(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(n){function e(e){for(var r,i,s=e[0],l=e[1],c=e[2],u=0,d=[];u<s.length;u++)i=s[u],Object.prototype.hasOwnProperty.call(a,i)&&a[i]&&d.push(a[i][0]),a[i]=0;for(r in l)Object.prototype.hasOwnProperty.call(l,r)&&(n[r]=l[r]);for(p&&p(e);d.length;)d.shift()();return o.push.apply(o,c||[]),t()}function t(){for(var n,e=0;e<o.length;e++){for(var t=o[e],r=!0,s=1;s<t.length;s++){var l=t[s];0!==a[l]&&(r=!1)}r&&(o.splice(e--,1),n=i(i.s=t[0]))}return n}var r={},a={2:0},o=[];function i(e){if(r[e])return r[e].exports;var t=r[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,i),t.l=!0,t.exports}i.e=function(n){var e=[],t=a[n];if(0!==t)if(t)e.push(t[2]);else{var r=new Promise((function(e,r){t=a[n]=[e,r]}));e.push(t[2]=r);var o,s=document.createElement("script");s.charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.src=function(n){return i.p+"assets/js/"+({}[n]||n)+"."+{1:"8757f81f",3:"9efa3b27",4:"95c562bd",5:"a1a0df33",6:"e534639a",7:"bc868936",8:"ce0f4f25",9:"a57a9cf0",10:"36ed4133",11:"18e1c33d",12:"b32071c5",13:"20e964b9",14:"d0932ff0",15:"7747d51d",16:"487f9c88",17:"9ae2b89b",18:"801c9b12",19:"867ef46b",20:"ec371d7f",21:"73f810bc",22:"8c016772",23:"9aa3d5ef",24:"a6dcda10",25:"2fe13775",26:"2296389e",27:"21f2d66f",28:"3209357f",29:"c7065b7f",30:"92914cc9",31:"13c12633",32:"b016f60e",33:"ed48a654",34:"ec572062",35:"5b55607b",36:"f50b6e38",37:"34409ac4",38:"c7073f77",39:"12f063e9",40:"1bbd5377",41:"6331a8cf",42:"28acd6f5",43:"905d2114",44:"041132c5",45:"d1a66b92",46:"1c9100fe",47:"3e75f768",48:"7d7248fb",49:"8d465809",50:"4aa56564",51:"59674bf5",52:"f3990683",53:"092fe0ad",54:"ac62e2a7",55:"3530fa5a",56:"6e9bc6f3"}[n]+".js"}(n);var l=new Error;o=function(e){s.onerror=s.onload=null,clearTimeout(c);var t=a[n];if(0!==t){if(t){var r=e&&("load"===e.type?"missing":e.type),o=e&&e.target&&e.target.src;l.message="Loading chunk "+n+" failed.\n("+r+": "+o+")",l.name="ChunkLoadError",l.type=r,l.request=o,t[1](l)}a[n]=void 0}};var c=setTimeout((function(){o({type:"timeout",target:s})}),12e4);s.onerror=s.onload=o,document.head.appendChild(s)}return Promise.all(e)},i.m=n,i.c=r,i.d=function(n,e,t){i.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},i.r=function(n){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},i.t=function(n,e){if(1&e&&(n=i(n)),8&e)return n;if(4&e&&"object"==typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(i.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var r in n)i.d(t,r,function(e){return n[e]}.bind(null,r));return t},i.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return i.d(e,"a",e),e},i.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},i.p="./",i.oe=function(n){throw console.error(n),n};var s=window.webpackJsonp=window.webpackJsonp||[],l=s.push.bind(s);s.push=e,s=s.slice();for(var c=0;c<s.length;c++)e(s[c]);var p=l;o.push([137,0]),t()}([function(n,e,t){"use strict";t.d(e,"e",(function(){return b})),t.d(e,"d",(function(){return k})),t.d(e,"c",(function(){return y})),t.d(e,"f",(function(){return S})),t.d(e,"a",(function(){return x})),t.d(e,"g",(function(){return w})),t.d(e,"b",(function(){return E})),t.d(e,"h",(function(){return D})),t.d(e,"i",(function(){return C}));t(17),t(136);var r=t(1),a={NotFound:()=>Promise.all([t.e(0),t.e(28)]).then(t.bind(null,1559)),Category:()=>Promise.all([t.e(0),t.e(1),t.e(20)]).then(t.bind(null,1560)),Layout:()=>Promise.all([t.e(0),t.e(1),t.e(13)]).then(t.bind(null,1557)),Tag:()=>Promise.all([t.e(0),t.e(1),t.e(17)]).then(t.bind(null,1561)),Tags:()=>Promise.all([t.e(0),t.e(1),t.e(18)]).then(t.bind(null,1562)),TimeLines:()=>Promise.all([t.e(0),t.e(1),t.e(29)]).then(t.bind(null,1563))},o={"v-aae13ec4":()=>t.e(39).then(t.bind(null,1565)),"v-af65573c":()=>t.e(37).then(t.bind(null,1566)),"v-0ea853f1":()=>t.e(40).then(t.bind(null,1567)),"v-0cbdd054":()=>t.e(43).then(t.bind(null,1568)),"v-0bf5ebde":()=>t.e(42).then(t.bind(null,1569)),"v-46e67ace":()=>t.e(38).then(t.bind(null,1570)),"v-6048fa40":()=>t.e(44).then(t.bind(null,1571)),"v-5ed1fdaa":()=>t.e(41).then(t.bind(null,1572)),"v-7d8ca27f":()=>t.e(7).then(t.bind(null,1573)),"v-dffff514":()=>t.e(27).then(t.bind(null,1574)),"v-33a310e8":()=>t.e(5).then(t.bind(null,1575)),"v-700a68a1":()=>t.e(30).then(t.bind(null,1576)),"v-2c8e9208":()=>t.e(46).then(t.bind(null,1577)),"v-01787dc2":()=>t.e(25).then(t.bind(null,1578)),"v-83d8af48":()=>t.e(9).then(t.bind(null,1579)),"v-c38fec2a":()=>t.e(26).then(t.bind(null,1580)),"v-7e69d236":()=>t.e(45).then(t.bind(null,1581)),"v-cdeeea2e":()=>t.e(47).then(t.bind(null,1582)),"v-10e8b782":()=>t.e(22).then(t.bind(null,1583)),"v-851571e2":()=>t.e(21).then(t.bind(null,1584)),"v-7350f07e":()=>t.e(15).then(t.bind(null,1585)),"v-22a43ff6":()=>t.e(19).then(t.bind(null,1586)),"v-0ddfb2e2":()=>t.e(12).then(t.bind(null,1587)),"v-41ccc7e2":()=>t.e(48).then(t.bind(null,1588)),"v-29f87cb0":()=>t.e(23).then(t.bind(null,1589)),"v-2e09059c":()=>t.e(32).then(t.bind(null,1590)),"v-7ea9e72a":()=>t.e(49).then(t.bind(null,1591)),"v-63f06f0b":()=>t.e(11).then(t.bind(null,1592)),"v-61c5b94b":()=>t.e(24).then(t.bind(null,1593)),"v-5692179e":()=>t.e(31).then(t.bind(null,1594)),"v-227baaf0":()=>t.e(14).then(t.bind(null,1595)),"v-e7d27b94":()=>t.e(51).then(t.bind(null,1596)),"v-7f30b557":()=>t.e(8).then(t.bind(null,1597)),"v-4f7f9be4":()=>t.e(36).then(t.bind(null,1598)),"v-24ffb3db":()=>t.e(6).then(t.bind(null,1599)),"v-e463dc58":()=>t.e(52).then(t.bind(null,1600)),"v-007b24d3":()=>t.e(10).then(t.bind(null,1601)),"v-305ac8e0":()=>t.e(50).then(t.bind(null,1602)),"v-4fd15c58":()=>t.e(53).then(t.bind(null,1603)),"v-36184f82":()=>t.e(16).then(t.bind(null,1604)),"v-49f14a1b":()=>t.e(4).then(t.bind(null,1605)),"v-9f20b3be":()=>t.e(3).then(t.bind(null,1606))};function i(n){const e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}const s=/-(\w)/g,l=i(n=>n.replace(s,(n,e)=>e?e.toUpperCase():"")),c=/\B([A-Z])/g,p=i(n=>n.replace(c,"-$1").toLowerCase()),u=i(n=>n.charAt(0).toUpperCase()+n.slice(1));function d(n,e){if(!e)return;if(n(e))return n(e);return e.includes("-")?n(u(l(e))):n(u(e))||n(p(e))}const m=Object.assign({},a,o),g=n=>m[n],f=n=>o[n],h=n=>a[n],v=n=>r.b.component(n);function b(n){return d(f,n)}function k(n){return d(h,n)}function y(n){return d(g,n)}function S(n){return d(v,n)}function x(...n){return Promise.all(n.filter(n=>n).map(async n=>{if(!S(n)&&y(n)){const e=await y(n)();r.b.component(n,e.default)}}))}function w(n,e,t){switch(e){case"components":n[e]||(n[e]={}),Object.assign(n[e],t);break;case"mixins":n[e]||(n[e]=[]),n[e].push(...t);break;default:throw new Error("Unknown option name.")}}function E(n,e){for(let t=0;t<n.length;t++){const r=n[t];if(r.key===e)return r}return{path:"",frontmatter:{}}}function D(n,e){const{$localePath:t}=n;return"object"==typeof e&&e[t]?e[t]:e}function C(n,e){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[n]=e)}},function(n,e,t){"use strict";t.d(e,"a",(function(){return Wn})),t.d(e,"b",(function(){return tr})),t.d(e,"c",(function(){return Ye})),t.d(e,"d",(function(){return dn})),t.d(e,"e",(function(){return Ge})),t.d(e,"f",(function(){return Je})),t.d(e,"g",(function(){return jn})),t.d(e,"h",(function(){return zn})),t.d(e,"i",(function(){return Vn}));
/*!
 * Vue.js v2.7.14
 * (c) 2014-2022 Evan You
 * Released under the MIT License.
 */
var r=Object.freeze({}),a=Array.isArray;function o(n){return null==n}function i(n){return null!=n}function s(n){return!0===n}function l(n){return"string"==typeof n||"number"==typeof n||"symbol"==typeof n||"boolean"==typeof n}function c(n){return"function"==typeof n}function p(n){return null!==n&&"object"==typeof n}var u=Object.prototype.toString;function d(n){return"[object Object]"===u.call(n)}function m(n){return"[object RegExp]"===u.call(n)}function g(n){var e=parseFloat(String(n));return e>=0&&Math.floor(e)===e&&isFinite(n)}function f(n){return i(n)&&"function"==typeof n.then&&"function"==typeof n.catch}function h(n){return null==n?"":Array.isArray(n)||d(n)&&n.toString===u?JSON.stringify(n,null,2):String(n)}function v(n){var e=parseFloat(n);return isNaN(e)?n:e}function b(n,e){for(var t=Object.create(null),r=n.split(","),a=0;a<r.length;a++)t[r[a]]=!0;return e?function(n){return t[n.toLowerCase()]}:function(n){return t[n]}}b("slot,component",!0);var k=b("key,ref,slot,slot-scope,is");function y(n,e){var t=n.length;if(t){if(e===n[t-1])return void(n.length=t-1);var r=n.indexOf(e);if(r>-1)return n.splice(r,1)}}var S=Object.prototype.hasOwnProperty;function x(n,e){return S.call(n,e)}function w(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var E=/-(\w)/g,D=w((function(n){return n.replace(E,(function(n,e){return e?e.toUpperCase():""}))})),C=w((function(n){return n.charAt(0).toUpperCase()+n.slice(1)})),I=/\B([A-Z])/g,T=w((function(n){return n.replace(I,"-$1").toLowerCase()}));var O=Function.prototype.bind?function(n,e){return n.bind(e)}:function(n,e){function t(t){var r=arguments.length;return r?r>1?n.apply(e,arguments):n.call(e,t):n.call(e)}return t._length=n.length,t};function A(n,e){e=e||0;for(var t=n.length-e,r=new Array(t);t--;)r[t]=n[t+e];return r}function _(n,e){for(var t in e)n[t]=e[t];return n}function R(n){for(var e={},t=0;t<n.length;t++)n[t]&&_(e,n[t]);return e}function P(n,e,t){}var F=function(n,e,t){return!1},B=function(n){return n};function M(n,e){if(n===e)return!0;var t=p(n),r=p(e);if(!t||!r)return!t&&!r&&String(n)===String(e);try{var a=Array.isArray(n),o=Array.isArray(e);if(a&&o)return n.length===e.length&&n.every((function(n,t){return M(n,e[t])}));if(n instanceof Date&&e instanceof Date)return n.getTime()===e.getTime();if(a||o)return!1;var i=Object.keys(n),s=Object.keys(e);return i.length===s.length&&i.every((function(t){return M(n[t],e[t])}))}catch(n){return!1}}function j(n,e){for(var t=0;t<n.length;t++)if(M(n[t],e))return t;return-1}function L(n){var e=!1;return function(){e||(e=!0,n.apply(this,arguments))}}function N(n,e){return n===e?0===n&&1/n!=1/e:n==n||e==e}var $=["component","directive","filter"],U=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],z={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:F,isReservedAttr:F,isUnknownElement:F,getTagNamespace:P,parsePlatformTagName:B,mustUseProp:F,async:!0,_lifecycleHooks:U},H=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function q(n){var e=(n+"").charCodeAt(0);return 36===e||95===e}function V(n,e,t,r){Object.defineProperty(n,e,{value:t,enumerable:!!r,writable:!0,configurable:!0})}var K=new RegExp("[^".concat(H.source,".$_\\d]"));var W="__proto__"in{},G="undefined"!=typeof window,J=G&&window.navigator.userAgent.toLowerCase(),Y=J&&/msie|trident/.test(J),Q=J&&J.indexOf("msie 9.0")>0,X=J&&J.indexOf("edge/")>0;J&&J.indexOf("android");var Z=J&&/iphone|ipad|ipod|ios/.test(J);J&&/chrome\/\d+/.test(J),J&&/phantomjs/.test(J);var nn,en=J&&J.match(/firefox\/(\d+)/),tn={}.watch,rn=!1;if(G)try{var an={};Object.defineProperty(an,"passive",{get:function(){rn=!0}}),window.addEventListener("test-passive",null,an)}catch(n){}var on=function(){return void 0===nn&&(nn=!G&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),nn},sn=G&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function ln(n){return"function"==typeof n&&/native code/.test(n.toString())}var cn,pn="undefined"!=typeof Symbol&&ln(Symbol)&&"undefined"!=typeof Reflect&&ln(Reflect.ownKeys);cn="undefined"!=typeof Set&&ln(Set)?Set:function(){function n(){this.set=Object.create(null)}return n.prototype.has=function(n){return!0===this.set[n]},n.prototype.add=function(n){this.set[n]=!0},n.prototype.clear=function(){this.set=Object.create(null)},n}();var un=null;function dn(){return un&&{proxy:un}}function mn(n){void 0===n&&(n=null),n||un&&un._scope.off(),un=n,n&&n._scope.on()}var gn=function(){function n(n,e,t,r,a,o,i,s){this.tag=n,this.data=e,this.children=t,this.text=r,this.elm=a,this.ns=void 0,this.context=o,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=i,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(n.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),n}(),fn=function(n){void 0===n&&(n="");var e=new gn;return e.text=n,e.isComment=!0,e};function hn(n){return new gn(void 0,void 0,void 0,String(n))}function vn(n){var e=new gn(n.tag,n.data,n.children&&n.children.slice(),n.text,n.elm,n.context,n.componentOptions,n.asyncFactory);return e.ns=n.ns,e.isStatic=n.isStatic,e.key=n.key,e.isComment=n.isComment,e.fnContext=n.fnContext,e.fnOptions=n.fnOptions,e.fnScopeId=n.fnScopeId,e.asyncMeta=n.asyncMeta,e.isCloned=!0,e}var bn=0,kn=[],yn=function(){function n(){this._pending=!1,this.id=bn++,this.subs=[]}return n.prototype.addSub=function(n){this.subs.push(n)},n.prototype.removeSub=function(n){this.subs[this.subs.indexOf(n)]=null,this._pending||(this._pending=!0,kn.push(this))},n.prototype.depend=function(e){n.target&&n.target.addDep(this)},n.prototype.notify=function(n){var e=this.subs.filter((function(n){return n}));for(var t=0,r=e.length;t<r;t++){0,e[t].update()}},n}();yn.target=null;var Sn=[];function xn(n){Sn.push(n),yn.target=n}function wn(){Sn.pop(),yn.target=Sn[Sn.length-1]}var En=Array.prototype,Dn=Object.create(En);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(n){var e=En[n];V(Dn,n,(function(){for(var t=[],r=0;r<arguments.length;r++)t[r]=arguments[r];var a,o=e.apply(this,t),i=this.__ob__;switch(n){case"push":case"unshift":a=t;break;case"splice":a=t.slice(2)}return a&&i.observeArray(a),i.dep.notify(),o}))}));var Cn=Object.getOwnPropertyNames(Dn),In={},Tn=!0;function On(n){Tn=n}var An={notify:P,depend:P,addSub:P,removeSub:P},_n=function(){function n(n,e,t){if(void 0===e&&(e=!1),void 0===t&&(t=!1),this.value=n,this.shallow=e,this.mock=t,this.dep=t?An:new yn,this.vmCount=0,V(n,"__ob__",this),a(n)){if(!t)if(W)n.__proto__=Dn;else for(var r=0,o=Cn.length;r<o;r++){V(n,s=Cn[r],Dn[s])}e||this.observeArray(n)}else{var i=Object.keys(n);for(r=0;r<i.length;r++){var s;Pn(n,s=i[r],In,void 0,e,t)}}}return n.prototype.observeArray=function(n){for(var e=0,t=n.length;e<t;e++)Rn(n[e],!1,this.mock)},n}();function Rn(n,e,t){return n&&x(n,"__ob__")&&n.__ob__ instanceof _n?n.__ob__:!Tn||!t&&on()||!a(n)&&!d(n)||!Object.isExtensible(n)||n.__v_skip||Un(n)||n instanceof gn?void 0:new _n(n,e,t)}function Pn(n,e,t,r,o,i){var s=new yn,l=Object.getOwnPropertyDescriptor(n,e);if(!l||!1!==l.configurable){var c=l&&l.get,p=l&&l.set;c&&!p||t!==In&&2!==arguments.length||(t=n[e]);var u=!o&&Rn(t,!1,i);return Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){var e=c?c.call(n):t;return yn.target&&(s.depend(),u&&(u.dep.depend(),a(e)&&Mn(e))),Un(e)&&!o?e.value:e},set:function(e){var r=c?c.call(n):t;if(N(r,e)){if(p)p.call(n,e);else{if(c)return;if(!o&&Un(r)&&!Un(e))return void(r.value=e);t=e}u=!o&&Rn(e,!1,i),s.notify()}}}),s}}function Fn(n,e,t){if(!$n(n)){var r=n.__ob__;return a(n)&&g(e)?(n.length=Math.max(n.length,e),n.splice(e,1,t),r&&!r.shallow&&r.mock&&Rn(t,!1,!0),t):e in n&&!(e in Object.prototype)?(n[e]=t,t):n._isVue||r&&r.vmCount?t:r?(Pn(r.value,e,t,void 0,r.shallow,r.mock),r.dep.notify(),t):(n[e]=t,t)}}function Bn(n,e){if(a(n)&&g(e))n.splice(e,1);else{var t=n.__ob__;n._isVue||t&&t.vmCount||$n(n)||x(n,e)&&(delete n[e],t&&t.dep.notify())}}function Mn(n){for(var e=void 0,t=0,r=n.length;t<r;t++)(e=n[t])&&e.__ob__&&e.__ob__.dep.depend(),a(e)&&Mn(e)}function jn(n){return Nn(n,!1),n}function Ln(n){return Nn(n,!0),V(n,"__v_isShallow",!0),n}function Nn(n,e){if(!$n(n)){Rn(n,e,on());0}}function $n(n){return!(!n||!n.__v_isReadonly)}function Un(n){return!(!n||!0!==n.__v_isRef)}function zn(n){return Hn(n,!1)}function Hn(n,e){if(Un(n))return n;var t={};return V(t,"__v_isRef",!0),V(t,"__v_isShallow",e),V(t,"dep",Pn(t,"value",n,null,e,on())),t}function qn(n,e,t){Object.defineProperty(n,t,{enumerable:!0,configurable:!0,get:function(){var n=e[t];if(Un(n))return n.value;var r=n&&n.__ob__;return r&&r.dep.depend(),n},set:function(n){var r=e[t];Un(r)&&!Un(n)?r.value=n:e[t]=n}})}function Vn(n){var e=a(n)?new Array(n.length):{};for(var t in n)e[t]=Kn(n,t);return e}function Kn(n,e,t){var r=n[e];if(Un(r))return r;var a={get value(){var r=n[e];return void 0===r?t:r},set value(t){n[e]=t}};return V(a,"__v_isRef",!0),a}function Wn(n,e){var t,r,a=c(n);a?(t=n,r=P):(t=n.get,r=n.set);var o=on()?null:new et(un,t,P,{lazy:!0});var i={effect:o,get value(){return o?(o.dirty&&o.evaluate(),yn.target&&o.depend(),o.value):t()},set value(n){r(n)}};return V(i,"__v_isRef",!0),V(i,"__v_isReadonly",a),i}"".concat("watcher"," callback"),"".concat("watcher"," getter"),"".concat("watcher"," cleanup");var Gn;var Jn=function(){function n(n){void 0===n&&(n=!1),this.detached=n,this.active=!0,this.effects=[],this.cleanups=[],this.parent=Gn,!n&&Gn&&(this.index=(Gn.scopes||(Gn.scopes=[])).push(this)-1)}return n.prototype.run=function(n){if(this.active){var e=Gn;try{return Gn=this,n()}finally{Gn=e}}else 0},n.prototype.on=function(){Gn=this},n.prototype.off=function(){Gn=this.parent},n.prototype.stop=function(n){if(this.active){var e=void 0,t=void 0;for(e=0,t=this.effects.length;e<t;e++)this.effects[e].teardown();for(e=0,t=this.cleanups.length;e<t;e++)this.cleanups[e]();if(this.scopes)for(e=0,t=this.scopes.length;e<t;e++)this.scopes[e].stop(!0);if(!this.detached&&this.parent&&!n){var r=this.parent.scopes.pop();r&&r!==this&&(this.parent.scopes[this.index]=r,r.index=this.index)}this.parent=void 0,this.active=!1}},n}();function Yn(n){var e=n._provided,t=n.$parent&&n.$parent._provided;return t===e?n._provided=Object.create(t):e}var Qn=w((function(n){var e="&"===n.charAt(0),t="~"===(n=e?n.slice(1):n).charAt(0),r="!"===(n=t?n.slice(1):n).charAt(0);return{name:n=r?n.slice(1):n,once:t,capture:r,passive:e}}));function Xn(n,e){function t(){var n=t.fns;if(!a(n))return Fe(n,null,arguments,e,"v-on handler");for(var r=n.slice(),o=0;o<r.length;o++)Fe(r[o],null,arguments,e,"v-on handler")}return t.fns=n,t}function Zn(n,e,t,r,a,i){var l,c,p,u;for(l in n)c=n[l],p=e[l],u=Qn(l),o(c)||(o(p)?(o(c.fns)&&(c=n[l]=Xn(c,i)),s(u.once)&&(c=n[l]=a(u.name,c,u.capture)),t(u.name,c,u.capture,u.passive,u.params)):c!==p&&(p.fns=c,n[l]=p));for(l in e)o(n[l])&&r((u=Qn(l)).name,e[l],u.capture)}function ne(n,e,t){var r;n instanceof gn&&(n=n.data.hook||(n.data.hook={}));var a=n[e];function l(){t.apply(this,arguments),y(r.fns,l)}o(a)?r=Xn([l]):i(a.fns)&&s(a.merged)?(r=a).fns.push(l):r=Xn([a,l]),r.merged=!0,n[e]=r}function ee(n,e,t,r,a){if(i(e)){if(x(e,t))return n[t]=e[t],a||delete e[t],!0;if(x(e,r))return n[t]=e[r],a||delete e[r],!0}return!1}function te(n){return l(n)?[hn(n)]:a(n)?function n(e,t){var r,c,p,u,d=[];for(r=0;r<e.length;r++)o(c=e[r])||"boolean"==typeof c||(p=d.length-1,u=d[p],a(c)?c.length>0&&(re((c=n(c,"".concat(t||"","_").concat(r)))[0])&&re(u)&&(d[p]=hn(u.text+c[0].text),c.shift()),d.push.apply(d,c)):l(c)?re(u)?d[p]=hn(u.text+c):""!==c&&d.push(hn(c)):re(c)&&re(u)?d[p]=hn(u.text+c.text):(s(e._isVList)&&i(c.tag)&&o(c.key)&&i(t)&&(c.key="__vlist".concat(t,"_").concat(r,"__")),d.push(c)));return d}(n):void 0}function re(n){return i(n)&&i(n.text)&&!1===n.isComment}function ae(n,e){var t,r,o,s,l=null;if(a(n)||"string"==typeof n)for(l=new Array(n.length),t=0,r=n.length;t<r;t++)l[t]=e(n[t],t);else if("number"==typeof n)for(l=new Array(n),t=0;t<n;t++)l[t]=e(t+1,t);else if(p(n))if(pn&&n[Symbol.iterator]){l=[];for(var c=n[Symbol.iterator](),u=c.next();!u.done;)l.push(e(u.value,l.length)),u=c.next()}else for(o=Object.keys(n),l=new Array(o.length),t=0,r=o.length;t<r;t++)s=o[t],l[t]=e(n[s],s,t);return i(l)||(l=[]),l._isVList=!0,l}function oe(n,e,t,r){var a,o=this.$scopedSlots[n];o?(t=t||{},r&&(t=_(_({},r),t)),a=o(t)||(c(e)?e():e)):a=this.$slots[n]||(c(e)?e():e);var i=t&&t.slot;return i?this.$createElement("template",{slot:i},a):a}function ie(n){return $t(this.$options,"filters",n,!0)||B}function se(n,e){return a(n)?-1===n.indexOf(e):n!==e}function le(n,e,t,r,a){var o=z.keyCodes[e]||t;return a&&r&&!z.keyCodes[e]?se(a,r):o?se(o,n):r?T(r)!==e:void 0===n}function ce(n,e,t,r,o){if(t)if(p(t)){a(t)&&(t=R(t));var i=void 0,s=function(a){if("class"===a||"style"===a||k(a))i=n;else{var s=n.attrs&&n.attrs.type;i=r||z.mustUseProp(e,s,a)?n.domProps||(n.domProps={}):n.attrs||(n.attrs={})}var l=D(a),c=T(a);l in i||c in i||(i[a]=t[a],o&&((n.on||(n.on={}))["update:".concat(a)]=function(n){t[a]=n}))};for(var l in t)s(l)}else;return n}function pe(n,e){var t=this._staticTrees||(this._staticTrees=[]),r=t[n];return r&&!e||de(r=t[n]=this.$options.staticRenderFns[n].call(this._renderProxy,this._c,this),"__static__".concat(n),!1),r}function ue(n,e,t){return de(n,"__once__".concat(e).concat(t?"_".concat(t):""),!0),n}function de(n,e,t){if(a(n))for(var r=0;r<n.length;r++)n[r]&&"string"!=typeof n[r]&&me(n[r],"".concat(e,"_").concat(r),t);else me(n,e,t)}function me(n,e,t){n.isStatic=!0,n.key=e,n.isOnce=t}function ge(n,e){if(e)if(d(e)){var t=n.on=n.on?_({},n.on):{};for(var r in e){var a=t[r],o=e[r];t[r]=a?[].concat(a,o):o}}else;return n}function fe(n,e,t,r){e=e||{$stable:!t};for(var o=0;o<n.length;o++){var i=n[o];a(i)?fe(i,e,t):i&&(i.proxy&&(i.fn.proxy=!0),e[i.key]=i.fn)}return r&&(e.$key=r),e}function he(n,e){for(var t=0;t<e.length;t+=2){var r=e[t];"string"==typeof r&&r&&(n[e[t]]=e[t+1])}return n}function ve(n,e){return"string"==typeof n?e+n:n}function be(n){n._o=ue,n._n=v,n._s=h,n._l=ae,n._t=oe,n._q=M,n._i=j,n._m=pe,n._f=ie,n._k=le,n._b=ce,n._v=hn,n._e=fn,n._u=fe,n._g=ge,n._d=he,n._p=ve}function ke(n,e){if(!n||!n.length)return{};for(var t={},r=0,a=n.length;r<a;r++){var o=n[r],i=o.data;if(i&&i.attrs&&i.attrs.slot&&delete i.attrs.slot,o.context!==e&&o.fnContext!==e||!i||null==i.slot)(t.default||(t.default=[])).push(o);else{var s=i.slot,l=t[s]||(t[s]=[]);"template"===o.tag?l.push.apply(l,o.children||[]):l.push(o)}}for(var c in t)t[c].every(ye)&&delete t[c];return t}function ye(n){return n.isComment&&!n.asyncFactory||" "===n.text}function Se(n){return n.isComment&&n.asyncFactory}function xe(n,e,t,a){var o,i=Object.keys(t).length>0,s=e?!!e.$stable:!i,l=e&&e.$key;if(e){if(e._normalized)return e._normalized;if(s&&a&&a!==r&&l===a.$key&&!i&&!a.$hasNormal)return a;for(var c in o={},e)e[c]&&"$"!==c[0]&&(o[c]=we(n,t,c,e[c]))}else o={};for(var p in t)p in o||(o[p]=Ee(t,p));return e&&Object.isExtensible(e)&&(e._normalized=o),V(o,"$stable",s),V(o,"$key",l),V(o,"$hasNormal",i),o}function we(n,e,t,r){var o=function(){var e=un;mn(n);var t=arguments.length?r.apply(null,arguments):r({}),o=(t=t&&"object"==typeof t&&!a(t)?[t]:te(t))&&t[0];return mn(e),t&&(!o||1===t.length&&o.isComment&&!Se(o))?void 0:t};return r.proxy&&Object.defineProperty(e,t,{get:o,enumerable:!0,configurable:!0}),o}function Ee(n,e){return function(){return n[e]}}function De(n){return{get attrs(){if(!n._attrsProxy){var e=n._attrsProxy={};V(e,"_v_attr_proxy",!0),Ce(e,n.$attrs,r,n,"$attrs")}return n._attrsProxy},get listeners(){n._listenersProxy||Ce(n._listenersProxy={},n.$listeners,r,n,"$listeners");return n._listenersProxy},get slots(){return function(n){n._slotsProxy||Te(n._slotsProxy={},n.$scopedSlots);return n._slotsProxy}(n)},emit:O(n.$emit,n),expose:function(e){e&&Object.keys(e).forEach((function(t){return qn(n,e,t)}))}}}function Ce(n,e,t,r,a){var o=!1;for(var i in e)i in n?e[i]!==t[i]&&(o=!0):(o=!0,Ie(n,i,r,a));for(var i in n)i in e||(o=!0,delete n[i]);return o}function Ie(n,e,t,r){Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){return t[r][e]}})}function Te(n,e){for(var t in e)n[t]=e[t];for(var t in n)t in e||delete n[t]}var Oe=null;function Ae(n,e){return(n.__esModule||pn&&"Module"===n[Symbol.toStringTag])&&(n=n.default),p(n)?e.extend(n):n}function _e(n){if(a(n))for(var e=0;e<n.length;e++){var t=n[e];if(i(t)&&(i(t.componentOptions)||Se(t)))return t}}function Re(n,e,t,r,u,d){return(a(t)||l(t))&&(u=r,r=t,t=void 0),s(d)&&(u=2),function(n,e,t,r,l){if(i(t)&&i(t.__ob__))return fn();i(t)&&i(t.is)&&(e=t.is);if(!e)return fn();0;a(r)&&c(r[0])&&((t=t||{}).scopedSlots={default:r[0]},r.length=0);2===l?r=te(r):1===l&&(r=function(n){for(var e=0;e<n.length;e++)if(a(n[e]))return Array.prototype.concat.apply([],n);return n}(r));var u,d;if("string"==typeof e){var m=void 0;d=n.$vnode&&n.$vnode.ns||z.getTagNamespace(e),u=z.isReservedTag(e)?new gn(z.parsePlatformTagName(e),t,r,void 0,void 0,n):t&&t.pre||!i(m=$t(n.$options,"components",e))?new gn(e,t,r,void 0,void 0,n):At(m,t,n,r,e)}else u=At(e,t,n,r);return a(u)?u:i(u)?(i(d)&&function n(e,t,r){e.ns=t,"foreignObject"===e.tag&&(t=void 0,r=!0);if(i(e.children))for(var a=0,l=e.children.length;a<l;a++){var c=e.children[a];i(c.tag)&&(o(c.ns)||s(r)&&"svg"!==c.tag)&&n(c,t,r)}}(u,d),i(t)&&function(n){p(n.style)&&Xe(n.style);p(n.class)&&Xe(n.class)}(t),u):fn()}(n,e,t,r,u)}function Pe(n,e,t){xn();try{if(e)for(var r=e;r=r.$parent;){var a=r.$options.errorCaptured;if(a)for(var o=0;o<a.length;o++)try{if(!1===a[o].call(r,n,e,t))return}catch(n){Be(n,r,"errorCaptured hook")}}Be(n,e,t)}finally{wn()}}function Fe(n,e,t,r,a){var o;try{(o=t?n.apply(e,t):n.call(e))&&!o._isVue&&f(o)&&!o._handled&&(o.catch((function(n){return Pe(n,r,a+" (Promise/async)")})),o._handled=!0)}catch(n){Pe(n,r,a)}return o}function Be(n,e,t){if(z.errorHandler)try{return z.errorHandler.call(null,n,e,t)}catch(e){e!==n&&Me(e,null,"config.errorHandler")}Me(n,e,t)}function Me(n,e,t){if(!G||"undefined"==typeof console)throw n;console.error(n)}var je,Le=!1,Ne=[],$e=!1;function Ue(){$e=!1;var n=Ne.slice(0);Ne.length=0;for(var e=0;e<n.length;e++)n[e]()}if("undefined"!=typeof Promise&&ln(Promise)){var ze=Promise.resolve();je=function(){ze.then(Ue),Z&&setTimeout(P)},Le=!0}else if(Y||"undefined"==typeof MutationObserver||!ln(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())je="undefined"!=typeof setImmediate&&ln(setImmediate)?function(){setImmediate(Ue)}:function(){setTimeout(Ue,0)};else{var He=1,qe=new MutationObserver(Ue),Ve=document.createTextNode(String(He));qe.observe(Ve,{characterData:!0}),je=function(){He=(He+1)%2,Ve.data=String(He)},Le=!0}function Ke(n,e){var t;if(Ne.push((function(){if(n)try{n.call(e)}catch(n){Pe(n,e,"nextTick")}else t&&t(e)})),$e||($e=!0,je()),!n&&"undefined"!=typeof Promise)return new Promise((function(n){t=n}))}function We(n){return function(e,t){if(void 0===t&&(t=un),t)return function(n,e,t){var r=n.$options;r[e]=Mt(r[e],t)}(t,n,e)}}We("beforeMount");var Ge=We("mounted"),Je=(We("beforeUpdate"),We("updated"));We("beforeDestroy"),We("destroyed"),We("activated"),We("deactivated"),We("serverPrefetch"),We("renderTracked"),We("renderTriggered"),We("errorCaptured");function Ye(n){return n}var Qe=new cn;function Xe(n){return function n(e,t){var r,o,i=a(e);if(!i&&!p(e)||e.__v_skip||Object.isFrozen(e)||e instanceof gn)return;if(e.__ob__){var s=e.__ob__.dep.id;if(t.has(s))return;t.add(s)}if(i)for(r=e.length;r--;)n(e[r],t);else if(Un(e))n(e.value,t);else for(o=Object.keys(e),r=o.length;r--;)n(e[o[r]],t)}(n,Qe),Qe.clear(),n}var Ze,nt=0,et=function(){function n(n,e,t,r,a){var o,i;o=this,void 0===(i=Gn&&!Gn._vm?Gn:n?n._scope:void 0)&&(i=Gn),i&&i.active&&i.effects.push(o),(this.vm=n)&&a&&(n._watcher=this),r?(this.deep=!!r.deep,this.user=!!r.user,this.lazy=!!r.lazy,this.sync=!!r.sync,this.before=r.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++nt,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new cn,this.newDepIds=new cn,this.expression="",c(e)?this.getter=e:(this.getter=function(n){if(!K.test(n)){var e=n.split(".");return function(n){for(var t=0;t<e.length;t++){if(!n)return;n=n[e[t]]}return n}}}(e),this.getter||(this.getter=P)),this.value=this.lazy?void 0:this.get()}return n.prototype.get=function(){var n;xn(this);var e=this.vm;try{n=this.getter.call(e,e)}catch(n){if(!this.user)throw n;Pe(n,e,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&Xe(n),wn(),this.cleanupDeps()}return n},n.prototype.addDep=function(n){var e=n.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(n),this.depIds.has(e)||n.addSub(this))},n.prototype.cleanupDeps=function(){for(var n=this.deps.length;n--;){var e=this.deps[n];this.newDepIds.has(e.id)||e.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},n.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():xt(this)},n.prototype.run=function(){if(this.active){var n=this.get();if(n!==this.value||p(n)||this.deep){var e=this.value;if(this.value=n,this.user){var t='callback for watcher "'.concat(this.expression,'"');Fe(this.cb,this.vm,[n,e],this.vm,t)}else this.cb.call(this.vm,n,e)}}},n.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},n.prototype.depend=function(){for(var n=this.deps.length;n--;)this.deps[n].depend()},n.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&y(this.vm._scope.effects,this),this.active){for(var n=this.deps.length;n--;)this.deps[n].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},n}();function tt(n,e){Ze.$on(n,e)}function rt(n,e){Ze.$off(n,e)}function at(n,e){var t=Ze;return function r(){var a=e.apply(null,arguments);null!==a&&t.$off(n,r)}}function ot(n,e,t){Ze=n,Zn(e,t||{},tt,rt,at,n),Ze=void 0}var it=null;function st(n){var e=it;return it=n,function(){it=e}}function lt(n){for(;n&&(n=n.$parent);)if(n._inactive)return!0;return!1}function ct(n,e){if(e){if(n._directInactive=!1,lt(n))return}else if(n._directInactive)return;if(n._inactive||null===n._inactive){n._inactive=!1;for(var t=0;t<n.$children.length;t++)ct(n.$children[t]);pt(n,"activated")}}function pt(n,e,t,r){void 0===r&&(r=!0),xn();var a=un;r&&mn(n);var o=n.$options[e],i="".concat(e," hook");if(o)for(var s=0,l=o.length;s<l;s++)Fe(o[s],n,t||null,n,i);n._hasHookEvent&&n.$emit("hook:"+e),r&&mn(a),wn()}var ut=[],dt=[],mt={},gt=!1,ft=!1,ht=0;var vt=0,bt=Date.now;if(G&&!Y){var kt=window.performance;kt&&"function"==typeof kt.now&&bt()>document.createEvent("Event").timeStamp&&(bt=function(){return kt.now()})}var yt=function(n,e){if(n.post){if(!e.post)return 1}else if(e.post)return-1;return n.id-e.id};function St(){var n,e;for(vt=bt(),ft=!0,ut.sort(yt),ht=0;ht<ut.length;ht++)(n=ut[ht]).before&&n.before(),e=n.id,mt[e]=null,n.run();var t=dt.slice(),r=ut.slice();ht=ut.length=dt.length=0,mt={},gt=ft=!1,function(n){for(var e=0;e<n.length;e++)n[e]._inactive=!0,ct(n[e],!0)}(t),function(n){var e=n.length;for(;e--;){var t=n[e],r=t.vm;r&&r._watcher===t&&r._isMounted&&!r._isDestroyed&&pt(r,"updated")}}(r),function(){for(var n=0;n<kn.length;n++){var e=kn[n];e.subs=e.subs.filter((function(n){return n})),e._pending=!1}kn.length=0}(),sn&&z.devtools&&sn.emit("flush")}function xt(n){var e=n.id;if(null==mt[e]&&(n!==yn.target||!n.noRecurse)){if(mt[e]=!0,ft){for(var t=ut.length-1;t>ht&&ut[t].id>n.id;)t--;ut.splice(t+1,0,n)}else ut.push(n);gt||(gt=!0,Ke(St))}}function wt(n,e){if(n){for(var t=Object.create(null),r=pn?Reflect.ownKeys(n):Object.keys(n),a=0;a<r.length;a++){var o=r[a];if("__ob__"!==o){var i=n[o].from;if(i in e._provided)t[o]=e._provided[i];else if("default"in n[o]){var s=n[o].default;t[o]=c(s)?s.call(e):s}else 0}}return t}}function Et(n,e,t,o,i){var l,c=this,p=i.options;x(o,"_uid")?(l=Object.create(o))._original=o:(l=o,o=o._original);var u=s(p._compiled),d=!u;this.data=n,this.props=e,this.children=t,this.parent=o,this.listeners=n.on||r,this.injections=wt(p.inject,o),this.slots=function(){return c.$slots||xe(o,n.scopedSlots,c.$slots=ke(t,o)),c.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return xe(o,n.scopedSlots,this.slots())}}),u&&(this.$options=p,this.$slots=this.slots(),this.$scopedSlots=xe(o,n.scopedSlots,this.$slots)),p._scopeId?this._c=function(n,e,t,r){var i=Re(l,n,e,t,r,d);return i&&!a(i)&&(i.fnScopeId=p._scopeId,i.fnContext=o),i}:this._c=function(n,e,t,r){return Re(l,n,e,t,r,d)}}function Dt(n,e,t,r,a){var o=vn(n);return o.fnContext=t,o.fnOptions=r,e.slot&&((o.data||(o.data={})).slot=e.slot),o}function Ct(n,e){for(var t in e)n[D(t)]=e[t]}function It(n){return n.name||n.__name||n._componentTag}be(Et.prototype);var Tt={init:function(n,e){if(n.componentInstance&&!n.componentInstance._isDestroyed&&n.data.keepAlive){var t=n;Tt.prepatch(t,t)}else{(n.componentInstance=function(n,e){var t={_isComponent:!0,_parentVnode:n,parent:e},r=n.data.inlineTemplate;i(r)&&(t.render=r.render,t.staticRenderFns=r.staticRenderFns);return new n.componentOptions.Ctor(t)}(n,it)).$mount(e?n.elm:void 0,e)}},prepatch:function(n,e){var t=e.componentOptions;!function(n,e,t,a,o){var i=a.data.scopedSlots,s=n.$scopedSlots,l=!!(i&&!i.$stable||s!==r&&!s.$stable||i&&n.$scopedSlots.$key!==i.$key||!i&&n.$scopedSlots.$key),c=!!(o||n.$options._renderChildren||l),p=n.$vnode;n.$options._parentVnode=a,n.$vnode=a,n._vnode&&(n._vnode.parent=a),n.$options._renderChildren=o;var u=a.data.attrs||r;n._attrsProxy&&Ce(n._attrsProxy,u,p.data&&p.data.attrs||r,n,"$attrs")&&(c=!0),n.$attrs=u,t=t||r;var d=n.$options._parentListeners;if(n._listenersProxy&&Ce(n._listenersProxy,t,d||r,n,"$listeners"),n.$listeners=n.$options._parentListeners=t,ot(n,t,d),e&&n.$options.props){On(!1);for(var m=n._props,g=n.$options._propKeys||[],f=0;f<g.length;f++){var h=g[f],v=n.$options.props;m[h]=Ut(h,v,e,n)}On(!0),n.$options.propsData=e}c&&(n.$slots=ke(o,a.context),n.$forceUpdate())}(e.componentInstance=n.componentInstance,t.propsData,t.listeners,e,t.children)},insert:function(n){var e,t=n.context,r=n.componentInstance;r._isMounted||(r._isMounted=!0,pt(r,"mounted")),n.data.keepAlive&&(t._isMounted?((e=r)._inactive=!1,dt.push(e)):ct(r,!0))},destroy:function(n){var e=n.componentInstance;e._isDestroyed||(n.data.keepAlive?function n(e,t){if(!(t&&(e._directInactive=!0,lt(e))||e._inactive)){e._inactive=!0;for(var r=0;r<e.$children.length;r++)n(e.$children[r]);pt(e,"deactivated")}}(e,!0):e.$destroy())}},Ot=Object.keys(Tt);function At(n,e,t,l,c){if(!o(n)){var u=t.$options._base;if(p(n)&&(n=u.extend(n)),"function"==typeof n){var d;if(o(n.cid)&&void 0===(n=function(n,e){if(s(n.error)&&i(n.errorComp))return n.errorComp;if(i(n.resolved))return n.resolved;var t=Oe;if(t&&i(n.owners)&&-1===n.owners.indexOf(t)&&n.owners.push(t),s(n.loading)&&i(n.loadingComp))return n.loadingComp;if(t&&!i(n.owners)){var r=n.owners=[t],a=!0,l=null,c=null;t.$on("hook:destroyed",(function(){return y(r,t)}));var u=function(n){for(var e=0,t=r.length;e<t;e++)r[e].$forceUpdate();n&&(r.length=0,null!==l&&(clearTimeout(l),l=null),null!==c&&(clearTimeout(c),c=null))},d=L((function(t){n.resolved=Ae(t,e),a?r.length=0:u(!0)})),m=L((function(e){i(n.errorComp)&&(n.error=!0,u(!0))})),g=n(d,m);return p(g)&&(f(g)?o(n.resolved)&&g.then(d,m):f(g.component)&&(g.component.then(d,m),i(g.error)&&(n.errorComp=Ae(g.error,e)),i(g.loading)&&(n.loadingComp=Ae(g.loading,e),0===g.delay?n.loading=!0:l=setTimeout((function(){l=null,o(n.resolved)&&o(n.error)&&(n.loading=!0,u(!1))}),g.delay||200)),i(g.timeout)&&(c=setTimeout((function(){c=null,o(n.resolved)&&m(null)}),g.timeout)))),a=!1,n.loading?n.loadingComp:n.resolved}}(d=n,u)))return function(n,e,t,r,a){var o=fn();return o.asyncFactory=n,o.asyncMeta={data:e,context:t,children:r,tag:a},o}(d,e,t,l,c);e=e||{},er(n),i(e.model)&&function(n,e){var t=n.model&&n.model.prop||"value",r=n.model&&n.model.event||"input";(e.attrs||(e.attrs={}))[t]=e.model.value;var o=e.on||(e.on={}),s=o[r],l=e.model.callback;i(s)?(a(s)?-1===s.indexOf(l):s!==l)&&(o[r]=[l].concat(s)):o[r]=l}(n.options,e);var m=function(n,e,t){var r=e.options.props;if(!o(r)){var a={},s=n.attrs,l=n.props;if(i(s)||i(l))for(var c in r){var p=T(c);ee(a,l,c,p,!0)||ee(a,s,c,p,!1)}return a}}(e,n);if(s(n.options.functional))return function(n,e,t,o,s){var l=n.options,c={},p=l.props;if(i(p))for(var u in p)c[u]=Ut(u,p,e||r);else i(t.attrs)&&Ct(c,t.attrs),i(t.props)&&Ct(c,t.props);var d=new Et(t,c,s,o,n),m=l.render.call(null,d._c,d);if(m instanceof gn)return Dt(m,t,d.parent,l,d);if(a(m)){for(var g=te(m)||[],f=new Array(g.length),h=0;h<g.length;h++)f[h]=Dt(g[h],t,d.parent,l,d);return f}}(n,m,e,t,l);var g=e.on;if(e.on=e.nativeOn,s(n.options.abstract)){var h=e.slot;e={},h&&(e.slot=h)}!function(n){for(var e=n.hook||(n.hook={}),t=0;t<Ot.length;t++){var r=Ot[t],a=e[r],o=Tt[r];a===o||a&&a._merged||(e[r]=a?_t(o,a):o)}}(e);var v=It(n.options)||c;return new gn("vue-component-".concat(n.cid).concat(v?"-".concat(v):""),e,void 0,void 0,void 0,t,{Ctor:n,propsData:m,listeners:g,tag:c,children:l},d)}}}function _t(n,e){var t=function(t,r){n(t,r),e(t,r)};return t._merged=!0,t}var Rt=P,Pt=z.optionMergeStrategies;function Ft(n,e,t){if(void 0===t&&(t=!0),!e)return n;for(var r,a,o,i=pn?Reflect.ownKeys(e):Object.keys(e),s=0;s<i.length;s++)"__ob__"!==(r=i[s])&&(a=n[r],o=e[r],t&&x(n,r)?a!==o&&d(a)&&d(o)&&Ft(a,o):Fn(n,r,o));return n}function Bt(n,e,t){return t?function(){var r=c(e)?e.call(t,t):e,a=c(n)?n.call(t,t):n;return r?Ft(r,a):a}:e?n?function(){return Ft(c(e)?e.call(this,this):e,c(n)?n.call(this,this):n)}:e:n}function Mt(n,e){var t=e?n?n.concat(e):a(e)?e:[e]:n;return t?function(n){for(var e=[],t=0;t<n.length;t++)-1===e.indexOf(n[t])&&e.push(n[t]);return e}(t):t}function jt(n,e,t,r){var a=Object.create(n||null);return e?_(a,e):a}Pt.data=function(n,e,t){return t?Bt(n,e,t):e&&"function"!=typeof e?n:Bt(n,e)},U.forEach((function(n){Pt[n]=Mt})),$.forEach((function(n){Pt[n+"s"]=jt})),Pt.watch=function(n,e,t,r){if(n===tn&&(n=void 0),e===tn&&(e=void 0),!e)return Object.create(n||null);if(!n)return e;var o={};for(var i in _(o,n),e){var s=o[i],l=e[i];s&&!a(s)&&(s=[s]),o[i]=s?s.concat(l):a(l)?l:[l]}return o},Pt.props=Pt.methods=Pt.inject=Pt.computed=function(n,e,t,r){if(!n)return e;var a=Object.create(null);return _(a,n),e&&_(a,e),a},Pt.provide=function(n,e){return n?function(){var t=Object.create(null);return Ft(t,c(n)?n.call(this):n),e&&Ft(t,c(e)?e.call(this):e,!1),t}:e};var Lt=function(n,e){return void 0===e?n:e};function Nt(n,e,t){if(c(e)&&(e=e.options),function(n,e){var t=n.props;if(t){var r,o,i={};if(a(t))for(r=t.length;r--;)"string"==typeof(o=t[r])&&(i[D(o)]={type:null});else if(d(t))for(var s in t)o=t[s],i[D(s)]=d(o)?o:{type:o};else 0;n.props=i}}(e),function(n,e){var t=n.inject;if(t){var r=n.inject={};if(a(t))for(var o=0;o<t.length;o++)r[t[o]]={from:t[o]};else if(d(t))for(var i in t){var s=t[i];r[i]=d(s)?_({from:i},s):{from:s}}else 0}}(e),function(n){var e=n.directives;if(e)for(var t in e){var r=e[t];c(r)&&(e[t]={bind:r,update:r})}}(e),!e._base&&(e.extends&&(n=Nt(n,e.extends,t)),e.mixins))for(var r=0,o=e.mixins.length;r<o;r++)n=Nt(n,e.mixins[r],t);var i,s={};for(i in n)l(i);for(i in e)x(n,i)||l(i);function l(r){var a=Pt[r]||Lt;s[r]=a(n[r],e[r],t,r)}return s}function $t(n,e,t,r){if("string"==typeof t){var a=n[e];if(x(a,t))return a[t];var o=D(t);if(x(a,o))return a[o];var i=C(o);return x(a,i)?a[i]:a[t]||a[o]||a[i]}}function Ut(n,e,t,r){var a=e[n],o=!x(t,n),i=t[n],s=Vt(Boolean,a.type);if(s>-1)if(o&&!x(a,"default"))i=!1;else if(""===i||i===T(n)){var l=Vt(String,a.type);(l<0||s<l)&&(i=!0)}if(void 0===i){i=function(n,e,t){if(!x(e,"default"))return;var r=e.default;0;if(n&&n.$options.propsData&&void 0===n.$options.propsData[t]&&void 0!==n._props[t])return n._props[t];return c(r)&&"Function"!==Ht(e.type)?r.call(n):r}(r,a,n);var p=Tn;On(!0),Rn(i),On(p)}return i}var zt=/^\s*function (\w+)/;function Ht(n){var e=n&&n.toString().match(zt);return e?e[1]:""}function qt(n,e){return Ht(n)===Ht(e)}function Vt(n,e){if(!a(e))return qt(e,n)?0:-1;for(var t=0,r=e.length;t<r;t++)if(qt(e[t],n))return t;return-1}var Kt={enumerable:!0,configurable:!0,get:P,set:P};function Wt(n,e,t){Kt.get=function(){return this[e][t]},Kt.set=function(n){this[e][t]=n},Object.defineProperty(n,t,Kt)}function Gt(n){var e=n.$options;if(e.props&&function(n,e){var t=n.$options.propsData||{},r=n._props=Ln({}),a=n.$options._propKeys=[];n.$parent&&On(!1);var o=function(o){a.push(o);var i=Ut(o,e,t,n);Pn(r,o,i),o in n||Wt(n,"_props",o)};for(var i in e)o(i);On(!0)}(n,e.props),function(n){var e=n.$options,t=e.setup;if(t){var r=n._setupContext=De(n);mn(n),xn();var a=Fe(t,null,[n._props||Ln({}),r],n,"setup");if(wn(),mn(),c(a))e.render=a;else if(p(a))if(n._setupState=a,a.__sfc){var o=n._setupProxy={};for(var i in a)"__sfc"!==i&&qn(o,a,i)}else for(var i in a)q(i)||qn(n,a,i);else 0}}(n),e.methods&&function(n,e){n.$options.props;for(var t in e)n[t]="function"!=typeof e[t]?P:O(e[t],n)}(n,e.methods),e.data)!function(n){var e=n.$options.data;d(e=n._data=c(e)?function(n,e){xn();try{return n.call(e,e)}catch(n){return Pe(n,e,"data()"),{}}finally{wn()}}(e,n):e||{})||(e={});var t=Object.keys(e),r=n.$options.props,a=(n.$options.methods,t.length);for(;a--;){var o=t[a];0,r&&x(r,o)||q(o)||Wt(n,"_data",o)}var i=Rn(e);i&&i.vmCount++}(n);else{var t=Rn(n._data={});t&&t.vmCount++}e.computed&&function(n,e){var t=n._computedWatchers=Object.create(null),r=on();for(var a in e){var o=e[a],i=c(o)?o:o.get;0,r||(t[a]=new et(n,i||P,P,Jt)),a in n||Yt(n,a,o)}}(n,e.computed),e.watch&&e.watch!==tn&&function(n,e){for(var t in e){var r=e[t];if(a(r))for(var o=0;o<r.length;o++)Zt(n,t,r[o]);else Zt(n,t,r)}}(n,e.watch)}var Jt={lazy:!0};function Yt(n,e,t){var r=!on();c(t)?(Kt.get=r?Qt(e):Xt(t),Kt.set=P):(Kt.get=t.get?r&&!1!==t.cache?Qt(e):Xt(t.get):P,Kt.set=t.set||P),Object.defineProperty(n,e,Kt)}function Qt(n){return function(){var e=this._computedWatchers&&this._computedWatchers[n];if(e)return e.dirty&&e.evaluate(),yn.target&&e.depend(),e.value}}function Xt(n){return function(){return n.call(this,this)}}function Zt(n,e,t,r){return d(t)&&(r=t,t=t.handler),"string"==typeof t&&(t=n[t]),n.$watch(e,t,r)}var nr=0;function er(n){var e=n.options;if(n.super){var t=er(n.super);if(t!==n.superOptions){n.superOptions=t;var r=function(n){var e,t=n.options,r=n.sealedOptions;for(var a in t)t[a]!==r[a]&&(e||(e={}),e[a]=t[a]);return e}(n);r&&_(n.extendOptions,r),(e=n.options=Nt(t,n.extendOptions)).name&&(e.components[e.name]=n)}}return e}function tr(n){this._init(n)}function rr(n){n.cid=0;var e=1;n.extend=function(n){n=n||{};var t=this,r=t.cid,a=n._Ctor||(n._Ctor={});if(a[r])return a[r];var o=It(n)||It(t.options);var i=function(n){this._init(n)};return(i.prototype=Object.create(t.prototype)).constructor=i,i.cid=e++,i.options=Nt(t.options,n),i.super=t,i.options.props&&function(n){var e=n.options.props;for(var t in e)Wt(n.prototype,"_props",t)}(i),i.options.computed&&function(n){var e=n.options.computed;for(var t in e)Yt(n.prototype,t,e[t])}(i),i.extend=t.extend,i.mixin=t.mixin,i.use=t.use,$.forEach((function(n){i[n]=t[n]})),o&&(i.options.components[o]=i),i.superOptions=t.options,i.extendOptions=n,i.sealedOptions=_({},i.options),a[r]=i,i}}function ar(n){return n&&(It(n.Ctor.options)||n.tag)}function or(n,e){return a(n)?n.indexOf(e)>-1:"string"==typeof n?n.split(",").indexOf(e)>-1:!!m(n)&&n.test(e)}function ir(n,e){var t=n.cache,r=n.keys,a=n._vnode;for(var o in t){var i=t[o];if(i){var s=i.name;s&&!e(s)&&sr(t,o,r,a)}}}function sr(n,e,t,r){var a=n[e];!a||r&&a.tag===r.tag||a.componentInstance.$destroy(),n[e]=null,y(t,e)}!function(n){n.prototype._init=function(n){var e=this;e._uid=nr++,e._isVue=!0,e.__v_skip=!0,e._scope=new Jn(!0),e._scope._vm=!0,n&&n._isComponent?function(n,e){var t=n.$options=Object.create(n.constructor.options),r=e._parentVnode;t.parent=e.parent,t._parentVnode=r;var a=r.componentOptions;t.propsData=a.propsData,t._parentListeners=a.listeners,t._renderChildren=a.children,t._componentTag=a.tag,e.render&&(t.render=e.render,t.staticRenderFns=e.staticRenderFns)}(e,n):e.$options=Nt(er(e.constructor),n||{},e),e._renderProxy=e,e._self=e,function(n){var e=n.$options,t=e.parent;if(t&&!e.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(n)}n.$parent=t,n.$root=t?t.$root:n,n.$children=[],n.$refs={},n._provided=t?t._provided:Object.create(null),n._watcher=null,n._inactive=null,n._directInactive=!1,n._isMounted=!1,n._isDestroyed=!1,n._isBeingDestroyed=!1}(e),function(n){n._events=Object.create(null),n._hasHookEvent=!1;var e=n.$options._parentListeners;e&&ot(n,e)}(e),function(n){n._vnode=null,n._staticTrees=null;var e=n.$options,t=n.$vnode=e._parentVnode,a=t&&t.context;n.$slots=ke(e._renderChildren,a),n.$scopedSlots=t?xe(n.$parent,t.data.scopedSlots,n.$slots):r,n._c=function(e,t,r,a){return Re(n,e,t,r,a,!1)},n.$createElement=function(e,t,r,a){return Re(n,e,t,r,a,!0)};var o=t&&t.data;Pn(n,"$attrs",o&&o.attrs||r,null,!0),Pn(n,"$listeners",e._parentListeners||r,null,!0)}(e),pt(e,"beforeCreate",void 0,!1),function(n){var e=wt(n.$options.inject,n);e&&(On(!1),Object.keys(e).forEach((function(t){Pn(n,t,e[t])})),On(!0))}(e),Gt(e),function(n){var e=n.$options.provide;if(e){var t=c(e)?e.call(n):e;if(!p(t))return;for(var r=Yn(n),a=pn?Reflect.ownKeys(t):Object.keys(t),o=0;o<a.length;o++){var i=a[o];Object.defineProperty(r,i,Object.getOwnPropertyDescriptor(t,i))}}}(e),pt(e,"created"),e.$options.el&&e.$mount(e.$options.el)}}(tr),function(n){var e={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(n.prototype,"$data",e),Object.defineProperty(n.prototype,"$props",t),n.prototype.$set=Fn,n.prototype.$delete=Bn,n.prototype.$watch=function(n,e,t){if(d(e))return Zt(this,n,e,t);(t=t||{}).user=!0;var r=new et(this,n,e,t);if(t.immediate){var a='callback for immediate watcher "'.concat(r.expression,'"');xn(),Fe(e,this,[r.value],this,a),wn()}return function(){r.teardown()}}}(tr),function(n){var e=/^hook:/;n.prototype.$on=function(n,t){var r=this;if(a(n))for(var o=0,i=n.length;o<i;o++)r.$on(n[o],t);else(r._events[n]||(r._events[n]=[])).push(t),e.test(n)&&(r._hasHookEvent=!0);return r},n.prototype.$once=function(n,e){var t=this;function r(){t.$off(n,r),e.apply(t,arguments)}return r.fn=e,t.$on(n,r),t},n.prototype.$off=function(n,e){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(a(n)){for(var r=0,o=n.length;r<o;r++)t.$off(n[r],e);return t}var i,s=t._events[n];if(!s)return t;if(!e)return t._events[n]=null,t;for(var l=s.length;l--;)if((i=s[l])===e||i.fn===e){s.splice(l,1);break}return t},n.prototype.$emit=function(n){var e=this,t=e._events[n];if(t){t=t.length>1?A(t):t;for(var r=A(arguments,1),a='event handler for "'.concat(n,'"'),o=0,i=t.length;o<i;o++)Fe(t[o],e,r,e,a)}return e}}(tr),function(n){n.prototype._update=function(n,e){var t=this,r=t.$el,a=t._vnode,o=st(t);t._vnode=n,t.$el=a?t.__patch__(a,n):t.__patch__(t.$el,n,e,!1),o(),r&&(r.__vue__=null),t.$el&&(t.$el.__vue__=t);for(var i=t;i&&i.$vnode&&i.$parent&&i.$vnode===i.$parent._vnode;)i.$parent.$el=i.$el,i=i.$parent},n.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},n.prototype.$destroy=function(){var n=this;if(!n._isBeingDestroyed){pt(n,"beforeDestroy"),n._isBeingDestroyed=!0;var e=n.$parent;!e||e._isBeingDestroyed||n.$options.abstract||y(e.$children,n),n._scope.stop(),n._data.__ob__&&n._data.__ob__.vmCount--,n._isDestroyed=!0,n.__patch__(n._vnode,null),pt(n,"destroyed"),n.$off(),n.$el&&(n.$el.__vue__=null),n.$vnode&&(n.$vnode.parent=null)}}}(tr),function(n){be(n.prototype),n.prototype.$nextTick=function(n){return Ke(n,this)},n.prototype._render=function(){var n,e=this,t=e.$options,r=t.render,o=t._parentVnode;o&&e._isMounted&&(e.$scopedSlots=xe(e.$parent,o.data.scopedSlots,e.$slots,e.$scopedSlots),e._slotsProxy&&Te(e._slotsProxy,e.$scopedSlots)),e.$vnode=o;try{mn(e),Oe=e,n=r.call(e._renderProxy,e.$createElement)}catch(t){Pe(t,e,"render"),n=e._vnode}finally{Oe=null,mn()}return a(n)&&1===n.length&&(n=n[0]),n instanceof gn||(n=fn()),n.parent=o,n}}(tr);var lr=[String,RegExp,Array],cr={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:lr,exclude:lr,max:[String,Number]},methods:{cacheVNode:function(){var n=this.cache,e=this.keys,t=this.vnodeToCache,r=this.keyToCache;if(t){var a=t.tag,o=t.componentInstance,i=t.componentOptions;n[r]={name:ar(i),tag:a,componentInstance:o},e.push(r),this.max&&e.length>parseInt(this.max)&&sr(n,e[0],e,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var n in this.cache)sr(this.cache,n,this.keys)},mounted:function(){var n=this;this.cacheVNode(),this.$watch("include",(function(e){ir(n,(function(n){return or(e,n)}))})),this.$watch("exclude",(function(e){ir(n,(function(n){return!or(e,n)}))}))},updated:function(){this.cacheVNode()},render:function(){var n=this.$slots.default,e=_e(n),t=e&&e.componentOptions;if(t){var r=ar(t),a=this.include,o=this.exclude;if(a&&(!r||!or(a,r))||o&&r&&or(o,r))return e;var i=this.cache,s=this.keys,l=null==e.key?t.Ctor.cid+(t.tag?"::".concat(t.tag):""):e.key;i[l]?(e.componentInstance=i[l].componentInstance,y(s,l),s.push(l)):(this.vnodeToCache=e,this.keyToCache=l),e.data.keepAlive=!0}return e||n&&n[0]}}};!function(n){var e={get:function(){return z}};Object.defineProperty(n,"config",e),n.util={warn:Rt,extend:_,mergeOptions:Nt,defineReactive:Pn},n.set=Fn,n.delete=Bn,n.nextTick=Ke,n.observable=function(n){return Rn(n),n},n.options=Object.create(null),$.forEach((function(e){n.options[e+"s"]=Object.create(null)})),n.options._base=n,_(n.options.components,cr),function(n){n.use=function(n){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(n)>-1)return this;var t=A(arguments,1);return t.unshift(this),c(n.install)?n.install.apply(n,t):c(n)&&n.apply(null,t),e.push(n),this}}(n),function(n){n.mixin=function(n){return this.options=Nt(this.options,n),this}}(n),rr(n),function(n){$.forEach((function(e){n[e]=function(n,t){return t?("component"===e&&d(t)&&(t.name=t.name||n,t=this.options._base.extend(t)),"directive"===e&&c(t)&&(t={bind:t,update:t}),this.options[e+"s"][n]=t,t):this.options[e+"s"][n]}}))}(n)}(tr),Object.defineProperty(tr.prototype,"$isServer",{get:on}),Object.defineProperty(tr.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(tr,"FunctionalRenderContext",{value:Et}),tr.version="2.7.14";var pr=b("style,class"),ur=b("input,textarea,option,select,progress"),dr=b("contenteditable,draggable,spellcheck"),mr=b("events,caret,typing,plaintext-only"),gr=b("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),fr="http://www.w3.org/1999/xlink",hr=function(n){return":"===n.charAt(5)&&"xlink"===n.slice(0,5)},vr=function(n){return hr(n)?n.slice(6,n.length):""},br=function(n){return null==n||!1===n};function kr(n){for(var e=n.data,t=n,r=n;i(r.componentInstance);)(r=r.componentInstance._vnode)&&r.data&&(e=yr(r.data,e));for(;i(t=t.parent);)t&&t.data&&(e=yr(e,t.data));return function(n,e){if(i(n)||i(e))return Sr(n,xr(e));return""}(e.staticClass,e.class)}function yr(n,e){return{staticClass:Sr(n.staticClass,e.staticClass),class:i(n.class)?[n.class,e.class]:e.class}}function Sr(n,e){return n?e?n+" "+e:n:e||""}function xr(n){return Array.isArray(n)?function(n){for(var e,t="",r=0,a=n.length;r<a;r++)i(e=xr(n[r]))&&""!==e&&(t&&(t+=" "),t+=e);return t}(n):p(n)?function(n){var e="";for(var t in n)n[t]&&(e&&(e+=" "),e+=t);return e}(n):"string"==typeof n?n:""}var wr={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},Er=b("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),Dr=b("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Cr=function(n){return Er(n)||Dr(n)};var Ir=Object.create(null);var Tr=b("text,number,password,search,email,tel,url");var Or=Object.freeze({__proto__:null,createElement:function(n,e){var t=document.createElement(n);return"select"!==n||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(n,e){return document.createElementNS(wr[n],e)},createTextNode:function(n){return document.createTextNode(n)},createComment:function(n){return document.createComment(n)},insertBefore:function(n,e,t){n.insertBefore(e,t)},removeChild:function(n,e){n.removeChild(e)},appendChild:function(n,e){n.appendChild(e)},parentNode:function(n){return n.parentNode},nextSibling:function(n){return n.nextSibling},tagName:function(n){return n.tagName},setTextContent:function(n,e){n.textContent=e},setStyleScope:function(n,e){n.setAttribute(e,"")}}),Ar={create:function(n,e){_r(e)},update:function(n,e){n.data.ref!==e.data.ref&&(_r(n,!0),_r(e))},destroy:function(n){_r(n,!0)}};function _r(n,e){var t=n.data.ref;if(i(t)){var r=n.context,o=n.componentInstance||n.elm,s=e?null:o,l=e?void 0:o;if(c(t))Fe(t,r,[s],r,"template ref function");else{var p=n.data.refInFor,u="string"==typeof t||"number"==typeof t,d=Un(t),m=r.$refs;if(u||d)if(p){var g=u?m[t]:t.value;e?a(g)&&y(g,o):a(g)?g.includes(o)||g.push(o):u?(m[t]=[o],Rr(r,t,m[t])):t.value=[o]}else if(u){if(e&&m[t]!==o)return;m[t]=l,Rr(r,t,s)}else if(d){if(e&&t.value!==o)return;t.value=s}else 0}}}function Rr(n,e,t){var r=n._setupState;r&&x(r,e)&&(Un(r[e])?r[e].value=t:r[e]=t)}var Pr=new gn("",{},[]),Fr=["create","activate","update","remove","destroy"];function Br(n,e){return n.key===e.key&&n.asyncFactory===e.asyncFactory&&(n.tag===e.tag&&n.isComment===e.isComment&&i(n.data)===i(e.data)&&function(n,e){if("input"!==n.tag)return!0;var t,r=i(t=n.data)&&i(t=t.attrs)&&t.type,a=i(t=e.data)&&i(t=t.attrs)&&t.type;return r===a||Tr(r)&&Tr(a)}(n,e)||s(n.isAsyncPlaceholder)&&o(e.asyncFactory.error))}function Mr(n,e,t){var r,a,o={};for(r=e;r<=t;++r)i(a=n[r].key)&&(o[a]=r);return o}var jr={create:Lr,update:Lr,destroy:function(n){Lr(n,Pr)}};function Lr(n,e){(n.data.directives||e.data.directives)&&function(n,e){var t,r,a,o=n===Pr,i=e===Pr,s=$r(n.data.directives,n.context),l=$r(e.data.directives,e.context),c=[],p=[];for(t in l)r=s[t],a=l[t],r?(a.oldValue=r.value,a.oldArg=r.arg,zr(a,"update",e,n),a.def&&a.def.componentUpdated&&p.push(a)):(zr(a,"bind",e,n),a.def&&a.def.inserted&&c.push(a));if(c.length){var u=function(){for(var t=0;t<c.length;t++)zr(c[t],"inserted",e,n)};o?ne(e,"insert",u):u()}p.length&&ne(e,"postpatch",(function(){for(var t=0;t<p.length;t++)zr(p[t],"componentUpdated",e,n)}));if(!o)for(t in s)l[t]||zr(s[t],"unbind",n,n,i)}(n,e)}var Nr=Object.create(null);function $r(n,e){var t,r,a=Object.create(null);if(!n)return a;for(t=0;t<n.length;t++){if((r=n[t]).modifiers||(r.modifiers=Nr),a[Ur(r)]=r,e._setupState&&e._setupState.__sfc){var o=r.def||$t(e,"_setupState","v-"+r.name);r.def="function"==typeof o?{bind:o,update:o}:o}r.def=r.def||$t(e.$options,"directives",r.name)}return a}function Ur(n){return n.rawName||"".concat(n.name,".").concat(Object.keys(n.modifiers||{}).join("."))}function zr(n,e,t,r,a){var o=n.def&&n.def[e];if(o)try{o(t.elm,n,t,r,a)}catch(r){Pe(r,t.context,"directive ".concat(n.name," ").concat(e," hook"))}}var Hr=[Ar,jr];function qr(n,e){var t=e.componentOptions;if(!(i(t)&&!1===t.Ctor.options.inheritAttrs||o(n.data.attrs)&&o(e.data.attrs))){var r,a,l=e.elm,c=n.data.attrs||{},p=e.data.attrs||{};for(r in(i(p.__ob__)||s(p._v_attr_proxy))&&(p=e.data.attrs=_({},p)),p)a=p[r],c[r]!==a&&Vr(l,r,a,e.data.pre);for(r in(Y||X)&&p.value!==c.value&&Vr(l,"value",p.value),c)o(p[r])&&(hr(r)?l.removeAttributeNS(fr,vr(r)):dr(r)||l.removeAttribute(r))}}function Vr(n,e,t,r){r||n.tagName.indexOf("-")>-1?Kr(n,e,t):gr(e)?br(t)?n.removeAttribute(e):(t="allowfullscreen"===e&&"EMBED"===n.tagName?"true":e,n.setAttribute(e,t)):dr(e)?n.setAttribute(e,function(n,e){return br(e)||"false"===e?"false":"contenteditable"===n&&mr(e)?e:"true"}(e,t)):hr(e)?br(t)?n.removeAttributeNS(fr,vr(e)):n.setAttributeNS(fr,e,t):Kr(n,e,t)}function Kr(n,e,t){if(br(t))n.removeAttribute(e);else{if(Y&&!Q&&"TEXTAREA"===n.tagName&&"placeholder"===e&&""!==t&&!n.__ieph){var r=function(e){e.stopImmediatePropagation(),n.removeEventListener("input",r)};n.addEventListener("input",r),n.__ieph=!0}n.setAttribute(e,t)}}var Wr={create:qr,update:qr};function Gr(n,e){var t=e.elm,r=e.data,a=n.data;if(!(o(r.staticClass)&&o(r.class)&&(o(a)||o(a.staticClass)&&o(a.class)))){var s=kr(e),l=t._transitionClasses;i(l)&&(s=Sr(s,xr(l))),s!==t._prevClass&&(t.setAttribute("class",s),t._prevClass=s)}}var Jr,Yr={create:Gr,update:Gr};function Qr(n,e,t){var r=Jr;return function a(){var o=e.apply(null,arguments);null!==o&&na(n,a,t,r)}}var Xr=Le&&!(en&&Number(en[1])<=53);function Zr(n,e,t,r){if(Xr){var a=vt,o=e;e=o._wrapper=function(n){if(n.target===n.currentTarget||n.timeStamp>=a||n.timeStamp<=0||n.target.ownerDocument!==document)return o.apply(this,arguments)}}Jr.addEventListener(n,e,rn?{capture:t,passive:r}:t)}function na(n,e,t,r){(r||Jr).removeEventListener(n,e._wrapper||e,t)}function ea(n,e){if(!o(n.data.on)||!o(e.data.on)){var t=e.data.on||{},r=n.data.on||{};Jr=e.elm||n.elm,function(n){if(i(n.__r)){var e=Y?"change":"input";n[e]=[].concat(n.__r,n[e]||[]),delete n.__r}i(n.__c)&&(n.change=[].concat(n.__c,n.change||[]),delete n.__c)}(t),Zn(t,r,Zr,na,Qr,e.context),Jr=void 0}}var ta,ra={create:ea,update:ea,destroy:function(n){return ea(n,Pr)}};function aa(n,e){if(!o(n.data.domProps)||!o(e.data.domProps)){var t,r,a=e.elm,l=n.data.domProps||{},c=e.data.domProps||{};for(t in(i(c.__ob__)||s(c._v_attr_proxy))&&(c=e.data.domProps=_({},c)),l)t in c||(a[t]="");for(t in c){if(r=c[t],"textContent"===t||"innerHTML"===t){if(e.children&&(e.children.length=0),r===l[t])continue;1===a.childNodes.length&&a.removeChild(a.childNodes[0])}if("value"===t&&"PROGRESS"!==a.tagName){a._value=r;var p=o(r)?"":String(r);oa(a,p)&&(a.value=p)}else if("innerHTML"===t&&Dr(a.tagName)&&o(a.innerHTML)){(ta=ta||document.createElement("div")).innerHTML="<svg>".concat(r,"</svg>");for(var u=ta.firstChild;a.firstChild;)a.removeChild(a.firstChild);for(;u.firstChild;)a.appendChild(u.firstChild)}else if(r!==l[t])try{a[t]=r}catch(n){}}}}function oa(n,e){return!n.composing&&("OPTION"===n.tagName||function(n,e){var t=!0;try{t=document.activeElement!==n}catch(n){}return t&&n.value!==e}(n,e)||function(n,e){var t=n.value,r=n._vModifiers;if(i(r)){if(r.number)return v(t)!==v(e);if(r.trim)return t.trim()!==e.trim()}return t!==e}(n,e))}var ia={create:aa,update:aa},sa=w((function(n){var e={},t=/:(.+)/;return n.split(/;(?![^(]*\))/g).forEach((function(n){if(n){var r=n.split(t);r.length>1&&(e[r[0].trim()]=r[1].trim())}})),e}));function la(n){var e=ca(n.style);return n.staticStyle?_(n.staticStyle,e):e}function ca(n){return Array.isArray(n)?R(n):"string"==typeof n?sa(n):n}var pa,ua=/^--/,da=/\s*!important$/,ma=function(n,e,t){if(ua.test(e))n.style.setProperty(e,t);else if(da.test(t))n.style.setProperty(T(e),t.replace(da,""),"important");else{var r=fa(e);if(Array.isArray(t))for(var a=0,o=t.length;a<o;a++)n.style[r]=t[a];else n.style[r]=t}},ga=["Webkit","Moz","ms"],fa=w((function(n){if(pa=pa||document.createElement("div").style,"filter"!==(n=D(n))&&n in pa)return n;for(var e=n.charAt(0).toUpperCase()+n.slice(1),t=0;t<ga.length;t++){var r=ga[t]+e;if(r in pa)return r}}));function ha(n,e){var t=e.data,r=n.data;if(!(o(t.staticStyle)&&o(t.style)&&o(r.staticStyle)&&o(r.style))){var a,s,l=e.elm,c=r.staticStyle,p=r.normalizedStyle||r.style||{},u=c||p,d=ca(e.data.style)||{};e.data.normalizedStyle=i(d.__ob__)?_({},d):d;var m=function(n,e){var t,r={};if(e)for(var a=n;a.componentInstance;)(a=a.componentInstance._vnode)&&a.data&&(t=la(a.data))&&_(r,t);(t=la(n.data))&&_(r,t);for(var o=n;o=o.parent;)o.data&&(t=la(o.data))&&_(r,t);return r}(e,!0);for(s in u)o(m[s])&&ma(l,s,"");for(s in m)(a=m[s])!==u[s]&&ma(l,s,null==a?"":a)}}var va={create:ha,update:ha},ba=/\s+/;function ka(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(ba).forEach((function(e){return n.classList.add(e)})):n.classList.add(e);else{var t=" ".concat(n.getAttribute("class")||""," ");t.indexOf(" "+e+" ")<0&&n.setAttribute("class",(t+e).trim())}}function ya(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(ba).forEach((function(e){return n.classList.remove(e)})):n.classList.remove(e),n.classList.length||n.removeAttribute("class");else{for(var t=" ".concat(n.getAttribute("class")||""," "),r=" "+e+" ";t.indexOf(r)>=0;)t=t.replace(r," ");(t=t.trim())?n.setAttribute("class",t):n.removeAttribute("class")}}function Sa(n){if(n){if("object"==typeof n){var e={};return!1!==n.css&&_(e,xa(n.name||"v")),_(e,n),e}return"string"==typeof n?xa(n):void 0}}var xa=w((function(n){return{enterClass:"".concat(n,"-enter"),enterToClass:"".concat(n,"-enter-to"),enterActiveClass:"".concat(n,"-enter-active"),leaveClass:"".concat(n,"-leave"),leaveToClass:"".concat(n,"-leave-to"),leaveActiveClass:"".concat(n,"-leave-active")}})),wa=G&&!Q,Ea="transition",Da="transitionend",Ca="animation",Ia="animationend";wa&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Ea="WebkitTransition",Da="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(Ca="WebkitAnimation",Ia="webkitAnimationEnd"));var Ta=G?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(n){return n()};function Oa(n){Ta((function(){Ta(n)}))}function Aa(n,e){var t=n._transitionClasses||(n._transitionClasses=[]);t.indexOf(e)<0&&(t.push(e),ka(n,e))}function _a(n,e){n._transitionClasses&&y(n._transitionClasses,e),ya(n,e)}function Ra(n,e,t){var r=Fa(n,e),a=r.type,o=r.timeout,i=r.propCount;if(!a)return t();var s="transition"===a?Da:Ia,l=0,c=function(){n.removeEventListener(s,p),t()},p=function(e){e.target===n&&++l>=i&&c()};setTimeout((function(){l<i&&c()}),o+1),n.addEventListener(s,p)}var Pa=/\b(transform|all)(,|$)/;function Fa(n,e){var t,r=window.getComputedStyle(n),a=(r[Ea+"Delay"]||"").split(", "),o=(r[Ea+"Duration"]||"").split(", "),i=Ba(a,o),s=(r[Ca+"Delay"]||"").split(", "),l=(r[Ca+"Duration"]||"").split(", "),c=Ba(s,l),p=0,u=0;return"transition"===e?i>0&&(t="transition",p=i,u=o.length):"animation"===e?c>0&&(t="animation",p=c,u=l.length):u=(t=(p=Math.max(i,c))>0?i>c?"transition":"animation":null)?"transition"===t?o.length:l.length:0,{type:t,timeout:p,propCount:u,hasTransform:"transition"===t&&Pa.test(r[Ea+"Property"])}}function Ba(n,e){for(;n.length<e.length;)n=n.concat(n);return Math.max.apply(null,e.map((function(e,t){return Ma(e)+Ma(n[t])})))}function Ma(n){return 1e3*Number(n.slice(0,-1).replace(",","."))}function ja(n,e){var t=n.elm;i(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var r=Sa(n.data.transition);if(!o(r)&&!i(t._enterCb)&&1===t.nodeType){for(var a=r.css,s=r.type,l=r.enterClass,u=r.enterToClass,d=r.enterActiveClass,m=r.appearClass,g=r.appearToClass,f=r.appearActiveClass,h=r.beforeEnter,b=r.enter,k=r.afterEnter,y=r.enterCancelled,S=r.beforeAppear,x=r.appear,w=r.afterAppear,E=r.appearCancelled,D=r.duration,C=it,I=it.$vnode;I&&I.parent;)C=I.context,I=I.parent;var T=!C._isMounted||!n.isRootInsert;if(!T||x||""===x){var O=T&&m?m:l,A=T&&f?f:d,_=T&&g?g:u,R=T&&S||h,P=T&&c(x)?x:b,F=T&&w||k,B=T&&E||y,M=v(p(D)?D.enter:D);0;var j=!1!==a&&!Q,N=$a(P),$=t._enterCb=L((function(){j&&(_a(t,_),_a(t,A)),$.cancelled?(j&&_a(t,O),B&&B(t)):F&&F(t),t._enterCb=null}));n.data.show||ne(n,"insert",(function(){var e=t.parentNode,r=e&&e._pending&&e._pending[n.key];r&&r.tag===n.tag&&r.elm._leaveCb&&r.elm._leaveCb(),P&&P(t,$)})),R&&R(t),j&&(Aa(t,O),Aa(t,A),Oa((function(){_a(t,O),$.cancelled||(Aa(t,_),N||(Na(M)?setTimeout($,M):Ra(t,s,$)))}))),n.data.show&&(e&&e(),P&&P(t,$)),j||N||$()}}}function La(n,e){var t=n.elm;i(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var r=Sa(n.data.transition);if(o(r)||1!==t.nodeType)return e();if(!i(t._leaveCb)){var a=r.css,s=r.type,l=r.leaveClass,c=r.leaveToClass,u=r.leaveActiveClass,d=r.beforeLeave,m=r.leave,g=r.afterLeave,f=r.leaveCancelled,h=r.delayLeave,b=r.duration,k=!1!==a&&!Q,y=$a(m),S=v(p(b)?b.leave:b);0;var x=t._leaveCb=L((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[n.key]=null),k&&(_a(t,c),_a(t,u)),x.cancelled?(k&&_a(t,l),f&&f(t)):(e(),g&&g(t)),t._leaveCb=null}));h?h(w):w()}function w(){x.cancelled||(!n.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[n.key]=n),d&&d(t),k&&(Aa(t,l),Aa(t,u),Oa((function(){_a(t,l),x.cancelled||(Aa(t,c),y||(Na(S)?setTimeout(x,S):Ra(t,s,x)))}))),m&&m(t,x),k||y||x())}}function Na(n){return"number"==typeof n&&!isNaN(n)}function $a(n){if(o(n))return!1;var e=n.fns;return i(e)?$a(Array.isArray(e)?e[0]:e):(n._length||n.length)>1}function Ua(n,e){!0!==e.data.show&&ja(e)}var za=function(n){var e,t,r={},c=n.modules,p=n.nodeOps;for(e=0;e<Fr.length;++e)for(r[Fr[e]]=[],t=0;t<c.length;++t)i(c[t][Fr[e]])&&r[Fr[e]].push(c[t][Fr[e]]);function u(n){var e=p.parentNode(n);i(e)&&p.removeChild(e,n)}function d(n,e,t,a,o,l,c){if(i(n.elm)&&i(l)&&(n=l[c]=vn(n)),n.isRootInsert=!o,!function(n,e,t,a){var o=n.data;if(i(o)){var l=i(n.componentInstance)&&o.keepAlive;if(i(o=o.hook)&&i(o=o.init)&&o(n,!1),i(n.componentInstance))return m(n,e),g(t,n.elm,a),s(l)&&function(n,e,t,a){var o,s=n;for(;s.componentInstance;)if(s=s.componentInstance._vnode,i(o=s.data)&&i(o=o.transition)){for(o=0;o<r.activate.length;++o)r.activate[o](Pr,s);e.push(s);break}g(t,n.elm,a)}(n,e,t,a),!0}}(n,e,t,a)){var u=n.data,d=n.children,h=n.tag;i(h)?(n.elm=n.ns?p.createElementNS(n.ns,h):p.createElement(h,n),k(n),f(n,d,e),i(u)&&v(n,e),g(t,n.elm,a)):s(n.isComment)?(n.elm=p.createComment(n.text),g(t,n.elm,a)):(n.elm=p.createTextNode(n.text),g(t,n.elm,a))}}function m(n,e){i(n.data.pendingInsert)&&(e.push.apply(e,n.data.pendingInsert),n.data.pendingInsert=null),n.elm=n.componentInstance.$el,h(n)?(v(n,e),k(n)):(_r(n),e.push(n))}function g(n,e,t){i(n)&&(i(t)?p.parentNode(t)===n&&p.insertBefore(n,e,t):p.appendChild(n,e))}function f(n,e,t){if(a(e)){0;for(var r=0;r<e.length;++r)d(e[r],t,n.elm,null,!0,e,r)}else l(n.text)&&p.appendChild(n.elm,p.createTextNode(String(n.text)))}function h(n){for(;n.componentInstance;)n=n.componentInstance._vnode;return i(n.tag)}function v(n,t){for(var a=0;a<r.create.length;++a)r.create[a](Pr,n);i(e=n.data.hook)&&(i(e.create)&&e.create(Pr,n),i(e.insert)&&t.push(n))}function k(n){var e;if(i(e=n.fnScopeId))p.setStyleScope(n.elm,e);else for(var t=n;t;)i(e=t.context)&&i(e=e.$options._scopeId)&&p.setStyleScope(n.elm,e),t=t.parent;i(e=it)&&e!==n.context&&e!==n.fnContext&&i(e=e.$options._scopeId)&&p.setStyleScope(n.elm,e)}function y(n,e,t,r,a,o){for(;r<=a;++r)d(t[r],o,n,e,!1,t,r)}function S(n){var e,t,a=n.data;if(i(a))for(i(e=a.hook)&&i(e=e.destroy)&&e(n),e=0;e<r.destroy.length;++e)r.destroy[e](n);if(i(e=n.children))for(t=0;t<n.children.length;++t)S(n.children[t])}function x(n,e,t){for(;e<=t;++e){var r=n[e];i(r)&&(i(r.tag)?(w(r),S(r)):u(r.elm))}}function w(n,e){if(i(e)||i(n.data)){var t,a=r.remove.length+1;for(i(e)?e.listeners+=a:e=function(n,e){function t(){0==--t.listeners&&u(n)}return t.listeners=e,t}(n.elm,a),i(t=n.componentInstance)&&i(t=t._vnode)&&i(t.data)&&w(t,e),t=0;t<r.remove.length;++t)r.remove[t](n,e);i(t=n.data.hook)&&i(t=t.remove)?t(n,e):e()}else u(n.elm)}function E(n,e,t,r){for(var a=t;a<r;a++){var o=e[a];if(i(o)&&Br(n,o))return a}}function D(n,e,t,a,l,c){if(n!==e){i(e.elm)&&i(a)&&(e=a[l]=vn(e));var u=e.elm=n.elm;if(s(n.isAsyncPlaceholder))i(e.asyncFactory.resolved)?T(n.elm,e,t):e.isAsyncPlaceholder=!0;else if(s(e.isStatic)&&s(n.isStatic)&&e.key===n.key&&(s(e.isCloned)||s(e.isOnce)))e.componentInstance=n.componentInstance;else{var m,g=e.data;i(g)&&i(m=g.hook)&&i(m=m.prepatch)&&m(n,e);var f=n.children,v=e.children;if(i(g)&&h(e)){for(m=0;m<r.update.length;++m)r.update[m](n,e);i(m=g.hook)&&i(m=m.update)&&m(n,e)}o(e.text)?i(f)&&i(v)?f!==v&&function(n,e,t,r,a){var s,l,c,u=0,m=0,g=e.length-1,f=e[0],h=e[g],v=t.length-1,b=t[0],k=t[v],S=!a;for(0;u<=g&&m<=v;)o(f)?f=e[++u]:o(h)?h=e[--g]:Br(f,b)?(D(f,b,r,t,m),f=e[++u],b=t[++m]):Br(h,k)?(D(h,k,r,t,v),h=e[--g],k=t[--v]):Br(f,k)?(D(f,k,r,t,v),S&&p.insertBefore(n,f.elm,p.nextSibling(h.elm)),f=e[++u],k=t[--v]):Br(h,b)?(D(h,b,r,t,m),S&&p.insertBefore(n,h.elm,f.elm),h=e[--g],b=t[++m]):(o(s)&&(s=Mr(e,u,g)),o(l=i(b.key)?s[b.key]:E(b,e,u,g))?d(b,r,n,f.elm,!1,t,m):Br(c=e[l],b)?(D(c,b,r,t,m),e[l]=void 0,S&&p.insertBefore(n,c.elm,f.elm)):d(b,r,n,f.elm,!1,t,m),b=t[++m]);u>g?y(n,o(t[v+1])?null:t[v+1].elm,t,m,v,r):m>v&&x(e,u,g)}(u,f,v,t,c):i(v)?(i(n.text)&&p.setTextContent(u,""),y(u,null,v,0,v.length-1,t)):i(f)?x(f,0,f.length-1):i(n.text)&&p.setTextContent(u,""):n.text!==e.text&&p.setTextContent(u,e.text),i(g)&&i(m=g.hook)&&i(m=m.postpatch)&&m(n,e)}}}function C(n,e,t){if(s(t)&&i(n.parent))n.parent.data.pendingInsert=e;else for(var r=0;r<e.length;++r)e[r].data.hook.insert(e[r])}var I=b("attrs,class,staticClass,staticStyle,key");function T(n,e,t,r){var a,o=e.tag,l=e.data,c=e.children;if(r=r||l&&l.pre,e.elm=n,s(e.isComment)&&i(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(i(l)&&(i(a=l.hook)&&i(a=a.init)&&a(e,!0),i(a=e.componentInstance)))return m(e,t),!0;if(i(o)){if(i(c))if(n.hasChildNodes())if(i(a=l)&&i(a=a.domProps)&&i(a=a.innerHTML)){if(a!==n.innerHTML)return!1}else{for(var p=!0,u=n.firstChild,d=0;d<c.length;d++){if(!u||!T(u,c[d],t,r)){p=!1;break}u=u.nextSibling}if(!p||u)return!1}else f(e,c,t);if(i(l)){var g=!1;for(var h in l)if(!I(h)){g=!0,v(e,t);break}!g&&l.class&&Xe(l.class)}}else n.data!==e.text&&(n.data=e.text);return!0}return function(n,e,t,a){if(!o(e)){var l,c=!1,u=[];if(o(n))c=!0,d(e,u);else{var m=i(n.nodeType);if(!m&&Br(n,e))D(n,e,u,null,null,a);else{if(m){if(1===n.nodeType&&n.hasAttribute("data-server-rendered")&&(n.removeAttribute("data-server-rendered"),t=!0),s(t)&&T(n,e,u))return C(e,u,!0),n;l=n,n=new gn(p.tagName(l).toLowerCase(),{},[],void 0,l)}var g=n.elm,f=p.parentNode(g);if(d(e,u,g._leaveCb?null:f,p.nextSibling(g)),i(e.parent))for(var v=e.parent,b=h(e);v;){for(var k=0;k<r.destroy.length;++k)r.destroy[k](v);if(v.elm=e.elm,b){for(var y=0;y<r.create.length;++y)r.create[y](Pr,v);var w=v.data.hook.insert;if(w.merged)for(var E=1;E<w.fns.length;E++)w.fns[E]()}else _r(v);v=v.parent}i(f)?x([n],0,0):i(n.tag)&&S(n)}}return C(e,u,c),e.elm}i(n)&&S(n)}}({nodeOps:Or,modules:[Wr,Yr,ra,ia,va,G?{create:Ua,activate:Ua,remove:function(n,e){!0!==n.data.show?La(n,e):e()}}:{}].concat(Hr)});Q&&document.addEventListener("selectionchange",(function(){var n=document.activeElement;n&&n.vmodel&&Ya(n,"input")}));var Ha={inserted:function(n,e,t,r){"select"===t.tag?(r.elm&&!r.elm._vOptions?ne(t,"postpatch",(function(){Ha.componentUpdated(n,e,t)})):qa(n,e,t.context),n._vOptions=[].map.call(n.options,Wa)):("textarea"===t.tag||Tr(n.type))&&(n._vModifiers=e.modifiers,e.modifiers.lazy||(n.addEventListener("compositionstart",Ga),n.addEventListener("compositionend",Ja),n.addEventListener("change",Ja),Q&&(n.vmodel=!0)))},componentUpdated:function(n,e,t){if("select"===t.tag){qa(n,e,t.context);var r=n._vOptions,a=n._vOptions=[].map.call(n.options,Wa);if(a.some((function(n,e){return!M(n,r[e])})))(n.multiple?e.value.some((function(n){return Ka(n,a)})):e.value!==e.oldValue&&Ka(e.value,a))&&Ya(n,"change")}}};function qa(n,e,t){Va(n,e,t),(Y||X)&&setTimeout((function(){Va(n,e,t)}),0)}function Va(n,e,t){var r=e.value,a=n.multiple;if(!a||Array.isArray(r)){for(var o,i,s=0,l=n.options.length;s<l;s++)if(i=n.options[s],a)o=j(r,Wa(i))>-1,i.selected!==o&&(i.selected=o);else if(M(Wa(i),r))return void(n.selectedIndex!==s&&(n.selectedIndex=s));a||(n.selectedIndex=-1)}}function Ka(n,e){return e.every((function(e){return!M(e,n)}))}function Wa(n){return"_value"in n?n._value:n.value}function Ga(n){n.target.composing=!0}function Ja(n){n.target.composing&&(n.target.composing=!1,Ya(n.target,"input"))}function Ya(n,e){var t=document.createEvent("HTMLEvents");t.initEvent(e,!0,!0),n.dispatchEvent(t)}function Qa(n){return!n.componentInstance||n.data&&n.data.transition?n:Qa(n.componentInstance._vnode)}var Xa={model:Ha,show:{bind:function(n,e,t){var r=e.value,a=(t=Qa(t)).data&&t.data.transition,o=n.__vOriginalDisplay="none"===n.style.display?"":n.style.display;r&&a?(t.data.show=!0,ja(t,(function(){n.style.display=o}))):n.style.display=r?o:"none"},update:function(n,e,t){var r=e.value;!r!=!e.oldValue&&((t=Qa(t)).data&&t.data.transition?(t.data.show=!0,r?ja(t,(function(){n.style.display=n.__vOriginalDisplay})):La(t,(function(){n.style.display="none"}))):n.style.display=r?n.__vOriginalDisplay:"none")},unbind:function(n,e,t,r,a){a||(n.style.display=n.__vOriginalDisplay)}}},Za={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function no(n){var e=n&&n.componentOptions;return e&&e.Ctor.options.abstract?no(_e(e.children)):n}function eo(n){var e={},t=n.$options;for(var r in t.propsData)e[r]=n[r];var a=t._parentListeners;for(var r in a)e[D(r)]=a[r];return e}function to(n,e){if(/\d-keep-alive$/.test(e.tag))return n("keep-alive",{props:e.componentOptions.propsData})}var ro=function(n){return n.tag||Se(n)},ao=function(n){return"show"===n.name},oo={name:"transition",props:Za,abstract:!0,render:function(n){var e=this,t=this.$slots.default;if(t&&(t=t.filter(ro)).length){0;var r=this.mode;0;var a=t[0];if(function(n){for(;n=n.parent;)if(n.data.transition)return!0}(this.$vnode))return a;var o=no(a);if(!o)return a;if(this._leaving)return to(n,a);var i="__transition-".concat(this._uid,"-");o.key=null==o.key?o.isComment?i+"comment":i+o.tag:l(o.key)?0===String(o.key).indexOf(i)?o.key:i+o.key:o.key;var s=(o.data||(o.data={})).transition=eo(this),c=this._vnode,p=no(c);if(o.data.directives&&o.data.directives.some(ao)&&(o.data.show=!0),p&&p.data&&!function(n,e){return e.key===n.key&&e.tag===n.tag}(o,p)&&!Se(p)&&(!p.componentInstance||!p.componentInstance._vnode.isComment)){var u=p.data.transition=_({},s);if("out-in"===r)return this._leaving=!0,ne(u,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),to(n,a);if("in-out"===r){if(Se(o))return c;var d,m=function(){d()};ne(s,"afterEnter",m),ne(s,"enterCancelled",m),ne(u,"delayLeave",(function(n){d=n}))}}return a}}},io=_({tag:String,moveClass:String},Za);function so(n){n.elm._moveCb&&n.elm._moveCb(),n.elm._enterCb&&n.elm._enterCb()}function lo(n){n.data.newPos=n.elm.getBoundingClientRect()}function co(n){var e=n.data.pos,t=n.data.newPos,r=e.left-t.left,a=e.top-t.top;if(r||a){n.data.moved=!0;var o=n.elm.style;o.transform=o.WebkitTransform="translate(".concat(r,"px,").concat(a,"px)"),o.transitionDuration="0s"}}delete io.mode;var po={Transition:oo,TransitionGroup:{props:io,beforeMount:function(){var n=this,e=this._update;this._update=function(t,r){var a=st(n);n.__patch__(n._vnode,n.kept,!1,!0),n._vnode=n.kept,a(),e.call(n,t,r)}},render:function(n){for(var e=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),r=this.prevChildren=this.children,a=this.$slots.default||[],o=this.children=[],i=eo(this),s=0;s<a.length;s++){if((p=a[s]).tag)if(null!=p.key&&0!==String(p.key).indexOf("__vlist"))o.push(p),t[p.key]=p,(p.data||(p.data={})).transition=i;else;}if(r){var l=[],c=[];for(s=0;s<r.length;s++){var p;(p=r[s]).data.transition=i,p.data.pos=p.elm.getBoundingClientRect(),t[p.key]?l.push(p):c.push(p)}this.kept=n(e,null,l),this.removed=c}return n(e,null,o)},updated:function(){var n=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";n.length&&this.hasMove(n[0].elm,e)&&(n.forEach(so),n.forEach(lo),n.forEach(co),this._reflow=document.body.offsetHeight,n.forEach((function(n){if(n.data.moved){var t=n.elm,r=t.style;Aa(t,e),r.transform=r.WebkitTransform=r.transitionDuration="",t.addEventListener(Da,t._moveCb=function n(r){r&&r.target!==t||r&&!/transform$/.test(r.propertyName)||(t.removeEventListener(Da,n),t._moveCb=null,_a(t,e))})}})))},methods:{hasMove:function(n,e){if(!wa)return!1;if(this._hasMove)return this._hasMove;var t=n.cloneNode();n._transitionClasses&&n._transitionClasses.forEach((function(n){ya(t,n)})),ka(t,e),t.style.display="none",this.$el.appendChild(t);var r=Fa(t);return this.$el.removeChild(t),this._hasMove=r.hasTransform}}}};tr.config.mustUseProp=function(n,e,t){return"value"===t&&ur(n)&&"button"!==e||"selected"===t&&"option"===n||"checked"===t&&"input"===n||"muted"===t&&"video"===n},tr.config.isReservedTag=Cr,tr.config.isReservedAttr=pr,tr.config.getTagNamespace=function(n){return Dr(n)?"svg":"math"===n?"math":void 0},tr.config.isUnknownElement=function(n){if(!G)return!0;if(Cr(n))return!1;if(n=n.toLowerCase(),null!=Ir[n])return Ir[n];var e=document.createElement(n);return n.indexOf("-")>-1?Ir[n]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:Ir[n]=/HTMLUnknownElement/.test(e.toString())},_(tr.options.directives,Xa),_(tr.options.components,po),tr.prototype.__patch__=G?za:P,tr.prototype.$mount=function(n,e){return function(n,e,t){var r;n.$el=e,n.$options.render||(n.$options.render=fn),pt(n,"beforeMount"),r=function(){n._update(n._render(),t)},new et(n,r,P,{before:function(){n._isMounted&&!n._isDestroyed&&pt(n,"beforeUpdate")}},!0),t=!1;var a=n._preWatchers;if(a)for(var o=0;o<a.length;o++)a[o].run();return null==n.$vnode&&(n._isMounted=!0,pt(n,"mounted")),n}(this,n=n&&G?function(n){if("string"==typeof n){var e=document.querySelector(n);return e||document.createElement("div")}return n}(n):void 0,e)},G&&setTimeout((function(){z.devtools&&sn&&sn.emit("init",tr)}),0)},function(n,e,t){"use strict";t.d(e,"a",(function(){return u}));var r=t(1);
/**
  * vue-class-component v7.2.6
  * (c) 2015-present Evan You
  * @license MIT
  */function a(n){return(a="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n})(n)}function o(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}function i(n){return function(n){if(Array.isArray(n)){for(var e=0,t=new Array(n.length);e<n.length;e++)t[e]=n[e];return t}}(n)||function(n){if(Symbol.iterator in Object(n)||"[object Arguments]"===Object.prototype.toString.call(n))return Array.from(n)}(n)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance")}()}function s(){return"undefined"!=typeof Reflect&&Reflect.defineMetadata&&Reflect.getOwnMetadataKeys}function l(n,e){c(n,e),Object.getOwnPropertyNames(e.prototype).forEach((function(t){c(n.prototype,e.prototype,t)})),Object.getOwnPropertyNames(e).forEach((function(t){c(n,e,t)}))}function c(n,e,t){(t?Reflect.getOwnMetadataKeys(e,t):Reflect.getOwnMetadataKeys(e)).forEach((function(r){var a=t?Reflect.getOwnMetadata(r,e,t):Reflect.getOwnMetadata(r,e);t?Reflect.defineMetadata(r,a,n,t):Reflect.defineMetadata(r,a,n)}))}var p={__proto__:[]}instanceof Array;function u(n){return function(e,t,r){var a="function"==typeof e?e:e.constructor;a.__decorators__||(a.__decorators__=[]),"number"!=typeof r&&(r=void 0),a.__decorators__.push((function(e){return n(e,t,r)}))}}function d(n,e){var t=e.prototype._init;e.prototype._init=function(){var e=this,t=Object.getOwnPropertyNames(n);if(n.$options.props)for(var r in n.$options.props)n.hasOwnProperty(r)||t.push(r);t.forEach((function(t){Object.defineProperty(e,t,{get:function(){return n[t]},set:function(e){n[t]=e},configurable:!0})}))};var r=new e;e.prototype._init=t;var a={};return Object.keys(r).forEach((function(n){void 0!==r[n]&&(a[n]=r[n])})),a}var m=["data","beforeCreate","created","beforeMount","mounted","beforeDestroy","destroyed","beforeUpdate","updated","activated","deactivated","render","errorCaptured","serverPrefetch"];function g(n){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};e.name=e.name||n._componentTag||n.name;var t=n.prototype;Object.getOwnPropertyNames(t).forEach((function(n){if("constructor"!==n)if(m.indexOf(n)>-1)e[n]=t[n];else{var r=Object.getOwnPropertyDescriptor(t,n);void 0!==r.value?"function"==typeof r.value?(e.methods||(e.methods={}))[n]=r.value:(e.mixins||(e.mixins=[])).push({data:function(){return o({},n,r.value)}}):(r.get||r.set)&&((e.computed||(e.computed={}))[n]={get:r.get,set:r.set})}})),(e.mixins||(e.mixins=[])).push({data:function(){return d(this,n)}});var a=n.__decorators__;a&&(a.forEach((function(n){return n(e)})),delete n.__decorators__);var i=Object.getPrototypeOf(n.prototype),c=i instanceof r.b?i.constructor:r.b,p=c.extend(e);return h(p,n,c),s()&&l(p,n),p}var f={prototype:!0,arguments:!0,callee:!0,caller:!0};function h(n,e,t){Object.getOwnPropertyNames(e).forEach((function(r){if(!f[r]){var o=Object.getOwnPropertyDescriptor(n,r);if(!o||o.configurable){var i,s,l=Object.getOwnPropertyDescriptor(e,r);if(!p){if("cid"===r)return;var c=Object.getOwnPropertyDescriptor(t,r);if(i=l.value,s=a(i),null!=i&&("object"===s||"function"===s)&&c&&c.value===l.value)return}0,Object.defineProperty(n,r,l)}}}))}function v(n){return"function"==typeof n?g(n):function(e){return g(e,n)}}v.registerHooks=function(n){m.push.apply(m,i(n))},e.b=v},function(n,e,t){"use strict";function r(n,e,t,r,a,o,i,s){var l,c="function"==typeof n?n.options:n;if(e&&(c.render=e,c.staticRenderFns=t,c._compiled=!0),r&&(c.functional=!0),o&&(c._scopeId="data-v-"+o),i?(l=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),a&&a.call(this,n),n&&n._registeredComponents&&n._registeredComponents.add(i)},c._ssrRegister=l):a&&(l=s?function(){a.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:a),l)if(c.functional){c._injectStyles=l;var p=c.render;c.render=function(n,e){return l.call(e),p(n,e)}}else{var u=c.beforeCreate;c.beforeCreate=u?[].concat(u,l):[l]}return{exports:n,options:c}}t.d(e,"a",(function(){return r}))},function(n,e,t){var r=t(72),a=r.all;n.exports=r.IS_HTMLDDA?function(n){return"function"==typeof n||n===a}:function(n){return"function"==typeof n}},function(n,e,t){"use strict";var r=t(117),a=Object.prototype.toString;function o(n){return"[object Array]"===a.call(n)}function i(n){return void 0===n}function s(n){return null!==n&&"object"==typeof n}function l(n){if("[object Object]"!==a.call(n))return!1;var e=Object.getPrototypeOf(n);return null===e||e===Object.prototype}function c(n){return"[object Function]"===a.call(n)}function p(n,e){if(null!=n)if("object"!=typeof n&&(n=[n]),o(n))for(var t=0,r=n.length;t<r;t++)e.call(null,n[t],t,n);else for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&e.call(null,n[a],a,n)}n.exports={isArray:o,isArrayBuffer:function(n){return"[object ArrayBuffer]"===a.call(n)},isBuffer:function(n){return null!==n&&!i(n)&&null!==n.constructor&&!i(n.constructor)&&"function"==typeof n.constructor.isBuffer&&n.constructor.isBuffer(n)},isFormData:function(n){return"undefined"!=typeof FormData&&n instanceof FormData},isArrayBufferView:function(n){return"undefined"!=typeof ArrayBuffer&&ArrayBuffer.isView?ArrayBuffer.isView(n):n&&n.buffer&&n.buffer instanceof ArrayBuffer},isString:function(n){return"string"==typeof n},isNumber:function(n){return"number"==typeof n},isObject:s,isPlainObject:l,isUndefined:i,isDate:function(n){return"[object Date]"===a.call(n)},isFile:function(n){return"[object File]"===a.call(n)},isBlob:function(n){return"[object Blob]"===a.call(n)},isFunction:c,isStream:function(n){return s(n)&&c(n.pipe)},isURLSearchParams:function(n){return"undefined"!=typeof URLSearchParams&&n instanceof URLSearchParams},isStandardBrowserEnv:function(){return("undefined"==typeof navigator||"ReactNative"!==navigator.product&&"NativeScript"!==navigator.product&&"NS"!==navigator.product)&&("undefined"!=typeof window&&"undefined"!=typeof document)},forEach:p,merge:function n(){var e={};function t(t,r){l(e[r])&&l(t)?e[r]=n(e[r],t):l(t)?e[r]=n({},t):o(t)?e[r]=t.slice():e[r]=t}for(var r=0,a=arguments.length;r<a;r++)p(arguments[r],t);return e},extend:function(n,e,t){return p(e,(function(e,a){n[a]=t&&"function"==typeof e?r(e,t):e})),n},trim:function(n){return n.trim?n.trim():n.replace(/^\s+|\s+$/g,"")},stripBOM:function(n){return 65279===n.charCodeAt(0)&&(n=n.slice(1)),n}}},function(n,e){n.exports=function(n){try{return!!n()}catch(n){return!0}}},function(n,e,t){var r=t(44),a=Function.prototype,o=a.call,i=r&&a.bind.bind(o,o);n.exports=r?i:function(n){return function(){return o.apply(n,arguments)}}},function(n,e){var t=function(n){return n&&n.Math==Math&&n};n.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||this||Function("return this")()},function(n,e,t){var r=t(6);n.exports=!r((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(n,e,t){n.exports=t(313)},function(n,e,t){var r=t(90),a="object"==typeof self&&self&&self.Object===Object&&self,o=r||a||Function("return this")();n.exports=o},function(n,e){var t=Array.isArray;n.exports=t},function(n,e,t){var r=t(7),a=t(33),o=r({}.hasOwnProperty);n.exports=Object.hasOwn||function(n,e){return o(a(n),e)}},function(n,e,t){var r=t(4),a=t(72),o=a.all;n.exports=a.IS_HTMLDDA?function(n){return"object"==typeof n?null!==n:r(n)||n===o}:function(n){return"object"==typeof n?null!==n:r(n)}},function(n,e,t){var r=t(195),a=t(198);n.exports=function(n,e){var t=a(n,e);return r(t)?t:void 0}},function(n,e,t){var r=t(301),a=t(115),o=/[T ]/,i=/:/,s=/^(\d{2})$/,l=[/^([+-]\d{2})$/,/^([+-]\d{3})$/,/^([+-]\d{4})$/],c=/^(\d{4})/,p=[/^([+-]\d{4})/,/^([+-]\d{5})/,/^([+-]\d{6})/],u=/^-(\d{2})$/,d=/^-?(\d{3})$/,m=/^-?(\d{2})-?(\d{2})$/,g=/^-?W(\d{2})$/,f=/^-?W(\d{2})-?(\d{1})$/,h=/^(\d{2}([.,]\d*)?)$/,v=/^(\d{2}):?(\d{2}([.,]\d*)?)$/,b=/^(\d{2}):?(\d{2}):?(\d{2}([.,]\d*)?)$/,k=/([Z+-].*)$/,y=/^(Z)$/,S=/^([+-])(\d{2})$/,x=/^([+-])(\d{2}):?(\d{2})$/;function w(n,e,t){e=e||0,t=t||0;var r=new Date(0);r.setUTCFullYear(n,0,4);var a=7*e+t+1-(r.getUTCDay()||7);return r.setUTCDate(r.getUTCDate()+a),r}n.exports=function(n,e){if(a(n))return new Date(n.getTime());if("string"!=typeof n)return new Date(n);var t=(e||{}).additionalDigits;t=null==t?2:Number(t);var E=function(n){var e,t={},r=n.split(o);i.test(r[0])?(t.date=null,e=r[0]):(t.date=r[0],e=r[1]);if(e){var a=k.exec(e);a?(t.time=e.replace(a[1],""),t.timezone=a[1]):t.time=e}return t}(n),D=function(n,e){var t,r=l[e],a=p[e];if(t=c.exec(n)||a.exec(n)){var o=t[1];return{year:parseInt(o,10),restDateString:n.slice(o.length)}}if(t=s.exec(n)||r.exec(n)){var i=t[1];return{year:100*parseInt(i,10),restDateString:n.slice(i.length)}}return{year:null}}(E.date,t),C=D.year,I=function(n,e){if(null===e)return null;var t,r,a,o;if(0===n.length)return(r=new Date(0)).setUTCFullYear(e),r;if(t=u.exec(n))return r=new Date(0),a=parseInt(t[1],10)-1,r.setUTCFullYear(e,a),r;if(t=d.exec(n)){r=new Date(0);var i=parseInt(t[1],10);return r.setUTCFullYear(e,0,i),r}if(t=m.exec(n)){r=new Date(0),a=parseInt(t[1],10)-1;var s=parseInt(t[2],10);return r.setUTCFullYear(e,a,s),r}if(t=g.exec(n))return o=parseInt(t[1],10)-1,w(e,o);if(t=f.exec(n)){o=parseInt(t[1],10)-1;var l=parseInt(t[2],10)-1;return w(e,o,l)}return null}(D.restDateString,C);if(I){var T,O=I.getTime(),A=0;if(E.time&&(A=function(n){var e,t,r;if(e=h.exec(n))return(t=parseFloat(e[1].replace(",",".")))%24*36e5;if(e=v.exec(n))return t=parseInt(e[1],10),r=parseFloat(e[2].replace(",",".")),t%24*36e5+6e4*r;if(e=b.exec(n)){t=parseInt(e[1],10),r=parseInt(e[2],10);var a=parseFloat(e[3].replace(",","."));return t%24*36e5+6e4*r+1e3*a}return null}(E.time)),E.timezone)T=6e4*function(n){var e,t;if(e=y.exec(n))return 0;if(e=S.exec(n))return t=60*parseInt(e[2],10),"+"===e[1]?-t:t;if(e=x.exec(n))return t=60*parseInt(e[2],10)+parseInt(e[3],10),"+"===e[1]?-t:t;return 0}(E.timezone);else{var _=O+A,R=new Date(_);T=r(R);var P=new Date(_);P.setDate(R.getDate()+1);var F=r(P)-r(R);F>0&&(T+=F)}return new Date(O+A+T)}return new Date(n)}},function(n,e,t){"use strict";var r=t(23),a=t(33),o=t(35),i=t(161),s=t(163);r({target:"Array",proto:!0,arity:1,forced:t(6)((function(){return 4294967297!==[].push.call({length:4294967296},1)}))||!function(){try{Object.defineProperty([],"length",{writable:!1}).push()}catch(n){return n instanceof TypeError}}()},{push:function(n){var e=a(this),t=o(e),r=arguments.length;s(t+r);for(var l=0;l<r;l++)e[t]=arguments[l],t++;return i(e,t),t}})},function(n,e,t){"use strict";t.d(e,"c",(function(){return o})),t.d(e,"i",(function(){return i})),t.d(e,"f",(function(){return l})),t.d(e,"g",(function(){return c})),t.d(e,"h",(function(){return p})),t.d(e,"d",(function(){return u})),t.d(e,"e",(function(){return d})),t.d(e,"k",(function(){return m})),t.d(e,"l",(function(){return g})),t.d(e,"j",(function(){return f})),t.d(e,"b",(function(){return v})),t.d(e,"a",(function(){return b}));t(17);const r=/#.*$/,a=/\.(md|html)$/,o=/\/$/,i=/^(https?:|mailto:|tel:)/;function s(n){return decodeURI(n).replace(r,"").replace(a,"")}function l(n){return i.test(n)}function c(n){return/^mailto:/.test(n)}function p(n){return/^tel:/.test(n)}function u(n){if(l(n))return n;const e=n.match(r),t=e?e[0]:"",a=s(n);return o.test(a)?n:a+".html"+t}function d(n,e){const t=n.hash,a=function(n){const e=n.match(r);if(e)return e[0]}(e);if(a&&t!==a)return!1;return s(n.path)===s(e)}function m(n,e,t){t&&(e=function(n,e,t){const r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;const a=e.split("/");t&&a[a.length-1]||a.pop();const o=n.replace(/^\//,"").split("/");for(let n=0;n<o.length;n++){const e=o[n];".."===e?a.pop():"."!==e&&a.push(e)}""!==a[0]&&a.unshift("");return a.join("/")}(e,t));const r=s(e);for(let e=0;e<n.length;e++)if(s(n[e].regularPath)===r)return Object.assign({},n[e],{type:"page",path:u(n[e].path)});return console.error(`[vuepress] No matching page found for sidebar item "${e}"`),{}}function g(n,e,t,r){const{pages:a,themeConfig:o}=t,i=(r&&o.locales&&o.locales[r]||o).sidebar||o.sidebar,{base:s,config:l}=function(n,e){if(Array.isArray(e))return{base:"/",config:e};for(const r in e)if(0===(t=n,/(\.html|\/)$/.test(t)?t:t+"/").indexOf(encodeURI(r)))return{base:r,config:e[r]};var t;return{}}(e,i);return l?l.map(n=>function n(e,t,r,a=1){if("string"==typeof e)return m(t,e,r);if(Array.isArray(e))return Object.assign(m(t,e[0],r),{title:e[1]});{a>3&&console.error("[vuepress] detected a too deep nested sidebar group.");const o=e.children||[];return 0===o.length&&e.path?Object.assign(m(t,e.path,r),{title:e.title}):{type:"group",path:e.path,title:e.title,sidebarDepth:e.sidebarDepth,children:o.map(e=>n(e,t,r,a+1)),collapsable:!1!==e.collapsable}}}(n,a,s)):[]}function f(n){return Object.assign(n,{type:n.items&&n.items.length?"links":"link"})}function h(n){return n?new Date(n).getTime():0}function v(n,e){const t=h(n.frontmatter.date),r=h(e.frontmatter.date);return 0===t||0===r?0:r-t}function b(n){const e=document.createElement("link");e.rel="stylesheet",e.href=n,document.head.append(e)}},function(n,e,t){var r=t(9),a=t(81),o=t(83),i=t(25),s=t(71),l=TypeError,c=Object.defineProperty,p=Object.getOwnPropertyDescriptor;e.f=r?o?function(n,e,t){if(i(n),e=s(e),i(t),"function"==typeof n&&"prototype"===e&&"value"in t&&"writable"in t&&!t.writable){var r=p(n,e);r&&r.writable&&(n[e]=t.value,t={configurable:"configurable"in t?t.configurable:r.configurable,enumerable:"enumerable"in t?t.enumerable:r.enumerable,writable:!1})}return c(n,e,t)}:c:function(n,e,t){if(i(n),e=s(e),i(t),a)try{return c(n,e,t)}catch(n){}if("get"in t||"set"in t)throw l("Accessors not supported");return"value"in t&&(n[e]=t.value),n}},function(n,e){n.exports=function(n){return null!=n&&"object"==typeof n}},function(n,e,t){"use strict";var r=t(288),a=t(299),o=t(63);n.exports={formats:o,parse:a,stringify:r}},function(n,e,t){"use strict";var r=t(1);e.a=new r.b},function(n,e,t){var r=t(8),a=t(68).f,o=t(24),i=t(144),s=t(49),l=t(85),c=t(157);n.exports=function(n,e){var t,p,u,d,m,g=n.target,f=n.global,h=n.stat;if(t=f?r:h?r[g]||s(g,{}):(r[g]||{}).prototype)for(p in e){if(d=e[p],u=n.dontCallGetSet?(m=a(t,p))&&m.value:t[p],!c(f?p:g+(h?".":"#")+p,n.forced)&&void 0!==u){if(typeof d==typeof u)continue;l(d,u)}(n.sham||u&&u.sham)&&o(d,"sham",!0),i(t,p,d,n)}}},function(n,e,t){var r=t(9),a=t(19),o=t(45);n.exports=r?function(n,e,t){return a.f(n,e,o(1,t))}:function(n,e,t){return n[e]=t,n}},function(n,e,t){var r=t(14),a=String,o=TypeError;n.exports=function(n){if(r(n))return n;throw o(a(n)+" is not an object")}},function(n,e,t){var r=t(11).Symbol;n.exports=r},function(n,e,t){var r=t(26),a=t(180),o=t(181),i=r?r.toStringTag:void 0;n.exports=function(n){return null==n?void 0===n?"[object Undefined]":"[object Null]":i&&i in Object(n)?a(n):o(n)}},function(n,e,t){"use strict";t.d(e,"a",(function(){return a})),t.d(e,"c",(function(){return o})),t.d(e,"b",(function(){return i}));var r=t(18);function a(n,e){return n=n.filter((t,r)=>{const{title:a,frontmatter:{home:o,date:i,publish:s}}=t;if(n.indexOf(t)!==r)return!1;{const n=!0===o||null==a||!1===s;return!0===e?!(n||void 0===i):!n}})}function o(n){n.sort((n,e)=>{const t=n.frontmatter.sticky,a=e.frontmatter.sticky;return t&&a?t==a?Object(r.b)(n,e):t-a:t&&!a?-1:!t&&a?1:Object(r.b)(n,e)})}function i(n){n.sort((n,e)=>Object(r.b)(n,e))}},function(n,e,t){var r=t(8),a=t(78),o=t(13),i=t(80),s=t(76),l=t(75),c=r.Symbol,p=a("wks"),u=l?c.for||c:c&&c.withoutSetter||i;n.exports=function(n){return o(p,n)||(p[n]=s&&o(c,n)?c[n]:u("Symbol."+n)),p[n]}},function(n,e,t){var r=t(69),a=t(46);n.exports=function(n){return r(a(n))}},function(n,e,t){var r=t(7),a=r({}.toString),o=r("".slice);n.exports=function(n){return o(a(n),8,-1)}},function(n,e,t){var r=t(8),a=t(4),o=function(n){return a(n)?n:void 0};n.exports=function(n,e){return arguments.length<2?o(r[n]):r[n]&&r[n][e]}},function(n,e,t){var r=t(46),a=Object;n.exports=function(n){return a(r(n))}},function(n,e,t){var r=t(154);n.exports=function(n){var e=+n;return e!=e||0===e?0:r(e)}},function(n,e,t){var r=t(155);n.exports=function(n){return r(n.length)}},function(n,e,t){var r=t(185),a=t(186),o=t(187),i=t(188),s=t(189);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=a,l.prototype.get=o,l.prototype.has=i,l.prototype.set=s,n.exports=l},function(n,e,t){var r=t(92);n.exports=function(n,e){for(var t=n.length;t--;)if(r(n[t][0],e))return t;return-1}},function(n,e,t){var r=t(15)(Object,"create");n.exports=r},function(n,e,t){var r=t(207);n.exports=function(n,e){var t=n.__data__;return r(e)?t["string"==typeof e?"string":"hash"]:t.map}},function(n,e,t){var r=t(59);n.exports=function(n){if("string"==typeof n||r(n))return n;var e=n+"";return"0"==e&&1/n==-1/0?"-0":e}},function(n,e,t){var r,a;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(a="function"==typeof(r=function(){var n,e,t={version:"0.2.0"},r=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function a(n,e,t){return n<e?e:n>t?t:n}function o(n){return 100*(-1+n)}t.configure=function(n){var e,t;for(e in n)void 0!==(t=n[e])&&n.hasOwnProperty(e)&&(r[e]=t);return this},t.status=null,t.set=function(n){var e=t.isStarted();n=a(n,r.minimum,1),t.status=1===n?null:n;var l=t.render(!e),c=l.querySelector(r.barSelector),p=r.speed,u=r.easing;return l.offsetWidth,i((function(e){""===r.positionUsing&&(r.positionUsing=t.getPositioningCSS()),s(c,function(n,e,t){var a;return(a="translate3d"===r.positionUsing?{transform:"translate3d("+o(n)+"%,0,0)"}:"translate"===r.positionUsing?{transform:"translate("+o(n)+"%,0)"}:{"margin-left":o(n)+"%"}).transition="all "+e+"ms "+t,a}(n,p,u)),1===n?(s(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){s(l,{transition:"all "+p+"ms linear",opacity:0}),setTimeout((function(){t.remove(),e()}),p)}),p)):setTimeout(e,p)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var n=function(){setTimeout((function(){t.status&&(t.trickle(),n())}),r.trickleSpeed)};return r.trickle&&n(),this},t.done=function(n){return n||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(n){var e=t.status;return e?("number"!=typeof n&&(n=(1-e)*a(Math.random()*e,.1,.95)),e=a(e+n,0,.994),t.set(e)):t.start()},t.trickle=function(){return t.inc(Math.random()*r.trickleRate)},n=0,e=0,t.promise=function(r){return r&&"resolved"!==r.state()?(0===e&&t.start(),n++,e++,r.always((function(){0==--e?(n=0,t.done()):t.set((n-e)/n)})),this):this},t.render=function(n){if(t.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var e=document.createElement("div");e.id="nprogress",e.innerHTML=r.template;var a,i=e.querySelector(r.barSelector),l=n?"-100":o(t.status||0),p=document.querySelector(r.parent);return s(i,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),r.showSpinner||(a=e.querySelector(r.spinnerSelector))&&d(a),p!=document.body&&c(p,"nprogress-custom-parent"),p.appendChild(e),e},t.remove=function(){p(document.documentElement,"nprogress-busy"),p(document.querySelector(r.parent),"nprogress-custom-parent");var n=document.getElementById("nprogress");n&&d(n)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var n=document.body.style,e="WebkitTransform"in n?"Webkit":"MozTransform"in n?"Moz":"msTransform"in n?"ms":"OTransform"in n?"O":"";return e+"Perspective"in n?"translate3d":e+"Transform"in n?"translate":"margin"};var i=function(){var n=[];function e(){var t=n.shift();t&&t(e)}return function(t){n.push(t),1==n.length&&e()}}(),s=function(){var n=["Webkit","O","Moz","ms"],e={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(n,e){return e.toUpperCase()})),e[t]||(e[t]=function(e){var t=document.body.style;if(e in t)return e;for(var r,a=n.length,o=e.charAt(0).toUpperCase()+e.slice(1);a--;)if((r=n[a]+o)in t)return r;return e}(t))}function r(n,e,r){e=t(e),n.style[e]=r}return function(n,e){var t,a,o=arguments;if(2==o.length)for(t in e)void 0!==(a=e[t])&&e.hasOwnProperty(t)&&r(n,t,a);else r(n,o[1],o[2])}}();function l(n,e){return("string"==typeof n?n:u(n)).indexOf(" "+e+" ")>=0}function c(n,e){var t=u(n),r=t+e;l(t,e)||(n.className=r.substring(1))}function p(n,e){var t,r=u(n);l(n,e)&&(t=r.replace(" "+e+" "," "),n.className=t.substring(1,t.length-1))}function u(n){return(" "+(n.className||"")+" ").replace(/\s+/gi," ")}function d(n){n&&n.parentNode&&n.parentNode.removeChild(n)}return t})?r.call(e,t,e,n):r)||(n.exports=a)},function(n,e,t){"use strict";t.d(e,"b",(function(){return r})),t.d(e,"c",(function(){return a})),t.d(e,"a",(function(){return o}));t(17),t(18);function r(){const n=["#e15b64","#f47e60","#f8b26a","#abbd81","#849b87","#e15b64","#f47e60","#f8b26a","#f26d6d","#67cc86","#fb9b5f","#3498db"];return n[Math.floor(Math.random()*n.length)]}function a(n){const e=n.__proto__.push;n.__proto__.push=function(n){return e.call(this,n).catch(n=>n)}}function o(n){const e=n.getRoutes();n.beforeEach((n,t,r)=>{const a=e.find(e=>e.regex.test(n.path));return/\.html$/.test(n.path)||a&&"*"!==a.path&&!a.redirect?r():decodeURIComponent(n.path)!==n.path?r(Object.assign({},n,{path:decodeURIComponent(n.path),fullPath:decodeURIComponent(n.fullPath)})):void r()})}},function(n,e,t){var r=t(44),a=Function.prototype.call;n.exports=r?a.bind(a):function(){return a.apply(a,arguments)}},function(n,e,t){var r=t(6);n.exports=!r((function(){var n=function(){}.bind();return"function"!=typeof n||n.hasOwnProperty("prototype")}))},function(n,e){n.exports=function(n,e){return{enumerable:!(1&n),configurable:!(2&n),writable:!(4&n),value:e}}},function(n,e,t){var r=t(70),a=TypeError;n.exports=function(n){if(r(n))throw a("Can't call method on "+n);return n}},function(n,e,t){var r=t(4),a=t(142),o=TypeError;n.exports=function(n){if(r(n))return n;throw o(a(n)+" is not a function")}},function(n,e,t){var r=t(8),a=t(49),o=r["__core-js_shared__"]||a("__core-js_shared__",{});n.exports=o},function(n,e,t){var r=t(8),a=Object.defineProperty;n.exports=function(n,e){try{a(r,n,{value:e,configurable:!0,writable:!0})}catch(t){r[n]=e}return e}},function(n,e){n.exports={}},function(n,e){n.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(n,e,t){var r=t(179),a=t(20),o=Object.prototype,i=o.hasOwnProperty,s=o.propertyIsEnumerable,l=r(function(){return arguments}())?r:function(n){return a(n)&&i.call(n,"callee")&&!s.call(n,"callee")};n.exports=l},function(n,e,t){var r=t(15)(t(11),"Map");n.exports=r},function(n,e){n.exports=function(n){var e=typeof n;return null!=n&&("object"==e||"function"==e)}},function(n,e,t){var r=t(199),a=t(206),o=t(208),i=t(209),s=t(210);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=a,l.prototype.get=o,l.prototype.has=i,l.prototype.set=s,n.exports=l},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n){t[++e]=n})),t}},function(n,e){n.exports=function(n){return"number"==typeof n&&n>-1&&n%1==0&&n<=9007199254740991}},function(n,e,t){var r=t(12),a=t(59),o=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,i=/^\w*$/;n.exports=function(n,e){if(r(n))return!1;var t=typeof n;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=n&&!a(n))||(i.test(n)||!o.test(n)||null!=e&&n in Object(e))}},function(n,e,t){var r=t(27),a=t(20);n.exports=function(n){return"symbol"==typeof n||a(n)&&"[object Symbol]"==r(n)}},function(n,e){n.exports=function(n){return n}},function(n,e,t){"use strict";var r=SyntaxError,a=Function,o=TypeError,i=function(n){try{return a('"use strict"; return ('+n+").constructor;")()}catch(n){}},s=Object.getOwnPropertyDescriptor;if(s)try{s({},"")}catch(n){s=null}var l=function(){throw new o},c=s?function(){try{return l}catch(n){try{return s(arguments,"callee").get}catch(n){return l}}}():l,p=t(290)(),u=t(292)(),d=Object.getPrototypeOf||(u?function(n){return n.__proto__}:null),m={},g="undefined"!=typeof Uint8Array&&d?d(Uint8Array):void 0,f={"%AggregateError%":"undefined"==typeof AggregateError?void 0:AggregateError,"%Array%":Array,"%ArrayBuffer%":"undefined"==typeof ArrayBuffer?void 0:ArrayBuffer,"%ArrayIteratorPrototype%":p&&d?d([][Symbol.iterator]()):void 0,"%AsyncFromSyncIteratorPrototype%":void 0,"%AsyncFunction%":m,"%AsyncGenerator%":m,"%AsyncGeneratorFunction%":m,"%AsyncIteratorPrototype%":m,"%Atomics%":"undefined"==typeof Atomics?void 0:Atomics,"%BigInt%":"undefined"==typeof BigInt?void 0:BigInt,"%BigInt64Array%":"undefined"==typeof BigInt64Array?void 0:BigInt64Array,"%BigUint64Array%":"undefined"==typeof BigUint64Array?void 0:BigUint64Array,"%Boolean%":Boolean,"%DataView%":"undefined"==typeof DataView?void 0:DataView,"%Date%":Date,"%decodeURI%":decodeURI,"%decodeURIComponent%":decodeURIComponent,"%encodeURI%":encodeURI,"%encodeURIComponent%":encodeURIComponent,"%Error%":Error,"%eval%":eval,"%EvalError%":EvalError,"%Float32Array%":"undefined"==typeof Float32Array?void 0:Float32Array,"%Float64Array%":"undefined"==typeof Float64Array?void 0:Float64Array,"%FinalizationRegistry%":"undefined"==typeof FinalizationRegistry?void 0:FinalizationRegistry,"%Function%":a,"%GeneratorFunction%":m,"%Int8Array%":"undefined"==typeof Int8Array?void 0:Int8Array,"%Int16Array%":"undefined"==typeof Int16Array?void 0:Int16Array,"%Int32Array%":"undefined"==typeof Int32Array?void 0:Int32Array,"%isFinite%":isFinite,"%isNaN%":isNaN,"%IteratorPrototype%":p&&d?d(d([][Symbol.iterator]())):void 0,"%JSON%":"object"==typeof JSON?JSON:void 0,"%Map%":"undefined"==typeof Map?void 0:Map,"%MapIteratorPrototype%":"undefined"!=typeof Map&&p&&d?d((new Map)[Symbol.iterator]()):void 0,"%Math%":Math,"%Number%":Number,"%Object%":Object,"%parseFloat%":parseFloat,"%parseInt%":parseInt,"%Promise%":"undefined"==typeof Promise?void 0:Promise,"%Proxy%":"undefined"==typeof Proxy?void 0:Proxy,"%RangeError%":RangeError,"%ReferenceError%":ReferenceError,"%Reflect%":"undefined"==typeof Reflect?void 0:Reflect,"%RegExp%":RegExp,"%Set%":"undefined"==typeof Set?void 0:Set,"%SetIteratorPrototype%":"undefined"!=typeof Set&&p&&d?d((new Set)[Symbol.iterator]()):void 0,"%SharedArrayBuffer%":"undefined"==typeof SharedArrayBuffer?void 0:SharedArrayBuffer,"%String%":String,"%StringIteratorPrototype%":p&&d?d(""[Symbol.iterator]()):void 0,"%Symbol%":p?Symbol:void 0,"%SyntaxError%":r,"%ThrowTypeError%":c,"%TypedArray%":g,"%TypeError%":o,"%Uint8Array%":"undefined"==typeof Uint8Array?void 0:Uint8Array,"%Uint8ClampedArray%":"undefined"==typeof Uint8ClampedArray?void 0:Uint8ClampedArray,"%Uint16Array%":"undefined"==typeof Uint16Array?void 0:Uint16Array,"%Uint32Array%":"undefined"==typeof Uint32Array?void 0:Uint32Array,"%URIError%":URIError,"%WeakMap%":"undefined"==typeof WeakMap?void 0:WeakMap,"%WeakRef%":"undefined"==typeof WeakRef?void 0:WeakRef,"%WeakSet%":"undefined"==typeof WeakSet?void 0:WeakSet};if(d)try{null.error}catch(n){var h=d(d(n));f["%Error.prototype%"]=h}var v={"%ArrayBufferPrototype%":["ArrayBuffer","prototype"],"%ArrayPrototype%":["Array","prototype"],"%ArrayProto_entries%":["Array","prototype","entries"],"%ArrayProto_forEach%":["Array","prototype","forEach"],"%ArrayProto_keys%":["Array","prototype","keys"],"%ArrayProto_values%":["Array","prototype","values"],"%AsyncFunctionPrototype%":["AsyncFunction","prototype"],"%AsyncGenerator%":["AsyncGeneratorFunction","prototype"],"%AsyncGeneratorPrototype%":["AsyncGeneratorFunction","prototype","prototype"],"%BooleanPrototype%":["Boolean","prototype"],"%DataViewPrototype%":["DataView","prototype"],"%DatePrototype%":["Date","prototype"],"%ErrorPrototype%":["Error","prototype"],"%EvalErrorPrototype%":["EvalError","prototype"],"%Float32ArrayPrototype%":["Float32Array","prototype"],"%Float64ArrayPrototype%":["Float64Array","prototype"],"%FunctionPrototype%":["Function","prototype"],"%Generator%":["GeneratorFunction","prototype"],"%GeneratorPrototype%":["GeneratorFunction","prototype","prototype"],"%Int8ArrayPrototype%":["Int8Array","prototype"],"%Int16ArrayPrototype%":["Int16Array","prototype"],"%Int32ArrayPrototype%":["Int32Array","prototype"],"%JSONParse%":["JSON","parse"],"%JSONStringify%":["JSON","stringify"],"%MapPrototype%":["Map","prototype"],"%NumberPrototype%":["Number","prototype"],"%ObjectPrototype%":["Object","prototype"],"%ObjProto_toString%":["Object","prototype","toString"],"%ObjProto_valueOf%":["Object","prototype","valueOf"],"%PromisePrototype%":["Promise","prototype"],"%PromiseProto_then%":["Promise","prototype","then"],"%Promise_all%":["Promise","all"],"%Promise_reject%":["Promise","reject"],"%Promise_resolve%":["Promise","resolve"],"%RangeErrorPrototype%":["RangeError","prototype"],"%ReferenceErrorPrototype%":["ReferenceError","prototype"],"%RegExpPrototype%":["RegExp","prototype"],"%SetPrototype%":["Set","prototype"],"%SharedArrayBufferPrototype%":["SharedArrayBuffer","prototype"],"%StringPrototype%":["String","prototype"],"%SymbolPrototype%":["Symbol","prototype"],"%SyntaxErrorPrototype%":["SyntaxError","prototype"],"%TypedArrayPrototype%":["TypedArray","prototype"],"%TypeErrorPrototype%":["TypeError","prototype"],"%Uint8ArrayPrototype%":["Uint8Array","prototype"],"%Uint8ClampedArrayPrototype%":["Uint8ClampedArray","prototype"],"%Uint16ArrayPrototype%":["Uint16Array","prototype"],"%Uint32ArrayPrototype%":["Uint32Array","prototype"],"%URIErrorPrototype%":["URIError","prototype"],"%WeakMapPrototype%":["WeakMap","prototype"],"%WeakSetPrototype%":["WeakSet","prototype"]},b=t(62),k=t(294),y=b.call(Function.call,Array.prototype.concat),S=b.call(Function.apply,Array.prototype.splice),x=b.call(Function.call,String.prototype.replace),w=b.call(Function.call,String.prototype.slice),E=b.call(Function.call,RegExp.prototype.exec),D=/[^%.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|%$))/g,C=/\\(\\)?/g,I=function(n){var e=w(n,0,1),t=w(n,-1);if("%"===e&&"%"!==t)throw new r("invalid intrinsic syntax, expected closing `%`");if("%"===t&&"%"!==e)throw new r("invalid intrinsic syntax, expected opening `%`");var a=[];return x(n,D,(function(n,e,t,r){a[a.length]=t?x(r,C,"$1"):e||n})),a},T=function(n,e){var t,a=n;if(k(v,a)&&(a="%"+(t=v[a])[0]+"%"),k(f,a)){var s=f[a];if(s===m&&(s=function n(e){var t;if("%AsyncFunction%"===e)t=i("async function () {}");else if("%GeneratorFunction%"===e)t=i("function* () {}");else if("%AsyncGeneratorFunction%"===e)t=i("async function* () {}");else if("%AsyncGenerator%"===e){var r=n("%AsyncGeneratorFunction%");r&&(t=r.prototype)}else if("%AsyncIteratorPrototype%"===e){var a=n("%AsyncGenerator%");a&&d&&(t=d(a.prototype))}return f[e]=t,t}(a)),void 0===s&&!e)throw new o("intrinsic "+n+" exists, but is not available. Please file an issue!");return{alias:t,name:a,value:s}}throw new r("intrinsic "+n+" does not exist!")};n.exports=function(n,e){if("string"!=typeof n||0===n.length)throw new o("intrinsic name must be a non-empty string");if(arguments.length>1&&"boolean"!=typeof e)throw new o('"allowMissing" argument must be a boolean');if(null===E(/^%?[^%]*%?$/,n))throw new r("`%` may not be present anywhere but at the beginning and end of the intrinsic name");var t=I(n),a=t.length>0?t[0]:"",i=T("%"+a+"%",e),l=i.name,c=i.value,p=!1,u=i.alias;u&&(a=u[0],S(t,y([0,1],u)));for(var d=1,m=!0;d<t.length;d+=1){var g=t[d],h=w(g,0,1),v=w(g,-1);if(('"'===h||"'"===h||"`"===h||'"'===v||"'"===v||"`"===v)&&h!==v)throw new r("property names with quotes must have matching quotes");if("constructor"!==g&&m||(p=!0),k(f,l="%"+(a+="."+g)+"%"))c=f[l];else if(null!=c){if(!(g in c)){if(!e)throw new o("base intrinsic for "+n+" exists, but the property is not available.");return}if(s&&d+1>=t.length){var b=s(c,g);c=(m=!!b)&&"get"in b&&!("originalValue"in b.get)?b.get:c[g]}else m=k(c,g),c=c[g];m&&!p&&(f[l]=c)}}return c}},function(n,e,t){"use strict";var r=t(293);n.exports=Function.prototype.bind||r},function(n,e,t){"use strict";var r=String.prototype.replace,a=/%20/g,o="RFC1738",i="RFC3986";n.exports={default:i,formatters:{RFC1738:function(n){return r.call(n,a,"+")},RFC3986:function(n){return String(n)}},RFC1738:o,RFC3986:i}},function(n,e,t){var r=t(306);n.exports=function(n){return r(n,{weekStartsOn:1})}},function(n,e,t){"use strict";var r=t(5),a=t(318),o=t(119),i={"Content-Type":"application/x-www-form-urlencoded"};function s(n,e){!r.isUndefined(n)&&r.isUndefined(n["Content-Type"])&&(n["Content-Type"]=e)}var l,c={transitional:{silentJSONParsing:!0,forcedJSONParsing:!0,clarifyTimeoutError:!1},adapter:(("undefined"!=typeof XMLHttpRequest||"undefined"!=typeof process&&"[object process]"===Object.prototype.toString.call(process))&&(l=t(120)),l),transformRequest:[function(n,e){return a(e,"Accept"),a(e,"Content-Type"),r.isFormData(n)||r.isArrayBuffer(n)||r.isBuffer(n)||r.isStream(n)||r.isFile(n)||r.isBlob(n)?n:r.isArrayBufferView(n)?n.buffer:r.isURLSearchParams(n)?(s(e,"application/x-www-form-urlencoded;charset=utf-8"),n.toString()):r.isObject(n)||e&&"application/json"===e["Content-Type"]?(s(e,"application/json"),function(n,e,t){if(r.isString(n))try{return(e||JSON.parse)(n),r.trim(n)}catch(n){if("SyntaxError"!==n.name)throw n}return(t||JSON.stringify)(n)}(n)):n}],transformResponse:[function(n){var e=this.transitional,t=e&&e.silentJSONParsing,a=e&&e.forcedJSONParsing,i=!t&&"json"===this.responseType;if(i||a&&r.isString(n)&&n.length)try{return JSON.parse(n)}catch(n){if(i){if("SyntaxError"===n.name)throw o(n,this,"E_JSON_PARSE");throw n}}return n}],timeout:0,xsrfCookieName:"XSRF-TOKEN",xsrfHeaderName:"X-XSRF-TOKEN",maxContentLength:-1,maxBodyLength:-1,validateStatus:function(n){return n>=200&&n<300}};c.headers={common:{Accept:"application/json, text/plain, */*"}},r.forEach(["delete","get","head"],(function(n){c.headers[n]={}})),r.forEach(["post","put","patch"],(function(n){c.headers[n]=r.merge(i)})),n.exports=c},function(n,e){n.exports=function(n){return n.webpackPolyfill||(n.deprecate=function(){},n.paths=[],n.children||(n.children=[]),Object.defineProperty(n,"loaded",{enumerable:!0,get:function(){return n.l}}),Object.defineProperty(n,"id",{enumerable:!0,get:function(){return n.i}}),n.webpackPolyfill=1),n}},function(n,e,t){"use strict";var r=t(23),a=t(158).left,o=t(159),i=t(77);r({target:"Array",proto:!0,forced:!t(160)&&i>79&&i<83||!o("reduce")},{reduce:function(n){var e=arguments.length;return a(this,n,e,e>1?arguments[1]:void 0)}})},function(n,e,t){var r=t(9),a=t(43),o=t(138),i=t(45),s=t(30),l=t(71),c=t(13),p=t(81),u=Object.getOwnPropertyDescriptor;e.f=r?u:function(n,e){if(n=s(n),e=l(e),p)try{return u(n,e)}catch(n){}if(c(n,e))return i(!a(o.f,n,e),n[e])}},function(n,e,t){var r=t(7),a=t(6),o=t(31),i=Object,s=r("".split);n.exports=a((function(){return!i("z").propertyIsEnumerable(0)}))?function(n){return"String"==o(n)?s(n,""):i(n)}:i},function(n,e){n.exports=function(n){return null==n}},function(n,e,t){var r=t(139),a=t(73);n.exports=function(n){var e=r(n,"string");return a(e)?e:e+""}},function(n,e){var t="object"==typeof document&&document.all,r=void 0===t&&void 0!==t;n.exports={all:t,IS_HTMLDDA:r}},function(n,e,t){var r=t(32),a=t(4),o=t(74),i=t(75),s=Object;n.exports=i?function(n){return"symbol"==typeof n}:function(n){var e=r("Symbol");return a(e)&&o(e.prototype,s(n))}},function(n,e,t){var r=t(7);n.exports=r({}.isPrototypeOf)},function(n,e,t){var r=t(76);n.exports=r&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(n,e,t){var r=t(77),a=t(6),o=t(8).String;n.exports=!!Object.getOwnPropertySymbols&&!a((function(){var n=Symbol();return!o(n)||!(Object(n)instanceof Symbol)||!Symbol.sham&&r&&r<41}))},function(n,e,t){var r,a,o=t(8),i=t(140),s=o.process,l=o.Deno,c=s&&s.versions||l&&l.version,p=c&&c.v8;p&&(a=(r=p.split("."))[0]>0&&r[0]<4?1:+(r[0]+r[1])),!a&&i&&(!(r=i.match(/Edge\/(\d+)/))||r[1]>=74)&&(r=i.match(/Chrome\/(\d+)/))&&(a=+r[1]),n.exports=a},function(n,e,t){var r=t(79),a=t(48);(n.exports=function(n,e){return a[n]||(a[n]=void 0!==e?e:{})})("versions",[]).push({version:"3.31.0",mode:r?"pure":"global",copyright:"© 2014-2023 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.31.0/LICENSE",source:"https://github.com/zloirock/core-js"})},function(n,e){n.exports=!1},function(n,e,t){var r=t(7),a=0,o=Math.random(),i=r(1..toString);n.exports=function(n){return"Symbol("+(void 0===n?"":n)+")_"+i(++a+o,36)}},function(n,e,t){var r=t(9),a=t(6),o=t(82);n.exports=!r&&!a((function(){return 7!=Object.defineProperty(o("div"),"a",{get:function(){return 7}}).a}))},function(n,e,t){var r=t(8),a=t(14),o=r.document,i=a(o)&&a(o.createElement);n.exports=function(n){return i?o.createElement(n):{}}},function(n,e,t){var r=t(9),a=t(6);n.exports=r&&a((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(n,e,t){var r=t(78),a=t(80),o=r("keys");n.exports=function(n){return o[n]||(o[n]=a(n))}},function(n,e,t){var r=t(13),a=t(150),o=t(68),i=t(19);n.exports=function(n,e,t){for(var s=a(e),l=i.f,c=o.f,p=0;p<s.length;p++){var u=s[p];r(n,u)||t&&r(t,u)||l(n,u,c(e,u))}}},function(n,e,t){var r=t(7),a=t(13),o=t(30),i=t(152).indexOf,s=t(50),l=r([].push);n.exports=function(n,e){var t,r=o(n),c=0,p=[];for(t in r)!a(s,t)&&a(r,t)&&l(p,t);for(;e.length>c;)a(r,t=e[c++])&&(~i(p,t)||l(p,t));return p}},function(n,e,t){var r=t(166),a=t(25),o=t(167);n.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var n,e=!1,t={};try{(n=r(Object.prototype,"__proto__","set"))(t,[]),e=t instanceof Array}catch(n){}return function(t,r){return a(t),o(r),e?n(t,r):t.__proto__=r,t}}():void 0)},function(n,e,t){var r=t(171),a=String;n.exports=function(n){if("Symbol"===r(n))throw TypeError("Cannot convert a Symbol value to a string");return a(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,r=e.length,a=n.length;++t<r;)n[a+t]=e[t];return n}},function(n,e){var t="object"==typeof global&&global&&global.Object===Object&&global;n.exports=t},function(n,e,t){var r=t(36),a=t(190),o=t(191),i=t(192),s=t(193),l=t(194);function c(n){var e=this.__data__=new r(n);this.size=e.size}c.prototype.clear=a,c.prototype.delete=o,c.prototype.get=i,c.prototype.has=s,c.prototype.set=l,n.exports=c},function(n,e){n.exports=function(n,e){return n===e||n!=n&&e!=e}},function(n,e,t){var r=t(27),a=t(54);n.exports=function(n){if(!a(n))return!1;var e=r(n);return"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e}},function(n,e){var t=Function.prototype.toString;n.exports=function(n){if(null!=n){try{return t.call(n)}catch(n){}try{return n+""}catch(n){}}return""}},function(n,e,t){var r=t(211),a=t(20);n.exports=function n(e,t,o,i,s){return e===t||(null==e||null==t||!a(e)&&!a(t)?e!=e&&t!=t:r(e,t,o,i,n,s))}},function(n,e,t){var r=t(97),a=t(214),o=t(98);n.exports=function(n,e,t,i,s,l){var c=1&t,p=n.length,u=e.length;if(p!=u&&!(c&&u>p))return!1;var d=l.get(n),m=l.get(e);if(d&&m)return d==e&&m==n;var g=-1,f=!0,h=2&t?new r:void 0;for(l.set(n,e),l.set(e,n);++g<p;){var v=n[g],b=e[g];if(i)var k=c?i(b,v,g,e,n,l):i(v,b,g,n,e,l);if(void 0!==k){if(k)continue;f=!1;break}if(h){if(!a(e,(function(n,e){if(!o(h,e)&&(v===n||s(v,n,t,i,l)))return h.push(e)}))){f=!1;break}}else if(v!==b&&!s(v,b,t,i,l)){f=!1;break}}return l.delete(n),l.delete(e),f}},function(n,e,t){var r=t(55),a=t(212),o=t(213);function i(n){var e=-1,t=null==n?0:n.length;for(this.__data__=new r;++e<t;)this.add(n[e])}i.prototype.add=i.prototype.push=a,i.prototype.has=o,n.exports=i},function(n,e){n.exports=function(n,e){return n.has(e)}},function(n,e,t){var r=t(224),a=t(230),o=t(103);n.exports=function(n){return o(n)?r(n):a(n)}},function(n,e,t){(function(n){var r=t(11),a=t(226),o=e&&!e.nodeType&&e,i=o&&"object"==typeof n&&n&&!n.nodeType&&n,s=i&&i.exports===o?r.Buffer:void 0,l=(s?s.isBuffer:void 0)||a;n.exports=l}).call(this,t(66)(n))},function(n,e){var t=/^(?:0|[1-9]\d*)$/;n.exports=function(n,e){var r=typeof n;return!!(e=null==e?9007199254740991:e)&&("number"==r||"symbol"!=r&&t.test(n))&&n>-1&&n%1==0&&n<e}},function(n,e,t){var r=t(227),a=t(228),o=t(229),i=o&&o.isTypedArray,s=i?a(i):r;n.exports=s},function(n,e,t){var r=t(93),a=t(57);n.exports=function(n){return null!=n&&a(n.length)&&!r(n)}},function(n,e,t){var r=t(15)(t(11),"Set");n.exports=r},function(n,e,t){var r=t(54);n.exports=function(n){return n==n&&!r(n)}},function(n,e){n.exports=function(n,e){return function(t){return null!=t&&(t[n]===e&&(void 0!==e||n in Object(t)))}}},function(n,e,t){var r=t(108),a=t(40);n.exports=function(n,e){for(var t=0,o=(e=r(e,n)).length;null!=n&&t<o;)n=n[a(e[t++])];return t&&t==o?n:void 0}},function(n,e,t){var r=t(12),a=t(58),o=t(241),i=t(244);n.exports=function(n,e){return r(n)?n:a(n,e)?[n]:o(i(n))}},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){"use strict";var r=t(63),a=Object.prototype.hasOwnProperty,o=Array.isArray,i=function(){for(var n=[],e=0;e<256;++e)n.push("%"+((e<16?"0":"")+e.toString(16)).toUpperCase());return n}(),s=function(n,e){for(var t=e&&e.plainObjects?Object.create(null):{},r=0;r<n.length;++r)void 0!==n[r]&&(t[r]=n[r]);return t};n.exports={arrayToObject:s,assign:function(n,e){return Object.keys(e).reduce((function(n,t){return n[t]=e[t],n}),n)},combine:function(n,e){return[].concat(n,e)},compact:function(n){for(var e=[{obj:{o:n},prop:"o"}],t=[],r=0;r<e.length;++r)for(var a=e[r],i=a.obj[a.prop],s=Object.keys(i),l=0;l<s.length;++l){var c=s[l],p=i[c];"object"==typeof p&&null!==p&&-1===t.indexOf(p)&&(e.push({obj:i,prop:c}),t.push(p))}return function(n){for(;n.length>1;){var e=n.pop(),t=e.obj[e.prop];if(o(t)){for(var r=[],a=0;a<t.length;++a)void 0!==t[a]&&r.push(t[a]);e.obj[e.prop]=r}}}(e),n},decode:function(n,e,t){var r=n.replace(/\+/g," ");if("iso-8859-1"===t)return r.replace(/%[0-9a-f]{2}/gi,unescape);try{return decodeURIComponent(r)}catch(n){return r}},encode:function(n,e,t,a,o){if(0===n.length)return n;var s=n;if("symbol"==typeof n?s=Symbol.prototype.toString.call(n):"string"!=typeof n&&(s=String(n)),"iso-8859-1"===t)return escape(s).replace(/%u[0-9a-f]{4}/gi,(function(n){return"%26%23"+parseInt(n.slice(2),16)+"%3B"}));for(var l="",c=0;c<s.length;++c){var p=s.charCodeAt(c);45===p||46===p||95===p||126===p||p>=48&&p<=57||p>=65&&p<=90||p>=97&&p<=122||o===r.RFC1738&&(40===p||41===p)?l+=s.charAt(c):p<128?l+=i[p]:p<2048?l+=i[192|p>>6]+i[128|63&p]:p<55296||p>=57344?l+=i[224|p>>12]+i[128|p>>6&63]+i[128|63&p]:(c+=1,p=65536+((1023&p)<<10|1023&s.charCodeAt(c)),l+=i[240|p>>18]+i[128|p>>12&63]+i[128|p>>6&63]+i[128|63&p])}return l},isBuffer:function(n){return!(!n||"object"!=typeof n)&&!!(n.constructor&&n.constructor.isBuffer&&n.constructor.isBuffer(n))},isRegExp:function(n){return"[object RegExp]"===Object.prototype.toString.call(n)},maybeMap:function(n,e){if(o(n)){for(var t=[],r=0;r<n.length;r+=1)t.push(e(n[r]));return t}return e(n)},merge:function n(e,t,r){if(!t)return e;if("object"!=typeof t){if(o(e))e.push(t);else{if(!e||"object"!=typeof e)return[e,t];(r&&(r.plainObjects||r.allowPrototypes)||!a.call(Object.prototype,t))&&(e[t]=!0)}return e}if(!e||"object"!=typeof e)return[e].concat(t);var i=e;return o(e)&&!o(t)&&(i=s(e,r)),o(e)&&o(t)?(t.forEach((function(t,o){if(a.call(e,o)){var i=e[o];i&&"object"==typeof i&&t&&"object"==typeof t?e[o]=n(i,t,r):e.push(t)}else e[o]=t})),e):Object.keys(t).reduce((function(e,o){var i=t[o];return a.call(e,o)?e[o]=n(e[o],i,r):e[o]=i,e}),i)}}},function(n,e){n.exports=function(n){return n instanceof Date}},function(n,e,t){var r=t(16),a=t(64);n.exports=function(n){var e=r(n),t=e.getFullYear(),o=new Date(0);o.setFullYear(t+1,0,4),o.setHours(0,0,0,0);var i=a(o),s=new Date(0);s.setFullYear(t,0,4),s.setHours(0,0,0,0);var l=a(s);return e.getTime()>=i.getTime()?t+1:e.getTime()>=l.getTime()?t:t-1}},function(n,e,t){"use strict";n.exports=function(n,e){return function(){for(var t=new Array(arguments.length),r=0;r<t.length;r++)t[r]=arguments[r];return n.apply(e,t)}}},function(n,e,t){"use strict";var r=t(5);function a(n){return encodeURIComponent(n).replace(/%3A/gi,":").replace(/%24/g,"$").replace(/%2C/gi,",").replace(/%20/g,"+").replace(/%5B/gi,"[").replace(/%5D/gi,"]")}n.exports=function(n,e,t){if(!e)return n;var o;if(t)o=t(e);else if(r.isURLSearchParams(e))o=e.toString();else{var i=[];r.forEach(e,(function(n,e){null!=n&&(r.isArray(n)?e+="[]":n=[n],r.forEach(n,(function(n){r.isDate(n)?n=n.toISOString():r.isObject(n)&&(n=JSON.stringify(n)),i.push(a(e)+"="+a(n))})))})),o=i.join("&")}if(o){var s=n.indexOf("#");-1!==s&&(n=n.slice(0,s)),n+=(-1===n.indexOf("?")?"?":"&")+o}return n}},function(n,e,t){"use strict";n.exports=function(n,e,t,r,a){return n.config=e,t&&(n.code=t),n.request=r,n.response=a,n.isAxiosError=!0,n.toJSON=function(){return{message:this.message,name:this.name,description:this.description,number:this.number,fileName:this.fileName,lineNumber:this.lineNumber,columnNumber:this.columnNumber,stack:this.stack,config:this.config,code:this.code}},n}},function(n,e,t){"use strict";var r=t(5),a=t(319),o=t(320),i=t(118),s=t(321),l=t(324),c=t(325),p=t(121);n.exports=function(n){return new Promise((function(e,t){var u=n.data,d=n.headers,m=n.responseType;r.isFormData(u)&&delete d["Content-Type"];var g=new XMLHttpRequest;if(n.auth){var f=n.auth.username||"",h=n.auth.password?unescape(encodeURIComponent(n.auth.password)):"";d.Authorization="Basic "+btoa(f+":"+h)}var v=s(n.baseURL,n.url);function b(){if(g){var r="getAllResponseHeaders"in g?l(g.getAllResponseHeaders()):null,o={data:m&&"text"!==m&&"json"!==m?g.response:g.responseText,status:g.status,statusText:g.statusText,headers:r,config:n,request:g};a(e,t,o),g=null}}if(g.open(n.method.toUpperCase(),i(v,n.params,n.paramsSerializer),!0),g.timeout=n.timeout,"onloadend"in g?g.onloadend=b:g.onreadystatechange=function(){g&&4===g.readyState&&(0!==g.status||g.responseURL&&0===g.responseURL.indexOf("file:"))&&setTimeout(b)},g.onabort=function(){g&&(t(p("Request aborted",n,"ECONNABORTED",g)),g=null)},g.onerror=function(){t(p("Network Error",n,null,g)),g=null},g.ontimeout=function(){var e="timeout of "+n.timeout+"ms exceeded";n.timeoutErrorMessage&&(e=n.timeoutErrorMessage),t(p(e,n,n.transitional&&n.transitional.clarifyTimeoutError?"ETIMEDOUT":"ECONNABORTED",g)),g=null},r.isStandardBrowserEnv()){var k=(n.withCredentials||c(v))&&n.xsrfCookieName?o.read(n.xsrfCookieName):void 0;k&&(d[n.xsrfHeaderName]=k)}"setRequestHeader"in g&&r.forEach(d,(function(n,e){void 0===u&&"content-type"===e.toLowerCase()?delete d[e]:g.setRequestHeader(e,n)})),r.isUndefined(n.withCredentials)||(g.withCredentials=!!n.withCredentials),m&&"json"!==m&&(g.responseType=n.responseType),"function"==typeof n.onDownloadProgress&&g.addEventListener("progress",n.onDownloadProgress),"function"==typeof n.onUploadProgress&&g.upload&&g.upload.addEventListener("progress",n.onUploadProgress),n.cancelToken&&n.cancelToken.promise.then((function(n){g&&(g.abort(),t(n),g=null)})),u||(u=null),g.send(u)}))}},function(n,e,t){"use strict";var r=t(119);n.exports=function(n,e,t,a,o){var i=new Error(n);return r(i,e,t,a,o)}},function(n,e,t){"use strict";n.exports=function(n){return!(!n||!n.__CANCEL__)}},function(n,e,t){"use strict";var r=t(5);n.exports=function(n,e){e=e||{};var t={},a=["url","method","data"],o=["headers","auth","proxy","params"],i=["baseURL","transformRequest","transformResponse","paramsSerializer","timeout","timeoutMessage","withCredentials","adapter","responseType","xsrfCookieName","xsrfHeaderName","onUploadProgress","onDownloadProgress","decompress","maxContentLength","maxBodyLength","maxRedirects","transport","httpAgent","httpsAgent","cancelToken","socketPath","responseEncoding"],s=["validateStatus"];function l(n,e){return r.isPlainObject(n)&&r.isPlainObject(e)?r.merge(n,e):r.isPlainObject(e)?r.merge({},e):r.isArray(e)?e.slice():e}function c(a){r.isUndefined(e[a])?r.isUndefined(n[a])||(t[a]=l(void 0,n[a])):t[a]=l(n[a],e[a])}r.forEach(a,(function(n){r.isUndefined(e[n])||(t[n]=l(void 0,e[n]))})),r.forEach(o,c),r.forEach(i,(function(a){r.isUndefined(e[a])?r.isUndefined(n[a])||(t[a]=l(void 0,n[a])):t[a]=l(void 0,e[a])})),r.forEach(s,(function(r){r in e?t[r]=l(n[r],e[r]):r in n&&(t[r]=l(void 0,n[r]))}));var p=a.concat(o).concat(i).concat(s),u=Object.keys(n).concat(Object.keys(e)).filter((function(n){return-1===p.indexOf(n)}));return r.forEach(u,c),t}},function(n,e,t){"use strict";function r(n){this.message=n}r.prototype.toString=function(){return"Cancel"+(this.message?": "+this.message:"")},r.prototype.__CANCEL__=!0,n.exports=r},function(n,e,t){},function(n,e,t){n.exports=function(){"use strict";var n=6e4,e=36e5,t="millisecond",r="second",a="minute",o="hour",i="day",s="week",l="month",c="quarter",p="year",u="date",d="Invalid Date",m=/^(\d{4})[-/]?(\d{1,2})?[-/]?(\d{0,2})[Tt\s]*(\d{1,2})?:?(\d{1,2})?:?(\d{1,2})?[.:]?(\d+)?$/,g=/\[([^\]]+)]|Y{1,4}|M{1,4}|D{1,2}|d{1,4}|H{1,2}|h{1,2}|a|A|m{1,2}|s{1,2}|Z{1,2}|SSS/g,f={name:"en",weekdays:"Sunday_Monday_Tuesday_Wednesday_Thursday_Friday_Saturday".split("_"),months:"January_February_March_April_May_June_July_August_September_October_November_December".split("_"),ordinal:function(n){var e=["th","st","nd","rd"],t=n%100;return"["+n+(e[(t-20)%10]||e[t]||e[0])+"]"}},h=function(n,e,t){var r=String(n);return!r||r.length>=e?n:""+Array(e+1-r.length).join(t)+n},v={s:h,z:function(n){var e=-n.utcOffset(),t=Math.abs(e),r=Math.floor(t/60),a=t%60;return(e<=0?"+":"-")+h(r,2,"0")+":"+h(a,2,"0")},m:function n(e,t){if(e.date()<t.date())return-n(t,e);var r=12*(t.year()-e.year())+(t.month()-e.month()),a=e.clone().add(r,l),o=t-a<0,i=e.clone().add(r+(o?-1:1),l);return+(-(r+(t-a)/(o?a-i:i-a))||0)},a:function(n){return n<0?Math.ceil(n)||0:Math.floor(n)},p:function(n){return{M:l,y:p,w:s,d:i,D:u,h:o,m:a,s:r,ms:t,Q:c}[n]||String(n||"").toLowerCase().replace(/s$/,"")},u:function(n){return void 0===n}},b="en",k={};k[b]=f;var y=function(n){return n instanceof E},S=function n(e,t,r){var a;if(!e)return b;if("string"==typeof e){var o=e.toLowerCase();k[o]&&(a=o),t&&(k[o]=t,a=o);var i=e.split("-");if(!a&&i.length>1)return n(i[0])}else{var s=e.name;k[s]=e,a=s}return!r&&a&&(b=a),a||!r&&b},x=function(n,e){if(y(n))return n.clone();var t="object"==typeof e?e:{};return t.date=n,t.args=arguments,new E(t)},w=v;w.l=S,w.i=y,w.w=function(n,e){return x(n,{locale:e.$L,utc:e.$u,x:e.$x,$offset:e.$offset})};var E=function(){function f(n){this.$L=S(n.locale,null,!0),this.parse(n)}var h=f.prototype;return h.parse=function(n){this.$d=function(n){var e=n.date,t=n.utc;if(null===e)return new Date(NaN);if(w.u(e))return new Date;if(e instanceof Date)return new Date(e);if("string"==typeof e&&!/Z$/i.test(e)){var r=e.match(m);if(r){var a=r[2]-1||0,o=(r[7]||"0").substring(0,3);return t?new Date(Date.UTC(r[1],a,r[3]||1,r[4]||0,r[5]||0,r[6]||0,o)):new Date(r[1],a,r[3]||1,r[4]||0,r[5]||0,r[6]||0,o)}}return new Date(e)}(n),this.$x=n.x||{},this.init()},h.init=function(){var n=this.$d;this.$y=n.getFullYear(),this.$M=n.getMonth(),this.$D=n.getDate(),this.$W=n.getDay(),this.$H=n.getHours(),this.$m=n.getMinutes(),this.$s=n.getSeconds(),this.$ms=n.getMilliseconds()},h.$utils=function(){return w},h.isValid=function(){return!(this.$d.toString()===d)},h.isSame=function(n,e){var t=x(n);return this.startOf(e)<=t&&t<=this.endOf(e)},h.isAfter=function(n,e){return x(n)<this.startOf(e)},h.isBefore=function(n,e){return this.endOf(e)<x(n)},h.$g=function(n,e,t){return w.u(n)?this[e]:this.set(t,n)},h.unix=function(){return Math.floor(this.valueOf()/1e3)},h.valueOf=function(){return this.$d.getTime()},h.startOf=function(n,e){var t=this,c=!!w.u(e)||e,d=w.p(n),m=function(n,e){var r=w.w(t.$u?Date.UTC(t.$y,e,n):new Date(t.$y,e,n),t);return c?r:r.endOf(i)},g=function(n,e){return w.w(t.toDate()[n].apply(t.toDate("s"),(c?[0,0,0,0]:[23,59,59,999]).slice(e)),t)},f=this.$W,h=this.$M,v=this.$D,b="set"+(this.$u?"UTC":"");switch(d){case p:return c?m(1,0):m(31,11);case l:return c?m(1,h):m(0,h+1);case s:var k=this.$locale().weekStart||0,y=(f<k?f+7:f)-k;return m(c?v-y:v+(6-y),h);case i:case u:return g(b+"Hours",0);case o:return g(b+"Minutes",1);case a:return g(b+"Seconds",2);case r:return g(b+"Milliseconds",3);default:return this.clone()}},h.endOf=function(n){return this.startOf(n,!1)},h.$set=function(n,e){var s,c=w.p(n),d="set"+(this.$u?"UTC":""),m=(s={},s[i]=d+"Date",s[u]=d+"Date",s[l]=d+"Month",s[p]=d+"FullYear",s[o]=d+"Hours",s[a]=d+"Minutes",s[r]=d+"Seconds",s[t]=d+"Milliseconds",s)[c],g=c===i?this.$D+(e-this.$W):e;if(c===l||c===p){var f=this.clone().set(u,1);f.$d[m](g),f.init(),this.$d=f.set(u,Math.min(this.$D,f.daysInMonth())).$d}else m&&this.$d[m](g);return this.init(),this},h.set=function(n,e){return this.clone().$set(n,e)},h.get=function(n){return this[w.p(n)]()},h.add=function(t,c){var u,d=this;t=Number(t);var m=w.p(c),g=function(n){var e=x(d);return w.w(e.date(e.date()+Math.round(n*t)),d)};if(m===l)return this.set(l,this.$M+t);if(m===p)return this.set(p,this.$y+t);if(m===i)return g(1);if(m===s)return g(7);var f=(u={},u[a]=n,u[o]=e,u[r]=1e3,u)[m]||1,h=this.$d.getTime()+t*f;return w.w(h,this)},h.subtract=function(n,e){return this.add(-1*n,e)},h.format=function(n){var e=this,t=this.$locale();if(!this.isValid())return t.invalidDate||d;var r=n||"YYYY-MM-DDTHH:mm:ssZ",a=w.z(this),o=this.$H,i=this.$m,s=this.$M,l=t.weekdays,c=t.months,p=function(n,t,a,o){return n&&(n[t]||n(e,r))||a[t].slice(0,o)},u=function(n){return w.s(o%12||12,n,"0")},m=t.meridiem||function(n,e,t){var r=n<12?"AM":"PM";return t?r.toLowerCase():r},f={YY:String(this.$y).slice(-2),YYYY:w.s(this.$y,4,"0"),M:s+1,MM:w.s(s+1,2,"0"),MMM:p(t.monthsShort,s,c,3),MMMM:p(c,s),D:this.$D,DD:w.s(this.$D,2,"0"),d:String(this.$W),dd:p(t.weekdaysMin,this.$W,l,2),ddd:p(t.weekdaysShort,this.$W,l,3),dddd:l[this.$W],H:String(o),HH:w.s(o,2,"0"),h:u(1),hh:u(2),a:m(o,i,!0),A:m(o,i,!1),m:String(i),mm:w.s(i,2,"0"),s:String(this.$s),ss:w.s(this.$s,2,"0"),SSS:w.s(this.$ms,3,"0"),Z:a};return r.replace(g,(function(n,e){return e||f[n]||a.replace(":","")}))},h.utcOffset=function(){return 15*-Math.round(this.$d.getTimezoneOffset()/15)},h.diff=function(t,u,d){var m,g=w.p(u),f=x(t),h=(f.utcOffset()-this.utcOffset())*n,v=this-f,b=w.m(this,f);return b=(m={},m[p]=b/12,m[l]=b,m[c]=b/3,m[s]=(v-h)/6048e5,m[i]=(v-h)/864e5,m[o]=v/e,m[a]=v/n,m[r]=v/1e3,m)[g]||v,d?b:w.a(b)},h.daysInMonth=function(){return this.endOf(l).$D},h.$locale=function(){return k[this.$L]},h.locale=function(n,e){if(!n)return this.$L;var t=this.clone(),r=S(n,e,!0);return r&&(t.$L=r),t},h.clone=function(){return w.w(this.$d,this)},h.toDate=function(){return new Date(this.valueOf())},h.toJSON=function(){return this.isValid()?this.toISOString():null},h.toISOString=function(){return this.$d.toISOString()},h.toString=function(){return this.$d.toUTCString()},f}(),D=E.prototype;return x.prototype=D,[["$ms",t],["$s",r],["$m",a],["$H",o],["$W",i],["$M",l],["$y",p],["$D",u]].forEach((function(n){D[n[1]]=function(e){return this.$g(e,n[0],n[1])}})),x.extend=function(n,e){return n.$i||(n(e,E,x),n.$i=!0),x},x.locale=S,x.isDayjs=y,x.unix=function(n){return x(1e3*n)},x.en=k[b],x.Ls=k,x.p={},x}()},function(n,e,t){},function(n,e,t){},function(n,e,t){var r=t(177),a=t(182),o=t(253),i=t(261),s=t(270),l=t(271),c=o((function(n){var e=l(n);return s(e)&&(e=void 0),i(r(n,1,s,!0),a(e,2))}));n.exports=c},function(n,e,t){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var r=/["'&<>]/;n.exports=function(n){var e,t=""+n,a=r.exec(t);if(!a)return t;var o="",i=0,s=0;for(i=a.index;i<t.length;i++){switch(t.charCodeAt(i)){case 34:e="&quot;";break;case 38:e="&amp;";break;case 39:e="&#39;";break;case 60:e="&lt;";break;case 62:e="&gt;";break;default:continue}s!==i&&(o+=t.substring(s,i)),s=i+1,o+=e}return s!==i?o+t.substring(s,i):o}},function(n,e){var t=/^\s+|\s+$/g,r=/^[-+]0x[0-9a-f]+$/i,a=/^0b[01]+$/i,o=/^0o[0-7]+$/i,i=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,c=s||l||Function("return this")(),p=Object.prototype.toString,u=Math.max,d=Math.min,m=function(){return c.Date.now()};function g(n){var e=typeof n;return!!n&&("object"==e||"function"==e)}function f(n){if("number"==typeof n)return n;if(function(n){return"symbol"==typeof n||function(n){return!!n&&"object"==typeof n}(n)&&"[object Symbol]"==p.call(n)}(n))return NaN;if(g(n)){var e="function"==typeof n.valueOf?n.valueOf():n;n=g(e)?e+"":e}if("string"!=typeof n)return 0===n?n:+n;n=n.replace(t,"");var s=a.test(n);return s||o.test(n)?i(n.slice(2),s?2:8):r.test(n)?NaN:+n}n.exports=function(n,e,t){var r,a,o,i,s,l,c=0,p=!1,h=!1,v=!0;if("function"!=typeof n)throw new TypeError("Expected a function");function b(e){var t=r,o=a;return r=a=void 0,c=e,i=n.apply(o,t)}function k(n){return c=n,s=setTimeout(S,e),p?b(n):i}function y(n){var t=n-l;return void 0===l||t>=e||t<0||h&&n-c>=o}function S(){var n=m();if(y(n))return x(n);s=setTimeout(S,function(n){var t=e-(n-l);return h?d(t,o-(n-c)):t}(n))}function x(n){return s=void 0,v&&r?b(n):(r=a=void 0,i)}function w(){var n=m(),t=y(n);if(r=arguments,a=this,l=n,t){if(void 0===s)return k(l);if(h)return s=setTimeout(S,e),b(l)}return void 0===s&&(s=setTimeout(S,e)),i}return e=f(e)||0,g(t)&&(p=!!t.leading,o=(h="maxWait"in t)?u(f(t.maxWait)||0,e):o,v="trailing"in t?!!t.trailing:v),w.cancel=function(){void 0!==s&&clearTimeout(s),c=0,r=l=a=s=void 0},w.flush=function(){return void 0===s?i:x(m())},w}},function(n,e,t){!function(){"use strict";n.exports={polyfill:function(){var n=window,e=document;if(!("scrollBehavior"in e.documentElement.style)||!0===n.__forceSmoothScrollPolyfill__){var t,r=n.HTMLElement||n.Element,a={scroll:n.scroll||n.scrollTo,scrollBy:n.scrollBy,elementScroll:r.prototype.scroll||s,scrollIntoView:r.prototype.scrollIntoView},o=n.performance&&n.performance.now?n.performance.now.bind(n.performance):Date.now,i=(t=n.navigator.userAgent,new RegExp(["MSIE ","Trident/","Edge/"].join("|")).test(t)?1:0);n.scroll=n.scrollTo=function(){void 0!==arguments[0]&&(!0!==l(arguments[0])?g.call(n,e.body,void 0!==arguments[0].left?~~arguments[0].left:n.scrollX||n.pageXOffset,void 0!==arguments[0].top?~~arguments[0].top:n.scrollY||n.pageYOffset):a.scroll.call(n,void 0!==arguments[0].left?arguments[0].left:"object"!=typeof arguments[0]?arguments[0]:n.scrollX||n.pageXOffset,void 0!==arguments[0].top?arguments[0].top:void 0!==arguments[1]?arguments[1]:n.scrollY||n.pageYOffset))},n.scrollBy=function(){void 0!==arguments[0]&&(l(arguments[0])?a.scrollBy.call(n,void 0!==arguments[0].left?arguments[0].left:"object"!=typeof arguments[0]?arguments[0]:0,void 0!==arguments[0].top?arguments[0].top:void 0!==arguments[1]?arguments[1]:0):g.call(n,e.body,~~arguments[0].left+(n.scrollX||n.pageXOffset),~~arguments[0].top+(n.scrollY||n.pageYOffset)))},r.prototype.scroll=r.prototype.scrollTo=function(){if(void 0!==arguments[0])if(!0!==l(arguments[0])){var n=arguments[0].left,e=arguments[0].top;g.call(this,this,void 0===n?this.scrollLeft:~~n,void 0===e?this.scrollTop:~~e)}else{if("number"==typeof arguments[0]&&void 0===arguments[1])throw new SyntaxError("Value could not be converted");a.elementScroll.call(this,void 0!==arguments[0].left?~~arguments[0].left:"object"!=typeof arguments[0]?~~arguments[0]:this.scrollLeft,void 0!==arguments[0].top?~~arguments[0].top:void 0!==arguments[1]?~~arguments[1]:this.scrollTop)}},r.prototype.scrollBy=function(){void 0!==arguments[0]&&(!0!==l(arguments[0])?this.scroll({left:~~arguments[0].left+this.scrollLeft,top:~~arguments[0].top+this.scrollTop,behavior:arguments[0].behavior}):a.elementScroll.call(this,void 0!==arguments[0].left?~~arguments[0].left+this.scrollLeft:~~arguments[0]+this.scrollLeft,void 0!==arguments[0].top?~~arguments[0].top+this.scrollTop:~~arguments[1]+this.scrollTop))},r.prototype.scrollIntoView=function(){if(!0!==l(arguments[0])){var t=d(this),r=t.getBoundingClientRect(),o=this.getBoundingClientRect();t!==e.body?(g.call(this,t,t.scrollLeft+o.left-r.left,t.scrollTop+o.top-r.top),"fixed"!==n.getComputedStyle(t).position&&n.scrollBy({left:r.left,top:r.top,behavior:"smooth"})):n.scrollBy({left:o.left,top:o.top,behavior:"smooth"})}else a.scrollIntoView.call(this,void 0===arguments[0]||arguments[0])}}function s(n,e){this.scrollLeft=n,this.scrollTop=e}function l(n){if(null===n||"object"!=typeof n||void 0===n.behavior||"auto"===n.behavior||"instant"===n.behavior)return!0;if("object"==typeof n&&"smooth"===n.behavior)return!1;throw new TypeError("behavior member of ScrollOptions "+n.behavior+" is not a valid value for enumeration ScrollBehavior.")}function c(n,e){return"Y"===e?n.clientHeight+i<n.scrollHeight:"X"===e?n.clientWidth+i<n.scrollWidth:void 0}function p(e,t){var r=n.getComputedStyle(e,null)["overflow"+t];return"auto"===r||"scroll"===r}function u(n){var e=c(n,"Y")&&p(n,"Y"),t=c(n,"X")&&p(n,"X");return e||t}function d(n){for(;n!==e.body&&!1===u(n);)n=n.parentNode||n.host;return n}function m(e){var t,r,a,i,s=(o()-e.startTime)/468;i=s=s>1?1:s,t=.5*(1-Math.cos(Math.PI*i)),r=e.startX+(e.x-e.startX)*t,a=e.startY+(e.y-e.startY)*t,e.method.call(e.scrollable,r,a),r===e.x&&a===e.y||n.requestAnimationFrame(m.bind(n,e))}function g(t,r,i){var l,c,p,u,d=o();t===e.body?(l=n,c=n.scrollX||n.pageXOffset,p=n.scrollY||n.pageYOffset,u=a.scroll):(l=t,c=t.scrollLeft,p=t.scrollTop,u=s),m({scrollable:l,method:u,startTime:d,startX:c,startY:p,x:r,y:i})}}}}()},function(n){n.exports=JSON.parse('{"en-US":{"author":"author","beforeAuthor":"Copyright © ","afterAuthor":"\\nLink: "},"zh-CN":{"author":"作者","beforeAuthor":"著作权归","afterAuthor":"所有。\\n链接："}}')},function(n,e,t){var r=t(300),a=t(305),o=t(116),i=t(16),s=t(308),l=t(309);var c={M:function(n){return n.getMonth()+1},MM:function(n){return d(n.getMonth()+1,2)},Q:function(n){return Math.ceil((n.getMonth()+1)/3)},D:function(n){return n.getDate()},DD:function(n){return d(n.getDate(),2)},DDD:function(n){return r(n)},DDDD:function(n){return d(r(n),3)},d:function(n){return n.getDay()},E:function(n){return n.getDay()||7},W:function(n){return a(n)},WW:function(n){return d(a(n),2)},YY:function(n){return d(n.getFullYear(),4).substr(2)},YYYY:function(n){return d(n.getFullYear(),4)},GG:function(n){return String(o(n)).substr(2)},GGGG:function(n){return o(n)},H:function(n){return n.getHours()},HH:function(n){return d(n.getHours(),2)},h:function(n){var e=n.getHours();return 0===e?12:e>12?e%12:e},hh:function(n){return d(c.h(n),2)},m:function(n){return n.getMinutes()},mm:function(n){return d(n.getMinutes(),2)},s:function(n){return n.getSeconds()},ss:function(n){return d(n.getSeconds(),2)},S:function(n){return Math.floor(n.getMilliseconds()/100)},SS:function(n){return d(Math.floor(n.getMilliseconds()/10),2)},SSS:function(n){return d(n.getMilliseconds(),3)},Z:function(n){return u(n.getTimezoneOffset(),":")},ZZ:function(n){return u(n.getTimezoneOffset())},X:function(n){return Math.floor(n.getTime()/1e3)},x:function(n){return n.getTime()}};function p(n){return n.match(/\[[\s\S]/)?n.replace(/^\[|]$/g,""):n.replace(/\\/g,"")}function u(n,e){e=e||"";var t=n>0?"-":"+",r=Math.abs(n),a=r%60;return t+d(Math.floor(r/60),2)+e+d(a,2)}function d(n,e){for(var t=Math.abs(n).toString();t.length<e;)t="0"+t;return t}n.exports=function(n,e,t){var r=e?String(e):"YYYY-MM-DDTHH:mm:ss.SSSZ",a=(t||{}).locale,o=l.format.formatters,u=l.format.formattingTokensRegExp;a&&a.format&&a.format.formatters&&(o=a.format.formatters,a.format.formattingTokensRegExp&&(u=a.format.formattingTokensRegExp));var d=i(n);return s(d)?function(n,e,t){var r,a,o=n.match(t),i=o.length;for(r=0;r<i;r++)a=e[o[r]]||c[o[r]],o[r]=a||p(o[r]);return function(n){for(var e="",t=0;t<i;t++)o[t]instanceof Function?e+=o[t](n,c):e+=o[t];return e}}(r,o,u)(d):"Invalid Date"}},function(n,e,t){function r(){var n;try{n=e.storage.debug}catch(n){}return!n&&"undefined"!=typeof process&&"env"in process&&(n=process.env.DEBUG),n}(e=n.exports=t(334)).log=function(){return"object"==typeof console&&console.log&&Function.prototype.apply.call(console.log,console,arguments)},e.formatArgs=function(n){var t=this.useColors;if(n[0]=(t?"%c":"")+this.namespace+(t?" %c":" ")+n[0]+(t?"%c ":" ")+"+"+e.humanize(this.diff),!t)return;var r="color: "+this.color;n.splice(1,0,r,"color: inherit");var a=0,o=0;n[0].replace(/%[a-zA-Z%]/g,(function(n){"%%"!==n&&(a++,"%c"===n&&(o=a))})),n.splice(o,0,r)},e.save=function(n){try{null==n?e.storage.removeItem("debug"):e.storage.debug=n}catch(n){}},e.load=r,e.useColors=function(){if("undefined"!=typeof window&&window.process&&"renderer"===window.process.type)return!0;return"undefined"!=typeof document&&document.documentElement&&document.documentElement.style&&document.documentElement.style.WebkitAppearance||"undefined"!=typeof window&&window.console&&(window.console.firebug||window.console.exception&&window.console.table)||"undefined"!=typeof navigator&&navigator.userAgent&&navigator.userAgent.toLowerCase().match(/firefox\/(\d+)/)&&parseInt(RegExp.$1,10)>=31||"undefined"!=typeof navigator&&navigator.userAgent&&navigator.userAgent.toLowerCase().match(/applewebkit\/(\d+)/)},e.storage="undefined"!=typeof chrome&&void 0!==chrome.storage?chrome.storage.local:function(){try{return window.localStorage}catch(n){}}(),e.colors=["lightseagreen","forestgreen","goldenrod","dodgerblue","darkorchid","crimson"],e.formatters.j=function(n){try{return JSON.stringify(n)}catch(n){return"[UnexpectedJSONParseError]: "+n.message}},e.enable(r())},function(n,e,t){var r=t(23),a=t(8),o=t(164),i=t(165),s=a.WebAssembly,l=7!==Error("e",{cause:7}).cause,c=function(n,e){var t={};t[n]=i(n,e,l),r({global:!0,constructor:!0,arity:1,forced:l},t)},p=function(n,e){if(s&&s[n]){var t={};t[n]=i("WebAssembly."+n,e,l),r({target:"WebAssembly",stat:!0,constructor:!0,arity:1,forced:l},t)}};c("Error",(function(n){return function(e){return o(n,this,arguments)}})),c("EvalError",(function(n){return function(e){return o(n,this,arguments)}})),c("RangeError",(function(n){return function(e){return o(n,this,arguments)}})),c("ReferenceError",(function(n){return function(e){return o(n,this,arguments)}})),c("SyntaxError",(function(n){return function(e){return o(n,this,arguments)}})),c("TypeError",(function(n){return function(e){return o(n,this,arguments)}})),c("URIError",(function(n){return function(e){return o(n,this,arguments)}})),p("CompileError",(function(n){return function(e){return o(n,this,arguments)}})),p("LinkError",(function(n){return function(e){return o(n,this,arguments)}})),p("RuntimeError",(function(n){return function(e){return o(n,this,arguments)}}))},function(n,e,t){n.exports=t(339)},function(n,e,t){"use strict";var r={}.propertyIsEnumerable,a=Object.getOwnPropertyDescriptor,o=a&&!r.call({1:2},1);e.f=o?function(n){var e=a(this,n);return!!e&&e.enumerable}:r},function(n,e,t){var r=t(43),a=t(14),o=t(73),i=t(141),s=t(143),l=t(29),c=TypeError,p=l("toPrimitive");n.exports=function(n,e){if(!a(n)||o(n))return n;var t,l=i(n,p);if(l){if(void 0===e&&(e="default"),t=r(l,n,e),!a(t)||o(t))return t;throw c("Can't convert object to primitive value")}return void 0===e&&(e="number"),s(n,e)}},function(n,e){n.exports="undefined"!=typeof navigator&&String(navigator.userAgent)||""},function(n,e,t){var r=t(47),a=t(70);n.exports=function(n,e){var t=n[e];return a(t)?void 0:r(t)}},function(n,e){var t=String;n.exports=function(n){try{return t(n)}catch(n){return"Object"}}},function(n,e,t){var r=t(43),a=t(4),o=t(14),i=TypeError;n.exports=function(n,e){var t,s;if("string"===e&&a(t=n.toString)&&!o(s=r(t,n)))return s;if(a(t=n.valueOf)&&!o(s=r(t,n)))return s;if("string"!==e&&a(t=n.toString)&&!o(s=r(t,n)))return s;throw i("Can't convert object to primitive value")}},function(n,e,t){var r=t(4),a=t(19),o=t(145),i=t(49);n.exports=function(n,e,t,s){s||(s={});var l=s.enumerable,c=void 0!==s.name?s.name:e;if(r(t)&&o(t,c,s),s.global)l?n[e]=t:i(e,t);else{try{s.unsafe?n[e]&&(l=!0):delete n[e]}catch(n){}l?n[e]=t:a.f(n,e,{value:t,enumerable:!1,configurable:!s.nonConfigurable,writable:!s.nonWritable})}return n}},function(n,e,t){var r=t(7),a=t(6),o=t(4),i=t(13),s=t(9),l=t(146).CONFIGURABLE,c=t(147),p=t(148),u=p.enforce,d=p.get,m=String,g=Object.defineProperty,f=r("".slice),h=r("".replace),v=r([].join),b=s&&!a((function(){return 8!==g((function(){}),"length",{value:8}).length})),k=String(String).split("String"),y=n.exports=function(n,e,t){"Symbol("===f(m(e),0,7)&&(e="["+h(m(e),/^Symbol\(([^)]*)\)/,"$1")+"]"),t&&t.getter&&(e="get "+e),t&&t.setter&&(e="set "+e),(!i(n,"name")||l&&n.name!==e)&&(s?g(n,"name",{value:e,configurable:!0}):n.name=e),b&&t&&i(t,"arity")&&n.length!==t.arity&&g(n,"length",{value:t.arity});try{t&&i(t,"constructor")&&t.constructor?s&&g(n,"prototype",{writable:!1}):n.prototype&&(n.prototype=void 0)}catch(n){}var r=u(n);return i(r,"source")||(r.source=v(k,"string"==typeof e?e:"")),n};Function.prototype.toString=y((function(){return o(this)&&d(this).source||c(this)}),"toString")},function(n,e,t){var r=t(9),a=t(13),o=Function.prototype,i=r&&Object.getOwnPropertyDescriptor,s=a(o,"name"),l=s&&"something"===function(){}.name,c=s&&(!r||r&&i(o,"name").configurable);n.exports={EXISTS:s,PROPER:l,CONFIGURABLE:c}},function(n,e,t){var r=t(7),a=t(4),o=t(48),i=r(Function.toString);a(o.inspectSource)||(o.inspectSource=function(n){return i(n)}),n.exports=o.inspectSource},function(n,e,t){var r,a,o,i=t(149),s=t(8),l=t(14),c=t(24),p=t(13),u=t(48),d=t(84),m=t(50),g=s.TypeError,f=s.WeakMap;if(i||u.state){var h=u.state||(u.state=new f);h.get=h.get,h.has=h.has,h.set=h.set,r=function(n,e){if(h.has(n))throw g("Object already initialized");return e.facade=n,h.set(n,e),e},a=function(n){return h.get(n)||{}},o=function(n){return h.has(n)}}else{var v=d("state");m[v]=!0,r=function(n,e){if(p(n,v))throw g("Object already initialized");return e.facade=n,c(n,v,e),e},a=function(n){return p(n,v)?n[v]:{}},o=function(n){return p(n,v)}}n.exports={set:r,get:a,has:o,enforce:function(n){return o(n)?a(n):r(n,{})},getterFor:function(n){return function(e){var t;if(!l(e)||(t=a(e)).type!==n)throw g("Incompatible receiver, "+n+" required");return t}}}},function(n,e,t){var r=t(8),a=t(4),o=r.WeakMap;n.exports=a(o)&&/native code/.test(String(o))},function(n,e,t){var r=t(32),a=t(7),o=t(151),i=t(156),s=t(25),l=a([].concat);n.exports=r("Reflect","ownKeys")||function(n){var e=o.f(s(n)),t=i.f;return t?l(e,t(n)):e}},function(n,e,t){var r=t(86),a=t(51).concat("length","prototype");e.f=Object.getOwnPropertyNames||function(n){return r(n,a)}},function(n,e,t){var r=t(30),a=t(153),o=t(35),i=function(n){return function(e,t,i){var s,l=r(e),c=o(l),p=a(i,c);if(n&&t!=t){for(;c>p;)if((s=l[p++])!=s)return!0}else for(;c>p;p++)if((n||p in l)&&l[p]===t)return n||p||0;return!n&&-1}};n.exports={includes:i(!0),indexOf:i(!1)}},function(n,e,t){var r=t(34),a=Math.max,o=Math.min;n.exports=function(n,e){var t=r(n);return t<0?a(t+e,0):o(t,e)}},function(n,e){var t=Math.ceil,r=Math.floor;n.exports=Math.trunc||function(n){var e=+n;return(e>0?r:t)(e)}},function(n,e,t){var r=t(34),a=Math.min;n.exports=function(n){return n>0?a(r(n),9007199254740991):0}},function(n,e){e.f=Object.getOwnPropertySymbols},function(n,e,t){var r=t(6),a=t(4),o=/#|\.prototype\./,i=function(n,e){var t=l[s(n)];return t==p||t!=c&&(a(e)?r(e):!!e)},s=i.normalize=function(n){return String(n).replace(o,".").toLowerCase()},l=i.data={},c=i.NATIVE="N",p=i.POLYFILL="P";n.exports=i},function(n,e,t){var r=t(47),a=t(33),o=t(69),i=t(35),s=TypeError,l=function(n){return function(e,t,l,c){r(t);var p=a(e),u=o(p),d=i(p),m=n?d-1:0,g=n?-1:1;if(l<2)for(;;){if(m in u){c=u[m],m+=g;break}if(m+=g,n?m<0:d<=m)throw s("Reduce of empty array with no initial value")}for(;n?m>=0:d>m;m+=g)m in u&&(c=t(c,u[m],m,p));return c}};n.exports={left:l(!1),right:l(!0)}},function(n,e,t){"use strict";var r=t(6);n.exports=function(n,e){var t=[][n];return!!t&&r((function(){t.call(null,e||function(){return 1},1)}))}},function(n,e,t){var r=t(31);n.exports="undefined"!=typeof process&&"process"==r(process)},function(n,e,t){"use strict";var r=t(9),a=t(162),o=TypeError,i=Object.getOwnPropertyDescriptor,s=r&&!function(){if(void 0!==this)return!0;try{Object.defineProperty([],"length",{writable:!1}).length=1}catch(n){return n instanceof TypeError}}();n.exports=s?function(n,e){if(a(n)&&!i(n,"length").writable)throw o("Cannot set read only .length");return n.length=e}:function(n,e){return n.length=e}},function(n,e,t){var r=t(31);n.exports=Array.isArray||function(n){return"Array"==r(n)}},function(n,e){var t=TypeError;n.exports=function(n){if(n>9007199254740991)throw t("Maximum allowed index exceeded");return n}},function(n,e,t){var r=t(44),a=Function.prototype,o=a.apply,i=a.call;n.exports="object"==typeof Reflect&&Reflect.apply||(r?i.bind(o):function(){return i.apply(o,arguments)})},function(n,e,t){"use strict";var r=t(32),a=t(13),o=t(24),i=t(74),s=t(87),l=t(85),c=t(168),p=t(169),u=t(170),d=t(173),m=t(174),g=t(9),f=t(79);n.exports=function(n,e,t,h){var v=h?2:1,b=n.split("."),k=b[b.length-1],y=r.apply(null,b);if(y){var S=y.prototype;if(!f&&a(S,"cause")&&delete S.cause,!t)return y;var x=r("Error"),w=e((function(n,e){var t=u(h?e:n,void 0),r=h?new y(n):new y;return void 0!==t&&o(r,"message",t),m(r,w,r.stack,2),this&&i(S,this)&&p(r,this,w),arguments.length>v&&d(r,arguments[v]),r}));if(w.prototype=S,"Error"!==k?s?s(w,x):l(w,x,{name:!0}):g&&"stackTraceLimit"in y&&(c(w,y,"stackTraceLimit"),c(w,y,"prepareStackTrace")),l(w,y),!f)try{S.name!==k&&o(S,"name",k),S.constructor=w}catch(n){}return w}}},function(n,e,t){var r=t(7),a=t(47);n.exports=function(n,e,t){try{return r(a(Object.getOwnPropertyDescriptor(n,e)[t]))}catch(n){}}},function(n,e,t){var r=t(4),a=String,o=TypeError;n.exports=function(n){if("object"==typeof n||r(n))return n;throw o("Can't set "+a(n)+" as a prototype")}},function(n,e,t){var r=t(19).f;n.exports=function(n,e,t){t in n||r(n,t,{configurable:!0,get:function(){return e[t]},set:function(n){e[t]=n}})}},function(n,e,t){var r=t(4),a=t(14),o=t(87);n.exports=function(n,e,t){var i,s;return o&&r(i=e.constructor)&&i!==t&&a(s=i.prototype)&&s!==t.prototype&&o(n,s),n}},function(n,e,t){var r=t(88);n.exports=function(n,e){return void 0===n?arguments.length<2?"":e:r(n)}},function(n,e,t){var r=t(172),a=t(4),o=t(31),i=t(29)("toStringTag"),s=Object,l="Arguments"==o(function(){return arguments}());n.exports=r?o:function(n){var e,t,r;return void 0===n?"Undefined":null===n?"Null":"string"==typeof(t=function(n,e){try{return n[e]}catch(n){}}(e=s(n),i))?t:l?o(e):"Object"==(r=o(e))&&a(e.callee)?"Arguments":r}},function(n,e,t){var r={};r[t(29)("toStringTag")]="z",n.exports="[object z]"===String(r)},function(n,e,t){var r=t(14),a=t(24);n.exports=function(n,e){r(e)&&"cause"in e&&a(n,"cause",e.cause)}},function(n,e,t){var r=t(24),a=t(175),o=t(176),i=Error.captureStackTrace;n.exports=function(n,e,t,s){o&&(i?i(n,e):r(n,"stack",a(t,s)))}},function(n,e,t){var r=t(7),a=Error,o=r("".replace),i=String(a("zxcasd").stack),s=/\n\s*at [^:]*:[^\n]*/,l=s.test(i);n.exports=function(n,e){if(l&&"string"==typeof n&&!a.prepareStackTrace)for(;e--;)n=o(n,s,"");return n}},function(n,e,t){var r=t(6),a=t(45);n.exports=!r((function(){var n=Error("a");return!("stack"in n)||(Object.defineProperty(n,"stack",a(1,7)),7!==n.stack)}))},function(n,e,t){var r=t(89),a=t(178);n.exports=function n(e,t,o,i,s){var l=-1,c=e.length;for(o||(o=a),s||(s=[]);++l<c;){var p=e[l];t>0&&o(p)?t>1?n(p,t-1,o,i,s):r(s,p):i||(s[s.length]=p)}return s}},function(n,e,t){var r=t(26),a=t(52),o=t(12),i=r?r.isConcatSpreadable:void 0;n.exports=function(n){return o(n)||a(n)||!!(i&&n&&n[i])}},function(n,e,t){var r=t(27),a=t(20);n.exports=function(n){return a(n)&&"[object Arguments]"==r(n)}},function(n,e,t){var r=t(26),a=Object.prototype,o=a.hasOwnProperty,i=a.toString,s=r?r.toStringTag:void 0;n.exports=function(n){var e=o.call(n,s),t=n[s];try{n[s]=void 0;var r=!0}catch(n){}var a=i.call(n);return r&&(e?n[s]=t:delete n[s]),a}},function(n,e){var t=Object.prototype.toString;n.exports=function(n){return t.call(n)}},function(n,e,t){var r=t(183),a=t(239),o=t(60),i=t(12),s=t(250);n.exports=function(n){return"function"==typeof n?n:null==n?o:"object"==typeof n?i(n)?a(n[0],n[1]):r(n):s(n)}},function(n,e,t){var r=t(184),a=t(238),o=t(106);n.exports=function(n){var e=a(n);return 1==e.length&&e[0][2]?o(e[0][0],e[0][1]):function(t){return t===n||r(t,n,e)}}},function(n,e,t){var r=t(91),a=t(95);n.exports=function(n,e,t,o){var i=t.length,s=i,l=!o;if(null==n)return!s;for(n=Object(n);i--;){var c=t[i];if(l&&c[2]?c[1]!==n[c[0]]:!(c[0]in n))return!1}for(;++i<s;){var p=(c=t[i])[0],u=n[p],d=c[1];if(l&&c[2]){if(void 0===u&&!(p in n))return!1}else{var m=new r;if(o)var g=o(u,d,p,n,e,m);if(!(void 0===g?a(d,u,3,o,m):g))return!1}}return!0}},function(n,e){n.exports=function(){this.__data__=[],this.size=0}},function(n,e,t){var r=t(37),a=Array.prototype.splice;n.exports=function(n){var e=this.__data__,t=r(e,n);return!(t<0)&&(t==e.length-1?e.pop():a.call(e,t,1),--this.size,!0)}},function(n,e,t){var r=t(37);n.exports=function(n){var e=this.__data__,t=r(e,n);return t<0?void 0:e[t][1]}},function(n,e,t){var r=t(37);n.exports=function(n){return r(this.__data__,n)>-1}},function(n,e,t){var r=t(37);n.exports=function(n,e){var t=this.__data__,a=r(t,n);return a<0?(++this.size,t.push([n,e])):t[a][1]=e,this}},function(n,e,t){var r=t(36);n.exports=function(){this.__data__=new r,this.size=0}},function(n,e){n.exports=function(n){var e=this.__data__,t=e.delete(n);return this.size=e.size,t}},function(n,e){n.exports=function(n){return this.__data__.get(n)}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e,t){var r=t(36),a=t(53),o=t(55);n.exports=function(n,e){var t=this.__data__;if(t instanceof r){var i=t.__data__;if(!a||i.length<199)return i.push([n,e]),this.size=++t.size,this;t=this.__data__=new o(i)}return t.set(n,e),this.size=t.size,this}},function(n,e,t){var r=t(93),a=t(196),o=t(54),i=t(94),s=/^\[object .+?Constructor\]$/,l=Function.prototype,c=Object.prototype,p=l.toString,u=c.hasOwnProperty,d=RegExp("^"+p.call(u).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");n.exports=function(n){return!(!o(n)||a(n))&&(r(n)?d:s).test(i(n))}},function(n,e,t){var r,a=t(197),o=(r=/[^.]+$/.exec(a&&a.keys&&a.keys.IE_PROTO||""))?"Symbol(src)_1."+r:"";n.exports=function(n){return!!o&&o in n}},function(n,e,t){var r=t(11)["__core-js_shared__"];n.exports=r},function(n,e){n.exports=function(n,e){return null==n?void 0:n[e]}},function(n,e,t){var r=t(200),a=t(36),o=t(53);n.exports=function(){this.size=0,this.__data__={hash:new r,map:new(o||a),string:new r}}},function(n,e,t){var r=t(201),a=t(202),o=t(203),i=t(204),s=t(205);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=a,l.prototype.get=o,l.prototype.has=i,l.prototype.set=s,n.exports=l},function(n,e,t){var r=t(38);n.exports=function(){this.__data__=r?r(null):{},this.size=0}},function(n,e){n.exports=function(n){var e=this.has(n)&&delete this.__data__[n];return this.size-=e?1:0,e}},function(n,e,t){var r=t(38),a=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;if(r){var t=e[n];return"__lodash_hash_undefined__"===t?void 0:t}return a.call(e,n)?e[n]:void 0}},function(n,e,t){var r=t(38),a=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;return r?void 0!==e[n]:a.call(e,n)}},function(n,e,t){var r=t(38);n.exports=function(n,e){var t=this.__data__;return this.size+=this.has(n)?0:1,t[n]=r&&void 0===e?"__lodash_hash_undefined__":e,this}},function(n,e,t){var r=t(39);n.exports=function(n){var e=r(this,n).delete(n);return this.size-=e?1:0,e}},function(n,e){n.exports=function(n){var e=typeof n;return"string"==e||"number"==e||"symbol"==e||"boolean"==e?"__proto__"!==n:null===n}},function(n,e,t){var r=t(39);n.exports=function(n){return r(this,n).get(n)}},function(n,e,t){var r=t(39);n.exports=function(n){return r(this,n).has(n)}},function(n,e,t){var r=t(39);n.exports=function(n,e){var t=r(this,n),a=t.size;return t.set(n,e),this.size+=t.size==a?0:1,this}},function(n,e,t){var r=t(91),a=t(96),o=t(215),i=t(218),s=t(234),l=t(12),c=t(100),p=t(102),u="[object Object]",d=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,m,g,f){var h=l(n),v=l(e),b=h?"[object Array]":s(n),k=v?"[object Array]":s(e),y=(b="[object Arguments]"==b?u:b)==u,S=(k="[object Arguments]"==k?u:k)==u,x=b==k;if(x&&c(n)){if(!c(e))return!1;h=!0,y=!1}if(x&&!y)return f||(f=new r),h||p(n)?a(n,e,t,m,g,f):o(n,e,b,t,m,g,f);if(!(1&t)){var w=y&&d.call(n,"__wrapped__"),E=S&&d.call(e,"__wrapped__");if(w||E){var D=w?n.value():n,C=E?e.value():e;return f||(f=new r),g(D,C,t,m,f)}}return!!x&&(f||(f=new r),i(n,e,t,m,g,f))}},function(n,e){n.exports=function(n){return this.__data__.set(n,"__lodash_hash_undefined__"),this}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length;++t<r;)if(e(n[t],t,n))return!0;return!1}},function(n,e,t){var r=t(26),a=t(216),o=t(92),i=t(96),s=t(217),l=t(56),c=r?r.prototype:void 0,p=c?c.valueOf:void 0;n.exports=function(n,e,t,r,c,u,d){switch(t){case"[object DataView]":if(n.byteLength!=e.byteLength||n.byteOffset!=e.byteOffset)return!1;n=n.buffer,e=e.buffer;case"[object ArrayBuffer]":return!(n.byteLength!=e.byteLength||!u(new a(n),new a(e)));case"[object Boolean]":case"[object Date]":case"[object Number]":return o(+n,+e);case"[object Error]":return n.name==e.name&&n.message==e.message;case"[object RegExp]":case"[object String]":return n==e+"";case"[object Map]":var m=s;case"[object Set]":var g=1&r;if(m||(m=l),n.size!=e.size&&!g)return!1;var f=d.get(n);if(f)return f==e;r|=2,d.set(n,e);var h=i(m(n),m(e),r,c,u,d);return d.delete(n),h;case"[object Symbol]":if(p)return p.call(n)==p.call(e)}return!1}},function(n,e,t){var r=t(11).Uint8Array;n.exports=r},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n,r){t[++e]=[r,n]})),t}},function(n,e,t){var r=t(219),a=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,o,i,s){var l=1&t,c=r(n),p=c.length;if(p!=r(e).length&&!l)return!1;for(var u=p;u--;){var d=c[u];if(!(l?d in e:a.call(e,d)))return!1}var m=s.get(n),g=s.get(e);if(m&&g)return m==e&&g==n;var f=!0;s.set(n,e),s.set(e,n);for(var h=l;++u<p;){var v=n[d=c[u]],b=e[d];if(o)var k=l?o(b,v,d,e,n,s):o(v,b,d,n,e,s);if(!(void 0===k?v===b||i(v,b,t,o,s):k)){f=!1;break}h||(h="constructor"==d)}if(f&&!h){var y=n.constructor,S=e.constructor;y==S||!("constructor"in n)||!("constructor"in e)||"function"==typeof y&&y instanceof y&&"function"==typeof S&&S instanceof S||(f=!1)}return s.delete(n),s.delete(e),f}},function(n,e,t){var r=t(220),a=t(221),o=t(99);n.exports=function(n){return r(n,o,a)}},function(n,e,t){var r=t(89),a=t(12);n.exports=function(n,e,t){var o=e(n);return a(n)?o:r(o,t(n))}},function(n,e,t){var r=t(222),a=t(223),o=Object.prototype.propertyIsEnumerable,i=Object.getOwnPropertySymbols,s=i?function(n){return null==n?[]:(n=Object(n),r(i(n),(function(e){return o.call(n,e)})))}:a;n.exports=s},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,a=0,o=[];++t<r;){var i=n[t];e(i,t,n)&&(o[a++]=i)}return o}},function(n,e){n.exports=function(){return[]}},function(n,e,t){var r=t(225),a=t(52),o=t(12),i=t(100),s=t(101),l=t(102),c=Object.prototype.hasOwnProperty;n.exports=function(n,e){var t=o(n),p=!t&&a(n),u=!t&&!p&&i(n),d=!t&&!p&&!u&&l(n),m=t||p||u||d,g=m?r(n.length,String):[],f=g.length;for(var h in n)!e&&!c.call(n,h)||m&&("length"==h||u&&("offset"==h||"parent"==h)||d&&("buffer"==h||"byteLength"==h||"byteOffset"==h)||s(h,f))||g.push(h);return g}},function(n,e){n.exports=function(n,e){for(var t=-1,r=Array(n);++t<n;)r[t]=e(t);return r}},function(n,e){n.exports=function(){return!1}},function(n,e,t){var r=t(27),a=t(57),o=t(20),i={};i["[object Float32Array]"]=i["[object Float64Array]"]=i["[object Int8Array]"]=i["[object Int16Array]"]=i["[object Int32Array]"]=i["[object Uint8Array]"]=i["[object Uint8ClampedArray]"]=i["[object Uint16Array]"]=i["[object Uint32Array]"]=!0,i["[object Arguments]"]=i["[object Array]"]=i["[object ArrayBuffer]"]=i["[object Boolean]"]=i["[object DataView]"]=i["[object Date]"]=i["[object Error]"]=i["[object Function]"]=i["[object Map]"]=i["[object Number]"]=i["[object Object]"]=i["[object RegExp]"]=i["[object Set]"]=i["[object String]"]=i["[object WeakMap]"]=!1,n.exports=function(n){return o(n)&&a(n.length)&&!!i[r(n)]}},function(n,e){n.exports=function(n){return function(e){return n(e)}}},function(n,e,t){(function(n){var r=t(90),a=e&&!e.nodeType&&e,o=a&&"object"==typeof n&&n&&!n.nodeType&&n,i=o&&o.exports===a&&r.process,s=function(){try{var n=o&&o.require&&o.require("util").types;return n||i&&i.binding&&i.binding("util")}catch(n){}}();n.exports=s}).call(this,t(66)(n))},function(n,e,t){var r=t(231),a=t(232),o=Object.prototype.hasOwnProperty;n.exports=function(n){if(!r(n))return a(n);var e=[];for(var t in Object(n))o.call(n,t)&&"constructor"!=t&&e.push(t);return e}},function(n,e){var t=Object.prototype;n.exports=function(n){var e=n&&n.constructor;return n===("function"==typeof e&&e.prototype||t)}},function(n,e,t){var r=t(233)(Object.keys,Object);n.exports=r},function(n,e){n.exports=function(n,e){return function(t){return n(e(t))}}},function(n,e,t){var r=t(235),a=t(53),o=t(236),i=t(104),s=t(237),l=t(27),c=t(94),p=c(r),u=c(a),d=c(o),m=c(i),g=c(s),f=l;(r&&"[object DataView]"!=f(new r(new ArrayBuffer(1)))||a&&"[object Map]"!=f(new a)||o&&"[object Promise]"!=f(o.resolve())||i&&"[object Set]"!=f(new i)||s&&"[object WeakMap]"!=f(new s))&&(f=function(n){var e=l(n),t="[object Object]"==e?n.constructor:void 0,r=t?c(t):"";if(r)switch(r){case p:return"[object DataView]";case u:return"[object Map]";case d:return"[object Promise]";case m:return"[object Set]";case g:return"[object WeakMap]"}return e}),n.exports=f},function(n,e,t){var r=t(15)(t(11),"DataView");n.exports=r},function(n,e,t){var r=t(15)(t(11),"Promise");n.exports=r},function(n,e,t){var r=t(15)(t(11),"WeakMap");n.exports=r},function(n,e,t){var r=t(105),a=t(99);n.exports=function(n){for(var e=a(n),t=e.length;t--;){var o=e[t],i=n[o];e[t]=[o,i,r(i)]}return e}},function(n,e,t){var r=t(95),a=t(240),o=t(247),i=t(58),s=t(105),l=t(106),c=t(40);n.exports=function(n,e){return i(n)&&s(e)?l(c(n),e):function(t){var i=a(t,n);return void 0===i&&i===e?o(t,n):r(e,i,3)}}},function(n,e,t){var r=t(107);n.exports=function(n,e,t){var a=null==n?void 0:r(n,e);return void 0===a?t:a}},function(n,e,t){var r=t(242),a=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,o=/\\(\\)?/g,i=r((function(n){var e=[];return 46===n.charCodeAt(0)&&e.push(""),n.replace(a,(function(n,t,r,a){e.push(r?a.replace(o,"$1"):t||n)})),e}));n.exports=i},function(n,e,t){var r=t(243);n.exports=function(n){var e=r(n,(function(n){return 500===t.size&&t.clear(),n})),t=e.cache;return e}},function(n,e,t){var r=t(55);function a(n,e){if("function"!=typeof n||null!=e&&"function"!=typeof e)throw new TypeError("Expected a function");var t=function(){var r=arguments,a=e?e.apply(this,r):r[0],o=t.cache;if(o.has(a))return o.get(a);var i=n.apply(this,r);return t.cache=o.set(a,i)||o,i};return t.cache=new(a.Cache||r),t}a.Cache=r,n.exports=a},function(n,e,t){var r=t(245);n.exports=function(n){return null==n?"":r(n)}},function(n,e,t){var r=t(26),a=t(246),o=t(12),i=t(59),s=r?r.prototype:void 0,l=s?s.toString:void 0;n.exports=function n(e){if("string"==typeof e)return e;if(o(e))return a(e,n)+"";if(i(e))return l?l.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,a=Array(r);++t<r;)a[t]=e(n[t],t,n);return a}},function(n,e,t){var r=t(248),a=t(249);n.exports=function(n,e){return null!=n&&a(n,e,r)}},function(n,e){n.exports=function(n,e){return null!=n&&e in Object(n)}},function(n,e,t){var r=t(108),a=t(52),o=t(12),i=t(101),s=t(57),l=t(40);n.exports=function(n,e,t){for(var c=-1,p=(e=r(e,n)).length,u=!1;++c<p;){var d=l(e[c]);if(!(u=null!=n&&t(n,d)))break;n=n[d]}return u||++c!=p?u:!!(p=null==n?0:n.length)&&s(p)&&i(d,p)&&(o(n)||a(n))}},function(n,e,t){var r=t(251),a=t(252),o=t(58),i=t(40);n.exports=function(n){return o(n)?r(i(n)):a(n)}},function(n,e){n.exports=function(n){return function(e){return null==e?void 0:e[n]}}},function(n,e,t){var r=t(107);n.exports=function(n){return function(e){return r(e,n)}}},function(n,e,t){var r=t(60),a=t(254),o=t(256);n.exports=function(n,e){return o(a(n,e,r),n+"")}},function(n,e,t){var r=t(255),a=Math.max;n.exports=function(n,e,t){return e=a(void 0===e?n.length-1:e,0),function(){for(var o=arguments,i=-1,s=a(o.length-e,0),l=Array(s);++i<s;)l[i]=o[e+i];i=-1;for(var c=Array(e+1);++i<e;)c[i]=o[i];return c[e]=t(l),r(n,this,c)}}},function(n,e){n.exports=function(n,e,t){switch(t.length){case 0:return n.call(e);case 1:return n.call(e,t[0]);case 2:return n.call(e,t[0],t[1]);case 3:return n.call(e,t[0],t[1],t[2])}return n.apply(e,t)}},function(n,e,t){var r=t(257),a=t(260)(r);n.exports=a},function(n,e,t){var r=t(258),a=t(259),o=t(60),i=a?function(n,e){return a(n,"toString",{configurable:!0,enumerable:!1,value:r(e),writable:!0})}:o;n.exports=i},function(n,e){n.exports=function(n){return function(){return n}}},function(n,e,t){var r=t(15),a=function(){try{var n=r(Object,"defineProperty");return n({},"",{}),n}catch(n){}}();n.exports=a},function(n,e){var t=Date.now;n.exports=function(n){var e=0,r=0;return function(){var a=t(),o=16-(a-r);if(r=a,o>0){if(++e>=800)return arguments[0]}else e=0;return n.apply(void 0,arguments)}}},function(n,e,t){var r=t(97),a=t(262),o=t(267),i=t(98),s=t(268),l=t(56);n.exports=function(n,e,t){var c=-1,p=a,u=n.length,d=!0,m=[],g=m;if(t)d=!1,p=o;else if(u>=200){var f=e?null:s(n);if(f)return l(f);d=!1,p=i,g=new r}else g=e?[]:m;n:for(;++c<u;){var h=n[c],v=e?e(h):h;if(h=t||0!==h?h:0,d&&v==v){for(var b=g.length;b--;)if(g[b]===v)continue n;e&&g.push(v),m.push(h)}else p(g,v,t)||(g!==m&&g.push(v),m.push(h))}return m}},function(n,e,t){var r=t(263);n.exports=function(n,e){return!!(null==n?0:n.length)&&r(n,e,0)>-1}},function(n,e,t){var r=t(264),a=t(265),o=t(266);n.exports=function(n,e,t){return e==e?o(n,e,t):r(n,a,t)}},function(n,e){n.exports=function(n,e,t,r){for(var a=n.length,o=t+(r?1:-1);r?o--:++o<a;)if(e(n[o],o,n))return o;return-1}},function(n,e){n.exports=function(n){return n!=n}},function(n,e){n.exports=function(n,e,t){for(var r=t-1,a=n.length;++r<a;)if(n[r]===e)return r;return-1}},function(n,e){n.exports=function(n,e,t){for(var r=-1,a=null==n?0:n.length;++r<a;)if(t(e,n[r]))return!0;return!1}},function(n,e,t){var r=t(104),a=t(269),o=t(56),i=r&&1/o(new r([,-0]))[1]==1/0?function(n){return new r(n)}:a;n.exports=i},function(n,e){n.exports=function(){}},function(n,e,t){var r=t(103),a=t(20);n.exports=function(n){return a(n)&&r(n)}},function(n,e){n.exports=function(n){var e=null==n?0:n.length;return e?n[e-1]:void 0}},function(n,e,t){},function(n,e,t){"use strict";t(109)},function(n,e,t){},function(n,e,t){"use strict";t(110)},function(n,e,t){},function(n,e,t){"use strict";t(111)},function(n,e,t){"use strict";var r=t(23),a=t(33),o=t(35),i=t(34),s=t(279);r({target:"Array",proto:!0},{at:function(n){var e=a(this),t=o(e),r=i(n),s=r>=0?r:t+r;return s<0||s>=t?void 0:e[s]}}),s("at")},function(n,e,t){var r=t(29),a=t(280),o=t(19).f,i=r("unscopables"),s=Array.prototype;null==s[i]&&o(s,i,{configurable:!0,value:a(null)}),n.exports=function(n){s[i][n]=!0}},function(n,e,t){var r,a=t(25),o=t(281),i=t(51),s=t(50),l=t(283),c=t(82),p=t(84),u=p("IE_PROTO"),d=function(){},m=function(n){return"<script>"+n+"<\/script>"},g=function(n){n.write(m("")),n.close();var e=n.parentWindow.Object;return n=null,e},f=function(){try{r=new ActiveXObject("htmlfile")}catch(n){}var n,e;f="undefined"!=typeof document?document.domain&&r?g(r):((e=c("iframe")).style.display="none",l.appendChild(e),e.src=String("javascript:"),(n=e.contentWindow.document).open(),n.write(m("document.F=Object")),n.close(),n.F):g(r);for(var t=i.length;t--;)delete f.prototype[i[t]];return f()};s[u]=!0,n.exports=Object.create||function(n,e){var t;return null!==n?(d.prototype=a(n),t=new d,d.prototype=null,t[u]=n):t=f(),void 0===e?t:o.f(t,e)}},function(n,e,t){var r=t(9),a=t(83),o=t(19),i=t(25),s=t(30),l=t(282);e.f=r&&!a?Object.defineProperties:function(n,e){i(n);for(var t,r=s(e),a=l(e),c=a.length,p=0;c>p;)o.f(n,t=a[p++],r[t]);return n}},function(n,e,t){var r=t(86),a=t(51);n.exports=Object.keys||function(n){return r(n,a)}},function(n,e,t){var r=t(32);n.exports=r("document","documentElement")},function(n,e,t){"use strict";var r=t(23),a=t(7),o=t(46),i=t(34),s=t(88),l=t(6),c=a("".charAt);r({target:"String",proto:!0,forced:l((function(){return"\ud842"!=="𠮷".at(-2)}))},{at:function(n){var e=s(o(this)),t=e.length,r=i(n),a=r>=0?r:t+r;return a<0||a>=t?void 0:c(e,a)}})},function(n,e,t){"use strict";t(112)},function(n,e,t){
/*!
 * Valine v1.5.1
 * (c) 2017-2022 xCss
 * Released under the GPL-2.0 License.
 * Last Update: 2022-7-21 3:43:59 ├F10: PM┤
 */
n.exports=function(n){function e(r){if(t[r])return t[r].exports;var a=t[r]={i:r,l:!1,exports:{}};return n[r].call(a.exports,a,a.exports,e),a.l=!0,a.exports}var t={};return e.m=n,e.c=t,e.i=function(n){return n},e.d=function(n,t,r){e.o(n,t)||Object.defineProperty(n,t,{configurable:!1,enumerable:!0,get:r})},e.n=function(n){var t=n&&n.__esModule?function(){return n.default}:function(){return n};return e.d(t,"a",t),t},e.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},e.p="",e(e.s=108)}([function(n,e,t){"use strict";var r=SyntaxError,a=Function,o=TypeError,i=function(n){try{return a('"use strict"; return ('+n+").constructor;")()}catch(n){}},s=Object.getOwnPropertyDescriptor;if(s)try{s({},"")}catch(n){s=null}var l=function(){throw new o},c=s?function(){try{return l}catch(n){try{return s(arguments,"callee").get}catch(n){return l}}}():l,p=t(22)(),u=Object.getPrototypeOf||function(n){return n.__proto__},d={},m="undefined"==typeof Uint8Array?void 0:u(Uint8Array),g={"%AggregateError%":"undefined"==typeof AggregateError?void 0:AggregateError,"%Array%":Array,"%ArrayBuffer%":"undefined"==typeof ArrayBuffer?void 0:ArrayBuffer,"%ArrayIteratorPrototype%":p?u([][Symbol.iterator]()):void 0,"%AsyncFromSyncIteratorPrototype%":void 0,"%AsyncFunction%":d,"%AsyncGenerator%":d,"%AsyncGeneratorFunction%":d,"%AsyncIteratorPrototype%":d,"%Atomics%":"undefined"==typeof Atomics?void 0:Atomics,"%BigInt%":"undefined"==typeof BigInt?void 0:BigInt,"%Boolean%":Boolean,"%DataView%":"undefined"==typeof DataView?void 0:DataView,"%Date%":Date,"%decodeURI%":decodeURI,"%decodeURIComponent%":decodeURIComponent,"%encodeURI%":encodeURI,"%encodeURIComponent%":encodeURIComponent,"%Error%":Error,"%eval%":eval,"%EvalError%":EvalError,"%Float32Array%":"undefined"==typeof Float32Array?void 0:Float32Array,"%Float64Array%":"undefined"==typeof Float64Array?void 0:Float64Array,"%FinalizationRegistry%":"undefined"==typeof FinalizationRegistry?void 0:FinalizationRegistry,"%Function%":a,"%GeneratorFunction%":d,"%Int8Array%":"undefined"==typeof Int8Array?void 0:Int8Array,"%Int16Array%":"undefined"==typeof Int16Array?void 0:Int16Array,"%Int32Array%":"undefined"==typeof Int32Array?void 0:Int32Array,"%isFinite%":isFinite,"%isNaN%":isNaN,"%IteratorPrototype%":p?u(u([][Symbol.iterator]())):void 0,"%JSON%":"object"==typeof JSON?JSON:void 0,"%Map%":"undefined"==typeof Map?void 0:Map,"%MapIteratorPrototype%":"undefined"!=typeof Map&&p?u((new Map)[Symbol.iterator]()):void 0,"%Math%":Math,"%Number%":Number,"%Object%":Object,"%parseFloat%":parseFloat,"%parseInt%":parseInt,"%Promise%":"undefined"==typeof Promise?void 0:Promise,"%Proxy%":"undefined"==typeof Proxy?void 0:Proxy,"%RangeError%":RangeError,"%ReferenceError%":ReferenceError,"%Reflect%":"undefined"==typeof Reflect?void 0:Reflect,"%RegExp%":RegExp,"%Set%":"undefined"==typeof Set?void 0:Set,"%SetIteratorPrototype%":"undefined"!=typeof Set&&p?u((new Set)[Symbol.iterator]()):void 0,"%SharedArrayBuffer%":"undefined"==typeof SharedArrayBuffer?void 0:SharedArrayBuffer,"%String%":String,"%StringIteratorPrototype%":p?u(""[Symbol.iterator]()):void 0,"%Symbol%":p?Symbol:void 0,"%SyntaxError%":r,"%ThrowTypeError%":c,"%TypedArray%":m,"%TypeError%":o,"%Uint8Array%":"undefined"==typeof Uint8Array?void 0:Uint8Array,"%Uint8ClampedArray%":"undefined"==typeof Uint8ClampedArray?void 0:Uint8ClampedArray,"%Uint16Array%":"undefined"==typeof Uint16Array?void 0:Uint16Array,"%Uint32Array%":"undefined"==typeof Uint32Array?void 0:Uint32Array,"%URIError%":URIError,"%WeakMap%":"undefined"==typeof WeakMap?void 0:WeakMap,"%WeakRef%":"undefined"==typeof WeakRef?void 0:WeakRef,"%WeakSet%":"undefined"==typeof WeakSet?void 0:WeakSet},f=function n(e){var t;if("%AsyncFunction%"===e)t=i("async function () {}");else if("%GeneratorFunction%"===e)t=i("function* () {}");else if("%AsyncGeneratorFunction%"===e)t=i("async function* () {}");else if("%AsyncGenerator%"===e){var r=n("%AsyncGeneratorFunction%");r&&(t=r.prototype)}else if("%AsyncIteratorPrototype%"===e){var a=n("%AsyncGenerator%");a&&(t=u(a.prototype))}return g[e]=t,t},h={"%ArrayBufferPrototype%":["ArrayBuffer","prototype"],"%ArrayPrototype%":["Array","prototype"],"%ArrayProto_entries%":["Array","prototype","entries"],"%ArrayProto_forEach%":["Array","prototype","forEach"],"%ArrayProto_keys%":["Array","prototype","keys"],"%ArrayProto_values%":["Array","prototype","values"],"%AsyncFunctionPrototype%":["AsyncFunction","prototype"],"%AsyncGenerator%":["AsyncGeneratorFunction","prototype"],"%AsyncGeneratorPrototype%":["AsyncGeneratorFunction","prototype","prototype"],"%BooleanPrototype%":["Boolean","prototype"],"%DataViewPrototype%":["DataView","prototype"],"%DatePrototype%":["Date","prototype"],"%ErrorPrototype%":["Error","prototype"],"%EvalErrorPrototype%":["EvalError","prototype"],"%Float32ArrayPrototype%":["Float32Array","prototype"],"%Float64ArrayPrototype%":["Float64Array","prototype"],"%FunctionPrototype%":["Function","prototype"],"%Generator%":["GeneratorFunction","prototype"],"%GeneratorPrototype%":["GeneratorFunction","prototype","prototype"],"%Int8ArrayPrototype%":["Int8Array","prototype"],"%Int16ArrayPrototype%":["Int16Array","prototype"],"%Int32ArrayPrototype%":["Int32Array","prototype"],"%JSONParse%":["JSON","parse"],"%JSONStringify%":["JSON","stringify"],"%MapPrototype%":["Map","prototype"],"%NumberPrototype%":["Number","prototype"],"%ObjectPrototype%":["Object","prototype"],"%ObjProto_toString%":["Object","prototype","toString"],"%ObjProto_valueOf%":["Object","prototype","valueOf"],"%PromisePrototype%":["Promise","prototype"],"%PromiseProto_then%":["Promise","prototype","then"],"%Promise_all%":["Promise","all"],"%Promise_reject%":["Promise","reject"],"%Promise_resolve%":["Promise","resolve"],"%RangeErrorPrototype%":["RangeError","prototype"],"%ReferenceErrorPrototype%":["ReferenceError","prototype"],"%RegExpPrototype%":["RegExp","prototype"],"%SetPrototype%":["Set","prototype"],"%SharedArrayBufferPrototype%":["SharedArrayBuffer","prototype"],"%StringPrototype%":["String","prototype"],"%SymbolPrototype%":["Symbol","prototype"],"%SyntaxErrorPrototype%":["SyntaxError","prototype"],"%TypedArrayPrototype%":["TypedArray","prototype"],"%TypeErrorPrototype%":["TypeError","prototype"],"%Uint8ArrayPrototype%":["Uint8Array","prototype"],"%Uint8ClampedArrayPrototype%":["Uint8ClampedArray","prototype"],"%Uint16ArrayPrototype%":["Uint16Array","prototype"],"%Uint32ArrayPrototype%":["Uint32Array","prototype"],"%URIErrorPrototype%":["URIError","prototype"],"%WeakMapPrototype%":["WeakMap","prototype"],"%WeakSetPrototype%":["WeakSet","prototype"]},v=t(9),b=t(25),k=v.call(Function.call,Array.prototype.concat),y=v.call(Function.apply,Array.prototype.splice),S=v.call(Function.call,String.prototype.replace),x=v.call(Function.call,String.prototype.slice),w=/[^%.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|%$))/g,E=/\\(\\)?/g,D=function(n){var e=x(n,0,1),t=x(n,-1);if("%"===e&&"%"!==t)throw new r("invalid intrinsic syntax, expected closing `%`");if("%"===t&&"%"!==e)throw new r("invalid intrinsic syntax, expected opening `%`");var a=[];return S(n,w,(function(n,e,t,r){a[a.length]=t?S(r,E,"$1"):e||n})),a},C=function(n,e){var t,a=n;if(b(h,a)&&(a="%"+(t=h[a])[0]+"%"),b(g,a)){var i=g[a];if(i===d&&(i=f(a)),void 0===i&&!e)throw new o("intrinsic "+n+" exists, but is not available. Please file an issue!");return{alias:t,name:a,value:i}}throw new r("intrinsic "+n+" does not exist!")};n.exports=function(n,e){if("string"!=typeof n||0===n.length)throw new o("intrinsic name must be a non-empty string");if(arguments.length>1&&"boolean"!=typeof e)throw new o('"allowMissing" argument must be a boolean');var t=D(n),a=t.length>0?t[0]:"",i=C("%"+a+"%",e),l=i.name,c=i.value,p=!1,u=i.alias;u&&(a=u[0],y(t,k([0,1],u)));for(var d=1,m=!0;d<t.length;d+=1){var f=t[d],h=x(f,0,1),v=x(f,-1);if(('"'===h||"'"===h||"`"===h||'"'===v||"'"===v||"`"===v)&&h!==v)throw new r("property names with quotes must have matching quotes");if("constructor"!==f&&m||(p=!0),b(g,l="%"+(a+="."+f)+"%"))c=g[l];else if(null!=c){if(!(f in c)){if(!e)throw new o("base intrinsic for "+n+" exists, but the property is not available.");return}if(s&&d+1>=t.length){var S=s(c,f);c=(m=!!S)&&"get"in S&&!("originalValue"in S.get)?S.get:c[f]}else m=b(c,f),c=c[f];m&&!p&&(g[l]=c)}}return c}},function(n,e,t){"use strict";var r=t(0),a=t(4),o=a(r("String.prototype.indexOf"));n.exports=function(n,e){var t=r(n,!!e);return"function"==typeof t&&o(n,".prototype.")>-1?a(t):t}},function(n,e,t){"use strict";var r=t(88),a="function"==typeof Symbol&&"symbol"==typeof Symbol("foo"),o=Object.prototype.toString,i=Array.prototype.concat,s=Object.defineProperty,l=s&&function(){var n={};try{for(var e in s(n,"x",{enumerable:!1,value:n}),n)return!1;return n.x===n}catch(n){return!1}}(),c=function(n,e,t,r){(!(e in n)||function(n){return"function"==typeof n&&"[object Function]"===o.call(n)}(r)&&r())&&(l?s(n,e,{configurable:!0,enumerable:!1,value:t,writable:!0}):n[e]=t)},p=function(n,e){var t=arguments.length>2?arguments[2]:{},o=r(e);a&&(o=i.call(o,Object.getOwnPropertySymbols(e)));for(var s=0;s<o.length;s+=1)c(n,o[s],e[o[s]],t[o[s]])};p.supportsDescriptors=!!l,n.exports=p},function(n,e,t){"use strict";function r(n){return n&&n.__esModule?n:{default:n}}e.__esModule=!0;var a="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},o=r(t(85)),i=r(t(49)),s=r(t(46)),l=r(t(48)),c=r(t(45)),p=document,u=navigator,d=/[&<>"'`\\]/g,m=RegExp(d.source),g=/&(?:amp|lt|gt|quot|#39|#x60|#x5c);/g,f=RegExp(g.source),h={"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&quot;","'":"&#39;","`":"&#x60;","\\":"&#x5c;"},v={};for(var b in h)v[h[b]]=b;var k=null;Array.prototype.forEach||(Array.prototype.forEach=function(n,e){var t,r;if(null==this)throw new TypeError(" this is null or not defined");var a=Object(this),o=a.length>>>0;if("function"!=typeof n)throw new TypeError(n+" is not a function");for(arguments.length>1&&(t=e),r=0;r<o;){var i;r in a&&(i=a[r],n.call(t,i,r,a)),r++}}),window.NodeList&&!NodeList.prototype.forEach&&(NodeList.prototype.forEach=Array.prototype.forEach),String.prototype.trim||(String.prototype.trim=function(){return this.replace(/^[\s\uFEFF\xA0]+|[\s\uFEFF\xA0]+$/g,"")}),(0,o.default)(i.default.fn,{prepend:function(n){return n instanceof HTMLElement||(n=n[0]),this.forEach((function(e){e.insertAdjacentElement("afterBegin",n)})),this},append:function(n){return n instanceof HTMLElement||(n=n[0]),this.forEach((function(e){e.insertAdjacentElement("beforeEnd",n)})),this},remove:function(){return this.forEach((function(n){try{n.parentNode.removeChild(n)}catch(n){}})),this},find:function(n){return(0,i.default)(n,this)},show:function(){return this.forEach((function(n){n.style.display="block"})),this},hide:function(){return this.forEach((function(n){n.style.display="none"})),this},on:function(n,e,t){return i.default.fn.off(n,e,t),this.forEach((function(r){n.split(" ").forEach((function(n){r.addEventListener?r.addEventListener(n,e,t||!1):r.attachEvent?r.attachEvent("on"+n,e):r["on"+n]=e}))})),this},off:function(n,e,t){return this.forEach((function(r){n.split(" ").forEach((function(n){r.removeEventListener?r.removeEventListener(n,e,t||!1):r.detachEvent?r.detachEvent("on"+n,e):r["on"+n]=null}))})),this},html:function(n){return void 0!==n?(this.forEach((function(e){e.innerHTML=n})),this):this[0].innerHTML},text:function(n){return void 0!==n?(this.forEach((function(e){e.innerText=n})),this):this[0].innerText},empty:function(n){return n=n||0,this.forEach((function(e){setTimeout((function(n){e.innerText=""}),n)})),this},val:function(n){return void 0!==n?(this.forEach((function(e){e.value=n})),this):this[0].value||""},attr:function(){var n=arguments;if("object"==a(arguments[0])){var e=arguments[0],t=this;return Object.keys(e).forEach((function(n){t.forEach((function(t){t.setAttribute(n,e[n])}))})),this}return"string"==typeof arguments[0]&&arguments.length<2?this[0].getAttribute(arguments[0])||"":(this.forEach((function(e){e.setAttribute(n[0],n[1])})),this)},removeAttr:function(n){return this.forEach((function(e){var t,r=0,a=n&&n.match(/[^\x20\t\r\n\f\*\/\\]+/g);if(a&&1===e.nodeType)for(;t=a[r++];)e.removeAttribute(t)})),this},hasClass:function(n){return!!this[0]&&new RegExp("(\\s|^)"+n+"(\\s|$)").test(this[0].getAttribute("class"))},addClass:function(n){return this.forEach((function(e){var t=(0,i.default)(e),r=t.attr("class");t.hasClass(n)||t.attr("class",r+=" "+n)})),this},removeClass:function(n){return this.forEach((function(e){var t=(0,i.default)(e),r=t.attr("class");if(t.hasClass(n)){var a=new RegExp("(\\s|^)"+n+"(\\s|$)");t.attr("class",r.replace(a,""))}})),this}}),(0,o.default)(i.default,{extend:o.default,noop:function(){},navi:u,ua:u.userAgent,lang:u.language||u.languages[0],detect:s.default,store:l.default,escape:function(n){return n&&m.test(n)?n.replace(d,(function(n){return h[n]})):n},unescape:function(n){return n&&f.test(n)?n.replace(g,(function(n){return v[n]})):n},dynamicLoadSource:function(n,e){if((0,i.default)('script[src="'+n+'"]').length)e&&e();else{var t=p.createElement("script");t.onload=t.onreadystatechange=function(){this.onload=this.onreadystatechange=null,e&&e(),(0,i.default)(t).remove()},t.async=!0,t.setAttribute("referrerPolicy","no-referrer"),(0,i.default)("head")[0].appendChild(t),t.src=n}},sdkLoader:function(n,e,t){e in window&&window[e]?(k&&clearTimeout(k),t&&t()):i.default.dynamicLoadSource(n,(function(){k=setTimeout(i.default.sdkLoader(n,e,t),100)}))},deleteInWin:function(n,e){var t=function(e){if(n in window)try{delete window[n]}catch(e){window[n]=null}};0===e?t():setTimeout(t,e||500)},ajax:c.default}),e.default=i.default},function(n,e,t){"use strict";var r=t(9),a=t(0),o=a("%Function.prototype.apply%"),i=a("%Function.prototype.call%"),s=a("%Reflect.apply%",!0)||r.call(i,o),l=a("%Object.getOwnPropertyDescriptor%",!0),c=a("%Object.defineProperty%",!0),p=a("%Math.max%");if(c)try{c({},"a",{value:1})}catch(n){c=null}n.exports=function(n){var e=s(r,i,arguments);return l&&c&&l(e,"length").configurable&&c(e,"length",{value:1+p(0,n.length-(arguments.length-1))}),e};var u=function(){return s(r,o,arguments)};c?c(n.exports,"apply",{value:u}):n.exports.apply=u},function(n,e,t){"use strict";n.exports=t(62)},function(n,e,t){"use strict";e.__esModule=!0,e.DEFAULT_EMOJI_CDN="//img.t.sinajs.cn/t4/appstyle/expression/ext/normal/",e.DB_NAME="Comment",e.CONFIG={lang:"zh-CN",langMode:null,appId:"",appKey:"",clazzName:"Comment",meta:["nick","mail","link"],path:location.pathname,placeholder:"Just Go Go",pageSize:10,recordIP:!0,serverURLs:"",visitor:!1,mathJax:!1,emojiCDN:"",emojiMaps:void 0,enableQQ:!1,requiredFields:[]},e.defaultMeta=["nick","mail","link"],e.QQCacheKey="_v_Cache_Q",e.MetaCacheKey="_v_Cache_Meta",e.RandomStr=function(n){return(Date.now()+Math.round(1e3*Math.random())).toString(32)},e.VERSION="1.5.1"},function(n,e,t){var r=t(16),a=t(50);for(var o in(e=n.exports=function(n,e){return new a(e).process(n)}).FilterCSS=a,r)e[o]=r[o];"undefined"!=typeof window&&(window.filterCSS=n.exports)},function(n,e,t){"use strict";var r=t(66);n.exports=function(n){return"symbol"==typeof n?"Symbol":"bigint"==typeof n?"BigInt":r(n)}},function(n,e,t){"use strict";var r=t(78);n.exports=Function.prototype.bind||r},function(n,e,t){"use strict";var r,a,o=Function.prototype.toString,i="object"==typeof Reflect&&null!==Reflect&&Reflect.apply;if("function"==typeof i&&"function"==typeof Object.defineProperty)try{r=Object.defineProperty({},"length",{get:function(){throw a}}),a={},i((function(){throw 42}),null,r)}catch(n){n!==a&&(i=null)}else i=null;var s=/^\s*class\b/,l=function(n){try{var e=o.call(n);return s.test(e)}catch(n){return!1}},c=Object.prototype.toString,p="function"==typeof Symbol&&!!Symbol.toStringTag,u="object"==typeof document&&void 0===document.all&&void 0!==document.all?document.all:{};n.exports=i?function(n){if(n===u)return!0;if(!n)return!1;if("function"!=typeof n&&"object"!=typeof n)return!1;if("function"==typeof n&&!n.prototype)return!0;try{i(n,null,r)}catch(n){if(n!==a)return!1}return!l(n)}:function(n){if(n===u)return!0;if(!n)return!1;if("function"!=typeof n&&"object"!=typeof n)return!1;if("function"==typeof n&&!n.prototype)return!0;if(p)return function(n){try{return!l(n)&&(o.call(n),!0)}catch(n){return!1}}(n);if(l(n))return!1;var e=c.call(n);return"[object Function]"===e||"[object GeneratorFunction]"===e}},function(n,e){n.exports={indexOf:function(n,e){var t,r;if(Array.prototype.indexOf)return n.indexOf(e);for(t=0,r=n.length;t<r;t++)if(n[t]===e)return t;return-1},forEach:function(n,e,t){var r,a;if(Array.prototype.forEach)return n.forEach(e,t);for(r=0,a=n.length;r<a;r++)e.call(t,n[r],r,n)},trim:function(n){return String.prototype.trim?n.trim():n.replace(/(^\s*)|(\s*$)/g,"")},spaceIndex:function(n){var e=/\s|\n|\t/.exec(n);return e?e.index:-1}}},function(n,e,t){"use strict";e.__esModule=!0;var r={cdn:t(6).DEFAULT_EMOJI_CDN,maps:t(97),parse:function(n,e){return String(n).replace(new RegExp(":("+Object.keys(r.maps).join("|")+"):","ig"),(function(n,t){return r.maps[t]?r.build(t,e):n}))},build:function(n,e){var t=/^(https?:)?\/\//i,a=r.maps[n],o=t.test(a)?a:r.cdn+a,i=' <img alt="'+n+'" referrerpolicy="no-referrer" class="vemoji" src="'+o+'" />';return t.test(o)?i:""}};e.default=r},function(n,e,t){"use strict";e.__esModule=!0;var r=function(n){return n&&n.__esModule?n:{default:n}}(t(95));e.default=function(n){return(0,r.default)(n,{onTagAttr:function(n,e,t,r){return a(n,e,t,r)},onIgnoreTagAttr:function(n,e,t,r){return a(n,e,t,r)}}).replace(/\<\/?div\>/gi,"")};var a=function(n,e,t,a){if(/video|audio/i.test(n))return"";if(/code|pre|span/i.test(n)){if("style"==e){var o=t.match(/color:([#a-z0-9]{3,7}|\s+[#a-z0-9]{3,8})/gi);return o&&o.length?'style="'+o[0]+'"':""}if("class"==e)return e+"='"+r.default.escapeAttrValue(t)+"'"}return"a"===n&&"class"==e&&"at"===t?e+"='"+r.default.escapeAttrValue(t)+"'":"img"===n&&/src|class/i.test(e)?e+"='"+r.default.escapeAttrValue(t)+"' referrerPolicy='no-referrer'":void 0}},function(n,e,t){"use strict";var r=t(0),a=t(1),o=r("%TypeError%"),i=t(52),s=t(18),l=t(53),c=t(55),p=t(56),u=t(60),d=t(20),m=t(81),g=a("String.prototype.split"),f=Object("a"),h="a"!==f[0]||!(0 in f);n.exports=function(n){var e,t=u(this),r=h&&m(this)?g(this,""):t,a=p(r);if(!c(n))throw new o("Array.prototype.forEach callback must be a function");arguments.length>1&&(e=arguments[1]);for(var f=0;f<a;){var v=d(f);if(l(r,v)){var b=s(r,v);i(n,e,[b,f,r])}f+=1}}},function(n,e,t){"use strict";var r=t(75),a=t(14);n.exports=function(){var n=Array.prototype.forEach;return r(n)?n:a}},function(n,e){function t(){var n={"align-content":!1,"align-items":!1,"align-self":!1,"alignment-adjust":!1,"alignment-baseline":!1,all:!1,"anchor-point":!1,animation:!1,"animation-delay":!1,"animation-direction":!1,"animation-duration":!1,"animation-fill-mode":!1,"animation-iteration-count":!1,"animation-name":!1,"animation-play-state":!1,"animation-timing-function":!1,azimuth:!1,"backface-visibility":!1,background:!0,"background-attachment":!0,"background-clip":!0,"background-color":!0,"background-image":!0,"background-origin":!0,"background-position":!0,"background-repeat":!0,"background-size":!0,"baseline-shift":!1,binding:!1,bleed:!1,"bookmark-label":!1,"bookmark-level":!1,"bookmark-state":!1,border:!0,"border-bottom":!0,"border-bottom-color":!0,"border-bottom-left-radius":!0,"border-bottom-right-radius":!0,"border-bottom-style":!0,"border-bottom-width":!0,"border-collapse":!0,"border-color":!0,"border-image":!0,"border-image-outset":!0,"border-image-repeat":!0,"border-image-slice":!0,"border-image-source":!0,"border-image-width":!0,"border-left":!0,"border-left-color":!0,"border-left-style":!0,"border-left-width":!0,"border-radius":!0,"border-right":!0,"border-right-color":!0,"border-right-style":!0,"border-right-width":!0,"border-spacing":!0,"border-style":!0,"border-top":!0,"border-top-color":!0,"border-top-left-radius":!0,"border-top-right-radius":!0,"border-top-style":!0,"border-top-width":!0,"border-width":!0,bottom:!1,"box-decoration-break":!0,"box-shadow":!0,"box-sizing":!0,"box-snap":!0,"box-suppress":!0,"break-after":!0,"break-before":!0,"break-inside":!0,"caption-side":!1,chains:!1,clear:!0,clip:!1,"clip-path":!1,"clip-rule":!1,color:!0,"color-interpolation-filters":!0,"column-count":!1,"column-fill":!1,"column-gap":!1,"column-rule":!1,"column-rule-color":!1,"column-rule-style":!1,"column-rule-width":!1,"column-span":!1,"column-width":!1,columns:!1,contain:!1,content:!1,"counter-increment":!1,"counter-reset":!1,"counter-set":!1,crop:!1,cue:!1,"cue-after":!1,"cue-before":!1,cursor:!1,direction:!1,display:!0,"display-inside":!0,"display-list":!0,"display-outside":!0,"dominant-baseline":!1,elevation:!1,"empty-cells":!1,filter:!1,flex:!1,"flex-basis":!1,"flex-direction":!1,"flex-flow":!1,"flex-grow":!1,"flex-shrink":!1,"flex-wrap":!1,float:!1,"float-offset":!1,"flood-color":!1,"flood-opacity":!1,"flow-from":!1,"flow-into":!1,font:!0,"font-family":!0,"font-feature-settings":!0,"font-kerning":!0,"font-language-override":!0,"font-size":!0,"font-size-adjust":!0,"font-stretch":!0,"font-style":!0,"font-synthesis":!0,"font-variant":!0,"font-variant-alternates":!0,"font-variant-caps":!0,"font-variant-east-asian":!0,"font-variant-ligatures":!0,"font-variant-numeric":!0,"font-variant-position":!0,"font-weight":!0,grid:!1,"grid-area":!1,"grid-auto-columns":!1,"grid-auto-flow":!1,"grid-auto-rows":!1,"grid-column":!1,"grid-column-end":!1,"grid-column-start":!1,"grid-row":!1,"grid-row-end":!1,"grid-row-start":!1,"grid-template":!1,"grid-template-areas":!1,"grid-template-columns":!1,"grid-template-rows":!1,"hanging-punctuation":!1,height:!0,hyphens:!1,icon:!1,"image-orientation":!1,"image-resolution":!1,"ime-mode":!1,"initial-letters":!1,"inline-box-align":!1,"justify-content":!1,"justify-items":!1,"justify-self":!1,left:!1,"letter-spacing":!0,"lighting-color":!0,"line-box-contain":!1,"line-break":!1,"line-grid":!1,"line-height":!1,"line-snap":!1,"line-stacking":!1,"line-stacking-ruby":!1,"line-stacking-shift":!1,"line-stacking-strategy":!1,"list-style":!0,"list-style-image":!0,"list-style-position":!0,"list-style-type":!0,margin:!0,"margin-bottom":!0,"margin-left":!0,"margin-right":!0,"margin-top":!0,"marker-offset":!1,"marker-side":!1,marks:!1,mask:!1,"mask-box":!1,"mask-box-outset":!1,"mask-box-repeat":!1,"mask-box-slice":!1,"mask-box-source":!1,"mask-box-width":!1,"mask-clip":!1,"mask-image":!1,"mask-origin":!1,"mask-position":!1,"mask-repeat":!1,"mask-size":!1,"mask-source-type":!1,"mask-type":!1,"max-height":!0,"max-lines":!1,"max-width":!0,"min-height":!0,"min-width":!0,"move-to":!1,"nav-down":!1,"nav-index":!1,"nav-left":!1,"nav-right":!1,"nav-up":!1,"object-fit":!1,"object-position":!1,opacity:!1,order:!1,orphans:!1,outline:!1,"outline-color":!1,"outline-offset":!1,"outline-style":!1,"outline-width":!1,overflow:!1,"overflow-wrap":!1,"overflow-x":!1,"overflow-y":!1,padding:!0,"padding-bottom":!0,"padding-left":!0,"padding-right":!0,"padding-top":!0,page:!1,"page-break-after":!1,"page-break-before":!1,"page-break-inside":!1,"page-policy":!1,pause:!1,"pause-after":!1,"pause-before":!1,perspective:!1,"perspective-origin":!1,pitch:!1,"pitch-range":!1,"play-during":!1,position:!1,"presentation-level":!1,quotes:!1,"region-fragment":!1,resize:!1,rest:!1,"rest-after":!1,"rest-before":!1,richness:!1,right:!1,rotation:!1,"rotation-point":!1,"ruby-align":!1,"ruby-merge":!1,"ruby-position":!1,"shape-image-threshold":!1,"shape-outside":!1,"shape-margin":!1,size:!1,speak:!1,"speak-as":!1,"speak-header":!1,"speak-numeral":!1,"speak-punctuation":!1,"speech-rate":!1,stress:!1,"string-set":!1,"tab-size":!1,"table-layout":!1,"text-align":!0,"text-align-last":!0,"text-combine-upright":!0,"text-decoration":!0,"text-decoration-color":!0,"text-decoration-line":!0,"text-decoration-skip":!0,"text-decoration-style":!0,"text-emphasis":!0,"text-emphasis-color":!0,"text-emphasis-position":!0,"text-emphasis-style":!0,"text-height":!0,"text-indent":!0,"text-justify":!0,"text-orientation":!0,"text-overflow":!0,"text-shadow":!0,"text-space-collapse":!0,"text-transform":!0,"text-underline-position":!0,"text-wrap":!0,top:!1,transform:!1,"transform-origin":!1,"transform-style":!1,transition:!1,"transition-delay":!1,"transition-duration":!1,"transition-property":!1,"transition-timing-function":!1,"unicode-bidi":!1,"vertical-align":!1,visibility:!1,"voice-balance":!1,"voice-duration":!1,"voice-family":!1,"voice-pitch":!1,"voice-range":!1,"voice-rate":!1,"voice-stress":!1,"voice-volume":!1,volume:!1,"white-space":!1,widows:!1,width:!0,"will-change":!1,"word-break":!0,"word-spacing":!0,"word-wrap":!0,"wrap-flow":!1,"wrap-through":!1,"writing-mode":!1,"z-index":!1};return n}var r=/javascript\s*\:/gim;e.whiteList=t(),e.getDefaultWhiteList=t,e.onAttr=function(n,e,t){},e.onIgnoreAttr=function(n,e,t){},e.safeAttrValue=function(n,e){return r.test(e)?"":e}},function(n,e){n.exports={indexOf:function(n,e){var t,r;if(Array.prototype.indexOf)return n.indexOf(e);for(t=0,r=n.length;t<r;t++)if(n[t]===e)return t;return-1},forEach:function(n,e,t){var r,a;if(Array.prototype.forEach)return n.forEach(e,t);for(r=0,a=n.length;r<a;r++)e.call(t,n[r],r,n)},trim:function(n){return String.prototype.trim?n.trim():n.replace(/(^\s*)|(\s*$)/g,"")},trimRight:function(n){return String.prototype.trimRight?n.trimRight():n.replace(/(\s*$)/g,"")}}},function(n,e,t){"use strict";var r=t(0)("%TypeError%"),a=t(86),o=t(19),i=t(8);n.exports=function(n,e){if("Object"!==i(n))throw new r("Assertion failed: Type(O) is not Object");if(!o(e))throw new r("Assertion failed: IsPropertyKey(P) is not true, got "+a(e));return n[e]}},function(n,e,t){"use strict";n.exports=function(n){return"string"==typeof n||"symbol"==typeof n}},function(n,e,t){"use strict";var r=t(0),a=r("%String%"),o=r("%TypeError%");n.exports=function(n){if("symbol"==typeof n)throw new o("Cannot convert a Symbol value to a string");return a(n)}},function(n,e,t){"use strict";n.exports=function(n){return null===n||"function"!=typeof n&&"object"!=typeof n}},function(n,e,t){"use strict";var r="undefined"!=typeof Symbol&&Symbol,a=t(23);n.exports=function(){return"function"==typeof r&&"function"==typeof Symbol&&"symbol"==typeof r("foo")&&"symbol"==typeof Symbol("bar")&&a()}},function(n,e,t){"use strict";n.exports=function(){if("function"!=typeof Symbol||"function"!=typeof Object.getOwnPropertySymbols)return!1;if("symbol"==typeof Symbol.iterator)return!0;var n={},e=Symbol("test"),t=Object(e);if("string"==typeof e)return!1;if("[object Symbol]"!==Object.prototype.toString.call(e))return!1;if("[object Symbol]"!==Object.prototype.toString.call(t))return!1;for(e in n[e]=42,n)return!1;if("function"==typeof Object.keys&&0!==Object.keys(n).length)return!1;if("function"==typeof Object.getOwnPropertyNames&&0!==Object.getOwnPropertyNames(n).length)return!1;var r=Object.getOwnPropertySymbols(n);if(1!==r.length||r[0]!==e)return!1;if(!Object.prototype.propertyIsEnumerable.call(n,e))return!1;if("function"==typeof Object.getOwnPropertyDescriptor){var a=Object.getOwnPropertyDescriptor(n,e);if(42!==a.value||!0!==a.enumerable)return!1}return!0}},function(n,e,t){"use strict";var r=t(23);n.exports=function(){return r()&&!!Symbol.toStringTag}},function(n,e,t){"use strict";var r=t(9);n.exports=r.call(Function.call,Object.prototype.hasOwnProperty)},function(n,e,t){"use strict";var r=Object.prototype.toString;n.exports=function(n){var e=r.call(n),t="[object Arguments]"===e;return t||(t="[object Array]"!==e&&null!==n&&"object"==typeof n&&"number"==typeof n.length&&n.length>=0&&"[object Function]"===r.call(n.callee)),t}},function(n,e,t){"use strict";var r=t(5),a=t(1),o=a("Object.prototype.propertyIsEnumerable"),i=a("Array.prototype.push");n.exports=function(n){var e=r(n),t=[];for(var a in e)o(e,a)&&i(t,[a,e[a]]);return t}},function(n,e,t){"use strict";var r=t(27);n.exports=function(){return"function"==typeof Object.entries?Object.entries:r}},function(n,e,t){"use strict";var r=t(5),a=t(20),o=t(1)("String.prototype.replace"),i=/^[\x09\x0A\x0B\x0C\x0D\x20\xA0\u1680\u180E\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200A\u202F\u205F\u3000\u2028\u2029\uFEFF]+/,s=/[\x09\x0A\x0B\x0C\x0D\x20\xA0\u1680\u180E\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200A\u202F\u205F\u3000\u2028\u2029\uFEFF]+$/;n.exports=function(){var n=a(r(this));return o(o(n,i,""),s,"")}},function(n,e,t){"use strict";var r=t(29);n.exports=function(){return String.prototype.trim&&"​"==="​".trim()?String.prototype.trim:r}},function(n,e,t){function r(){return{a:["target","href","title"],abbr:["title"],address:[],area:["shape","coords","href","alt"],article:[],aside:[],audio:["autoplay","controls","crossorigin","loop","muted","preload","src"],b:[],bdi:["dir"],bdo:["dir"],big:[],blockquote:["cite"],br:[],caption:[],center:[],cite:[],code:[],col:["align","valign","span","width"],colgroup:["align","valign","span","width"],dd:[],del:["datetime"],details:["open"],div:[],dl:[],dt:[],em:[],figcaption:[],figure:[],font:["color","size","face"],footer:[],h1:[],h2:[],h3:[],h4:[],h5:[],h6:[],header:[],hr:[],i:[],img:["src","alt","title","width","height"],ins:["datetime"],li:[],mark:[],nav:[],ol:[],p:[],pre:[],s:[],section:[],small:[],span:[],sub:[],summary:[],sup:[],strong:[],strike:[],table:["width","border","align","valign"],tbody:["align","valign"],td:["width","rowspan","colspan","align","valign"],tfoot:["align","valign"],th:["width","rowspan","colspan","align","valign"],thead:["align","valign"],tr:["rowspan","align","valign"],tt:[],u:[],ul:[],video:["autoplay","controls","crossorigin","loop","muted","playsinline","poster","preload","src","height","width"]}}function a(n){return n.replace(h,"&lt;").replace(v,"&gt;")}function o(n){return n.replace(b,"&quot;")}function i(n){return n.replace(k,'"')}function s(n){return n.replace(y,(function(n,e){return"x"===e[0]||"X"===e[0]?String.fromCharCode(parseInt(e.substr(1),16)):String.fromCharCode(parseInt(e,10))}))}function l(n){return n.replace(S,":").replace(x," ")}function c(n){for(var e="",t=0,r=n.length;t<r;t++)e+=n.charCodeAt(t)<32?" ":n.charAt(t);return g.trim(e)}function p(n){return c(n=l(n=s(n=i(n))))}function u(n){return a(n=o(n))}var d=t(7).FilterCSS,m=t(7).getDefaultWhiteList,g=t(11),f=new d,h=/</g,v=/>/g,b=/"/g,k=/&quot;/g,y=/&#([a-zA-Z0-9]*);?/gim,S=/&colon;?/gim,x=/&newline;?/gim,w=/((j\s*a\s*v\s*a|v\s*b|l\s*i\s*v\s*e)\s*s\s*c\s*r\s*i\s*p\s*t\s*|m\s*o\s*c\s*h\s*a)\:/gi,E=/e\s*x\s*p\s*r\s*e\s*s\s*s\s*i\s*o\s*n\s*\(.*/gi,D=/u\s*r\s*l\s*\(.*/gi;e.whiteList={a:["target","href","title"],abbr:["title"],address:[],area:["shape","coords","href","alt"],article:[],aside:[],audio:["autoplay","controls","crossorigin","loop","muted","preload","src"],b:[],bdi:["dir"],bdo:["dir"],big:[],blockquote:["cite"],br:[],caption:[],center:[],cite:[],code:[],col:["align","valign","span","width"],colgroup:["align","valign","span","width"],dd:[],del:["datetime"],details:["open"],div:[],dl:[],dt:[],em:[],figcaption:[],figure:[],font:["color","size","face"],footer:[],h1:[],h2:[],h3:[],h4:[],h5:[],h6:[],header:[],hr:[],i:[],img:["src","alt","title","width","height"],ins:["datetime"],li:[],mark:[],nav:[],ol:[],p:[],pre:[],s:[],section:[],small:[],span:[],sub:[],summary:[],sup:[],strong:[],strike:[],table:["width","border","align","valign"],tbody:["align","valign"],td:["width","rowspan","colspan","align","valign"],tfoot:["align","valign"],th:["width","rowspan","colspan","align","valign"],thead:["align","valign"],tr:["rowspan","align","valign"],tt:[],u:[],ul:[],video:["autoplay","controls","crossorigin","loop","muted","playsinline","poster","preload","src","height","width"]},e.getDefaultWhiteList=r,e.onTag=function(n,e,t){},e.onIgnoreTag=function(n,e,t){},e.onTagAttr=function(n,e,t){},e.onIgnoreTagAttr=function(n,e,t){},e.safeAttrValue=function(n,e,t,r){if(t=p(t),"href"===e||"src"===e){if("#"===(t=g.trim(t)))return"#";if("http://"!==t.substr(0,7)&&"https://"!==t.substr(0,8)&&"mailto:"!==t.substr(0,7)&&"tel:"!==t.substr(0,4)&&"data:image/"!==t.substr(0,11)&&"ftp://"!==t.substr(0,6)&&"./"!==t.substr(0,2)&&"../"!==t.substr(0,3)&&"#"!==t[0]&&"/"!==t[0])return""}else if("background"===e){if(w.lastIndex=0,w.test(t))return""}else if("style"===e){if(E.lastIndex=0,E.test(t))return"";if(D.lastIndex=0,D.test(t)&&(w.lastIndex=0,w.test(t)))return"";!1!==r&&(t=(r=r||f).process(t))}return u(t)},e.escapeHtml=a,e.escapeQuote=o,e.unescapeQuote=i,e.escapeHtmlEntities=s,e.escapeDangerHtml5Entities=l,e.clearNonPrintableCharacter=c,e.friendlyAttrValue=p,e.escapeAttrValue=u,e.onIgnoreTagStripAll=function(){return""},e.StripTagBody=function(n,e){function t(e){return!!r||-1!==g.indexOf(n,e)}"function"!=typeof e&&(e=function(){});var r=!Array.isArray(n),a=[],o=!1;return{onIgnoreTag:function(n,r,i){if(t(n)){if(i.isClosing){var s="[/removed]",l=i.position+s.length;return a.push([!1!==o?o:i.position,l]),o=!1,s}return o||(o=i.position),"[removed]"}return e(n,r,i)},remove:function(n){var e="",t=0;return g.forEach(a,(function(r){e+=n.slice(t,r[0]),t=r[1]})),e+=n.slice(t)}}},e.stripCommentTag=function(n){for(var e="",t=0;t<n.length;){var r=n.indexOf("\x3c!--",t);if(-1===r){e+=n.slice(t);break}e+=n.slice(t,r);var a=n.indexOf("--\x3e",r);if(-1===a)break;t=a+3}return e},e.stripBlankChar=function(n){var e=n.split("");return(e=e.filter((function(n){var e=n.charCodeAt(0);return!(127===e||e<=31&&10!==e&&13!==e)}))).join("")},e.cssFilter=f,e.getDefaultCSSWhiteList=m},function(n,e,t){function r(n){var e=l.spaceIndex(n);if(-1===e)var t=n.slice(1,-1);else t=n.slice(1,e+1);return"/"===(t=l.trim(t).toLowerCase()).slice(0,1)&&(t=t.slice(1)),"/"===t.slice(-1)&&(t=t.slice(0,-1)),t}function a(n){return"</"===n.slice(0,2)}function o(n,e){for(;e<n.length;e++){var t=n[e];if(" "!==t)return"="===t?e:-1}}function i(n,e){for(;e>0;e--){var t=n[e];if(" "!==t)return"="===t?e:-1}}function s(n){return function(n){return'"'===n[0]&&'"'===n[n.length-1]||"'"===n[0]&&"'"===n[n.length-1]}(n)?n.substr(1,n.length-2):n}var l=t(11),c=/[^a-zA-Z0-9_:\.\-]/gim;e.parseTag=function(n,e,t){"use strict";var o="",i=0,s=!1,l=!1,c=0,p=n.length,u="",d="";n:for(c=0;c<p;c++){var m=n.charAt(c);if(!1===s){if("<"===m){s=c;continue}}else if(!1===l){if("<"===m){o+=t(n.slice(i,c)),s=c,i=c;continue}if(">"===m){o+=t(n.slice(i,s)),u=r(d=n.slice(s,c+1)),o+=e(s,o.length,u,d,a(d)),i=c+1,s=!1;continue}if('"'===m||"'"===m)for(var g=1,f=n.charAt(c-g);""===f.trim()||"="===f;){if("="===f){l=m;continue n}f=n.charAt(c-++g)}}else if(m===l){l=!1;continue}}return i<n.length&&(o+=t(n.substr(i))),o},e.parseAttr=function(n,e){"use strict";function t(n,t){if(!((n=(n=l.trim(n)).replace(c,"").toLowerCase()).length<1)){var r=e(n,t||"");r&&a.push(r)}}for(var r=0,a=[],p=!1,u=n.length,d=0;d<u;d++){var m,g=n.charAt(d);if(!1!==p||"="!==g)if(!1===p||d!==r||'"'!==g&&"'"!==g||"="!==n.charAt(d-1)){if(/\s|\n|\t/.test(g)){if(n=n.replace(/\s|\n|\t/g," "),!1===p){if(-1===(m=o(n,d))){t(l.trim(n.slice(r,d))),p=!1,r=d+1;continue}d=m-1;continue}if(-1===(m=i(n,d-1))){t(p,s(l.trim(n.slice(r,d)))),p=!1,r=d+1;continue}}}else{if(-1===(m=n.indexOf(g,d+1)))break;t(p,l.trim(n.slice(r+1,m))),p=!1,r=(d=m)+1}else p=n.slice(r,d),r=d+1}return r<n.length&&(!1===p?t(n.slice(r)):t(p,s(l.trim(n.slice(r))))),l.trim(a.join(" "))}},function(n,e,t){var r,a,o;
/*!
	autosize 4.0.4
	license: MIT
	http://www.jacklmoore.com/autosize
*/a=[n,e],r=function(n,e){"use strict";function t(n){function e(e){var t=n.style.width;n.style.width="0px",n.offsetWidth,n.style.width=t,n.style.overflowY=e}function t(){if(0!==n.scrollHeight){var e=function(n){for(var e=[];n&&n.parentNode&&n.parentNode instanceof Element;)n.parentNode.scrollTop&&e.push({node:n.parentNode,scrollTop:n.parentNode.scrollTop}),n=n.parentNode;return e}(n),t=document.documentElement&&document.documentElement.scrollTop;n.style.height="",n.style.height=n.scrollHeight+a+"px",s=n.clientWidth,e.forEach((function(n){n.node.scrollTop=n.scrollTop})),t&&(document.documentElement.scrollTop=t)}}function r(){t();var r=Math.round(parseFloat(n.style.height)),a=window.getComputedStyle(n,null),o="content-box"===a.boxSizing?Math.round(parseFloat(a.height)):n.offsetHeight;if(o<r?"hidden"===a.overflowY&&(e("scroll"),t(),o="content-box"===a.boxSizing?Math.round(parseFloat(window.getComputedStyle(n,null).height)):n.offsetHeight):"hidden"!==a.overflowY&&(e("hidden"),t(),o="content-box"===a.boxSizing?Math.round(parseFloat(window.getComputedStyle(n,null).height)):n.offsetHeight),l!==o){l=o;var s=i("autosize:resized");try{n.dispatchEvent(s)}catch(n){}}}if(n&&n.nodeName&&"TEXTAREA"===n.nodeName&&!o.has(n)){var a=null,s=null,l=null,c=function(){n.clientWidth!==s&&r()},p=function(e){window.removeEventListener("resize",c,!1),n.removeEventListener("input",r,!1),n.removeEventListener("keyup",r,!1),n.removeEventListener("autosize:destroy",p,!1),n.removeEventListener("autosize:update",r,!1),Object.keys(e).forEach((function(t){n.style[t]=e[t]})),o.delete(n)}.bind(n,{height:n.style.height,resize:n.style.resize,overflowY:n.style.overflowY,overflowX:n.style.overflowX,wordWrap:n.style.wordWrap});n.addEventListener("autosize:destroy",p,!1),"onpropertychange"in n&&"oninput"in n&&n.addEventListener("keyup",r,!1),window.addEventListener("resize",c,!1),n.addEventListener("input",r,!1),n.addEventListener("autosize:update",r,!1),n.style.overflowX="hidden",n.style.wordWrap="break-word",o.set(n,{destroy:p,update:r}),function(){var e=window.getComputedStyle(n,null);"vertical"===e.resize?n.style.resize="none":"both"===e.resize&&(n.style.resize="horizontal"),a="content-box"===e.boxSizing?-(parseFloat(e.paddingTop)+parseFloat(e.paddingBottom)):parseFloat(e.borderTopWidth)+parseFloat(e.borderBottomWidth),isNaN(a)&&(a=0),r()}()}}function r(n){var e=o.get(n);e&&e.destroy()}function a(n){var e=o.get(n);e&&e.update()}var o="function"==typeof Map?new Map:function(){var n=[],e=[];return{has:function(e){return n.indexOf(e)>-1},get:function(t){return e[n.indexOf(t)]},set:function(t,r){-1===n.indexOf(t)&&(n.push(t),e.push(r))},delete:function(t){var r=n.indexOf(t);r>-1&&(n.splice(r,1),e.splice(r,1))}}}(),i=function(n){return new Event(n,{bubbles:!0})};try{new Event("test")}catch(n){i=function(n){var e=document.createEvent("Event");return e.initEvent(n,!0,!1),e}}var s=null;"undefined"==typeof window||"function"!=typeof window.getComputedStyle?((s=function(n){return n}).destroy=function(n){return n},s.update=function(n){return n}):((s=function(n,e){return n&&Array.prototype.forEach.call(n.length?n:[n],(function(n){return t(n)})),n}).destroy=function(n){return n&&Array.prototype.forEach.call(n.length?n:[n],r),n},s.update=function(n){return n&&Array.prototype.forEach.call(n.length?n:[n],a),n}),e.default=s,n.exports=e.default},void 0!==(o="function"==typeof r?r.apply(e,a):r)&&(n.exports=o)},function(n,e,t){"use strict";function r(n){return n&&n.__esModule?n:{default:n}}function a(n){return!!n&&this.init(n),this}function o(n){return new a(n)}var i=r(t(42)),s=r(t(33)),l=r(t(37)),c=r(t(12)),p=t(6),u=r(t(41)),d=r(t(36)),m=t(40),g=r(t(38)),f=r(t(3)),h=r(t(39)),v=r(t(13)),b=(r(t(35)),{comment:"",nick:"",mail:"",link:"",ua:f.default.ua,url:"",QQAvatar:""}),k="",y={},S={cdn:"https://gravatar.loli.net/avatar/",ds:["mp","identicon","monsterid","wavatar","robohash","retro",""],params:"",hide:!1};a.prototype.init=function(n){if("undefined"==typeof document)throw new Error("Sorry, Valine does not support Server-side rendering.");var e=this;return n&&(n=f.default.extend(p.CONFIG,n),e.i18n=(0,l.default)(n.lang||f.default.lang,n.langMode),e.cfg=n,c.default.maps=!!n.emojiMaps&&n.emojiMaps||c.default.maps,c.default.cdn=!!n.emojiCDN&&n.emojiCDN||c.default.cdn,e._init()),e},a.prototype._init=function(){var n=this;try{var e=n.cfg,t=e.avatar,r=e.avatarForce,a=e.avatar_cdn,o=e.visitor,i=e.path,s=void 0===i?location.pathname:i,l=e.pageSize,c=e.recordIP;n.cfg.path=s.replace(/index\.html?$/,"");var u=S.ds,h=r?"&q="+(0,p.RandomStr)():"";S.params="?d="+(u.indexOf(t)>-1?t:"mp")+"&v="+p.VERSION+h,S.hide="hide"===t,S.cdn=/^https?\:\/\//.test(a)?a:S.cdn,n.cfg.pageSize=isNaN(l)||l<1?10:l,c&&(0,m.recordIPFn)((function(n){return b.ip=n}));var v=n.cfg.el||null,k=(0,f.default)(v);if(v=v instanceof HTMLElement?v:k[k.length-1]||null){n.$el=(0,f.default)(v),n.$el.addClass("v").attr("data-class","v"),S.hide&&n.$el.addClass("hide-avatar"),n.cfg.meta=(n.cfg.guest_info||n.cfg.meta||p.defaultMeta).filter((function(n){return p.defaultMeta.indexOf(n)>-1})),n.cfg.requiredFields=n.cfg.requiredFields.filter((function(n){return p.defaultMeta.indexOf(n)>-1}));var y=(0==n.cfg.meta.length?p.defaultMeta:n.cfg.meta).map((function(e){var t="mail"==e?"email":"text";return p.defaultMeta.indexOf(e)>-1?'<input name="'+e+'" placeholder="'+n.i18n.t(e)+'" class="v'+e+' vinput" type="'+t+'">':""})),x='<div class="vpanel"><div class="vwrap"><p class="cancel-reply text-right" style="display:none;" title="'+n.i18n.t("cancelReply")+'"><svg class="vicon cancel-reply-btn" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4220" width="22" height="22"><path d="M796.454 985H227.545c-50.183 0-97.481-19.662-133.183-55.363-35.7-35.701-55.362-83-55.362-133.183V227.545c0-50.183 19.662-97.481 55.363-133.183 35.701-35.7 83-55.362 133.182-55.362h568.909c50.183 0 97.481 19.662 133.183 55.363 35.701 35.702 55.363 83 55.363 133.183v568.909c0 50.183-19.662 97.481-55.363 133.183S846.637 985 796.454 985zM227.545 91C152.254 91 91 152.254 91 227.545v568.909C91 871.746 152.254 933 227.545 933h568.909C871.746 933 933 871.746 933 796.454V227.545C933 152.254 871.746 91 796.454 91H227.545z" p-id="4221"></path><path d="M568.569 512l170.267-170.267c15.556-15.556 15.556-41.012 0-56.569s-41.012-15.556-56.569 0L512 455.431 341.733 285.165c-15.556-15.556-41.012-15.556-56.569 0s-15.556 41.012 0 56.569L455.431 512 285.165 682.267c-15.556 15.556-15.556 41.012 0 56.569 15.556 15.556 41.012 15.556 56.569 0L512 568.569l170.267 170.267c15.556 15.556 41.012 15.556 56.569 0 15.556-15.556 15.556-41.012 0-56.569L568.569 512z" p-id="4222" ></path></svg></p><div class="vheader item'+y.length+'">'+y.join("")+'</div><div class="vedit"><textarea id="veditor" class="veditor vinput" placeholder="'+n.cfg.placeholder+'"></textarea><div class="vrow"><div class="vcol vcol-60 status-bar"></div><div class="vcol vcol-40 vctrl text-right"><span title="'+n.i18n.t("emoji")+'"  class="vicon vemoji-btn"><svg  viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="16172" width="22" height="22" ><path d="M512 1024a512 512 0 1 1 512-512 512 512 0 0 1-512 512zM512 56.888889a455.111111 455.111111 0 1 0 455.111111 455.111111 455.111111 455.111111 0 0 0-455.111111-455.111111zM312.888889 512A85.333333 85.333333 0 1 1 398.222222 426.666667 85.333333 85.333333 0 0 1 312.888889 512z" p-id="16173"></path><path d="M512 768A142.222222 142.222222 0 0 1 369.777778 625.777778a28.444444 28.444444 0 0 1 56.888889 0 85.333333 85.333333 0 0 0 170.666666 0 28.444444 28.444444 0 0 1 56.888889 0A142.222222 142.222222 0 0 1 512 768z" p-id="16174"></path><path d="M782.222222 391.964444l-113.777778 59.733334a29.013333 29.013333 0 0 1-38.684444-10.808889 28.444444 28.444444 0 0 1 10.24-38.684445l113.777778-56.888888a28.444444 28.444444 0 0 1 38.684444 10.24 28.444444 28.444444 0 0 1-10.24 36.408888z" p-id="16175"></path><path d="M640.568889 451.697778l113.777778 56.888889a27.875556 27.875556 0 0 0 38.684444-10.24 27.875556 27.875556 0 0 0-10.24-38.684445l-113.777778-56.888889a28.444444 28.444444 0 0 0-38.684444 10.808889 28.444444 28.444444 0 0 0 10.24 38.115556z" p-id="16176"></path></svg></span><span title="'+n.i18n.t("preview")+'" class="vicon vpreview-btn"><svg  viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="17688" width="22" height="22"><path d="M502.390154 935.384615a29.538462 29.538462 0 1 1 0 59.076923H141.430154C79.911385 994.461538 29.538462 946.254769 29.538462 886.153846V137.846154C29.538462 77.745231 79.950769 29.538462 141.390769 29.538462h741.218462c61.44 0 111.852308 48.206769 111.852307 108.307692v300.268308a29.538462 29.538462 0 1 1-59.076923 0V137.846154c0-26.899692-23.355077-49.230769-52.775384-49.230769H141.390769c-29.420308 0-52.775385 22.331077-52.775384 49.230769v748.307692c0 26.899692 23.355077 49.230769 52.775384 49.230769h360.999385z" p-id="17689"></path><path d="M196.923077 216.615385m29.538461 0l374.153847 0q29.538462 0 29.538461 29.538461l0 0q0 29.538462-29.538461 29.538462l-374.153847 0q-29.538462 0-29.538461-29.538462l0 0q0-29.538462 29.538461-29.538461Z" p-id="17690"></path><path d="M649.846154 846.769231a216.615385 216.615385 0 1 0 0-433.230769 216.615385 216.615385 0 0 0 0 433.230769z m0 59.076923a275.692308 275.692308 0 1 1 0-551.384616 275.692308 275.692308 0 0 1 0 551.384616z" p-id="17691"></path><path d="M807.398383 829.479768m20.886847-20.886846l0 0q20.886846-20.886846 41.773692 0l125.321079 125.321079q20.886846 20.886846 0 41.773693l0 0q-20.886846 20.886846-41.773693 0l-125.321078-125.321079q-20.886846-20.886846 0-41.773693Z" p-id="17692"></path></svg></span></div></div></div><div class="vrow"><div class="vcol vcol-30" ><a alt="Markdown is supported" href="https://guides.github.com/features/mastering-markdown/" class="vicon" target="_blank"><svg class="markdown" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14.85 3H1.15C.52 3 0 3.52 0 4.15v7.69C0 12.48.52 13 1.15 13h13.69c.64 0 1.15-.52 1.15-1.15v-7.7C16 3.52 15.48 3 14.85 3zM9 11H7V8L5.5 9.92 4 8v3H2V5h2l1.5 2L7 5h2v6zm2.99.5L9.5 8H11V5h2v3h1.5l-2.51 3.5z"></path></svg></a></div><div class="vcol vcol-70 text-right"><button type="button"  title="Cmd|Ctrl+Enter" class="vsubmit vbtn">'+n.i18n.t("submit")+'</button></div></div><div class="vemojis" style="display:none;"></div><div class="vinput vpreview" style="display:none;"></div></div></div><div class="vcount" style="display:none;"><span class="vnum">0</span> '+n.i18n.t("comments")+'</div><div class="vload-top text-center" style="display:none;"><i class="vspinner" style="width:30px;height:30px;"></i></div><div class="vcards"></div><div class="vload-bottom text-center" style="display:none;"><i class="vspinner" style="width:30px;height:30px;"></i></div><div class="vempty" style="display:none;"></div><div class="vpage txt-center" style="display:none"><button type="button" class="vmore vbtn">'+n.i18n.t("more")+'</button></div><div class="vpower txt-right">Powered By <a href="https://valine.js.org" target="_blank">Valine</a><br>v'+p.VERSION+"</div>";n.$el.html(x),n.$el.find(".cancel-reply").on("click",(function(e){n.reset()}));var E=n.$el.find(".vempty");n.$nodata={show:function(e){return E.html(e||n.i18n.t("sofa")).show(),n},hide:function(){return E.hide(),n}};var D=n.$el.find(".vload-bottom"),C=n.$el.find(".vload-top");n.$loading={show:function(e){return e&&C.show()||D.show(),n.$nodata.hide(),n},hide:function(){return C.hide(),D.hide(),0===n.$el.find(".vcard").length&&n.$nodata.show(),n}}}(0,d.default)(n.cfg,(function(e){var t=(0,f.default)(".valine-comment-count"),r=0;!function e(t){var a=t[r++];if(a){var o=(0,f.default)(a).attr("data-xid");o&&n.Q(o).count().then((function(n){a.innerText=n,e(t)})).catch((function(n){a.innerText=0}))}}(t),o&&w.add(AV.Object.extend("Counter"),n.cfg.path),n.$el&&n.bind()}))}catch(e){(0,g.default)(n,e,"init")}};var x=function(n,e){var t=new n,r=new AV.ACL;r.setPublicReadAccess(!0),r.setPublicWriteAccess(!0),t.setACL(r),t.set("url",e.url),t.set("xid",e.xid),t.set("title",e.title),t.set("time",1),t.save().then((function(n){(0,f.default)(e.el).find(".leancloud-visitors-count").text(1)})).catch((function(n){}))},w={add:function(n,e){var t=this,r=(0,f.default)(".leancloud_visitors,.leancloud-visitors");if(1===r.length){var a=r[0],o=decodeURI((0,f.default)(a).attr("id")),i=(0,f.default)(a).attr("data-flag-title"),s=encodeURI(o),l={el:a,url:o,xid:s,title:i};if(decodeURI(o)===decodeURI(e)){var c=new AV.Query(n);c.equalTo("url",o),c.find().then((function(e){if(e.length>0){var t=e[0];t.increment("time"),t.save().then((function(n){(0,f.default)(a).find(".leancloud-visitors-count").text(n.get("time"))})).catch((function(n){}))}else x(n,l)})).catch((function(e){101==e.code?x(n,l):(0,g.default)(t,e)}))}else w.show(n,r)}else w.show(n,r)},show:function(n,e){var t=[];if(e.forEach((function(n){var e=(0,f.default)(n).find(".leancloud-visitors-count");e&&e.text("0"),t.push(/\%/.test((0,f.default)(n).attr("id"))?decodeURI((0,f.default)(n).attr("id")):(0,f.default)(n).attr("id"))})),t.length){var r=new AV.Query(n);r.containedIn("url",t),r.find().then((function(n){n.length>0&&e.forEach((function(e){n.forEach((function(n){var t=n.get("xid")||encodeURI(n.get("url")),r=n.get("time"),a=(0,f.default)(e),o=a.attr("id");if((/\%/.test(o)?o:encodeURI(o))==t){var i=a.find(".leancloud-visitors-count");i&&i.text(r)}}))}))})).catch((function(n){}))}}};a.prototype.Q=function(n){var e=this,t=arguments.length,r=e.cfg.clazzName;if(1==t){var a=new AV.Query(r);a.doesNotExist("rid");var o=new AV.Query(r);o.equalTo("rid","");var i=AV.Query.or(a,o);return"*"===n?i.exists("url"):i.equalTo("url",decodeURI(n)),i.addDescending("createdAt"),i.addDescending("insertedAt"),i}var s=JSON.stringify(arguments[1]).replace(/(\[|\])/g,""),l="select * from "+r+" where rid in ("+s+") order by -createdAt,-createdAt";return AV.Query.doCloudQuery(l)},a.prototype.installLocale=function(n,e){return this.i18n(n,e),this},a.prototype.setPath=function(n){return this.config.path=n,this},a.prototype.bind=function(){var n=this,e=n.$el.find(".vemojis"),t=n.$el.find(".vpreview"),r=n.$el.find(".vemoji-btn"),a=n.$el.find(".vpreview-btn"),o=n.$el.find(".veditor"),l=c.default.maps,d=!1;n.$emoji={show:function(){return!d&&function(n){var t=[];for(var r in l)l.hasOwnProperty(r)&&c.default.build(r)&&t.push('<i title="'+r+'" >'+c.default.build(r)+"</i>");e.html(t.join("")),d=!0,e.find("i").on("click",(function(n){n.preventDefault(),T(o[0]," :"+(0,f.default)(this).attr("title")+":")}))}(),n.$preview.hide(),e.show(),r.addClass("actived"),n.$emoji},hide:function(){return r.removeClass("actived"),e.hide(),n.$emoji}},n.$preview={show:function(){return k?(n.$emoji.hide(),a.addClass("actived"),t.html((0,h.default)(k)).show(),M()):n.$preview.hide(),n.$preview},hide:function(){return a.removeClass("actived"),t.hide().html(""),n.$preview}};var x=function(e){var r=e.val()||"";r||n.$preview.hide(),k!=r&&(k=r,a.hasClass("actived")>-1&&k!=t.html()&&t.html((0,h.default)(k)),M())};r.on("click",(function(e){r.hasClass("actived")?n.$emoji.hide():n.$emoji.show()})),a.on("click",(function(e){a.hasClass("actived")?n.$preview.hide():n.$preview.show()}));var w=n.cfg.meta,E={},D={veditor:"comment"};for(var C in w.forEach((function(n){D["v"+n]=n})),D)D.hasOwnProperty(C)&&function(){var e=D[C],t=n.$el.find("."+C);E[e]=t,t.on("input change blur propertychange",(function(r){n.cfg.enableQQ&&"blur"===r.type&&"nick"===e&&(t.val()&&!isNaN(t.val())?(0,m.fetchQQFn)(t.val(),(function(n){var e=n.nick||t.val(),r=n.qq+"@qq.com";(0,f.default)(".vnick").val(e),(0,f.default)(".vmail").val(r),b.nick=e,b.mail=r,b.QQAvatar=n.pic})):f.default.store.get(p.QQCacheKey)&&f.default.store.get(p.QQCacheKey).nick!=t.val()&&(f.default.store.remove(p.QQCacheKey),b.nick=t.val(),b.mail="",b.QQAvatar="")),"comment"===e?((0,s.default)(t[0]),I((function(n){x(t)}))()):b[e]=(0,v.default)(t.val().replace(/(^\s*)|(\s*$)/g,"").substring(0,35))}))}();var I=function(n){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:300,t=void 0;return function(){var r=this,a=arguments;t&&clearTimeout(t),t=setTimeout((function(){n.apply(r,a)}),e)}},T=function(n,e){if(document.selection)n.focus(),document.selection.createRange().text=e,n.focus();else if(n.selectionStart||"0"==n.selectionStart){var t=n.selectionStart,r=n.selectionEnd,a=n.scrollTop;n.value=n.value.substring(0,t)+e+n.value.substring(r,n.value.length),n.focus(),n.selectionStart=t+e.length,n.selectionEnd=t+e.length,n.scrollTop=a}else n.focus(),n.value+=e;I((function(e){x((0,f.default)(n))}))()},O={no:1,size:n.cfg.pageSize,skip:n.cfg.pageSize},A=n.$el.find(".vpage");A.on("click",(function(n){A.hide(),O.no++,_()}));var _=function(){var e=O.size,t=O.no,r=Number(n.$el.find(".vnum").text());n.$loading.show();var a=n.Q(n.cfg.path);a.limit(e),a.skip((t-1)*e),a.find().then((function(a){if(O.skip=O.size,a&&a.length){var o=[];a.forEach((function(e){o.push(e.id),P(e,n.$el.find(".vcards"),!0)})),n.Q(n.cfg.path,o).then((function(n){(n&&n.results||[]).forEach((function(n){P(n,(0,f.default)('.vquote[data-self-id="'+n.get("rid")+'"]'))}))})).catch((function(n){})),e*t<r?A.show():A.hide(),M()}else n.$nodata.show();n.$loading.hide()})).catch((function(e){n.$loading.hide(),(0,g.default)(n,e,"query")}))};n.Q(n.cfg.path).count().then((function(e){e>0?(n.$el.find(".vcount").show().find(".vnum").text(e),_()):n.$loading.hide()})).catch((function(e){(0,g.default)(n,e,"count")}));var R=function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"",e=/(https?|http):\/\/[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]/g,t=n.match(e)||[];return t.length>0?t[0]:""},P=function(e,t,r){var a=(0,f.default)('<div class="vcard" id="'+e.id+'"></div>'),o=(0,v.default)(e.get("ua")),s="";o&&!/ja/.test(n.cfg.lang)&&(s=(o=f.default.detect(o)).version?o.os?'<span class="vsys">'+o.browser+" "+o.version+'</span> <span class="vsys">'+o.os+" "+o.osVersion+"</span>":"":'<span class="vsys">'+o.browser+"</span>"),"*"===n.cfg.path&&(s='<a href="'+e.get("url")+'" class="vsys">'+e.get("url")+"</a>");var l=e.get("link")?/^https?\:\/\//.test(e.get("link"))?e.get("link"):"http://"+e.get("link"):"",c=f.default.escape((0,v.default)(e.get("nick").substring(0,30))),p=l?'<a class="vnick" rel="nofollow" href="'+R(l)+'" target="_blank" >'+c+"</a>":'<span class="vnick">'+c+"</span>",d=(S.hide?"":n.cfg.enableQQ&&e.get("QQAvatar")?(0,v.default)('<img class="vimg" src="'+R(e.get("QQAvatar"))+'" referrerPolicy="no-referrer"/>'):'<img class="vimg" src="'+(S.cdn+(0,i.default)(e.get("mail"))+S.params)+'">')+'<div class="vh"><div class="vhead">'+p+" "+s+'</div><div class="vmeta"><span class="vtime" >'+(0,u.default)(e.get("insertedAt"),n.i18n)+'</span><span class="vat" data-vm-id="'+(e.get("rid")||e.id)+'" data-self-id="'+e.id+'">'+n.i18n.t("reply")+'</span></div><div class="vcontent" data-expand="'+n.i18n.t("expand")+'">'+(0,h.default)(e.get("comment"))+'</div><div class="vreply-wrapper" data-self-id="'+e.id+'"></div><div class="vquote" data-self-id="'+e.id+'"></div></div>';a.html(d);var m=a.find(".vat");a.find("a:not(.at)").forEach((function(n){(0,f.default)(n).attr({target:"_blank",rel:"noopener"})})),r?t.append(a):t.prepend(a);var g=a.find(".vcontent");g&&j(g),m&&B(m,e)},F={},B=function(e,t){e.on("click",(function(r){var a=e.attr("data-vm-id"),o=e.attr("data-self-id"),i=n.$el.find(".vwrap"),s="@"+f.default.escape(t.get("nick"));(0,f.default)('.vreply-wrapper[data-self-id="'+o+'"]').append(i).find(".cancel-reply").show(),F={at:f.default.escape(s)+" ",rid:a,pid:o,rmail:t.get("mail")},E.comment.attr({placeholder:s})[0].focus()}))},M=function(){setTimeout((function(){try{n.cfg.mathjax&&"MathJax"in window&&"version"in window.MathJax&&(/^3.*/.test(window.MathJax.version)&&MathJax.typeset()||MathJax.Hub.Queue(["Typeset",MathJax.Hub,document.querySelector(".v")])),"renderMathInElement"in window&&renderMathInElement((0,f.default)(".v")[0],{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})}catch(n){}}),100)},j=function(n){setTimeout((function(){n[0].offsetHeight>200&&(n.addClass("expand"),n.on("click",(function(e){n.removeClass("expand")})))}))};!function(e){if(e=f.default.store.get(p.MetaCacheKey)||e)for(var t in w)if(w.hasOwnProperty(t)){var r=w[t];n.$el.find(".v"+r).val(f.default.unescape(e[r])),b[r]=e[r]}var a=f.default.store.get(p.QQCacheKey);b.QQAvatar=n.cfg.enableQQ&&!!a&&a.pic||""}(),n.reset=function(){b.comment="",E.comment.val(""),x(E.comment),E.comment.attr("placeholder",n.cfg.placeholder),F={},n.$preview.hide(),n.$el.find(".vpanel").append(n.$el.find(".vwrap")),n.$el.find(".cancel-reply").hide(),k="",s.default.update(E.comment[0])};var L=n.$el.find(".vsubmit"),N=function(e){if(n.cfg.requiredFields.indexOf("nick")>-1&&b.nick.length<3)return E.nick[0].focus(),void n.$el.find(".status-bar").text(""+n.i18n.t("nickFail")).empty(3e3);if(n.cfg.requiredFields.indexOf("mail")>-1&&!/[\w-\.]+@([\w-]+\.)+[a-z]{2,3}/.test(b.mail))return E.mail[0].focus(),void n.$el.find(".status-bar").text(""+n.i18n.t("mailFail")).empty(3e3);if(""!=k){for(var t in y)if(y.hasOwnProperty(t)){var r=y[t];k=k.replace(t,r),URL.revokeObjectURL(t)}y={},b.comment=(0,v.default)(k),b.nick=b.nick||"Anonymous";var a=f.default.store.get("vlx");a&&Date.now()/1e3-a/1e3<20?n.$el.find(".status-bar").text(n.i18n.t("busy")).empty(3e3):$()}else E.comment[0].focus()},$=function(){f.default.store.set("vlx",Date.now()),L.attr({disabled:!0}),n.$loading.show(!0);var e=new(AV.Object.extend(n.cfg.clazzName||"Comment"));if(b.url=decodeURI(n.cfg.path),b.insertedAt=new Date,F.rid){var t=F.pid||F.rid;e.set("rid",F.rid),e.set("pid",t),b.comment=k.replace("<p>",'<p><a class="at" href="#'+t+'">'+F.at+"</a> , ")}for(var r in b)if(b.hasOwnProperty(r)){var a=b[r];e.set(r,a)}e.setACL(function(){var n=new AV.ACL;return n.setPublicReadAccess(!0),n.setPublicWriteAccess(!1),n}()),e.save().then((function(e){"Anonymous"!=b.nick&&f.default.store.set(p.MetaCacheKey,{nick:b.nick,link:b.link,mail:b.mail});var t=n.$el.find(".vnum");try{F.rid?P(e,(0,f.default)('.vquote[data-self-id="'+F.rid+'"]'),!0):(Number(t.text())?t.text(Number(t.text())+1):n.$el.find(".vcount").show().find(".vnum").text(Number(t.text())+1),P(e,n.$el.find(".vcards")),O.skip++),L.removeAttr("disabled"),n.$loading.hide(),n.reset()}catch(e){(0,g.default)(n,e,"save")}})).catch((function(e){(0,g.default)(n,e,"commitEvt")}))};L.on("click",N),(0,f.default)(document).on("keydown",(function(n){var e=(n=window.event||n).keyCode||n.which||n.charCode;(n.ctrlKey||n.metaKey)&&13===e&&N(),9===e&&"veditor"==(document.activeElement.id||"")&&(n.preventDefault(),T(o[0],"    "))})).on("paste",(function(n){var e="clipboardData"in n?n.clipboardData:n.originalEvent&&n.originalEvent.clipboardData||window.clipboardData;e&&U(e.items,!0)})),o.on("dragenter dragleave dragover drop",(function(n){n.stopPropagation(),n.preventDefault(),"drop"===n.type&&U(n.dataTransfer.items)}));var U=function(n,e){for(var t=0,r=n.length;t<r;t++){var a=n[t];if("string"===a.kind&&a.type.match("^text/html"))!e&&a.getAsString((function(n){n&&T(o[0],n.replace(/<[^>]+>/g,""))}));else if(a.type.indexOf("image")>-1){z(a.getAsFile());continue}}},z=function(n){try{var e=URL.createObjectURL(n),t="![image]("+e+") ",r=new FileReader;T(o[0],t),r.onload=function(){y[e]=r.result},r.readAsDataURL(n)}catch(n){}}},n.exports=o,n.exports.default=o},function(n,e,t){"use strict";e.__esModule=!0;var r=function(n){return n&&n.__esModule?n:{default:n}}(t(3));e.default={getApi:function(n,e){r.default.ajax({url:"https://app-router.com/2/route",body:{appId:n}}).then((function(n){n.json().then((function(n){return e&&e("//"+n.api_server)}))}))}}},function(n,e,t){"use strict";e.__esModule=!0;var r=function(n){return n&&n.__esModule?n:{default:n}}(t(3)),a=!1;e.default=function(n,e){if("AV"in window){var t=window.AV.version||window.AV.VERSION;parseInt(t.split(".")[0])>2?a=!!AV.applicationId&&!!AV.applicationKey:r.default.deleteInWin("AV",0)}a?e&&e():r.default.sdkLoader("//unpkg.com/leancloud-storage@3/dist/av-min.js","AV",(function(t){var r,o="https://",i=n.app_id||n.appId,s=n.app_key||n.appKey;if(!n.serverURLs)switch(i.slice(-9)){case"-9Nh9j0Va":o+="tab.";break;case"-MdYXbMMI":o+="us."}r=n.serverURLs||o+"leancloud.cn",AV.init({appId:i,appKey:s,serverURLs:r}),a=!0,e&&e()}))}},function(n,e,t){"use strict";function r(n){return n&&n.__esModule?n:{default:n}}e.__esModule=!0;var a=r(t(84)),o=r(t(100)),i=r(t(101)),s=r(t(98)),l=r(t(99)),c={zh:o.default,"zh-cn":o.default,"zh-CN":o.default,"zh-TW":i.default,en:s.default,"en-US":s.default,ja:l.default,"ja-JP":l.default};e.default=function(n,e){return!c[n]&&n&&e&&(c[n]=e),new a.default({phrases:c[n||"zh"],locale:n})}},function(n,e,t){"use strict";e.__esModule=!0,e.default=function(n,e){if(n.$el&&n.$loading.hide().$nodata.hide(),"[object Error]"==={}.toString.call(e)){var t=e.code||e.message||e.error||"";if(isNaN(t))n.$el&&n.$nodata.show('<pre style="text-align:left;"> '+JSON.stringify(e)+"</pre>");else{var r=n.i18n.t("code-"+t),a=(r=="code-"+t?void 0:r)||e.message||e.error||"";101==t||-1==t?n.$nodata.show():n.$el&&n.$nodata.show('<pre style="text-align:left;">Code '+t+": "+a+"</pre>")}}else n.$el&&n.$nodata.show('<pre style="text-align:left;">'+JSON.stringify(e)+"</pre>")}},function(n,e,t){"use strict";function r(n){return n&&n.__esModule?n:{default:n}}e.__esModule=!0;var a=t(83),o=r(t(79)),i=r(t(3)),s=r(t(12)),l=r(t(13)),c=new a.marked.Renderer;c.code=function(n,e){return'<pre><code class="hljs language-'+e+'">'+(e&&hljs.getLanguage(e)?hljs.highlight(e,n).value:i.default.escape(n))+"</code></pre>"},a.marked.setOptions({renderer:"hljs"in window?c:new a.marked.Renderer,highlight:function(n,e){return"hljs"in window?e&&hljs.getLanguage(e)&&hljs.highlight(e,n,!0).value||hljs.highlightAuto(n).value:(0,o.default)(n)},gfm:!0,tables:!0,breaks:!0,pedantic:!1,sanitize:!1,smartLists:!0,smartypants:!0,headerPrefix:"v-"}),e.default=function(n){return(0,l.default)((0,a.marked)(s.default.parse(n,!0)))}},function(n,e,t){"use strict";e.__esModule=!0,e.recordIPFn=e.fetchQQFn=void 0;var r=function(n){return n&&n.__esModule?n:{default:n}}(t(3)),a=t(6);e.fetchQQFn=function(n,e){var t=r.default.store.get(a.QQCacheKey);t&&t.qq==n?e&&e(t):r.default.ajax({url:"//valine.api.ioliu.cn/getqqinfo",method:"POST",body:{qq:n}}).then((function(n){n.json().then((function(n){n.errmsg||(r.default.store.set(a.QQCacheKey,n),e&&e(n))}))}))},e.recordIPFn=function(n){r.default.ajax({url:"https://forge.speedtest.cn/api/location/info",method:"get"}).then((function(n){return n.json()})).then((function(e){n&&n(e.ip)}))}},function(n,e,t){"use strict";e.__esModule=!0,e.default=function(n,e){if(!n)return"Invalid Date.";try{var t=a(n).getTime();if(isNaN(t))return"Invalid Date.";var o=(new Date).getTime()-t,i=Math.floor(o/864e5);if(0===i){var s=o%864e5,l=Math.floor(s/36e5);if(0===l){var c=s%36e5,p=Math.floor(c/6e4);if(0===p){var u=c%6e4;return Math.round(u/1e3)+" "+e.t("seconds")}return p+" "+e.t("minutes")}return l+" "+e.t("hours")}return i<0?e.t("now"):i<8?i+" "+e.t("days"):r(n)}catch(n){}};var r=function(n){var e=o(n.getDate(),2),t=o(n.getMonth()+1,2);return o(n.getFullYear(),2)+"-"+t+"-"+e},a=function n(e){return e instanceof Date?e:!isNaN(e)||/^\d+$/.test(e)?new Date(parseInt(e)):/GMT/.test(e||"")?n(new Date(e).getTime()):(e=(e||"").replace(/(^\s*)|(\s*$)/g,"").replace(/\.\d+/,"").replace(/-/,"/").replace(/-/,"/").replace(/(\d)T(\d)/,"$1 $2").replace(/Z/," UTC").replace(/([+-]\d\d):?(\d\d)/," $1$2"),new Date(e))},o=function(n,e){for(var t=n.toString();t.length<e;)t="0"+t;return t}},function(n,e,t){var r;!function(a){"use strict";function o(n,e){var t=(65535&n)+(65535&e);return(n>>16)+(e>>16)+(t>>16)<<16|65535&t}function i(n,e,t,r,a,i){return o(function(n,e){return n<<e|n>>>32-e}(o(o(e,n),o(r,i)),a),t)}function s(n,e,t,r,a,o,s){return i(e&t|~e&r,n,e,a,o,s)}function l(n,e,t,r,a,o,s){return i(e&r|t&~r,n,e,a,o,s)}function c(n,e,t,r,a,o,s){return i(e^t^r,n,e,a,o,s)}function p(n,e,t,r,a,o,s){return i(t^(e|~r),n,e,a,o,s)}function u(n,e){n[e>>5]|=128<<e%32,n[14+(e+64>>>9<<4)]=e;var t,r,a,i,u,d=1732584193,m=-271733879,g=-1732584194,f=271733878;for(t=0;t<n.length;t+=16)r=d,a=m,i=g,u=f,d=s(d,m,g,f,n[t],7,-680876936),f=s(f,d,m,g,n[t+1],12,-389564586),g=s(g,f,d,m,n[t+2],17,606105819),m=s(m,g,f,d,n[t+3],22,-1044525330),d=s(d,m,g,f,n[t+4],7,-176418897),f=s(f,d,m,g,n[t+5],12,1200080426),g=s(g,f,d,m,n[t+6],17,-1473231341),m=s(m,g,f,d,n[t+7],22,-45705983),d=s(d,m,g,f,n[t+8],7,1770035416),f=s(f,d,m,g,n[t+9],12,-1958414417),g=s(g,f,d,m,n[t+10],17,-42063),m=s(m,g,f,d,n[t+11],22,-1990404162),d=s(d,m,g,f,n[t+12],7,1804603682),f=s(f,d,m,g,n[t+13],12,-40341101),g=s(g,f,d,m,n[t+14],17,-1502002290),d=l(d,m=s(m,g,f,d,n[t+15],22,1236535329),g,f,n[t+1],5,-165796510),f=l(f,d,m,g,n[t+6],9,-1069501632),g=l(g,f,d,m,n[t+11],14,643717713),m=l(m,g,f,d,n[t],20,-373897302),d=l(d,m,g,f,n[t+5],5,-701558691),f=l(f,d,m,g,n[t+10],9,38016083),g=l(g,f,d,m,n[t+15],14,-660478335),m=l(m,g,f,d,n[t+4],20,-405537848),d=l(d,m,g,f,n[t+9],5,568446438),f=l(f,d,m,g,n[t+14],9,-1019803690),g=l(g,f,d,m,n[t+3],14,-187363961),m=l(m,g,f,d,n[t+8],20,1163531501),d=l(d,m,g,f,n[t+13],5,-1444681467),f=l(f,d,m,g,n[t+2],9,-51403784),g=l(g,f,d,m,n[t+7],14,1735328473),d=c(d,m=l(m,g,f,d,n[t+12],20,-1926607734),g,f,n[t+5],4,-378558),f=c(f,d,m,g,n[t+8],11,-2022574463),g=c(g,f,d,m,n[t+11],16,1839030562),m=c(m,g,f,d,n[t+14],23,-35309556),d=c(d,m,g,f,n[t+1],4,-1530992060),f=c(f,d,m,g,n[t+4],11,1272893353),g=c(g,f,d,m,n[t+7],16,-155497632),m=c(m,g,f,d,n[t+10],23,-1094730640),d=c(d,m,g,f,n[t+13],4,681279174),f=c(f,d,m,g,n[t],11,-358537222),g=c(g,f,d,m,n[t+3],16,-722521979),m=c(m,g,f,d,n[t+6],23,76029189),d=c(d,m,g,f,n[t+9],4,-640364487),f=c(f,d,m,g,n[t+12],11,-421815835),g=c(g,f,d,m,n[t+15],16,530742520),d=p(d,m=c(m,g,f,d,n[t+2],23,-995338651),g,f,n[t],6,-198630844),f=p(f,d,m,g,n[t+7],10,1126891415),g=p(g,f,d,m,n[t+14],15,-1416354905),m=p(m,g,f,d,n[t+5],21,-57434055),d=p(d,m,g,f,n[t+12],6,1700485571),f=p(f,d,m,g,n[t+3],10,-1894986606),g=p(g,f,d,m,n[t+10],15,-1051523),m=p(m,g,f,d,n[t+1],21,-2054922799),d=p(d,m,g,f,n[t+8],6,1873313359),f=p(f,d,m,g,n[t+15],10,-30611744),g=p(g,f,d,m,n[t+6],15,-1560198380),m=p(m,g,f,d,n[t+13],21,1309151649),d=p(d,m,g,f,n[t+4],6,-145523070),f=p(f,d,m,g,n[t+11],10,-1120210379),g=p(g,f,d,m,n[t+2],15,718787259),m=p(m,g,f,d,n[t+9],21,-343485551),d=o(d,r),m=o(m,a),g=o(g,i),f=o(f,u);return[d,m,g,f]}function d(n){var e,t="",r=32*n.length;for(e=0;e<r;e+=8)t+=String.fromCharCode(n[e>>5]>>>e%32&255);return t}function m(n){var e,t=[];for(t[(n.length>>2)-1]=void 0,e=0;e<t.length;e+=1)t[e]=0;var r=8*n.length;for(e=0;e<r;e+=8)t[e>>5]|=(255&n.charCodeAt(e/8))<<e%32;return t}function g(n){var e,t,r="0123456789abcdef",a="";for(t=0;t<n.length;t+=1)e=n.charCodeAt(t),a+=r.charAt(e>>>4&15)+r.charAt(15&e);return a}function f(n){return unescape(encodeURIComponent(n))}function h(n){return function(n){return d(u(m(n),8*n.length))}(f(n))}function v(n,e){return function(n,e){var t,r,a=m(n),o=[],i=[];for(o[15]=i[15]=void 0,a.length>16&&(a=u(a,8*n.length)),t=0;t<16;t+=1)o[t]=909522486^a[t],i[t]=1549556828^a[t];return r=u(o.concat(m(e)),512+8*e.length),d(u(i.concat(r),640))}(f(n),f(e))}function b(n,e,t){return e?t?v(e,n):function(n,e){return g(v(n,e))}(e,n):t?h(n):function(n){return g(h(n))}(n)}void 0!==(r=function(){return b}.call(e,t,e,n))&&(n.exports=r)}()},function(n,e,t){"use strict";var r=t(2),a=t(4),o=t(1),i=t(5),s=t(14),l=t(15),c=l(),p=t(44),u=o("Array.prototype.slice"),d=a.apply(c),m=function(n,e){return i(n),d(n,u(arguments,1))};r(m,{getPolyfill:l,implementation:s,shim:p}),n.exports=m},function(n,e,t){"use strict";var r=t(2),a=t(15);n.exports=function(){var n=a();return r(Array.prototype,{forEach:n},{forEach:function(){return Array.prototype.forEach!==n}}),n}},function(n,e,t){"use strict";e.__esModule=!0;var r=function(n){return n&&n.__esModule?n:{default:n}}(t(47));e.default=function(n){return n=(0,r.default)({url:"",method:"get",body:{}},n),new Promise((function(e,t){if("jsonp"==n.method){var r="cb_"+(Date.now()+Math.round(1e3*Math.random())).toString(32),a=document,i=a.body,s=a.createElement("script");return s.async=!0,s.defer=!0,n.url.indexOf("?")>-1?n.url+="&"+o({callback:r,t:Date.now()}):n.url+="?"+o({callback:r,t:Date.now()}),s.src=n.url,window[r]=function(n){window[r]=null,i.removeChild(s),e(n)},void i.appendChild(s)}var l="XMLHttpRequest"in window?new XMLHttpRequest:new ActiveXObject("Microsoft.XMLHTTP"),c=[],p=[],u={};for(var d in o(n.body)&&(n.url=n.url+"?"+("get"==n.method?o(n.body):"")),l.open(n.method||"get",n.url),"blob"==n.dataType&&(l.responseType="blob"),l.onload=function(){l.getAllResponseHeaders().replace(/^(.*?):[^\S\n]*([\s\S]*?)$/gm,(function(n,e,t){c.push(e=e.toLowerCase()),p.push([e,t]),u[e]=u[e]?u[e]+","+t:t})),e(function n(){return{ok:2==(l.status/100|0),statusText:l.statusText,status:l.status,url:l.responseURL,text:function(){return Promise.resolve(l.responseText)},json:function(){return Promise.resolve(l.responseText).then(JSON.parse)},blob:function(){return Promise.resolve(new Blob([l.response]))},clone:n,headers:{keys:function(){return c},entries:function(){return p},get:function(n){return u[n.toLowerCase()]},has:function(n){return n.toLowerCase()in u}}}}())},l.onerror=t,l.withCredentials="include"==n.credentials,n.headers)l.setRequestHeader(d,n.headers[d]);l.send("post"==n.method?n.body:"get"==n.method?"":o(n.body))}))};var a=encodeURIComponent,o=function(n){var e=[];for(var t in n)n.hasOwnProperty(t)&&e.push(a(t)+"="+a(n[t]));return(e=e.join("&").replace(/%20/g,"+"))||""}},function(n,e,t){"use strict";e.__esModule=!0,e.default=function(n){var e={},t={Trident:(n=n||navigator.userAgent).indexOf("Trident")>-1||n.indexOf("NET CLR")>-1,Presto:n.indexOf("Presto")>-1,WebKit:n.indexOf("AppleWebKit")>-1,Gecko:n.indexOf("Gecko/")>-1,Safari:n.indexOf("Safari")>-1,Edge:n.indexOf("Edge")>-1||n.indexOf("Edg")>-1,Chrome:n.indexOf("Chrome")>-1||n.indexOf("CriOS")>-1,IE:n.indexOf("MSIE")>-1||n.indexOf("Trident")>-1,Firefox:n.indexOf("Firefox")>-1||n.indexOf("FxiOS")>-1,"Firefox Focus":n.indexOf("Focus")>-1,Chromium:n.indexOf("Chromium")>-1,Opera:n.indexOf("Opera")>-1||n.indexOf("OPR")>-1,Vivaldi:n.indexOf("Vivaldi")>-1,Yandex:n.indexOf("YaBrowser")>-1,Kindle:n.indexOf("Kindle")>-1||n.indexOf("Silk/")>-1,360:n.indexOf("360EE")>-1||n.indexOf("360SE")>-1,UC:n.indexOf("UC")>-1||n.indexOf(" UBrowser")>-1,QQBrowser:n.indexOf("QQBrowser")>-1,QQ:n.indexOf("QQ/")>-1,Baidu:n.indexOf("Baidu")>-1||n.indexOf("BIDUBrowser")>-1,Maxthon:n.indexOf("Maxthon")>-1,Sogou:n.indexOf("MetaSr")>-1||n.indexOf("Sogou")>-1,LBBROWSER:n.indexOf("LBBROWSER")>-1,"2345Explorer":n.indexOf("2345Explorer")>-1,TheWorld:n.indexOf("TheWorld")>-1,XiaoMi:n.indexOf("MiuiBrowser")>-1,Quark:n.indexOf("Quark")>-1,Qiyu:n.indexOf("Qiyu")>-1,Wechat:n.indexOf("MicroMessenger")>-1,Taobao:n.indexOf("AliApp(TB")>-1,Alipay:n.indexOf("AliApp(AP")>-1,Weibo:n.indexOf("Weibo")>-1,Douban:n.indexOf("com.douban.frodo")>-1,Suning:n.indexOf("SNEBUY-APP")>-1,iQiYi:n.indexOf("IqiyiApp")>-1,Windows:n.indexOf("Windows")>-1,Linux:n.indexOf("Linux")>-1||n.indexOf("X11")>-1,macOS:n.indexOf("Macintosh")>-1,Android:n.indexOf("Android")>-1||n.indexOf("Adr")>-1,Ubuntu:n.indexOf("Ubuntu")>-1,FreeBSD:n.indexOf("FreeBSD")>-1,Debian:n.indexOf("Debian")>-1,"Windows Phone":n.indexOf("IEMobile")>-1||n.indexOf("Windows Phone")>-1,BlackBerry:n.indexOf("BlackBerry")>-1||n.indexOf("RIM")>-1||n.indexOf("BB10")>-1,MeeGo:n.indexOf("MeeGo")>-1,Symbian:n.indexOf("Symbian")>-1,iOS:n.indexOf("like Mac OS X")>-1,"Chrome OS":n.indexOf("CrOS")>-1,WebOS:n.indexOf("hpwOS")>-1,Mobile:n.indexOf("Mobi")>-1||n.indexOf("iPh")>-1||n.indexOf("480")>-1,Tablet:n.indexOf("Tablet")>-1||n.indexOf("Pad")>-1||n.indexOf("Nexus 7")>-1};t.Mobile&&(t.Mobile=!(n.indexOf("iPad")>-1));var r={browser:["Safari","Chrome","Edge","IE","Firefox","Firefox Focus","Chromium","Opera","Vivaldi","Yandex","Kindle","360","UC","QQBrowser","QQ","Baidu","Maxthon","Sogou","LBBROWSER","2345Explorer","TheWorld","XiaoMi","Quark","Qiyu","Wechat","Taobao","Alipay","Weibo","Douban","Suning","iQiYi"],os:["Windows","Linux","Mac OS","macOS","Android","Ubuntu","FreeBSD","Debian","iOS","Windows Phone","BlackBerry","MeeGo","Symbian","Chrome OS","WebOS"]};for(var a in r)if(r.hasOwnProperty(a))for(var o=0,i=r[a].length;o<i;o++){var s=r[a][o];t[s]&&(e[a]=s)}var l={Windows:function(){return{"10.0":"11",6.4:"10",6.3:"8.1",6.2:"8",6.1:"7","6.0":"Vista",5.2:"XP",5.1:"XP","5.0":"2000"}[n.replace(/^.*Windows NT ([\d.]+).*$/,"$1")]},Android:n.replace(/^.*Android ([\d.]+);.*$/,"$1"),iOS:n.replace(/^.*OS ([\d_]+) like.*$/,"$1").replace(/_/g,"."),Debian:n.replace(/^.*Debian\/([\d.]+).*$/,"$1"),"Windows Phone":n.replace(/^.*Windows Phone( OS)? ([\d.]+);.*$/,"$2"),macOS:n.replace(/^.*Mac OS X ([\d_]+).*$/,"$1").replace(/_/g,"."),WebOS:n.replace(/^.*hpwOS\/([\d.]+);.*$/,"$1"),BlackBerry:n.replace(/^.*BB([\d.]+);*$/,"$1")};e.osVersion="";var c=l[e.os];c&&(e.osVersion="function"==typeof c?c():c==n?"":c);var p={Safari:n.replace(/^.*Version\/([\d.]+).*$/,"$1"),Chrome:n.replace(/^.*Chrome\/([\d.]+).*$/,"$1").replace(/^.*CriOS\/([\d.]+).*$/,"$1"),IE:n.replace(/^.*MSIE ([\d.]+).*$/,"$1").replace(/^.*rv:([\d.]+).*$/,"$1"),Edge:n.replace(/^.*Edge?\/([\d.]+).*$/,"$1"),Firefox:n.replace(/^.*Firefox\/([\d.]+).*$/,"$1").replace(/^.*FxiOS\/([\d.]+).*$/,"$1"),"Firefox Focus":n.replace(/^.*Focus\/([\d.]+).*$/,"$1"),Chromium:n.replace(/^.*Chromium\/([\d.]+).*$/,"$1"),Opera:n.replace(/^.*Opera\/([\d.]+).*$/,"$1").replace(/^.*OPR\/([\d.]+).*$/,"$1"),Vivaldi:n.replace(/^.*Vivaldi\/([\d.]+).*$/,"$1"),Yandex:n.replace(/^.*YaBrowser\/([\d.]+).*$/,"$1"),Kindle:n.replace(/^.*Version\/([\d.]+).*$/,"$1"),Maxthon:n.replace(/^.*Maxthon\/([\d.]+).*$/,"$1"),QQBrowser:n.replace(/^.*QQBrowser\/([\d.]+).*$/,"$1"),QQ:n.replace(/^.*QQ\/([\d.]+).*$/,"$1"),Baidu:n.replace(/^.*BIDUBrowser[\s\/]([\d.]+).*$/,"$1"),UC:n.replace(/^.*UC?Browser\/([\d.]+).*$/,"$1"),Sogou:n.replace(/^.*SE ([\d.X]+).*$/,"$1").replace(/^.*SogouMobileBrowser\/([\d.]+).*$/,"$1"),"2345Explorer":n.replace(/^.*2345Explorer\/([\d.]+).*$/,"$1"),TheWorld:n.replace(/^.*TheWorld ([\d.]+).*$/,"$1"),XiaoMi:n.replace(/^.*MiuiBrowser\/([\d.]+).*$/,"$1"),Quark:n.replace(/^.*Quark\/([\d.]+).*$/,"$1"),Qiyu:n.replace(/^.*Qiyu\/([\d.]+).*$/,"$1"),Wechat:n.replace(/^.*MicroMessenger\/([\d.]+).*$/,"$1"),Taobao:n.replace(/^.*AliApp\(TB\/([\d.]+).*$/,"$1"),Alipay:n.replace(/^.*AliApp\(AP\/([\d.]+).*$/,"$1"),Weibo:n.replace(/^.*weibo__([\d.]+).*$/,"$1"),Douban:n.replace(/^.*com.douban.frodo\/([\d.]+).*$/,"$1"),Suning:n.replace(/^.*SNEBUY-APP([\d.]+).*$/,"$1"),iQiYi:n.replace(/^.*IqiyiVersion\/([\d.]+).*$/,"$1")};e.version="";var u=p[e.browser];return u&&(e.version="function"==typeof u?u():u==n?"":u),null==e.browser&&(e.browser="Unknow App"),e}},function(n,e,t){"use strict";e.__esModule=!0,e.default=function(n){n=Object(n);for(var e=1,t=arguments.length;e<t;e++){var r=arguments[e];if(r)for(var a in r)Object.prototype.hasOwnProperty.call(r,a)&&(n[a]=r[a])}return n}},function(n,e,t){"use strict";function r(n){return/^\{[\s\S]*\}$/.test(JSON.stringify(n))}function a(n){return"[object Function]"==={}.toString.call(n)}function o(n){return"[object Array]"==={}.toString.call(n)}function i(n){if("string"==typeof n)try{return JSON.parse(n)}catch(e){return n}}function s(){if(!(this instanceof s))return new s}function l(n,e){var t=arguments,i=null;if(p||(p=s()),0===t.length)return p.get();if(1===t.length){if("string"==typeof n)return p.get(n);if(r(n))return p.set(n)}if(2===t.length&&"string"==typeof n){if(!e)return p.remove(n);if(e&&"string"==typeof e)return p.set(n,e);e&&a(e)&&(i=null,i=e(n,p.get(n)),l.set(n,i))}if(2===t.length&&o(n)&&a(e))for(var c=0,u=n.length;c<u;c++)i=e(n[c],p.get(n[c])),l.set(n[c],i);return l}e.__esModule=!0;var c=window.localStorage;c=function(n){var e="_Is_Incognit";try{n.setItem(e,"yes")}catch(e){if(["QuotaExceededError","NS_ERROR_DOM_QUOTA_REACHED"].indexOf(e.name)>-1){var t=function(){};n.__proto__={setItem:t,getItem:t,removeItem:t,clear:t}}}finally{"yes"===n.getItem(e)&&n.removeItem(e)}return n}(c),s.prototype={set:function(n,e){if(n&&!r(n))c.setItem(n,function(n){return void 0===n||"function"==typeof n?n+"":JSON.stringify(n)}(e));else if(r(n))for(var t in n)this.set(t,n[t]);return this},get:function(n){if(!n){var e={};return this.each((function(n,t){return e[n]=t})),e}if("?"===n.charAt(0))return this.has(n.substr(1));var t=arguments;if(t.length>1){for(var r={},a=0,o=t.length;a<o;a++){var s=i(c.getItem(t[a]));s&&(r[t[a]]=s)}return r}return i(c.getItem(n))},clear:function(){return c.clear(),this},remove:function(n){var e=this.get(n);return c.removeItem(n),e},has:function(n){return{}.hasOwnProperty.call(this.get(),n)},keys:function(){var n=[];return this.each((function(e){n.push(e)})),n},each:function(n){for(var e=0,t=c.length;e<t;e++){var r=c.key(e);n(r,this.get(r))}return this},search:function(n){for(var e=this.keys(),t={},r=0,a=e.length;r<a;r++)e[r].indexOf(n)>-1&&(t[e[r]]=this.get(e[r]));return t}};var p=null;for(var u in s.prototype)l[u]=s.prototype[u];e.default=l},function(n,e,t){var r,a;a=function(n,e,t){function r(e,a,o){return o=Object.create(r.fn),e&&o.push.apply(o,e.addEventListener?[e]:""+e===e?/</.test(e)?((a=n.createElement(a)).innerHTML=e,a.children):a?(a=r(a)[0])?a[t](e):o:n[t](e):e),o}return r.fn=[],r.one=function(n,e){return r(n,e)[0]||null},r}(document,0,"querySelectorAll"),void 0!==(r=function(){return a}.apply(e,[]))&&(n.exports=r)},function(n,e,t){function r(n){return null==n}function a(n){(n=function(n){var e={};for(var t in n)e[t]=n[t];return e}(n||{})).whiteList=n.whiteList||o.whiteList,n.onAttr=n.onAttr||o.onAttr,n.onIgnoreAttr=n.onIgnoreAttr||o.onIgnoreAttr,n.safeAttrValue=n.safeAttrValue||o.safeAttrValue,this.options=n}var o=t(16),i=t(51);t(17),a.prototype.process=function(n){if(!(n=(n=n||"").toString()))return"";var e=this.options,t=e.whiteList,a=e.onAttr,o=e.onIgnoreAttr,s=e.safeAttrValue;return i(n,(function(n,e,i,l,c){var p=t[i],u=!1;if(!0===p?u=p:"function"==typeof p?u=p(l):p instanceof RegExp&&(u=p.test(l)),!0!==u&&(u=!1),l=s(i,l)){var d,m={position:e,sourcePosition:n,source:c,isWhite:u};return u?r(d=a(i,l,m))?i+":"+l:d:r(d=o(i,l,m))?void 0:d}}))},n.exports=a},function(n,e,t){var r=t(17);n.exports=function(n,e){function t(){if(!o){var t=r.trim(n.slice(i,s)),a=t.indexOf(":");if(-1!==a){var c=r.trim(t.slice(0,a)),p=r.trim(t.slice(a+1));if(c){var u=e(i,l.length,c,p,t);u&&(l+=u+"; ")}}}i=s+1}";"!==(n=r.trimRight(n))[n.length-1]&&(n+=";");for(var a=n.length,o=!1,i=0,s=0,l="";s<a;s++){var c=n[s];if("/"===c&&"*"===n[s+1]){var p=n.indexOf("*/",s+2);if(-1===p)break;i=(s=p+1)+1,o=!1}else"("===c?o=!0:")"===c?o=!1:";"===c?o||t():"\n"===c&&t()}return r.trim(l)}},function(n,e,t){"use strict";var r=t(0),a=t(1),o=r("%TypeError%"),i=t(54),s=r("%Reflect.apply%",!0)||a("%Function.prototype.apply%");n.exports=function(n,e){var t=arguments.length>2?arguments[2]:[];if(!i(t))throw new o("Assertion failed: optional `argumentsList`, if provided, must be a List");return s(n,e,t)}},function(n,e,t){"use strict";var r=t(0)("%TypeError%"),a=t(19),o=t(8);n.exports=function(n,e){if("Object"!==o(n))throw new r("Assertion failed: `O` must be an Object");if(!a(e))throw new r("Assertion failed: `P` must be a Property Key");return e in n}},function(n,e,t){"use strict";var r=t(0)("%Array%"),a=!r.isArray&&t(1)("Object.prototype.toString");n.exports=r.isArray||function(n){return"[object Array]"===a(n)}},function(n,e,t){"use strict";n.exports=t(10)},function(n,e,t){"use strict";var r=t(0)("%TypeError%"),a=t(18),o=t(58),i=t(8);n.exports=function(n){if("Object"!==i(n))throw new r("Assertion failed: `obj` must be an Object");return o(a(n,"length"))}},function(n,e,t){"use strict";var r=t(63),a=t(59);n.exports=function(n){var e=a(n);return 0!==e&&(e=r(e)),0===e?0:e}},function(n,e,t){"use strict";var r=t(72),a=t(57);n.exports=function(n){var e=a(n);return e<=0?0:e>r?r:e}},function(n,e,t){"use strict";var r=t(0),a=r("%TypeError%"),o=r("%Number%"),i=r("%RegExp%"),s=r("%parseInt%"),l=t(1),c=t(73),p=t(71),u=l("String.prototype.slice"),d=c(/^0b[01]+$/i),m=c(/^0o[0-7]+$/i),g=c(/^[-+]0x[0-9a-f]+$/i),f=c(new i("["+["","​","￾"].join("")+"]","g")),h=["\t\n\v\f\r   ᠎    ","         　\u2028","\u2029\ufeff"].join(""),v=new RegExp("(^["+h+"]+)|(["+h+"]+$)","g"),b=l("String.prototype.replace"),k=t(61);n.exports=function n(e){var t=p(e)?e:k(e,o);if("symbol"==typeof t)throw new a("Cannot convert a Symbol value to a number");if("bigint"==typeof t)throw new a("Conversion from 'BigInt' to 'number' is not allowed.");if("string"==typeof t){if(d(t))return n(s(u(t,2),2));if(m(t))return n(s(u(t,2),8));if(f(t)||g(t))return NaN;var r=function(n){return b(n,v,"")}(t);if(r!==t)return n(r)}return o(t)}},function(n,e,t){"use strict";var r=t(0)("%Object%"),a=t(5);n.exports=function(n){return a(n),r(n)}},function(n,e,t){"use strict";var r=t(76);n.exports=function(n){return arguments.length>1?r(n,arguments[1]):r(n)}},function(n,e,t){"use strict";var r=t(0)("%TypeError%");n.exports=function(n,e){if(null==n)throw new r(e||"Cannot call method on "+n);return n}},function(n,e,t){"use strict";var r=t(67),a=t(68),o=t(64),i=t(70),s=t(69),l=t(74);n.exports=function(n){var e=o(n);return i(e)?0:0!==e&&s(e)?l(e)*a(r(e)):e}},function(n,e,t){"use strict";var r=t(65);n.exports=function(n){var e=r(n,Number);if("string"!=typeof e)return+e;var t=e.replace(/^[ \t\x0b\f\xa0\ufeff\n\r\u2028\u2029\u1680\u180e\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200a\u202f\u205f\u3000\u0085]+|[ \t\x0b\f\xa0\ufeff\n\r\u2028\u2029\u1680\u180e\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200a\u202f\u205f\u3000\u0085]+$/g,"");return/^0[ob]|^[+-]0x/.test(t)?NaN:+t}},function(n,e,t){"use strict";n.exports=t(77)},function(n,e,t){"use strict";n.exports=function(n){return null===n?"Null":void 0===n?"Undefined":"function"==typeof n||"object"==typeof n?"Object":"number"==typeof n?"Number":"boolean"==typeof n?"Boolean":"string"==typeof n?"String":void 0}},function(n,e,t){"use strict";var r=t(0)("%Math.abs%");n.exports=function(n){return r(n)}},function(n,e,t){"use strict";var r=Math.floor;n.exports=function(n){return r(n)}},function(n,e,t){"use strict";var r=Number.isNaN||function(n){return n!=n};n.exports=Number.isFinite||function(n){return"number"==typeof n&&!r(n)&&n!==1/0&&n!==-1/0}},function(n,e,t){"use strict";n.exports=Number.isNaN||function(n){return n!=n}},function(n,e,t){"use strict";n.exports=function(n){return null===n||"function"!=typeof n&&"object"!=typeof n}},function(n,e,t){"use strict";var r=t(0),a=r("%Math%"),o=r("%Number%");n.exports=o.MAX_SAFE_INTEGER||a.pow(2,53)-1},function(n,e,t){"use strict";var r=t(0)("RegExp.prototype.test"),a=t(4);n.exports=function(n){return a(r,n)}},function(n,e,t){"use strict";n.exports=function(n){return n>=0?1:-1}},function(n,e){n.exports=function(n){var e=!0,t=!0,r=!1;if("function"==typeof n){try{n.call("f",(function(n,t,r){"object"!=typeof r&&(e=!1)})),n.call([null],(function(){"use strict";t="string"==typeof this}),"x")}catch(n){r=!0}return!r&&e&&t}return!1}},function(n,e,t){"use strict";var r="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator,a=t(21),o=t(10),i=t(80),s=t(82),l=function(n,e){if(null==n)throw new TypeError("Cannot call method on "+n);if("string"!=typeof e||"number"!==e&&"string"!==e)throw new TypeError('hint must be "string" or "number"');var t,r,i,s="string"===e?["toString","valueOf"]:["valueOf","toString"];for(i=0;i<s.length;++i)if(t=n[s[i]],o(t)&&(r=t.call(n),a(r)))return r;throw new TypeError("No default value")},c=function(n,e){var t=n[e];if(null!=t){if(!o(t))throw new TypeError(t+" returned for property "+e+" of object "+n+" is not a function");return t}};n.exports=function(n){if(a(n))return n;var e,t="default";if(arguments.length>1&&(arguments[1]===String?t="string":arguments[1]===Number&&(t="number")),r&&(Symbol.toPrimitive?e=c(n,Symbol.toPrimitive):s(n)&&(e=Symbol.prototype.valueOf)),void 0!==e){var o=e.call(n,t);if(a(o))return o;throw new TypeError("unable to convert exotic object to primitive")}return"default"===t&&(i(n)||s(n))&&(t="string"),l(n,"default"===t?"number":t)}},function(n,e,t){"use strict";var r=Object.prototype.toString,a=t(21),o=t(10),i=function(n){var e;if((e=arguments.length>1?arguments[1]:"[object Date]"===r.call(n)?String:Number)===String||e===Number){var t,i,s=e===String?["toString","valueOf"]:["valueOf","toString"];for(i=0;i<s.length;++i)if(o(n[s[i]])&&(t=n[s[i]](),a(t)))return t;throw new TypeError("No default value")}throw new TypeError("invalid [[DefaultValue]] hint supplied")};n.exports=function(n){return a(n)?n:arguments.length>1?i(n,arguments[1]):i(n)}},function(n,e,t){"use strict";var r=Array.prototype.slice,a=Object.prototype.toString;n.exports=function(n){var e=this;if("function"!=typeof e||"[object Function]"!==a.call(e))throw new TypeError("Function.prototype.bind called on incompatible "+e);for(var t,o=r.call(arguments,1),i=function(){if(this instanceof t){var a=e.apply(this,o.concat(r.call(arguments)));return Object(a)===a?a:this}return e.apply(n,o.concat(r.call(arguments)))},s=Math.max(0,e.length-o.length),l=[],c=0;c<s;c++)l.push("$"+c);if(t=Function("binder","return function ("+l.join(",")+"){ return binder.apply(this,arguments); }")(i),e.prototype){var p=function(){};p.prototype=e.prototype,t.prototype=new p,p.prototype=null}return t}},function(n,e,t){n.exports=function(){"use strict";var n=function(n,e){return function(n){var e=n.exports=function(){return new RegExp("(?:"+e.line().source+")|(?:"+e.block().source+")","gm")};e.line=function(){return/(?:^|\s)\/\/(.+?)$/gm},e.block=function(){return/\/\*([\S\s]*?)\*\//gm}}(e={exports:{}}),e.exports}(),e=["23AC69","91C132","F19726","E8552D","1AAB8E","E1147F","2980C1","1BA1E6","9FA0A0","F19726","E30B20","E30B20","A3338B"];return function(t,r){void 0===r&&(r={});var a=r.colors;void 0===a&&(a=e);var o=0,i={},s=new RegExp("("+/[\u4E00-\u9FFF\u3400-\u4dbf\uf900-\ufaff\u3040-\u309f\uac00-\ud7af\u0400-\u04FF]+|\w+/.source+"|"+/</.source+")|("+n().source+")","gmi");return t.replace(s,(function(n,e,t){if(t)return function(n){return'<span style="color: slategray">'+n+"</span>"}(t);if("<"===e)return"&lt;";var r;i[e]?r=i[e]:(r=a[o],i[e]=r);var s='<span style="color: #'+r+'">'+e+"</span>";return o=++o%a.length,s}))}}()},function(n,e,t){"use strict";var r=Date.prototype.getDay,a=Object.prototype.toString,o=t(24)();n.exports=function(n){return"object"==typeof n&&null!==n&&(o?function(n){try{return r.call(n),!0}catch(n){return!1}}(n):"[object Date]"===a.call(n))}},function(n,e,t){"use strict";var r=String.prototype.valueOf,a=Object.prototype.toString,o=t(24)();n.exports=function(n){return"string"==typeof n||"object"==typeof n&&(o?function(n){try{return r.call(n),!0}catch(n){return!1}}(n):"[object String]"===a.call(n))}},function(n,e,t){"use strict";var r=Object.prototype.toString;if(t(22)()){var a=Symbol.prototype.toString,o=/^Symbol\(.*\)$/;n.exports=function(n){if("symbol"==typeof n)return!0;if("[object Symbol]"!==r.call(n))return!1;try{return function(n){return"symbol"==typeof n.valueOf()&&o.test(a.call(n))}(n)}catch(n){return!1}}}else n.exports=function(n){return!1}},function(n,e,t){!function(n){"use strict";function e(n,e){for(var t=0;t<e.length;t++){var r=e[t];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(n,r.key,r)}}function t(n,t,r){return t&&e(n.prototype,t),r&&e(n,r),Object.defineProperty(n,"prototype",{writable:!1}),n}function r(n,e){if(n){if("string"==typeof n)return a(n,e);var t=Object.prototype.toString.call(n).slice(8,-1);return"Object"===t&&n.constructor&&(t=n.constructor.name),"Map"===t||"Set"===t?Array.from(n):"Arguments"===t||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t)?a(n,e):void 0}}function a(n,e){(null==e||e>n.length)&&(e=n.length);for(var t=0,r=new Array(e);t<e;t++)r[t]=n[t];return r}function o(n,e){var t="undefined"!=typeof Symbol&&n[Symbol.iterator]||n["@@iterator"];if(t)return(t=t.call(n)).next.bind(t);if(Array.isArray(n)||(t=r(n))||e&&n&&"number"==typeof n.length){t&&(n=t);var a=0;return function(){return a>=n.length?{done:!0}:{done:!1,value:n[a++]}}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}function i(){return{baseUrl:null,breaks:!1,extensions:null,gfm:!0,headerIds:!0,headerPrefix:"",highlight:null,langPrefix:"language-",mangle:!0,pedantic:!1,renderer:null,sanitize:!1,sanitizer:null,silent:!1,smartLists:!1,smartypants:!1,tokenizer:null,walkTokens:null,xhtml:!1}}function s(e){n.defaults=e}function l(n,e){if(e){if(S.test(n))return n.replace(x,C)}else if(w.test(n))return n.replace(E,C);return n}function c(n){return n.replace(I,(function(n,e){return"colon"===(e=e.toLowerCase())?":":"#"===e.charAt(0)?"x"===e.charAt(1)?String.fromCharCode(parseInt(e.substring(2),16)):String.fromCharCode(+e.substring(1)):""}))}function p(n,e){n=n.source||n,e=e||"";var t={replace:function(e,r){return r=(r=r.source||r).replace(T,"$1"),n=n.replace(e,r),t},getRegex:function(){return new RegExp(n,e)}};return t}function u(n,e,t){if(n){var r;try{r=decodeURIComponent(c(t)).replace(O,"").toLowerCase()}catch(n){return null}if(0===r.indexOf("javascript:")||0===r.indexOf("vbscript:")||0===r.indexOf("data:"))return null}e&&!A.test(t)&&(t=function(n,e){_[" "+n]||(R.test(n)?_[" "+n]=n+"/":_[" "+n]=g(n,"/",!0));var t=-1===(n=_[" "+n]).indexOf(":");return"//"===e.substring(0,2)?t?e:n.replace(P,"$1")+e:"/"===e.charAt(0)?t?e:n.replace(F,"$1")+e:n+e}(e,t));try{t=encodeURI(t).replace(/%25/g,"%")}catch(n){return null}return t}function d(n){for(var e,t,r=1;r<arguments.length;r++)for(t in e=arguments[r])Object.prototype.hasOwnProperty.call(e,t)&&(n[t]=e[t]);return n}function m(n,e){var t=n.replace(/\|/g,(function(n,e,t){for(var r=!1,a=e;--a>=0&&"\\"===t[a];)r=!r;return r?"|":" |"})).split(/ \|/),r=0;if(t[0].trim()||t.shift(),t.length>0&&!t[t.length-1].trim()&&t.pop(),t.length>e)t.splice(e);else for(;t.length<e;)t.push("");for(;r<t.length;r++)t[r]=t[r].trim().replace(/\\\|/g,"|");return t}function g(n,e,t){var r=n.length;if(0===r)return"";for(var a=0;a<r;){var o=n.charAt(r-a-1);if(o!==e||t){if(o===e||!t)break;a++}else a++}return n.substr(0,r-a)}function f(n){n&&n.sanitize&&n.silent}function h(n,e){if(e<1)return"";for(var t="";e>1;)1&e&&(t+=n),e>>=1,n+=n;return t+n}function v(n,e,t,r){var a=e.href,o=e.title?l(e.title):null,i=n[1].replace(/\\([\[\]])/g,"$1");if("!"!==n[0].charAt(0)){r.state.inLink=!0;var s={type:"link",raw:t,href:a,title:o,text:i,tokens:r.inlineTokens(i,[])};return r.state.inLink=!1,s}return{type:"image",raw:t,href:a,title:o,text:l(i)}}function b(n){return n.replace(/---/g,"—").replace(/--/g,"–").replace(/(^|[-\u2014/(\[{"\s])'/g,"$1‘").replace(/'/g,"’").replace(/(^|[-\u2014/(\[{\u2018\s])"/g,"$1“").replace(/"/g,"”").replace(/\.{3}/g,"…")}function k(n){var e,t,r="",a=n.length;for(e=0;e<a;e++)t=n.charCodeAt(e),Math.random()>.5&&(t="x"+t.toString(16)),r+="&#"+t+";";return r}function y(n,e,t){if(null==n)throw new Error("marked(): input parameter is undefined or null");if("string"!=typeof n)throw new Error("marked(): input parameter is of type "+Object.prototype.toString.call(n)+", string expected");if("function"==typeof e&&(t=e,e=null),f(e=d({},y.defaults,e||{})),t){var r,a=e.highlight;try{r=N.lex(n,e)}catch(n){return t(n)}var o=function(n){var o;if(!n)try{e.walkTokens&&y.walkTokens(r,e.walkTokens),o=H.parse(r,e)}catch(e){n=e}return e.highlight=a,n?t(n):t(null,o)};if(!a||a.length<3)return o();if(delete e.highlight,!r.length)return o();var i=0;return y.walkTokens(r,(function(n){"code"===n.type&&(i++,setTimeout((function(){a(n.text,n.lang,(function(e,t){if(e)return o(e);null!=t&&t!==n.text&&(n.text=t,n.escaped=!0),0==--i&&o()}))}),0))})),void(0===i&&o())}try{var s=N.lex(n,e);return e.walkTokens&&y.walkTokens(s,e.walkTokens),H.parse(s,e)}catch(n){if(n.message+="\nPlease report this to https://github.com/markedjs/marked.",e.silent)return"<p>An error occurred:</p><pre>"+l(n.message+"",!0)+"</pre>";throw n}}n.defaults={baseUrl:null,breaks:!1,extensions:null,gfm:!0,headerIds:!0,headerPrefix:"",highlight:null,langPrefix:"language-",mangle:!0,pedantic:!1,renderer:null,sanitize:!1,sanitizer:null,silent:!1,smartLists:!1,smartypants:!1,tokenizer:null,walkTokens:null,xhtml:!1};var S=/[&<>"']/,x=/[&<>"']/g,w=/[<>"']|&(?!#?\w+;)/,E=/[<>"']|&(?!#?\w+;)/g,D={"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&quot;","'":"&#39;"},C=function(n){return D[n]},I=/&(#(?:\d+)|(?:#x[0-9A-Fa-f]+)|(?:\w+));?/gi,T=/(^|[^\[])\^/g,O=/[^\w:]/g,A=/^$|^[a-z][a-z0-9+.-]*:|^[?#]/i,_={},R=/^[^:]+:\/*[^/]*$/,P=/^([^:]+:)[\s\S]*$/,F=/^([^:]+:\/*[^/]*)[\s\S]*$/,B={exec:function(){}},M=function(){function e(e){this.options=e||n.defaults}var t=e.prototype;return t.space=function(n){var e=this.rules.block.newline.exec(n);if(e&&e[0].length>0)return{type:"space",raw:e[0]}},t.code=function(n){var e=this.rules.block.code.exec(n);if(e){var t=e[0].replace(/^ {1,4}/gm,"");return{type:"code",raw:e[0],codeBlockStyle:"indented",text:this.options.pedantic?t:g(t,"\n")}}},t.fences=function(n){var e=this.rules.block.fences.exec(n);if(e){var t=e[0],r=function(n,e){var t=n.match(/^(\s+)(?:```)/);if(null===t)return e;var r=t[1];return e.split("\n").map((function(n){var e=n.match(/^\s+/);return null===e?n:e[0].length>=r.length?n.slice(r.length):n})).join("\n")}(t,e[3]||"");return{type:"code",raw:t,lang:e[2]?e[2].trim():e[2],text:r}}},t.heading=function(n){var e=this.rules.block.heading.exec(n);if(e){var t=e[2].trim();if(/#$/.test(t)){var r=g(t,"#");this.options.pedantic?t=r.trim():r&&!/ $/.test(r)||(t=r.trim())}var a={type:"heading",raw:e[0],depth:e[1].length,text:t,tokens:[]};return this.lexer.inline(a.text,a.tokens),a}},t.hr=function(n){var e=this.rules.block.hr.exec(n);if(e)return{type:"hr",raw:e[0]}},t.blockquote=function(n){var e=this.rules.block.blockquote.exec(n);if(e){var t=e[0].replace(/^ *> ?/gm,"");return{type:"blockquote",raw:e[0],tokens:this.lexer.blockTokens(t,[]),text:t}}},t.list=function(n){var e=this.rules.block.list.exec(n);if(e){var t,r,a,i,s,l,c,p,u,d,m,g,f=e[1].trim(),h=f.length>1,v={type:"list",raw:"",ordered:h,start:h?+f.slice(0,-1):"",loose:!1,items:[]};f=h?"\\d{1,9}\\"+f.slice(-1):"\\"+f,this.options.pedantic&&(f=h?f:"[*+-]");for(var b=new RegExp("^( {0,3}"+f+")((?: [^\\n]*)?(?:\\n|$))");n&&(g=!1,e=b.exec(n))&&!this.rules.block.hr.test(n);){if(t=e[0],n=n.substring(t.length),p=e[2].split("\n",1)[0],u=n.split("\n",1)[0],this.options.pedantic?(i=2,m=p.trimLeft()):(i=(i=e[2].search(/[^ ]/))>4?1:i,m=p.slice(i),i+=e[1].length),l=!1,!p&&/^ *$/.test(u)&&(t+=u+"\n",n=n.substring(u.length+1),g=!0),!g)for(var k=new RegExp("^ {0,"+Math.min(3,i-1)+"}(?:[*+-]|\\d{1,9}[.)])");n&&(p=d=n.split("\n",1)[0],this.options.pedantic&&(p=p.replace(/^ {1,4}(?=( {4})*[^ ])/g,"  ")),!k.test(p));){if(p.search(/[^ ]/)>=i||!p.trim())m+="\n"+p.slice(i);else{if(l)break;m+="\n"+p}l||p.trim()||(l=!0),t+=d+"\n",n=n.substring(d.length+1)}v.loose||(c?v.loose=!0:/\n *\n *$/.test(t)&&(c=!0)),this.options.gfm&&(r=/^\[[ xX]\] /.exec(m))&&(a="[ ] "!==r[0],m=m.replace(/^\[[ xX]\] +/,"")),v.items.push({type:"list_item",raw:t,task:!!r,checked:a,loose:!1,text:m}),v.raw+=t}v.items[v.items.length-1].raw=t.trimRight(),v.items[v.items.length-1].text=m.trimRight(),v.raw=v.raw.trimRight();var y=v.items.length;for(s=0;s<y;s++){this.lexer.state.top=!1,v.items[s].tokens=this.lexer.blockTokens(v.items[s].text,[]);var S=v.items[s].tokens.filter((function(n){return"space"===n.type})),x=S.every((function(n){for(var e,t=0,r=o(n.raw.split(""));!(e=r()).done;)if("\n"===e.value&&(t+=1),t>1)return!0;return!1}));!v.loose&&S.length&&x&&(v.loose=!0,v.items[s].loose=!0)}return v}},t.html=function(n){var e=this.rules.block.html.exec(n);if(e){var t={type:"html",raw:e[0],pre:!this.options.sanitizer&&("pre"===e[1]||"script"===e[1]||"style"===e[1]),text:e[0]};return this.options.sanitize&&(t.type="paragraph",t.text=this.options.sanitizer?this.options.sanitizer(e[0]):l(e[0]),t.tokens=[],this.lexer.inline(t.text,t.tokens)),t}},t.def=function(n){var e=this.rules.block.def.exec(n);if(e)return e[3]&&(e[3]=e[3].substring(1,e[3].length-1)),{type:"def",tag:e[1].toLowerCase().replace(/\s+/g," "),raw:e[0],href:e[2],title:e[3]}},t.table=function(n){var e=this.rules.block.table.exec(n);if(e){var t={type:"table",header:m(e[1]).map((function(n){return{text:n}})),align:e[2].replace(/^ *|\| *$/g,"").split(/ *\| */),rows:e[3]&&e[3].trim()?e[3].replace(/\n[ \t]*$/,"").split("\n"):[]};if(t.header.length===t.align.length){t.raw=e[0];var r,a,o,i,s=t.align.length;for(r=0;r<s;r++)/^ *-+: *$/.test(t.align[r])?t.align[r]="right":/^ *:-+: *$/.test(t.align[r])?t.align[r]="center":/^ *:-+ *$/.test(t.align[r])?t.align[r]="left":t.align[r]=null;for(s=t.rows.length,r=0;r<s;r++)t.rows[r]=m(t.rows[r],t.header.length).map((function(n){return{text:n}}));for(s=t.header.length,a=0;a<s;a++)t.header[a].tokens=[],this.lexer.inlineTokens(t.header[a].text,t.header[a].tokens);for(s=t.rows.length,a=0;a<s;a++)for(i=t.rows[a],o=0;o<i.length;o++)i[o].tokens=[],this.lexer.inlineTokens(i[o].text,i[o].tokens);return t}}},t.lheading=function(n){var e=this.rules.block.lheading.exec(n);if(e){var t={type:"heading",raw:e[0],depth:"="===e[2].charAt(0)?1:2,text:e[1],tokens:[]};return this.lexer.inline(t.text,t.tokens),t}},t.paragraph=function(n){var e=this.rules.block.paragraph.exec(n);if(e){var t={type:"paragraph",raw:e[0],text:"\n"===e[1].charAt(e[1].length-1)?e[1].slice(0,-1):e[1],tokens:[]};return this.lexer.inline(t.text,t.tokens),t}},t.text=function(n){var e=this.rules.block.text.exec(n);if(e){var t={type:"text",raw:e[0],text:e[0],tokens:[]};return this.lexer.inline(t.text,t.tokens),t}},t.escape=function(n){var e=this.rules.inline.escape.exec(n);if(e)return{type:"escape",raw:e[0],text:l(e[1])}},t.tag=function(n){var e=this.rules.inline.tag.exec(n);if(e)return!this.lexer.state.inLink&&/^<a /i.test(e[0])?this.lexer.state.inLink=!0:this.lexer.state.inLink&&/^<\/a>/i.test(e[0])&&(this.lexer.state.inLink=!1),!this.lexer.state.inRawBlock&&/^<(pre|code|kbd|script)(\s|>)/i.test(e[0])?this.lexer.state.inRawBlock=!0:this.lexer.state.inRawBlock&&/^<\/(pre|code|kbd|script)(\s|>)/i.test(e[0])&&(this.lexer.state.inRawBlock=!1),{type:this.options.sanitize?"text":"html",raw:e[0],inLink:this.lexer.state.inLink,inRawBlock:this.lexer.state.inRawBlock,text:this.options.sanitize?this.options.sanitizer?this.options.sanitizer(e[0]):l(e[0]):e[0]}},t.link=function(n){var e=this.rules.inline.link.exec(n);if(e){var t=e[2].trim();if(!this.options.pedantic&&/^</.test(t)){if(!/>$/.test(t))return;var r=g(t.slice(0,-1),"\\");if((t.length-r.length)%2==0)return}else{var a=function(n,e){if(-1===n.indexOf(e[1]))return-1;for(var t=n.length,r=0,a=0;a<t;a++)if("\\"===n[a])a++;else if(n[a]===e[0])r++;else if(n[a]===e[1]&&--r<0)return a;return-1}(e[2],"()");if(a>-1){var o=(0===e[0].indexOf("!")?5:4)+e[1].length+a;e[2]=e[2].substring(0,a),e[0]=e[0].substring(0,o).trim(),e[3]=""}}var i=e[2],s="";if(this.options.pedantic){var l=/^([^'"]*[^\s])\s+(['"])(.*)\2/.exec(i);l&&(i=l[1],s=l[3])}else s=e[3]?e[3].slice(1,-1):"";return i=i.trim(),/^</.test(i)&&(i=this.options.pedantic&&!/>$/.test(t)?i.slice(1):i.slice(1,-1)),v(e,{href:i?i.replace(this.rules.inline._escapes,"$1"):i,title:s?s.replace(this.rules.inline._escapes,"$1"):s},e[0],this.lexer)}},t.reflink=function(n,e){var t;if((t=this.rules.inline.reflink.exec(n))||(t=this.rules.inline.nolink.exec(n))){var r=(t[2]||t[1]).replace(/\s+/g," ");if(!(r=e[r.toLowerCase()])||!r.href){var a=t[0].charAt(0);return{type:"text",raw:a,text:a}}return v(t,r,t[0],this.lexer)}},t.emStrong=function(n,e,t){void 0===t&&(t="");var r=this.rules.inline.emStrong.lDelim.exec(n);if(r&&(!r[3]||!t.match(/(?:[0-9A-Za-z\xAA\xB2\xB3\xB5\xB9\xBA\xBC-\xBE\xC0-\xD6\xD8-\xF6\xF8-\u02C1\u02C6-\u02D1\u02E0-\u02E4\u02EC\u02EE\u0370-\u0374\u0376\u0377\u037A-\u037D\u037F\u0386\u0388-\u038A\u038C\u038E-\u03A1\u03A3-\u03F5\u03F7-\u0481\u048A-\u052F\u0531-\u0556\u0559\u0560-\u0588\u05D0-\u05EA\u05EF-\u05F2\u0620-\u064A\u0660-\u0669\u066E\u066F\u0671-\u06D3\u06D5\u06E5\u06E6\u06EE-\u06FC\u06FF\u0710\u0712-\u072F\u074D-\u07A5\u07B1\u07C0-\u07EA\u07F4\u07F5\u07FA\u0800-\u0815\u081A\u0824\u0828\u0840-\u0858\u0860-\u086A\u0870-\u0887\u0889-\u088E\u08A0-\u08C9\u0904-\u0939\u093D\u0950\u0958-\u0961\u0966-\u096F\u0971-\u0980\u0985-\u098C\u098F\u0990\u0993-\u09A8\u09AA-\u09B0\u09B2\u09B6-\u09B9\u09BD\u09CE\u09DC\u09DD\u09DF-\u09E1\u09E6-\u09F1\u09F4-\u09F9\u09FC\u0A05-\u0A0A\u0A0F\u0A10\u0A13-\u0A28\u0A2A-\u0A30\u0A32\u0A33\u0A35\u0A36\u0A38\u0A39\u0A59-\u0A5C\u0A5E\u0A66-\u0A6F\u0A72-\u0A74\u0A85-\u0A8D\u0A8F-\u0A91\u0A93-\u0AA8\u0AAA-\u0AB0\u0AB2\u0AB3\u0AB5-\u0AB9\u0ABD\u0AD0\u0AE0\u0AE1\u0AE6-\u0AEF\u0AF9\u0B05-\u0B0C\u0B0F\u0B10\u0B13-\u0B28\u0B2A-\u0B30\u0B32\u0B33\u0B35-\u0B39\u0B3D\u0B5C\u0B5D\u0B5F-\u0B61\u0B66-\u0B6F\u0B71-\u0B77\u0B83\u0B85-\u0B8A\u0B8E-\u0B90\u0B92-\u0B95\u0B99\u0B9A\u0B9C\u0B9E\u0B9F\u0BA3\u0BA4\u0BA8-\u0BAA\u0BAE-\u0BB9\u0BD0\u0BE6-\u0BF2\u0C05-\u0C0C\u0C0E-\u0C10\u0C12-\u0C28\u0C2A-\u0C39\u0C3D\u0C58-\u0C5A\u0C5D\u0C60\u0C61\u0C66-\u0C6F\u0C78-\u0C7E\u0C80\u0C85-\u0C8C\u0C8E-\u0C90\u0C92-\u0CA8\u0CAA-\u0CB3\u0CB5-\u0CB9\u0CBD\u0CDD\u0CDE\u0CE0\u0CE1\u0CE6-\u0CEF\u0CF1\u0CF2\u0D04-\u0D0C\u0D0E-\u0D10\u0D12-\u0D3A\u0D3D\u0D4E\u0D54-\u0D56\u0D58-\u0D61\u0D66-\u0D78\u0D7A-\u0D7F\u0D85-\u0D96\u0D9A-\u0DB1\u0DB3-\u0DBB\u0DBD\u0DC0-\u0DC6\u0DE6-\u0DEF\u0E01-\u0E30\u0E32\u0E33\u0E40-\u0E46\u0E50-\u0E59\u0E81\u0E82\u0E84\u0E86-\u0E8A\u0E8C-\u0EA3\u0EA5\u0EA7-\u0EB0\u0EB2\u0EB3\u0EBD\u0EC0-\u0EC4\u0EC6\u0ED0-\u0ED9\u0EDC-\u0EDF\u0F00\u0F20-\u0F33\u0F40-\u0F47\u0F49-\u0F6C\u0F88-\u0F8C\u1000-\u102A\u103F-\u1049\u1050-\u1055\u105A-\u105D\u1061\u1065\u1066\u106E-\u1070\u1075-\u1081\u108E\u1090-\u1099\u10A0-\u10C5\u10C7\u10CD\u10D0-\u10FA\u10FC-\u1248\u124A-\u124D\u1250-\u1256\u1258\u125A-\u125D\u1260-\u1288\u128A-\u128D\u1290-\u12B0\u12B2-\u12B5\u12B8-\u12BE\u12C0\u12C2-\u12C5\u12C8-\u12D6\u12D8-\u1310\u1312-\u1315\u1318-\u135A\u1369-\u137C\u1380-\u138F\u13A0-\u13F5\u13F8-\u13FD\u1401-\u166C\u166F-\u167F\u1681-\u169A\u16A0-\u16EA\u16EE-\u16F8\u1700-\u1711\u171F-\u1731\u1740-\u1751\u1760-\u176C\u176E-\u1770\u1780-\u17B3\u17D7\u17DC\u17E0-\u17E9\u17F0-\u17F9\u1810-\u1819\u1820-\u1878\u1880-\u1884\u1887-\u18A8\u18AA\u18B0-\u18F5\u1900-\u191E\u1946-\u196D\u1970-\u1974\u1980-\u19AB\u19B0-\u19C9\u19D0-\u19DA\u1A00-\u1A16\u1A20-\u1A54\u1A80-\u1A89\u1A90-\u1A99\u1AA7\u1B05-\u1B33\u1B45-\u1B4C\u1B50-\u1B59\u1B83-\u1BA0\u1BAE-\u1BE5\u1C00-\u1C23\u1C40-\u1C49\u1C4D-\u1C7D\u1C80-\u1C88\u1C90-\u1CBA\u1CBD-\u1CBF\u1CE9-\u1CEC\u1CEE-\u1CF3\u1CF5\u1CF6\u1CFA\u1D00-\u1DBF\u1E00-\u1F15\u1F18-\u1F1D\u1F20-\u1F45\u1F48-\u1F4D\u1F50-\u1F57\u1F59\u1F5B\u1F5D\u1F5F-\u1F7D\u1F80-\u1FB4\u1FB6-\u1FBC\u1FBE\u1FC2-\u1FC4\u1FC6-\u1FCC\u1FD0-\u1FD3\u1FD6-\u1FDB\u1FE0-\u1FEC\u1FF2-\u1FF4\u1FF6-\u1FFC\u2070\u2071\u2074-\u2079\u207F-\u2089\u2090-\u209C\u2102\u2107\u210A-\u2113\u2115\u2119-\u211D\u2124\u2126\u2128\u212A-\u212D\u212F-\u2139\u213C-\u213F\u2145-\u2149\u214E\u2150-\u2189\u2460-\u249B\u24EA-\u24FF\u2776-\u2793\u2C00-\u2CE4\u2CEB-\u2CEE\u2CF2\u2CF3\u2CFD\u2D00-\u2D25\u2D27\u2D2D\u2D30-\u2D67\u2D6F\u2D80-\u2D96\u2DA0-\u2DA6\u2DA8-\u2DAE\u2DB0-\u2DB6\u2DB8-\u2DBE\u2DC0-\u2DC6\u2DC8-\u2DCE\u2DD0-\u2DD6\u2DD8-\u2DDE\u2E2F\u3005-\u3007\u3021-\u3029\u3031-\u3035\u3038-\u303C\u3041-\u3096\u309D-\u309F\u30A1-\u30FA\u30FC-\u30FF\u3105-\u312F\u3131-\u318E\u3192-\u3195\u31A0-\u31BF\u31F0-\u31FF\u3220-\u3229\u3248-\u324F\u3251-\u325F\u3280-\u3289\u32B1-\u32BF\u3400-\u4DBF\u4E00-\uA48C\uA4D0-\uA4FD\uA500-\uA60C\uA610-\uA62B\uA640-\uA66E\uA67F-\uA69D\uA6A0-\uA6EF\uA717-\uA71F\uA722-\uA788\uA78B-\uA7CA\uA7D0\uA7D1\uA7D3\uA7D5-\uA7D9\uA7F2-\uA801\uA803-\uA805\uA807-\uA80A\uA80C-\uA822\uA830-\uA835\uA840-\uA873\uA882-\uA8B3\uA8D0-\uA8D9\uA8F2-\uA8F7\uA8FB\uA8FD\uA8FE\uA900-\uA925\uA930-\uA946\uA960-\uA97C\uA984-\uA9B2\uA9CF-\uA9D9\uA9E0-\uA9E4\uA9E6-\uA9FE\uAA00-\uAA28\uAA40-\uAA42\uAA44-\uAA4B\uAA50-\uAA59\uAA60-\uAA76\uAA7A\uAA7E-\uAAAF\uAAB1\uAAB5\uAAB6\uAAB9-\uAABD\uAAC0\uAAC2\uAADB-\uAADD\uAAE0-\uAAEA\uAAF2-\uAAF4\uAB01-\uAB06\uAB09-\uAB0E\uAB11-\uAB16\uAB20-\uAB26\uAB28-\uAB2E\uAB30-\uAB5A\uAB5C-\uAB69\uAB70-\uABE2\uABF0-\uABF9\uAC00-\uD7A3\uD7B0-\uD7C6\uD7CB-\uD7FB\uF900-\uFA6D\uFA70-\uFAD9\uFB00-\uFB06\uFB13-\uFB17\uFB1D\uFB1F-\uFB28\uFB2A-\uFB36\uFB38-\uFB3C\uFB3E\uFB40\uFB41\uFB43\uFB44\uFB46-\uFBB1\uFBD3-\uFD3D\uFD50-\uFD8F\uFD92-\uFDC7\uFDF0-\uFDFB\uFE70-\uFE74\uFE76-\uFEFC\uFF10-\uFF19\uFF21-\uFF3A\uFF41-\uFF5A\uFF66-\uFFBE\uFFC2-\uFFC7\uFFCA-\uFFCF\uFFD2-\uFFD7\uFFDA-\uFFDC]|\uD800[\uDC00-\uDC0B\uDC0D-\uDC26\uDC28-\uDC3A\uDC3C\uDC3D\uDC3F-\uDC4D\uDC50-\uDC5D\uDC80-\uDCFA\uDD07-\uDD33\uDD40-\uDD78\uDD8A\uDD8B\uDE80-\uDE9C\uDEA0-\uDED0\uDEE1-\uDEFB\uDF00-\uDF23\uDF2D-\uDF4A\uDF50-\uDF75\uDF80-\uDF9D\uDFA0-\uDFC3\uDFC8-\uDFCF\uDFD1-\uDFD5]|\uD801[\uDC00-\uDC9D\uDCA0-\uDCA9\uDCB0-\uDCD3\uDCD8-\uDCFB\uDD00-\uDD27\uDD30-\uDD63\uDD70-\uDD7A\uDD7C-\uDD8A\uDD8C-\uDD92\uDD94\uDD95\uDD97-\uDDA1\uDDA3-\uDDB1\uDDB3-\uDDB9\uDDBB\uDDBC\uDE00-\uDF36\uDF40-\uDF55\uDF60-\uDF67\uDF80-\uDF85\uDF87-\uDFB0\uDFB2-\uDFBA]|\uD802[\uDC00-\uDC05\uDC08\uDC0A-\uDC35\uDC37\uDC38\uDC3C\uDC3F-\uDC55\uDC58-\uDC76\uDC79-\uDC9E\uDCA7-\uDCAF\uDCE0-\uDCF2\uDCF4\uDCF5\uDCFB-\uDD1B\uDD20-\uDD39\uDD80-\uDDB7\uDDBC-\uDDCF\uDDD2-\uDE00\uDE10-\uDE13\uDE15-\uDE17\uDE19-\uDE35\uDE40-\uDE48\uDE60-\uDE7E\uDE80-\uDE9F\uDEC0-\uDEC7\uDEC9-\uDEE4\uDEEB-\uDEEF\uDF00-\uDF35\uDF40-\uDF55\uDF58-\uDF72\uDF78-\uDF91\uDFA9-\uDFAF]|\uD803[\uDC00-\uDC48\uDC80-\uDCB2\uDCC0-\uDCF2\uDCFA-\uDD23\uDD30-\uDD39\uDE60-\uDE7E\uDE80-\uDEA9\uDEB0\uDEB1\uDF00-\uDF27\uDF30-\uDF45\uDF51-\uDF54\uDF70-\uDF81\uDFB0-\uDFCB\uDFE0-\uDFF6]|\uD804[\uDC03-\uDC37\uDC52-\uDC6F\uDC71\uDC72\uDC75\uDC83-\uDCAF\uDCD0-\uDCE8\uDCF0-\uDCF9\uDD03-\uDD26\uDD36-\uDD3F\uDD44\uDD47\uDD50-\uDD72\uDD76\uDD83-\uDDB2\uDDC1-\uDDC4\uDDD0-\uDDDA\uDDDC\uDDE1-\uDDF4\uDE00-\uDE11\uDE13-\uDE2B\uDE80-\uDE86\uDE88\uDE8A-\uDE8D\uDE8F-\uDE9D\uDE9F-\uDEA8\uDEB0-\uDEDE\uDEF0-\uDEF9\uDF05-\uDF0C\uDF0F\uDF10\uDF13-\uDF28\uDF2A-\uDF30\uDF32\uDF33\uDF35-\uDF39\uDF3D\uDF50\uDF5D-\uDF61]|\uD805[\uDC00-\uDC34\uDC47-\uDC4A\uDC50-\uDC59\uDC5F-\uDC61\uDC80-\uDCAF\uDCC4\uDCC5\uDCC7\uDCD0-\uDCD9\uDD80-\uDDAE\uDDD8-\uDDDB\uDE00-\uDE2F\uDE44\uDE50-\uDE59\uDE80-\uDEAA\uDEB8\uDEC0-\uDEC9\uDF00-\uDF1A\uDF30-\uDF3B\uDF40-\uDF46]|\uD806[\uDC00-\uDC2B\uDCA0-\uDCF2\uDCFF-\uDD06\uDD09\uDD0C-\uDD13\uDD15\uDD16\uDD18-\uDD2F\uDD3F\uDD41\uDD50-\uDD59\uDDA0-\uDDA7\uDDAA-\uDDD0\uDDE1\uDDE3\uDE00\uDE0B-\uDE32\uDE3A\uDE50\uDE5C-\uDE89\uDE9D\uDEB0-\uDEF8]|\uD807[\uDC00-\uDC08\uDC0A-\uDC2E\uDC40\uDC50-\uDC6C\uDC72-\uDC8F\uDD00-\uDD06\uDD08\uDD09\uDD0B-\uDD30\uDD46\uDD50-\uDD59\uDD60-\uDD65\uDD67\uDD68\uDD6A-\uDD89\uDD98\uDDA0-\uDDA9\uDEE0-\uDEF2\uDFB0\uDFC0-\uDFD4]|\uD808[\uDC00-\uDF99]|\uD809[\uDC00-\uDC6E\uDC80-\uDD43]|\uD80B[\uDF90-\uDFF0]|[\uD80C\uD81C-\uD820\uD822\uD840-\uD868\uD86A-\uD86C\uD86F-\uD872\uD874-\uD879\uD880-\uD883][\uDC00-\uDFFF]|\uD80D[\uDC00-\uDC2E]|\uD811[\uDC00-\uDE46]|\uD81A[\uDC00-\uDE38\uDE40-\uDE5E\uDE60-\uDE69\uDE70-\uDEBE\uDEC0-\uDEC9\uDED0-\uDEED\uDF00-\uDF2F\uDF40-\uDF43\uDF50-\uDF59\uDF5B-\uDF61\uDF63-\uDF77\uDF7D-\uDF8F]|\uD81B[\uDE40-\uDE96\uDF00-\uDF4A\uDF50\uDF93-\uDF9F\uDFE0\uDFE1\uDFE3]|\uD821[\uDC00-\uDFF7]|\uD823[\uDC00-\uDCD5\uDD00-\uDD08]|\uD82B[\uDFF0-\uDFF3\uDFF5-\uDFFB\uDFFD\uDFFE]|\uD82C[\uDC00-\uDD22\uDD50-\uDD52\uDD64-\uDD67\uDD70-\uDEFB]|\uD82F[\uDC00-\uDC6A\uDC70-\uDC7C\uDC80-\uDC88\uDC90-\uDC99]|\uD834[\uDEE0-\uDEF3\uDF60-\uDF78]|\uD835[\uDC00-\uDC54\uDC56-\uDC9C\uDC9E\uDC9F\uDCA2\uDCA5\uDCA6\uDCA9-\uDCAC\uDCAE-\uDCB9\uDCBB\uDCBD-\uDCC3\uDCC5-\uDD05\uDD07-\uDD0A\uDD0D-\uDD14\uDD16-\uDD1C\uDD1E-\uDD39\uDD3B-\uDD3E\uDD40-\uDD44\uDD46\uDD4A-\uDD50\uDD52-\uDEA5\uDEA8-\uDEC0\uDEC2-\uDEDA\uDEDC-\uDEFA\uDEFC-\uDF14\uDF16-\uDF34\uDF36-\uDF4E\uDF50-\uDF6E\uDF70-\uDF88\uDF8A-\uDFA8\uDFAA-\uDFC2\uDFC4-\uDFCB\uDFCE-\uDFFF]|\uD837[\uDF00-\uDF1E]|\uD838[\uDD00-\uDD2C\uDD37-\uDD3D\uDD40-\uDD49\uDD4E\uDE90-\uDEAD\uDEC0-\uDEEB\uDEF0-\uDEF9]|\uD839[\uDFE0-\uDFE6\uDFE8-\uDFEB\uDFED\uDFEE\uDFF0-\uDFFE]|\uD83A[\uDC00-\uDCC4\uDCC7-\uDCCF\uDD00-\uDD43\uDD4B\uDD50-\uDD59]|\uD83B[\uDC71-\uDCAB\uDCAD-\uDCAF\uDCB1-\uDCB4\uDD01-\uDD2D\uDD2F-\uDD3D\uDE00-\uDE03\uDE05-\uDE1F\uDE21\uDE22\uDE24\uDE27\uDE29-\uDE32\uDE34-\uDE37\uDE39\uDE3B\uDE42\uDE47\uDE49\uDE4B\uDE4D-\uDE4F\uDE51\uDE52\uDE54\uDE57\uDE59\uDE5B\uDE5D\uDE5F\uDE61\uDE62\uDE64\uDE67-\uDE6A\uDE6C-\uDE72\uDE74-\uDE77\uDE79-\uDE7C\uDE7E\uDE80-\uDE89\uDE8B-\uDE9B\uDEA1-\uDEA3\uDEA5-\uDEA9\uDEAB-\uDEBB]|\uD83C[\uDD00-\uDD0C]|\uD83E[\uDFF0-\uDFF9]|\uD869[\uDC00-\uDEDF\uDF00-\uDFFF]|\uD86D[\uDC00-\uDF38\uDF40-\uDFFF]|\uD86E[\uDC00-\uDC1D\uDC20-\uDFFF]|\uD873[\uDC00-\uDEA1\uDEB0-\uDFFF]|\uD87A[\uDC00-\uDFE0]|\uD87E[\uDC00-\uDE1D]|\uD884[\uDC00-\uDF4A])/))){var a=r[1]||r[2]||"";if(!a||a&&(""===t||this.rules.inline.punctuation.exec(t))){var o,i,s=r[0].length-1,l=s,c=0,p="*"===r[0][0]?this.rules.inline.emStrong.rDelimAst:this.rules.inline.emStrong.rDelimUnd;for(p.lastIndex=0,e=e.slice(-1*n.length+s);null!=(r=p.exec(e));)if(o=r[1]||r[2]||r[3]||r[4]||r[5]||r[6])if(i=o.length,r[3]||r[4])l+=i;else if(!((r[5]||r[6])&&s%3)||(s+i)%3){if(!((l-=i)>0)){if(i=Math.min(i,i+l+c),Math.min(s,i)%2){var u=n.slice(1,s+r.index+i);return{type:"em",raw:n.slice(0,s+r.index+i+1),text:u,tokens:this.lexer.inlineTokens(u,[])}}var d=n.slice(2,s+r.index+i-1);return{type:"strong",raw:n.slice(0,s+r.index+i+1),text:d,tokens:this.lexer.inlineTokens(d,[])}}}else c+=i}}},t.codespan=function(n){var e=this.rules.inline.code.exec(n);if(e){var t=e[2].replace(/\n/g," "),r=/[^ ]/.test(t),a=/^ /.test(t)&&/ $/.test(t);return r&&a&&(t=t.substring(1,t.length-1)),t=l(t,!0),{type:"codespan",raw:e[0],text:t}}},t.br=function(n){var e=this.rules.inline.br.exec(n);if(e)return{type:"br",raw:e[0]}},t.del=function(n){var e=this.rules.inline.del.exec(n);if(e)return{type:"del",raw:e[0],text:e[2],tokens:this.lexer.inlineTokens(e[2],[])}},t.autolink=function(n,e){var t,r,a=this.rules.inline.autolink.exec(n);if(a)return r="@"===a[2]?"mailto:"+(t=l(this.options.mangle?e(a[1]):a[1])):t=l(a[1]),{type:"link",raw:a[0],text:t,href:r,tokens:[{type:"text",raw:t,text:t}]}},t.url=function(n,e){var t;if(t=this.rules.inline.url.exec(n)){var r,a;if("@"===t[2])a="mailto:"+(r=l(this.options.mangle?e(t[0]):t[0]));else{var o;do{o=t[0],t[0]=this.rules.inline._backpedal.exec(t[0])[0]}while(o!==t[0]);r=l(t[0]),a="www."===t[1]?"http://"+r:r}return{type:"link",raw:t[0],text:r,href:a,tokens:[{type:"text",raw:r,text:r}]}}},t.inlineText=function(n,e){var t,r=this.rules.inline.text.exec(n);if(r)return t=this.lexer.state.inRawBlock?this.options.sanitize?this.options.sanitizer?this.options.sanitizer(r[0]):l(r[0]):r[0]:l(this.options.smartypants?e(r[0]):r[0]),{type:"text",raw:r[0],text:t}},e}(),j={newline:/^(?: *(?:\n|$))+/,code:/^( {4}[^\n]+(?:\n(?: *(?:\n|$))*)?)+/,fences:/^ {0,3}(`{3,}(?=[^`\n]*\n)|~{3,})([^\n]*)\n(?:|([\s\S]*?)\n)(?: {0,3}\1[~`]* *(?=\n|$)|$)/,hr:/^ {0,3}((?:- *){3,}|(?:_ *){3,}|(?:\* *){3,})(?:\n+|$)/,heading:/^ {0,3}(#{1,6})(?=\s|$)(.*)(?:\n+|$)/,blockquote:/^( {0,3}> ?(paragraph|[^\n]*)(?:\n|$))+/,list:/^( {0,3}bull)( [^\n]+?)?(?:\n|$)/,html:"^ {0,3}(?:<(script|pre|style|textarea)[\\s>][\\s\\S]*?(?:</\\1>[^\\n]*\\n+|$)|comment[^\\n]*(\\n+|$)|<\\?[\\s\\S]*?(?:\\?>\\n*|$)|<![A-Z][\\s\\S]*?(?:>\\n*|$)|<!\\[CDATA\\[[\\s\\S]*?(?:\\]\\]>\\n*|$)|</?(tag)(?: +|\\n|/?>)[\\s\\S]*?(?:(?:\\n *)+\\n|$)|<(?!script|pre|style|textarea)([a-z][\\w-]*)(?:attribute)*? */?>(?=[ \\t]*(?:\\n|$))[\\s\\S]*?(?:(?:\\n *)+\\n|$)|</(?!script|pre|style|textarea)[a-z][\\w-]*\\s*>(?=[ \\t]*(?:\\n|$))[\\s\\S]*?(?:(?:\\n *)+\\n|$))",def:/^ {0,3}\[(label)\]: *(?:\n *)?<?([^\s>]+)>?(?:(?: +(?:\n *)?| *\n *)(title))? *(?:\n+|$)/,table:B,lheading:/^([^\n]+)\n {0,3}(=+|-+) *(?:\n+|$)/,_paragraph:/^([^\n]+(?:\n(?!hr|heading|lheading|blockquote|fences|list|html|table| +\n)[^\n]+)*)/,text:/^[^\n]+/,_label:/(?!\s*\])(?:\\.|[^\[\]\\])+/,_title:/(?:"(?:\\"?|[^"\\])*"|'[^'\n]*(?:\n[^'\n]+)*\n?'|\([^()]*\))/};j.def=p(j.def).replace("label",j._label).replace("title",j._title).getRegex(),j.bullet=/(?:[*+-]|\d{1,9}[.)])/,j.listItemStart=p(/^( *)(bull) */).replace("bull",j.bullet).getRegex(),j.list=p(j.list).replace(/bull/g,j.bullet).replace("hr","\\n+(?=\\1?(?:(?:- *){3,}|(?:_ *){3,}|(?:\\* *){3,})(?:\\n+|$))").replace("def","\\n+(?="+j.def.source+")").getRegex(),j._tag="address|article|aside|base|basefont|blockquote|body|caption|center|col|colgroup|dd|details|dialog|dir|div|dl|dt|fieldset|figcaption|figure|footer|form|frame|frameset|h[1-6]|head|header|hr|html|iframe|legend|li|link|main|menu|menuitem|meta|nav|noframes|ol|optgroup|option|p|param|section|source|summary|table|tbody|td|tfoot|th|thead|title|tr|track|ul",j._comment=/<!--(?!-?>)[\s\S]*?(?:-->|$)/,j.html=p(j.html,"i").replace("comment",j._comment).replace("tag",j._tag).replace("attribute",/ +[a-zA-Z:_][\w.:-]*(?: *= *"[^"\n]*"| *= *'[^'\n]*'| *= *[^\s"'=<>`]+)?/).getRegex(),j.paragraph=p(j._paragraph).replace("hr",j.hr).replace("heading"," {0,3}#{1,6} ").replace("|lheading","").replace("|table","").replace("blockquote"," {0,3}>").replace("fences"," {0,3}(?:`{3,}(?=[^`\\n]*\\n)|~{3,})[^\\n]*\\n").replace("list"," {0,3}(?:[*+-]|1[.)]) ").replace("html","</?(?:tag)(?: +|\\n|/?>)|<(?:script|pre|style|textarea|!--)").replace("tag",j._tag).getRegex(),j.blockquote=p(j.blockquote).replace("paragraph",j.paragraph).getRegex(),j.normal=d({},j),j.gfm=d({},j.normal,{table:"^ *([^\\n ].*\\|.*)\\n {0,3}(?:\\| *)?(:?-+:? *(?:\\| *:?-+:? *)*)(?:\\| *)?(?:\\n((?:(?! *\\n|hr|heading|blockquote|code|fences|list|html).*(?:\\n|$))*)\\n*|$)"}),j.gfm.table=p(j.gfm.table).replace("hr",j.hr).replace("heading"," {0,3}#{1,6} ").replace("blockquote"," {0,3}>").replace("code"," {4}[^\\n]").replace("fences"," {0,3}(?:`{3,}(?=[^`\\n]*\\n)|~{3,})[^\\n]*\\n").replace("list"," {0,3}(?:[*+-]|1[.)]) ").replace("html","</?(?:tag)(?: +|\\n|/?>)|<(?:script|pre|style|textarea|!--)").replace("tag",j._tag).getRegex(),j.gfm.paragraph=p(j._paragraph).replace("hr",j.hr).replace("heading"," {0,3}#{1,6} ").replace("|lheading","").replace("table",j.gfm.table).replace("blockquote"," {0,3}>").replace("fences"," {0,3}(?:`{3,}(?=[^`\\n]*\\n)|~{3,})[^\\n]*\\n").replace("list"," {0,3}(?:[*+-]|1[.)]) ").replace("html","</?(?:tag)(?: +|\\n|/?>)|<(?:script|pre|style|textarea|!--)").replace("tag",j._tag).getRegex(),j.pedantic=d({},j.normal,{html:p("^ *(?:comment *(?:\\n|\\s*$)|<(tag)[\\s\\S]+?</\\1> *(?:\\n{2,}|\\s*$)|<tag(?:\"[^\"]*\"|'[^']*'|\\s[^'\"/>\\s]*)*?/?> *(?:\\n{2,}|\\s*$))").replace("comment",j._comment).replace(/tag/g,"(?!(?:a|em|strong|small|s|cite|q|dfn|abbr|data|time|code|var|samp|kbd|sub|sup|i|b|u|mark|ruby|rt|rp|bdi|bdo|span|br|wbr|ins|del|img)\\b)\\w+(?!:|[^\\w\\s@]*@)\\b").getRegex(),def:/^ *\[([^\]]+)\]: *<?([^\s>]+)>?(?: +(["(][^\n]+[")]))? *(?:\n+|$)/,heading:/^(#{1,6})(.*)(?:\n+|$)/,fences:B,paragraph:p(j.normal._paragraph).replace("hr",j.hr).replace("heading"," *#{1,6} *[^\n]").replace("lheading",j.lheading).replace("blockquote"," {0,3}>").replace("|fences","").replace("|list","").replace("|html","").getRegex()});var L={escape:/^\\([!"#$%&'()*+,\-./:;<=>?@\[\]\\^_`{|}~])/,autolink:/^<(scheme:[^\s\x00-\x1f<>]*|email)>/,url:B,tag:"^comment|^</[a-zA-Z][\\w:-]*\\s*>|^<[a-zA-Z][\\w-]*(?:attribute)*?\\s*/?>|^<\\?[\\s\\S]*?\\?>|^<![a-zA-Z]+\\s[\\s\\S]*?>|^<!\\[CDATA\\[[\\s\\S]*?\\]\\]>",link:/^!?\[(label)\]\(\s*(href)(?:\s+(title))?\s*\)/,reflink:/^!?\[(label)\]\[(ref)\]/,nolink:/^!?\[(ref)\](?:\[\])?/,reflinkSearch:"reflink|nolink(?!\\()",emStrong:{lDelim:/^(?:\*+(?:([punct_])|[^\s*]))|^_+(?:([punct*])|([^\s_]))/,rDelimAst:/^[^_*]*?\_\_[^_*]*?\*[^_*]*?(?=\_\_)|[punct_](\*+)(?=[\s]|$)|[^punct*_\s](\*+)(?=[punct_\s]|$)|[punct_\s](\*+)(?=[^punct*_\s])|[\s](\*+)(?=[punct_])|[punct_](\*+)(?=[punct_])|[^punct*_\s](\*+)(?=[^punct*_\s])/,rDelimUnd:/^[^_*]*?\*\*[^_*]*?\_[^_*]*?(?=\*\*)|[punct*](\_+)(?=[\s]|$)|[^punct*_\s](\_+)(?=[punct*\s]|$)|[punct*\s](\_+)(?=[^punct*_\s])|[\s](\_+)(?=[punct*])|[punct*](\_+)(?=[punct*])/},code:/^(`+)([^`]|[^`][\s\S]*?[^`])\1(?!`)/,br:/^( {2,}|\\)\n(?!\s*$)/,del:B,text:/^(`+|[^`])(?:(?= {2,}\n)|[\s\S]*?(?:(?=[\\<!\[`*_]|\b_|$)|[^ ](?= {2,}\n)))/,punctuation:/^([\spunctuation])/,_punctuation:"!\"#$%&'()+\\-.,/:;<=>?@\\[\\]`^{|}~"};L.punctuation=p(L.punctuation).replace(/punctuation/g,L._punctuation).getRegex(),L.blockSkip=/\[[^\]]*?\]\([^\)]*?\)|`[^`]*?`|<[^>]*?>/g,L.escapedEmSt=/\\\*|\\_/g,L._comment=p(j._comment).replace("(?:--\x3e|$)","--\x3e").getRegex(),L.emStrong.lDelim=p(L.emStrong.lDelim).replace(/punct/g,L._punctuation).getRegex(),L.emStrong.rDelimAst=p(L.emStrong.rDelimAst,"g").replace(/punct/g,L._punctuation).getRegex(),L.emStrong.rDelimUnd=p(L.emStrong.rDelimUnd,"g").replace(/punct/g,L._punctuation).getRegex(),L._escapes=/\\([!"#$%&'()*+,\-./:;<=>?@\[\]\\^_`{|}~])/g,L._scheme=/[a-zA-Z][a-zA-Z0-9+.-]{1,31}/,L._email=/[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+(@)[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)+(?![-_])/,L.autolink=p(L.autolink).replace("scheme",L._scheme).replace("email",L._email).getRegex(),L._attribute=/\s+[a-zA-Z:_][\w.:-]*(?:\s*=\s*"[^"]*"|\s*=\s*'[^']*'|\s*=\s*[^\s"'=<>`]+)?/,L.tag=p(L.tag).replace("comment",L._comment).replace("attribute",L._attribute).getRegex(),L._label=/(?:\[(?:\\.|[^\[\]\\])*\]|\\.|`[^`]*`|[^\[\]\\`])*?/,L._href=/<(?:\\.|[^\n<>\\])+>|[^\s\x00-\x1f]*/,L._title=/"(?:\\"?|[^"\\])*"|'(?:\\'?|[^'\\])*'|\((?:\\\)?|[^)\\])*\)/,L.link=p(L.link).replace("label",L._label).replace("href",L._href).replace("title",L._title).getRegex(),L.reflink=p(L.reflink).replace("label",L._label).replace("ref",j._label).getRegex(),L.nolink=p(L.nolink).replace("ref",j._label).getRegex(),L.reflinkSearch=p(L.reflinkSearch,"g").replace("reflink",L.reflink).replace("nolink",L.nolink).getRegex(),L.normal=d({},L),L.pedantic=d({},L.normal,{strong:{start:/^__|\*\*/,middle:/^__(?=\S)([\s\S]*?\S)__(?!_)|^\*\*(?=\S)([\s\S]*?\S)\*\*(?!\*)/,endAst:/\*\*(?!\*)/g,endUnd:/__(?!_)/g},em:{start:/^_|\*/,middle:/^()\*(?=\S)([\s\S]*?\S)\*(?!\*)|^_(?=\S)([\s\S]*?\S)_(?!_)/,endAst:/\*(?!\*)/g,endUnd:/_(?!_)/g},link:p(/^!?\[(label)\]\((.*?)\)/).replace("label",L._label).getRegex(),reflink:p(/^!?\[(label)\]\s*\[([^\]]*)\]/).replace("label",L._label).getRegex()}),L.gfm=d({},L.normal,{escape:p(L.escape).replace("])","~|])").getRegex(),_extended_email:/[A-Za-z0-9._+-]+(@)[a-zA-Z0-9-_]+(?:\.[a-zA-Z0-9-_]*[a-zA-Z0-9])+(?![-_])/,url:/^((?:ftp|https?):\/\/|www\.)(?:[a-zA-Z0-9\-]+\.?)+[^\s<]*|^email/,_backpedal:/(?:[^?!.,:;*_~()&]+|\([^)]*\)|&(?![a-zA-Z0-9]+;$)|[?!.,:;*_~)]+(?!$))+/,del:/^(~~?)(?=[^\s~])([\s\S]*?[^\s~])\1(?=[^~]|$)/,text:/^([`~]+|[^`~])(?:(?= {2,}\n)|(?=[a-zA-Z0-9.!#$%&'*+\/=?_`{\|}~-]+@)|[\s\S]*?(?:(?=[\\<!\[`*~_]|\b_|https?:\/\/|ftp:\/\/|www\.|$)|[^ ](?= {2,}\n)|[^a-zA-Z0-9.!#$%&'*+\/=?_`{\|}~-](?=[a-zA-Z0-9.!#$%&'*+\/=?_`{\|}~-]+@)))/}),L.gfm.url=p(L.gfm.url,"i").replace("email",L.gfm._extended_email).getRegex(),L.breaks=d({},L.gfm,{br:p(L.br).replace("{2,}","*").getRegex(),text:p(L.gfm.text).replace("\\b_","\\b_| {2,}\\n").replace(/\{2,\}/g,"*").getRegex()});var N=function(){function e(e){this.tokens=[],this.tokens.links=Object.create(null),this.options=e||n.defaults,this.options.tokenizer=this.options.tokenizer||new M,this.tokenizer=this.options.tokenizer,this.tokenizer.options=this.options,this.tokenizer.lexer=this,this.inlineQueue=[],this.state={inLink:!1,inRawBlock:!1,top:!0};var t={block:j.normal,inline:L.normal};this.options.pedantic?(t.block=j.pedantic,t.inline=L.pedantic):this.options.gfm&&(t.block=j.gfm,this.options.breaks?t.inline=L.breaks:t.inline=L.gfm),this.tokenizer.rules=t}e.lex=function(n,t){return new e(t).lex(n)},e.lexInline=function(n,t){return new e(t).inlineTokens(n)};var r=e.prototype;return r.lex=function(n){n=n.replace(/\r\n|\r/g,"\n").replace(/\t/g,"    "),this.blockTokens(n,this.tokens);for(var e;e=this.inlineQueue.shift();)this.inlineTokens(e.src,e.tokens);return this.tokens},r.blockTokens=function(n,e){var t,r,a,o,i=this;for(void 0===e&&(e=[]),this.options.pedantic&&(n=n.replace(/^ +$/gm,""));n;)if(!(this.options.extensions&&this.options.extensions.block&&this.options.extensions.block.some((function(r){return!!(t=r.call({lexer:i},n,e))&&(n=n.substring(t.raw.length),e.push(t),!0)}))))if(t=this.tokenizer.space(n))n=n.substring(t.raw.length),1===t.raw.length&&e.length>0?e[e.length-1].raw+="\n":e.push(t);else if(t=this.tokenizer.code(n))n=n.substring(t.raw.length),!(r=e[e.length-1])||"paragraph"!==r.type&&"text"!==r.type?e.push(t):(r.raw+="\n"+t.raw,r.text+="\n"+t.text,this.inlineQueue[this.inlineQueue.length-1].src=r.text);else if(t=this.tokenizer.fences(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.heading(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.hr(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.blockquote(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.list(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.html(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.def(n))n=n.substring(t.raw.length),!(r=e[e.length-1])||"paragraph"!==r.type&&"text"!==r.type?this.tokens.links[t.tag]||(this.tokens.links[t.tag]={href:t.href,title:t.title}):(r.raw+="\n"+t.raw,r.text+="\n"+t.raw,this.inlineQueue[this.inlineQueue.length-1].src=r.text);else if(t=this.tokenizer.table(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.lheading(n))n=n.substring(t.raw.length),e.push(t);else if(a=n,this.options.extensions&&this.options.extensions.startBlock&&function(){var e=1/0,t=n.slice(1),r=void 0;i.options.extensions.startBlock.forEach((function(n){"number"==typeof(r=n.call({lexer:this},t))&&r>=0&&(e=Math.min(e,r))})),e<1/0&&e>=0&&(a=n.substring(0,e+1))}(),this.state.top&&(t=this.tokenizer.paragraph(a)))r=e[e.length-1],o&&"paragraph"===r.type?(r.raw+="\n"+t.raw,r.text+="\n"+t.text,this.inlineQueue.pop(),this.inlineQueue[this.inlineQueue.length-1].src=r.text):e.push(t),o=a.length!==n.length,n=n.substring(t.raw.length);else if(t=this.tokenizer.text(n))n=n.substring(t.raw.length),(r=e[e.length-1])&&"text"===r.type?(r.raw+="\n"+t.raw,r.text+="\n"+t.text,this.inlineQueue.pop(),this.inlineQueue[this.inlineQueue.length-1].src=r.text):e.push(t);else if(n){var s="Infinite loop on byte: "+n.charCodeAt(0);if(this.options.silent)break;throw new Error(s)}return this.state.top=!0,e},r.inline=function(n,e){this.inlineQueue.push({src:n,tokens:e})},r.inlineTokens=function(n,e){var t=this;void 0===e&&(e=[]);var r,a,o,i,s,l,c=n;if(this.tokens.links){var p=Object.keys(this.tokens.links);if(p.length>0)for(;null!=(i=this.tokenizer.rules.inline.reflinkSearch.exec(c));)p.includes(i[0].slice(i[0].lastIndexOf("[")+1,-1))&&(c=c.slice(0,i.index)+"["+h("a",i[0].length-2)+"]"+c.slice(this.tokenizer.rules.inline.reflinkSearch.lastIndex))}for(;null!=(i=this.tokenizer.rules.inline.blockSkip.exec(c));)c=c.slice(0,i.index)+"["+h("a",i[0].length-2)+"]"+c.slice(this.tokenizer.rules.inline.blockSkip.lastIndex);for(;null!=(i=this.tokenizer.rules.inline.escapedEmSt.exec(c));)c=c.slice(0,i.index)+"++"+c.slice(this.tokenizer.rules.inline.escapedEmSt.lastIndex);for(;n;)if(s||(l=""),s=!1,!(this.options.extensions&&this.options.extensions.inline&&this.options.extensions.inline.some((function(a){return!!(r=a.call({lexer:t},n,e))&&(n=n.substring(r.raw.length),e.push(r),!0)}))))if(r=this.tokenizer.escape(n))n=n.substring(r.raw.length),e.push(r);else if(r=this.tokenizer.tag(n))n=n.substring(r.raw.length),(a=e[e.length-1])&&"text"===r.type&&"text"===a.type?(a.raw+=r.raw,a.text+=r.text):e.push(r);else if(r=this.tokenizer.link(n))n=n.substring(r.raw.length),e.push(r);else if(r=this.tokenizer.reflink(n,this.tokens.links))n=n.substring(r.raw.length),(a=e[e.length-1])&&"text"===r.type&&"text"===a.type?(a.raw+=r.raw,a.text+=r.text):e.push(r);else if(r=this.tokenizer.emStrong(n,c,l))n=n.substring(r.raw.length),e.push(r);else if(r=this.tokenizer.codespan(n))n=n.substring(r.raw.length),e.push(r);else if(r=this.tokenizer.br(n))n=n.substring(r.raw.length),e.push(r);else if(r=this.tokenizer.del(n))n=n.substring(r.raw.length),e.push(r);else if(r=this.tokenizer.autolink(n,k))n=n.substring(r.raw.length),e.push(r);else if(this.state.inLink||!(r=this.tokenizer.url(n,k))){if(o=n,this.options.extensions&&this.options.extensions.startInline&&function(){var e=1/0,r=n.slice(1),a=void 0;t.options.extensions.startInline.forEach((function(n){"number"==typeof(a=n.call({lexer:this},r))&&a>=0&&(e=Math.min(e,a))})),e<1/0&&e>=0&&(o=n.substring(0,e+1))}(),r=this.tokenizer.inlineText(o,b))n=n.substring(r.raw.length),"_"!==r.raw.slice(-1)&&(l=r.raw.slice(-1)),s=!0,(a=e[e.length-1])&&"text"===a.type?(a.raw+=r.raw,a.text+=r.text):e.push(r);else if(n){var u="Infinite loop on byte: "+n.charCodeAt(0);if(this.options.silent)break;throw new Error(u)}}else n=n.substring(r.raw.length),e.push(r);return e},t(e,null,[{key:"rules",get:function(){return{block:j,inline:L}}}]),e}(),$=function(){function e(e){this.options=e||n.defaults}var t=e.prototype;return t.code=function(n,e,t){var r=(e||"").match(/\S*/)[0];if(this.options.highlight){var a=this.options.highlight(n,r);null!=a&&a!==n&&(t=!0,n=a)}return n=n.replace(/\n$/,"")+"\n",r?'<pre><code class="'+this.options.langPrefix+l(r,!0)+'">'+(t?n:l(n,!0))+"</code></pre>\n":"<pre><code>"+(t?n:l(n,!0))+"</code></pre>\n"},t.blockquote=function(n){return"<blockquote>\n"+n+"</blockquote>\n"},t.html=function(n){return n},t.heading=function(n,e,t,r){return this.options.headerIds?"<h"+e+' id="'+this.options.headerPrefix+r.slug(t)+'">'+n+"</h"+e+">\n":"<h"+e+">"+n+"</h"+e+">\n"},t.hr=function(){return this.options.xhtml?"<hr/>\n":"<hr>\n"},t.list=function(n,e,t){var r=e?"ol":"ul";return"<"+r+(e&&1!==t?' start="'+t+'"':"")+">\n"+n+"</"+r+">\n"},t.listitem=function(n){return"<li>"+n+"</li>\n"},t.checkbox=function(n){return"<input "+(n?'checked="" ':"")+'disabled="" type="checkbox"'+(this.options.xhtml?" /":"")+"> "},t.paragraph=function(n){return"<p>"+n+"</p>\n"},t.table=function(n,e){return e&&(e="<tbody>"+e+"</tbody>"),"<table>\n<thead>\n"+n+"</thead>\n"+e+"</table>\n"},t.tablerow=function(n){return"<tr>\n"+n+"</tr>\n"},t.tablecell=function(n,e){var t=e.header?"th":"td";return(e.align?"<"+t+' align="'+e.align+'">':"<"+t+">")+n+"</"+t+">\n"},t.strong=function(n){return"<strong>"+n+"</strong>"},t.em=function(n){return"<em>"+n+"</em>"},t.codespan=function(n){return"<code>"+n+"</code>"},t.br=function(){return this.options.xhtml?"<br/>":"<br>"},t.del=function(n){return"<del>"+n+"</del>"},t.link=function(n,e,t){if(null===(n=u(this.options.sanitize,this.options.baseUrl,n)))return t;var r='<a href="'+l(n)+'"';return e&&(r+=' title="'+e+'"'),r+">"+t+"</a>"},t.image=function(n,e,t){if(null===(n=u(this.options.sanitize,this.options.baseUrl,n)))return t;var r='<img src="'+n+'" alt="'+t+'"';return e&&(r+=' title="'+e+'"'),r+(this.options.xhtml?"/>":">")},t.text=function(n){return n},e}(),U=function(){function n(){}var e=n.prototype;return e.strong=function(n){return n},e.em=function(n){return n},e.codespan=function(n){return n},e.del=function(n){return n},e.html=function(n){return n},e.text=function(n){return n},e.link=function(n,e,t){return""+t},e.image=function(n,e,t){return""+t},e.br=function(){return""},n}(),z=function(){function n(){this.seen={}}var e=n.prototype;return e.serialize=function(n){return n.toLowerCase().trim().replace(/<[!\/a-z].*?>/gi,"").replace(/[\u2000-\u206F\u2E00-\u2E7F\\'!"#$%&()*+,./:;<=>?@[\]^`{|}~]/g,"").replace(/\s/g,"-")},e.getNextSafeSlug=function(n,e){var t=n,r=0;if(this.seen.hasOwnProperty(t)){r=this.seen[n];do{t=n+"-"+ ++r}while(this.seen.hasOwnProperty(t))}return e||(this.seen[n]=r,this.seen[t]=0),t},e.slug=function(n,e){void 0===e&&(e={});var t=this.serialize(n);return this.getNextSafeSlug(t,e.dryrun)},n}(),H=function(){function e(e){this.options=e||n.defaults,this.options.renderer=this.options.renderer||new $,this.renderer=this.options.renderer,this.renderer.options=this.options,this.textRenderer=new U,this.slugger=new z}e.parse=function(n,t){return new e(t).parse(n)},e.parseInline=function(n,t){return new e(t).parseInline(n)};var t=e.prototype;return t.parse=function(n,e){void 0===e&&(e=!0);var t,r,a,o,i,s,l,p,u,d,m,g,f,h,v,b,k,y,S,x="",w=n.length;for(t=0;t<w;t++)if(d=n[t],!(this.options.extensions&&this.options.extensions.renderers&&this.options.extensions.renderers[d.type])||!1===(S=this.options.extensions.renderers[d.type].call({parser:this},d))&&["space","hr","heading","code","table","blockquote","list","html","paragraph","text"].includes(d.type))switch(d.type){case"space":continue;case"hr":x+=this.renderer.hr();continue;case"heading":x+=this.renderer.heading(this.parseInline(d.tokens),d.depth,c(this.parseInline(d.tokens,this.textRenderer)),this.slugger);continue;case"code":x+=this.renderer.code(d.text,d.lang,d.escaped);continue;case"table":for(p="",l="",o=d.header.length,r=0;r<o;r++)l+=this.renderer.tablecell(this.parseInline(d.header[r].tokens),{header:!0,align:d.align[r]});for(p+=this.renderer.tablerow(l),u="",o=d.rows.length,r=0;r<o;r++){for(l="",i=(s=d.rows[r]).length,a=0;a<i;a++)l+=this.renderer.tablecell(this.parseInline(s[a].tokens),{header:!1,align:d.align[a]});u+=this.renderer.tablerow(l)}x+=this.renderer.table(p,u);continue;case"blockquote":u=this.parse(d.tokens),x+=this.renderer.blockquote(u);continue;case"list":for(m=d.ordered,g=d.start,f=d.loose,o=d.items.length,u="",r=0;r<o;r++)b=(v=d.items[r]).checked,k=v.task,h="",v.task&&(y=this.renderer.checkbox(b),f?v.tokens.length>0&&"paragraph"===v.tokens[0].type?(v.tokens[0].text=y+" "+v.tokens[0].text,v.tokens[0].tokens&&v.tokens[0].tokens.length>0&&"text"===v.tokens[0].tokens[0].type&&(v.tokens[0].tokens[0].text=y+" "+v.tokens[0].tokens[0].text)):v.tokens.unshift({type:"text",text:y}):h+=y),h+=this.parse(v.tokens,f),u+=this.renderer.listitem(h,k,b);x+=this.renderer.list(u,m,g);continue;case"html":x+=this.renderer.html(d.text);continue;case"paragraph":x+=this.renderer.paragraph(this.parseInline(d.tokens));continue;case"text":for(u=d.tokens?this.parseInline(d.tokens):d.text;t+1<w&&"text"===n[t+1].type;)u+="\n"+((d=n[++t]).tokens?this.parseInline(d.tokens):d.text);x+=e?this.renderer.paragraph(u):u;continue;default:var E='Token with "'+d.type+'" type was not found.';if(this.options.silent)return;throw new Error(E)}else x+=S||"";return x},t.parseInline=function(n,e){e=e||this.renderer;var t,r,a,o="",i=n.length;for(t=0;t<i;t++)if(r=n[t],!(this.options.extensions&&this.options.extensions.renderers&&this.options.extensions.renderers[r.type])||!1===(a=this.options.extensions.renderers[r.type].call({parser:this},r))&&["escape","html","link","image","strong","em","codespan","br","del","text"].includes(r.type))switch(r.type){case"escape":o+=e.text(r.text);break;case"html":o+=e.html(r.text);break;case"link":o+=e.link(r.href,r.title,this.parseInline(r.tokens,e));break;case"image":o+=e.image(r.href,r.title,r.text);break;case"strong":o+=e.strong(this.parseInline(r.tokens,e));break;case"em":o+=e.em(this.parseInline(r.tokens,e));break;case"codespan":o+=e.codespan(r.text);break;case"br":o+=e.br();break;case"del":o+=e.del(this.parseInline(r.tokens,e));break;case"text":o+=e.text(r.text);break;default:var s='Token with "'+r.type+'" type was not found.';if(this.options.silent)return;throw new Error(s)}else o+=a||"";return o},e}();y.options=y.setOptions=function(n){return d(y.defaults,n),s(y.defaults),y},y.getDefaults=i,y.defaults=n.defaults,y.use=function(){for(var n=arguments.length,e=new Array(n),t=0;t<n;t++)e[t]=arguments[t];var r,a=d.apply(void 0,[{}].concat(e)),o=y.defaults.extensions||{renderers:{},childTokens:{}};e.forEach((function(n){if(n.extensions&&(r=!0,n.extensions.forEach((function(n){if(!n.name)throw new Error("extension name required");if(n.renderer){var e=o.renderers?o.renderers[n.name]:null;o.renderers[n.name]=e?function(){for(var t=arguments.length,r=new Array(t),a=0;a<t;a++)r[a]=arguments[a];var o=n.renderer.apply(this,r);return!1===o&&(o=e.apply(this,r)),o}:n.renderer}if(n.tokenizer){if(!n.level||"block"!==n.level&&"inline"!==n.level)throw new Error("extension level must be 'block' or 'inline'");o[n.level]?o[n.level].unshift(n.tokenizer):o[n.level]=[n.tokenizer],n.start&&("block"===n.level?o.startBlock?o.startBlock.push(n.start):o.startBlock=[n.start]:"inline"===n.level&&(o.startInline?o.startInline.push(n.start):o.startInline=[n.start]))}n.childTokens&&(o.childTokens[n.name]=n.childTokens)}))),n.renderer&&function(){var e=y.defaults.renderer||new $;for(var t in n.renderer)!function(t){var r=e[t];e[t]=function(){for(var a=arguments.length,o=new Array(a),i=0;i<a;i++)o[i]=arguments[i];var s=n.renderer[t].apply(e,o);return!1===s&&(s=r.apply(e,o)),s}}(t);a.renderer=e}(),n.tokenizer&&function(){var e=y.defaults.tokenizer||new M;for(var t in n.tokenizer)!function(t){var r=e[t];e[t]=function(){for(var a=arguments.length,o=new Array(a),i=0;i<a;i++)o[i]=arguments[i];var s=n.tokenizer[t].apply(e,o);return!1===s&&(s=r.apply(e,o)),s}}(t);a.tokenizer=e}(),n.walkTokens){var e=y.defaults.walkTokens;a.walkTokens=function(t){n.walkTokens.call(this,t),e&&e.call(this,t)}}r&&(a.extensions=o),y.setOptions(a)}))},y.walkTokens=function(n,e){for(var t,r=o(n);!(t=r()).done;)!function(){var n=t.value;switch(e.call(y,n),n.type){case"table":for(var r,a=o(n.header);!(r=a()).done;){var i=r.value;y.walkTokens(i.tokens,e)}for(var s,l=o(n.rows);!(s=l()).done;)for(var c,p=o(s.value);!(c=p()).done;){var u=c.value;y.walkTokens(u.tokens,e)}break;case"list":y.walkTokens(n.items,e);break;default:y.defaults.extensions&&y.defaults.extensions.childTokens&&y.defaults.extensions.childTokens[n.type]?y.defaults.extensions.childTokens[n.type].forEach((function(t){y.walkTokens(n[t],e)})):n.tokens&&y.walkTokens(n.tokens,e)}}()},y.parseInline=function(n,e){if(null==n)throw new Error("marked.parseInline(): input parameter is undefined or null");if("string"!=typeof n)throw new Error("marked.parseInline(): input parameter is of type "+Object.prototype.toString.call(n)+", string expected");f(e=d({},y.defaults,e||{}));try{var t=N.lexInline(n,e);return e.walkTokens&&y.walkTokens(t,e.walkTokens),H.parseInline(t,e)}catch(n){if(n.message+="\nPlease report this to https://github.com/markedjs/marked.",e.silent)return"<p>An error occurred:</p><pre>"+l(n.message+"",!0)+"</pre>";throw n}},y.Parser=H,y.parser=H.parse,y.Renderer=$,y.TextRenderer=U,y.Lexer=N,y.lexer=N.lex,y.Tokenizer=M,y.Slugger=z,y.parse=y;var q=y.options,V=y.setOptions,K=y.use,W=y.walkTokens,G=y.parseInline,J=y,Y=H.parse,Q=N.lex;n.Lexer=N,n.Parser=H,n.Renderer=$,n.Slugger=z,n.TextRenderer=U,n.Tokenizer=M,n.getDefaults=i,n.lexer=Q,n.marked=y,n.options=q,n.parse=J,n.parseInline=G,n.parser=Y,n.setOptions=V,n.use=K,n.walkTokens=W,Object.defineProperty(n,"__esModule",{value:!0})}(e)},function(n,e,t){"use strict";function r(n,e){var t=function(n){var e={};return s(l(n),(function(n){var t=n[0],r=n[1];s(r,(function(n){e[n]=t}))})),e}(n.pluralTypeToLanguages);return t[e]||t[g.call(e,/-/,1)[0]]||t.en}function a(n){return n.replace(/[.*+?^${}()|[\]\\]/g,"\\$&")}function o(n,e,t,r,a){if("string"!=typeof n)throw new TypeError("Polyglot.transformPhrase expects argument #1 to be string");if(null==e)return n;var o=n,i=r||k,s="number"==typeof e?{smart_count:e}:e;if(null!=s.smart_count&&n){var l=a||v,c=g.call(n,f),d=function(n,e,t){return n.pluralTypes[e](t)}(l,b(l,t||"en"),s.smart_count);o=u(c[d]||c[0])}return m.call(o,i,(function(n,e){return p(s,e)&&null!=s[e]?s[e]:n}))}function i(n){var e=n||{};this.phrases={},this.extend(e.phrases||{}),this.currentLocale=e.locale||"en";var t=e.allowMissing?o:null;this.onMissingKey="function"==typeof e.onMissingKey?e.onMissingKey:t,this.warn=e.warn||d,this.tokenRegex=function(n){var e=n&&n.prefix||"%{",t=n&&n.suffix||"}";if(e===f||t===f)throw new RangeError('"'+f+'" token is reserved for pluralization');return new RegExp(a(e)+"(.*?)"+a(t),"g")}(e.interpolation),this.pluralRules=e.pluralRules||v}var s=t(43),l=t(89),c=t(94),p=t(25),u=t(92),d=function(n){c(!1,n)},m=String.prototype.replace,g=String.prototype.split,f="||||",h=function(n){var e=n%100,t=e%10;return 11!==e&&1===t?0:2<=t&&t<=4&&!(e>=12&&e<=14)?1:2},v={pluralTypes:{arabic:function(n){if(n<3)return n;var e=n%100;return e>=3&&e<=10?3:e>=11?4:5},bosnian_serbian:h,chinese:function(){return 0},croatian:h,french:function(n){return n>=2?1:0},german:function(n){return 1!==n?1:0},russian:h,lithuanian:function(n){return n%10==1&&n%100!=11?0:n%10>=2&&n%10<=9&&(n%100<11||n%100>19)?1:2},czech:function(n){return 1===n?0:n>=2&&n<=4?1:2},polish:function(n){if(1===n)return 0;var e=n%10;return 2<=e&&e<=4&&(n%100<10||n%100>=20)?1:2},icelandic:function(n){return n%10!=1||n%100==11?1:0},slovenian:function(n){var e=n%100;return 1===e?0:2===e?1:3===e||4===e?2:3}},pluralTypeToLanguages:{arabic:["ar"],bosnian_serbian:["bs-Latn-BA","bs-Cyrl-BA","srl-RS","sr-RS"],chinese:["id","id-ID","ja","ko","ko-KR","lo","ms","th","th-TH","zh"],croatian:["hr","hr-HR"],german:["fa","da","de","en","es","fi","el","he","hi-IN","hu","hu-HU","it","nl","no","pt","sv","tr"],french:["fr","tl","pt-br"],russian:["ru","ru-RU"],lithuanian:["lt"],czech:["cs","cs-CZ","sk"],polish:["pl"],icelandic:["is"],slovenian:["sl-SL"]}},b=function(){var n={};return function(e,t){var a=n[t];return a&&!e.pluralTypes[a]&&(a=null,n[t]=a),a||(a=r(e,t))&&(n[t]=a),a}}(),k=/%\{(.*?)\}/g;i.prototype.locale=function(n){return n&&(this.currentLocale=n),this.currentLocale},i.prototype.extend=function(n,e){s(l(n||{}),(function(n){var t=n[0],r=n[1],a=e?e+"."+t:t;"object"==typeof r?this.extend(r,a):this.phrases[a]=r}),this)},i.prototype.unset=function(n,e){"string"==typeof n?delete this.phrases[n]:s(l(n||{}),(function(n){var t=n[0],r=n[1],a=e?e+"."+t:t;"object"==typeof r?this.unset(r,a):delete this.phrases[a]}),this)},i.prototype.clear=function(){this.phrases={}},i.prototype.replace=function(n){this.clear(),this.extend(n)},i.prototype.t=function(n,e){var t,r,a=null==e?{}:e;return"string"==typeof this.phrases[n]?t=this.phrases[n]:"string"==typeof a._?t=a._:this.onMissingKey?r=(0,this.onMissingKey)(n,a,this.currentLocale,this.tokenRegex,this.pluralRules):(this.warn('Missing translation for key: "'+n+'"'),r=n),"string"==typeof t&&(r=o(t,a,this.currentLocale,this.tokenRegex,this.pluralRules)),r},i.prototype.has=function(n){return p(this.phrases,n)},i.transformPhrase=function(n,e,t){return o(n,e,t)},n.exports=i},function(n,e,t){"use strict";function r(n){if(null==n)throw new TypeError("Object.assign cannot be called with null or undefined");return Object(n)}
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/var a=Object.getOwnPropertySymbols,o=Object.prototype.hasOwnProperty,i=Object.prototype.propertyIsEnumerable;n.exports=function(){try{if(!Object.assign)return!1;var n=new String("abc");if(n[5]="de","5"===Object.getOwnPropertyNames(n)[0])return!1;for(var e={},t=0;t<10;t++)e["_"+String.fromCharCode(t)]=t;if("0123456789"!==Object.getOwnPropertyNames(e).map((function(n){return e[n]})).join(""))return!1;var r={};return"abcdefghijklmnopqrst".split("").forEach((function(n){r[n]=n})),"abcdefghijklmnopqrst"===Object.keys(Object.assign({},r)).join("")}catch(n){return!1}}()?Object.assign:function(n,e){for(var t,s,l=r(n),c=1;c<arguments.length;c++){for(var p in t=Object(arguments[c]))o.call(t,p)&&(l[p]=t[p]);if(a){s=a(t);for(var u=0;u<s.length;u++)i.call(t,s[u])&&(l[s[u]]=t[s[u]])}}return l}},function(n,e,t){function r(n,e){if(n===1/0||n===-1/0||n!=n||n&&n>-1e3&&n<1e3||U.call(/e/,e))return e;var t=/[0-9](?=(?:[0-9]{3})+(?![0-9]))/g;if("number"==typeof n){var r=n<0?-V(-n):V(n);if(r!==n){var a=String(r),o=j.call(e,a.length+1);return L.call(a,t,"$&_")+"."+L.call(L.call(o,/([0-9]{3})/g,"$&_"),/_$/,"")}}return L.call(e,t,"$&_")}function a(n,e,t){var r="double"===(t.quoteStyle||e)?'"':"'";return r+n+r}function o(n){return L.call(String(n),/"/g,"&quot;")}function i(n){return!("[object Array]"!==c(n)||Y&&"object"==typeof n&&Y in n)}function s(n){if(J)return n&&"object"==typeof n&&n instanceof Symbol;if("symbol"==typeof n)return!0;if(!n||"object"!=typeof n||!G)return!1;try{return G.call(n),!0}catch(n){}return!1}function l(n,e){return en.call(n,e)}function c(n){return F.call(n)}function p(n){if(n.name)return n.name;var e=M.call(B.call(n),/^function\s*([\w$]+)/);return e?e[1]:null}function u(n,e){if(n.indexOf)return n.indexOf(e);for(var t=0,r=n.length;t<r;t++)if(n[t]===e)return t;return-1}function d(n){if(!A||!n||"object"!=typeof n)return!1;try{A.call(n,A);try{_.call(n,_)}catch(n){return!0}return n instanceof WeakMap}catch(n){}return!1}function m(n){if(!R||!n||"object"!=typeof n)return!1;try{return R.call(n),!0}catch(n){}return!1}function g(n){if(!_||!n||"object"!=typeof n)return!1;try{_.call(n,_);try{A.call(n,A)}catch(n){return!0}return n instanceof WeakSet}catch(n){}return!1}function f(n,e){if(n.length>e.maxStringLength){var t=n.length-e.maxStringLength,r="... "+t+" more character"+(t>1?"s":"");return f(j.call(n,0,e.maxStringLength),e)+r}return a(L.call(L.call(n,/(['\\])/g,"\\$1"),/[\x00-\x1f]/g,h),"single",e)}function h(n){var e=n.charCodeAt(0),t={8:"b",9:"t",10:"n",12:"f",13:"r"}[e];return t?"\\"+t:"\\x"+(e<16?"0":"")+N.call(e.toString(16))}function v(n){return"Object("+n+")"}function b(n){return n+" { ? }"}function k(n,e,t,r){return n+" ("+e+") {"+(r?y(t,r):H.call(t,", "))+"}"}function y(n,e){if(0===n.length)return"";var t="\n"+e.prev+e.base;return t+H.call(n,","+t)+"\n"+e.prev}function S(n,e){var t=i(n),r=[];if(t){r.length=n.length;for(var a=0;a<n.length;a++)r[a]=l(n,a)?e(n[a],n):""}var o,s="function"==typeof W?W(n):[];if(J){o={};for(var c=0;c<s.length;c++)o["$"+s[c]]=s[c]}for(var p in n)l(n,p)&&(t&&String(Number(p))===p&&p<n.length||J&&o["$"+p]instanceof Symbol||(U.call(/[^\w$]/,p)?r.push(e(p,n)+": "+e(n[p],n)):r.push(p+": "+e(n[p],n))));if("function"==typeof W)for(var u=0;u<s.length;u++)Q.call(n,s[u])&&r.push("["+e(s[u])+"]: "+e(n[s[u]],n));return r}var x="function"==typeof Map&&Map.prototype,w=Object.getOwnPropertyDescriptor&&x?Object.getOwnPropertyDescriptor(Map.prototype,"size"):null,E=x&&w&&"function"==typeof w.get?w.get:null,D=x&&Map.prototype.forEach,C="function"==typeof Set&&Set.prototype,I=Object.getOwnPropertyDescriptor&&C?Object.getOwnPropertyDescriptor(Set.prototype,"size"):null,T=C&&I&&"function"==typeof I.get?I.get:null,O=C&&Set.prototype.forEach,A="function"==typeof WeakMap&&WeakMap.prototype?WeakMap.prototype.has:null,_="function"==typeof WeakSet&&WeakSet.prototype?WeakSet.prototype.has:null,R="function"==typeof WeakRef&&WeakRef.prototype?WeakRef.prototype.deref:null,P=Boolean.prototype.valueOf,F=Object.prototype.toString,B=Function.prototype.toString,M=String.prototype.match,j=String.prototype.slice,L=String.prototype.replace,N=String.prototype.toUpperCase,$=String.prototype.toLowerCase,U=RegExp.prototype.test,z=Array.prototype.concat,H=Array.prototype.join,q=Array.prototype.slice,V=Math.floor,K="function"==typeof BigInt?BigInt.prototype.valueOf:null,W=Object.getOwnPropertySymbols,G="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?Symbol.prototype.toString:null,J="function"==typeof Symbol&&"object"==typeof Symbol.iterator,Y="function"==typeof Symbol&&Symbol.toStringTag&&(Symbol.toStringTag,1)?Symbol.toStringTag:null,Q=Object.prototype.propertyIsEnumerable,X=("function"==typeof Reflect?Reflect.getPrototypeOf:Object.getPrototypeOf)||([].__proto__===Array.prototype?function(n){return n.__proto__}:null),Z=t(102).custom,nn=Z&&s(Z)?Z:null;n.exports=function n(e,t,h,x){function w(e,t,r){if(t&&(x=q.call(x)).push(t),r){var a={depth:C.depth};return l(C,"quoteStyle")&&(a.quoteStyle=C.quoteStyle),n(e,a,h+1,x)}return n(e,C,h+1,x)}var C=t||{};if(l(C,"quoteStyle")&&"single"!==C.quoteStyle&&"double"!==C.quoteStyle)throw new TypeError('option "quoteStyle" must be "single" or "double"');if(l(C,"maxStringLength")&&("number"==typeof C.maxStringLength?C.maxStringLength<0&&C.maxStringLength!==1/0:null!==C.maxStringLength))throw new TypeError('option "maxStringLength", if provided, must be a positive integer, Infinity, or `null`');var I=!l(C,"customInspect")||C.customInspect;if("boolean"!=typeof I&&"symbol"!==I)throw new TypeError("option \"customInspect\", if provided, must be `true`, `false`, or `'symbol'`");if(l(C,"indent")&&null!==C.indent&&"\t"!==C.indent&&!(parseInt(C.indent,10)===C.indent&&C.indent>0))throw new TypeError('option "indent" must be "\\t", an integer > 0, or `null`');if(l(C,"numericSeparator")&&"boolean"!=typeof C.numericSeparator)throw new TypeError('option "numericSeparator", if provided, must be `true` or `false`');var A=C.numericSeparator;if(void 0===e)return"undefined";if(null===e)return"null";if("boolean"==typeof e)return e?"true":"false";if("string"==typeof e)return f(e,C);if("number"==typeof e){if(0===e)return 1/0/e>0?"0":"-0";var _=String(e);return A?r(e,_):_}if("bigint"==typeof e){var R=String(e)+"n";return A?r(e,R):R}var F=void 0===C.depth?5:C.depth;if(void 0===h&&(h=0),h>=F&&F>0&&"object"==typeof e)return i(e)?"[Array]":"[Object]";var B=function(n,e){var t;if("\t"===n.indent)t="\t";else{if(!("number"==typeof n.indent&&n.indent>0))return null;t=H.call(Array(n.indent+1)," ")}return{base:t,prev:H.call(Array(e+1),t)}}(C,h);if(void 0===x)x=[];else if(u(x,e)>=0)return"[Circular]";if("function"==typeof e){var M=p(e),N=S(e,w);return"[Function"+(M?": "+M:" (anonymous)")+"]"+(N.length>0?" { "+H.call(N,", ")+" }":"")}if(s(e)){var U=J?L.call(String(e),/^(Symbol\(.*\))_[^)]*$/,"$1"):G.call(e);return"object"!=typeof e||J?U:v(U)}if(function(n){return!(!n||"object"!=typeof n)&&("undefined"!=typeof HTMLElement&&n instanceof HTMLElement||"string"==typeof n.nodeName&&"function"==typeof n.getAttribute)}(e)){for(var V="<"+$.call(String(e.nodeName)),W=e.attributes||[],Z=0;Z<W.length;Z++)V+=" "+W[Z].name+"="+a(o(W[Z].value),"double",C);return V+=">",e.childNodes&&e.childNodes.length&&(V+="..."),V+"</"+$.call(String(e.nodeName))+">"}if(i(e)){if(0===e.length)return"[]";var en=S(e,w);return B&&!function(n){for(var e=0;e<n.length;e++)if(u(n[e],"\n")>=0)return!1;return!0}(en)?"["+y(en,B)+"]":"[ "+H.call(en,", ")+" ]"}if(function(n){return!("[object Error]"!==c(n)||Y&&"object"==typeof n&&Y in n)}(e)){var tn=S(e,w);return"cause"in e&&!Q.call(e,"cause")?"{ ["+String(e)+"] "+H.call(z.call("[cause]: "+w(e.cause),tn),", ")+" }":0===tn.length?"["+String(e)+"]":"{ ["+String(e)+"] "+H.call(tn,", ")+" }"}if("object"==typeof e&&I){if(nn&&"function"==typeof e[nn])return e[nn]();if("symbol"!==I&&"function"==typeof e.inspect)return e.inspect()}if(function(n){if(!E||!n||"object"!=typeof n)return!1;try{E.call(n);try{T.call(n)}catch(n){return!0}return n instanceof Map}catch(n){}return!1}(e)){var rn=[];return D.call(e,(function(n,t){rn.push(w(t,e,!0)+" => "+w(n,e))})),k("Map",E.call(e),rn,B)}if(function(n){if(!T||!n||"object"!=typeof n)return!1;try{T.call(n);try{E.call(n)}catch(n){return!0}return n instanceof Set}catch(n){}return!1}(e)){var an=[];return O.call(e,(function(n){an.push(w(n,e))})),k("Set",T.call(e),an,B)}if(d(e))return b("WeakMap");if(g(e))return b("WeakSet");if(m(e))return b("WeakRef");if(function(n){return!("[object Number]"!==c(n)||Y&&"object"==typeof n&&Y in n)}(e))return v(w(Number(e)));if(function(n){if(!n||"object"!=typeof n||!K)return!1;try{return K.call(n),!0}catch(n){}return!1}(e))return v(w(K.call(e)));if(function(n){return!("[object Boolean]"!==c(n)||Y&&"object"==typeof n&&Y in n)}(e))return v(P.call(e));if(function(n){return!("[object String]"!==c(n)||Y&&"object"==typeof n&&Y in n)}(e))return v(w(String(e)));if(!function(n){return!("[object Date]"!==c(n)||Y&&"object"==typeof n&&Y in n)}(e)&&!function(n){return!("[object RegExp]"!==c(n)||Y&&"object"==typeof n&&Y in n)}(e)){var on=S(e,w),sn=X?X(e)===Object.prototype:e instanceof Object||e.constructor===Object,ln=e instanceof Object?"":"null prototype",cn=!sn&&Y&&Object(e)===e&&Y in e?j.call(c(e),8,-1):ln?"Object":"",pn=(sn||"function"!=typeof e.constructor?"":e.constructor.name?e.constructor.name+" ":"")+(cn||ln?"["+H.call(z.call([],cn||[],ln||[]),": ")+"] ":"");return 0===on.length?pn+"{}":B?pn+"{"+y(on,B)+"}":pn+"{ "+H.call(on,", ")+" }"}return String(e)};var en=Object.prototype.hasOwnProperty||function(n){return n in this}},function(n,e,t){"use strict";var r;if(!Object.keys){var a=Object.prototype.hasOwnProperty,o=Object.prototype.toString,i=t(26),s=Object.prototype.propertyIsEnumerable,l=!s.call({toString:null},"toString"),c=s.call((function(){}),"prototype"),p=["toString","toLocaleString","valueOf","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","constructor"],u=function(n){var e=n.constructor;return e&&e.prototype===n},d={$applicationCache:!0,$console:!0,$external:!0,$frame:!0,$frameElement:!0,$frames:!0,$innerHeight:!0,$innerWidth:!0,$onmozfullscreenchange:!0,$onmozfullscreenerror:!0,$outerHeight:!0,$outerWidth:!0,$pageXOffset:!0,$pageYOffset:!0,$parent:!0,$scrollLeft:!0,$scrollTop:!0,$scrollX:!0,$scrollY:!0,$self:!0,$webkitIndexedDB:!0,$webkitStorageInfo:!0,$window:!0},m=function(){if("undefined"==typeof window)return!1;for(var n in window)try{if(!d["$"+n]&&a.call(window,n)&&null!==window[n]&&"object"==typeof window[n])try{u(window[n])}catch(n){return!0}}catch(n){return!0}return!1}(),g=function(n){if("undefined"==typeof window||!m)return u(n);try{return u(n)}catch(n){return!1}};r=function(n){var e=null!==n&&"object"==typeof n,t="[object Function]"===o.call(n),r=i(n),s=e&&"[object String]"===o.call(n),u=[];if(!e&&!t&&!r)throw new TypeError("Object.keys called on a non-object");var d=c&&t;if(s&&n.length>0&&!a.call(n,0))for(var m=0;m<n.length;++m)u.push(String(m));if(r&&n.length>0)for(var f=0;f<n.length;++f)u.push(String(f));else for(var h in n)d&&"prototype"===h||!a.call(n,h)||u.push(String(h));if(l)for(var v=g(n),b=0;b<p.length;++b)v&&"constructor"===p[b]||!a.call(n,p[b])||u.push(p[b]);return u}}n.exports=r},function(n,e,t){"use strict";var r=Array.prototype.slice,a=t(26),o=Object.keys,i=o?function(n){return o(n)}:t(87),s=Object.keys;i.shim=function(){return Object.keys?function(){var n=Object.keys(arguments);return n&&n.length===arguments.length}(1,2)||(Object.keys=function(n){return s(a(n)?r.call(n):n)}):Object.keys=i,Object.keys||i},n.exports=i},function(n,e,t){"use strict";var r=t(2),a=t(4),o=t(27),i=t(28),s=t(90),l=a(i(),Object);r(l,{getPolyfill:i,implementation:o,shim:s}),n.exports=l},function(n,e,t){"use strict";var r=t(28),a=t(2);n.exports=function(){var n=r();return a(Object,{entries:n},{entries:function(){return Object.entries!==n}}),n}},function(n,e){function t(){throw new Error("setTimeout has not been defined")}function r(){throw new Error("clearTimeout has not been defined")}function a(n){if(c===setTimeout)return setTimeout(n,0);if((c===t||!c)&&setTimeout)return c=setTimeout,setTimeout(n,0);try{return c(n,0)}catch(e){try{return c.call(null,n,0)}catch(e){return c.call(this,n,0)}}}function o(){g&&d&&(g=!1,d.length?m=d.concat(m):f=-1,m.length&&i())}function i(){if(!g){var n=a(o);g=!0;for(var e=m.length;e;){for(d=m,m=[];++f<e;)d&&d[f].run();f=-1,e=m.length}d=null,g=!1,function(n){if(p===clearTimeout)return clearTimeout(n);if((p===r||!p)&&clearTimeout)return p=clearTimeout,clearTimeout(n);try{p(n)}catch(e){try{return p.call(null,n)}catch(e){return p.call(this,n)}}}(n)}}function s(n,e){this.fun=n,this.array=e}function l(){}var c,p,u=n.exports={};!function(){try{c="function"==typeof setTimeout?setTimeout:t}catch(n){c=t}try{p="function"==typeof clearTimeout?clearTimeout:r}catch(n){p=r}}();var d,m=[],g=!1,f=-1;u.nextTick=function(n){var e=new Array(arguments.length-1);if(arguments.length>1)for(var t=1;t<arguments.length;t++)e[t-1]=arguments[t];m.push(new s(n,e)),1!==m.length||g||a(i)},s.prototype.run=function(){this.fun.apply(null,this.array)},u.title="browser",u.browser=!0,u.env={},u.argv=[],u.version="",u.versions={},u.on=l,u.addListener=l,u.once=l,u.off=l,u.removeListener=l,u.removeAllListeners=l,u.emit=l,u.prependListener=l,u.prependOnceListener=l,u.listeners=function(n){return[]},u.binding=function(n){throw new Error("process.binding is not supported")},u.cwd=function(){return"/"},u.chdir=function(n){throw new Error("process.chdir is not supported")},u.umask=function(){return 0}},function(n,e,t){"use strict";var r=t(4),a=t(2),o=t(29),i=t(30),s=t(93),l=r(i());a(l,{getPolyfill:i,implementation:o,shim:s}),n.exports=l},function(n,e,t){"use strict";var r=t(2),a=t(30);n.exports=function(){var n=a();return r(String.prototype,{trim:n},{trim:function(){return String.prototype.trim!==n}}),n}},function(n,e,t){"use strict";(function(e){var t=function(){};if("production"!==e.env.NODE_ENV){var r=function(n,e){var t=arguments.length;e=new Array(t>1?t-1:0);for(var r=1;r<t;r++)e[r-1]=arguments[r];var a=0,o="Warning: "+n.replace(/%s/g,(function(){return e[a++]}));try{throw new Error(o)}catch(n){}};t=function(n,e,t){var a=arguments.length;t=new Array(a>2?a-2:0);for(var o=2;o<a;o++)t[o-2]=arguments[o];if(void 0===e)throw new Error("`warning(condition, format, ...args)` requires a warning message argument");n||r.apply(null,[e].concat(t))}}n.exports=t}).call(e,t(91))},function(n,e,t){function r(n,e){return new i(e).process(n)}var a=t(31),o=t(32),i=t(96);for(var s in(e=n.exports=r).filterXSS=r,e.FilterXSS=i,a)e[s]=a[s];for(var s in o)e[s]=o[s];"undefined"!=typeof window&&(window.filterXSS=n.exports),"undefined"!=typeof self&&"undefined"!=typeof DedicatedWorkerGlobalScope&&self instanceof DedicatedWorkerGlobalScope&&(self.filterXSS=n.exports)},function(n,e,t){function r(n){return null==n}function a(n){(n=function(n){var e={};for(var t in n)e[t]=n[t];return e}(n||{})).stripIgnoreTag&&(n.onIgnoreTag,n.onIgnoreTag=i.onIgnoreTagStripAll),n.whiteList=n.whiteList||n.allowList||i.whiteList,n.onTag=n.onTag||i.onTag,n.onTagAttr=n.onTagAttr||i.onTagAttr,n.onIgnoreTag=n.onIgnoreTag||i.onIgnoreTag,n.onIgnoreTagAttr=n.onIgnoreTagAttr||i.onIgnoreTagAttr,n.safeAttrValue=n.safeAttrValue||i.safeAttrValue,n.escapeHtml=n.escapeHtml||i.escapeHtml,this.options=n,!1===n.css?this.cssFilter=!1:(n.css=n.css||{},this.cssFilter=new o(n.css))}var o=t(7).FilterCSS,i=t(31),s=t(32),l=s.parseTag,c=s.parseAttr,p=t(11);a.prototype.process=function(n){if(!(n=(n=n||"").toString()))return"";var e=this.options,t=e.whiteList,a=e.onTag,o=e.onIgnoreTag,s=e.onTagAttr,u=e.onIgnoreTagAttr,d=e.safeAttrValue,m=e.escapeHtml,g=this.cssFilter;e.stripBlankChar&&(n=i.stripBlankChar(n)),e.allowCommentTag||(n=i.stripCommentTag(n));var f=!1;e.stripIgnoreTagBody&&(f=i.StripTagBody(e.stripIgnoreTagBody,o),o=f.onIgnoreTag);var h=l(n,(function(n,e,i,l,f){var h,v={sourcePosition:n,position:e,isClosing:f,isWhite:t.hasOwnProperty(i)};if(!r(h=a(i,l,v)))return h;if(v.isWhite){if(v.isClosing)return"</"+i+">";var b=function(n){var e=p.spaceIndex(n);if(-1===e)return{html:"",closing:"/"===n[n.length-2]};var t="/"===(n=p.trim(n.slice(e+1,-1)))[n.length-1];return t&&(n=p.trim(n.slice(0,-1))),{html:n,closing:t}}(l),k=t[i],y=c(b.html,(function(n,e){var t,a=-1!==p.indexOf(k,n);return r(t=s(i,n,e,a))?a?(e=d(i,n,e,g))?n+'="'+e+'"':n:r(t=u(i,n,e,a))?void 0:t:t}));return l="<"+i,y&&(l+=" "+y),b.closing&&(l+=" /"),l+">"}return r(h=o(i,l,v))?m(l):h}),m);return f&&(h=f.remove(h)),h},n.exports=a},function(n,e){n.exports={smile:"e3/2018new_weixioa02_org.png",lovely:"09/2018new_keai_org.png",happy:"1e/2018new_taikaixin_org.png",clap:"6e/2018new_guzhang_thumb.png",whee:"33/2018new_xixi_thumb.png",haha:"8f/2018new_haha_thumb.png","laugh and cry":"4a/2018new_xiaoku_thumb.png",wink:"43/2018new_jiyan_org.png",greddy:"fa/2018new_chanzui_org.png",awkward:"a3/2018new_heixian_thumb.png",sweat:"28/2018new_han_org.png","pick nose":"9a/2018new_wabi_thumb.png",hum:"7c/2018new_heng_thumb.png",angry:"f6/2018new_nu_thumb.png",grievance:"a5/2018new_weiqu_thumb.png",poor:"96/2018new_kelian_org.png",disappoint:"aa/2018new_shiwang_thumb.png",sad:"ee/2018new_beishang_org.png",tear:"6e/2018new_leimu_org.png","no way":"83/2018new_kuxiao_org.png",shy:"c1/2018new_haixiu_org.png",dirt:"10/2018new_wu_thumb.png","love you":"f6/2018new_aini_org.png",kiss:"2c/2018new_qinqin_thumb.png",amorousness:"9d/2018new_huaxin_org.png",longing:"c9/2018new_chongjing_org.png",desire:"3e/2018new_tianping_thumb.png","bad laugh":"4d/2018new_huaixiao_org.png",blackness:"9e/2018new_yinxian_org.png","laugh without word":"2d/2018new_xiaoerbuyu_org.png",titter:"71/2018new_touxiao_org.png",cool:"c4/2018new_ku_org.png","not easy":"aa/2018new_bingbujiandan_thumb.png",think:"30/2018new_sikao_org.png",question:"b8/2018new_ningwen_org.png","no idea":"2a/2018new_wenhao_thumb.png",dizzy:"07/2018new_yun_thumb.png",bomb:"a2/2018new_shuai_thumb.png",bone:"a1/2018new_kulou_thumb.png","be quiet":"b0/2018new_xu_org.png","shut up":"62/2018new_bizui_org.png",stupid:"dd/2018new_shayan_org.png","surprise ":"49/2018new_chijing_org.png",vomit:"08/2018new_tu_org.png",cold:"40/2018new_kouzhao_thumb.png",sick:"3b/2018new_shengbing_thumb.png",bye:"fd/2018new_baibai_thumb.png","look down on":"da/2018new_bishi_org.png","white eye":"ef/2018new_landelini_org.png","left hum":"43/2018new_zuohengheng_thumb.png","right hum":"c1/2018new_youhengheng_thumb.png",crazy:"17/2018new_zhuakuang_org.png","scold ":"87/2018new_zhouma_thumb.png","hit on face":"cb/2018new_dalian_org.png",wow:"ae/2018new_ding_org.png",fan:"86/2018new_hufen02_org.png",money:"a2/2018new_qian_thumb.png",yawn:"55/2018new_dahaqian_org.png",sleepy:"3c/2018new_kun_thumb.png",sleep:"e2/2018new_shuijiao_thumb.png","watermelon ":"01/2018new_chigua_thumb.png",doge:"a1/2018new_doge02_org.png",dog:"22/2018new_erha_org.png",cat:"7b/2018new_miaomiao_thumb.png",thumb:"e6/2018new_zan_org.png",good:"8a/2018new_good_org.png",ok:"45/2018new_ok_org.png",yeah:"29/2018new_ye_thumb.png","shack hand":"e9/2018new_woshou_thumb.png",bow:"e7/2018new_zuoyi_org.png",come:"42/2018new_guolai_thumb.png",punch:"86/2018new_quantou_thumb.png"}},function(n,e){n.exports={nick:"NickName",mail:"E-Mail",link:"Website(http://)",nickFail:"NickName cannot be less than 3 bytes.",mailFail:"Please confirm your email address.",sofa:"No comment yet.",submit:"Submit",reply:"Reply",cancelReply:"Cancel reply",comments:"Comments",cancel:"Cancel",confirm:"Confirm",continue:"Continue",more:"Load More...",preview:"Preview",emoji:"Emoji",expand:"See more....",seconds:"seconds ago",minutes:"minutes ago",hours:"hours ago",days:"days ago",now:"just now",uploading:"Uploading ...",uploadDone:"Upload completed!",busy:"Submit is busy, please wait...","code-98":"Valine initialization failed, please check your version of av-min.js.","code-99":"Valine initialization failed, Please check the `el` element in the init method.","code-100":"Valine initialization failed, Please check your appId and appKey.","code-140":"The total number of API calls today has exceeded the development version limit.","code-401":"Unauthorized operation, Please check your appId and appKey.","code-403":"Access denied by API domain white list, Please check your security domain."}},function(n,e){n.exports={nick:"ニックネーム",mail:"メールアドレス",link:"サイト(http://)",nickFail:"3バイト以上のニックネームをご入力ください.",mailFail:"メールアドレスをご確認ください.",sofa:"コメントしましょう~",submit:"提出する",reply:"返信する",cancelReply:"キャンセル",comments:"コメント",cancel:"キャンセル",confirm:"確認する",continue:"继续",more:"さらに読み込む...",preview:"プレビュー",emoji:"絵文字",expand:"もっと見る",seconds:"秒前",minutes:"分前",hours:"時間前",days:"日前",now:"たっだ今",uploading:"アップロード中...",uploadDone:"アップロードが完了しました!",busy:"20 秒間隔で提出してください    ...","code-98":"ロードエラーです。av-min.js のバージョンを確認してください.","code-99":"ロードエラーです。initにある`el`エレメントを確認ください.","code-100":"ロードエラーです。AppIdとAppKeyを確認ください.","code-140":"今日のAPIコールの総数が開発バージョンの上限を超えた.","code-401":"権限が制限されています。AppIdとAppKeyを確認ください.","code-403":"アクセスがAPIなどに制限されました、ドメイン名のセキュリティ設定を確認ください"}},function(n,e){n.exports={nick:"昵称",mail:"邮箱",link:"网址(http://)",nickFail:"昵称不能少于3个字符",mailFail:"请填写正确的邮件地址",sofa:"来发评论吧~",submit:"提交",reply:"回复",cancelReply:"取消回复",comments:"评论",cancel:"取消",confirm:"确认",continue:"继续",more:"加载更多...",preview:"预览",emoji:"表情",expand:"查看更多...",seconds:"秒前",minutes:"分钟前",hours:"小时前",days:"天前",now:"刚刚",uploading:"正在传输...",uploadDone:"传输完成!",busy:"操作频繁，请稍候再试...","code-98":"Valine 初始化失败，请检查 av-min.js 版本","code-99":"Valine 初始化失败，请检查init中的`el`元素.","code-100":"Valine 初始化失败，请检查你的AppId和AppKey.","code-140":"今日 API 调用总次数已超过开发版限制.","code-401":"未经授权的操作，请检查你的AppId和AppKey.","code-403":"访问被API域名白名单拒绝，请检查你的安全域名设置."}},function(n,e){n.exports={nick:"暱稱",mail:"郵箱",link:"網址(http://)",nickFail:"昵稱不能少於3個字符",mailFail:"請填寫正確的郵件地址",sofa:"來發評論吧~",submit:"提交",reply:"回覆",cancelReply:"取消回覆",comments:"評論",cancel:"取消",confirm:"確認",continue:"繼續",more:"加載更多...",preview:"預覽",emoji:"表情",expand:"查看更多...",seconds:"秒前",minutes:"分鐘前",hours:"小時前",days:"天前",now:"剛剛",uploading:"正在上傳...",uploadDone:"上傳完成!",busy:"操作頻繁，請稍候再試...","code-98":"Valine 初始化失敗，請檢查 av-min.js 版本","code-99":"Valine 初始化失敗，請檢查init中的`el`元素.","code-100":"Valine 初始化失敗，請檢查你的AppId和AppKey.","code-140":"今日 API 調用總次數已超過開發版限制.","code-401":"未經授權的操作，請檢查你的AppId和AppKey.","code-403":"訪問被API域名白名單拒絕，請檢查你的安全域名設置."}},function(n,e){},function(n,e,t){var r=t(104);"string"==typeof r&&(r=[[n.i,r,""]]);var a={transform:void 0};t(106)(r,a),r.locals&&(n.exports=r.locals)},function(n,e,t){(e=t(105)(!1)).push([n.i,'.v[data-class=v]{font-size:16px;text-align:left}.v[data-class=v] *{-webkit-box-sizing:border-box;box-sizing:border-box;line-height:1.75}.v[data-class=v] .vinput,.v[data-class=v] .veditor,.v[data-class=v] p,.v[data-class=v] pre code,.v[data-class=v] .status-bar{color:#555}.v[data-class=v] .vtime,.v[data-class=v] .vsys{color:#b3b3b3}.v[data-class=v] .text-right{text-align:right}.v[data-class=v] .text-center{text-align:center}.v[data-class=v] img{max-width:100%;border:none}.v[data-class=v] hr{margin:.825em 0;border-color:#f6f6f6;border-style:dashed}.v[data-class=v].hide-avatar .vimg{display:none}.v[data-class=v] a{position:relative;cursor:pointer;color:#1abc9c;text-decoration:none;display:inline-block}.v[data-class=v] a:hover{color:#d7191a}.v[data-class=v] pre,.v[data-class=v] code{background-color:#f8f8f8;padding:.2em .4em;border-radius:3px;font-size:85%;margin:0}.v[data-class=v] pre{padding:10px;overflow:auto;line-height:1.45}.v[data-class=v] pre code{padding:0;background:transparent;white-space:pre-wrap;word-break:keep-all}.v[data-class=v] blockquote{color:#666;margin:.5em 0;padding:0 0 0 1em;border-left:8px solid rgba(238,238,238,.5)}.v[data-class=v] .vinput{border:none;resize:none;outline:none;padding:10px 5px;max-width:100%;font-size:.775em;-webkit-box-sizing:border-box;box-sizing:border-box}.v[data-class=v] input[type=checkbox],.v[data-class=v] input[type=radio]{display:inline-block;vertical-align:middle;margin-top:-2px}.v[data-class=v] .vicon{cursor:pointer;display:inline-block;overflow:hidden;fill:#555;vertical-align:middle}.v[data-class=v] .vicon+.vicon{margin-left:10px}.v[data-class=v] .vicon.actived{fill:#66b1ff}.v[data-class=v] .vrow{font-size:0;padding:10px 0}.v[data-class=v] .vrow .vcol{display:inline-block;vertical-align:middle;font-size:14px}.v[data-class=v] .vrow .vcol.vcol-20{width:20%}.v[data-class=v] .vrow .vcol.vcol-30{width:30%}.v[data-class=v] .vrow .vcol.vcol-40{width:40%}.v[data-class=v] .vrow .vcol.vcol-50{width:50%}.v[data-class=v] .vrow .vcol.vcol-60{width:60%}.v[data-class=v] .vrow .vcol.vcol-70{width:70%}.v[data-class=v] .vrow .vcol.vcol-80{width:80%}.v[data-class=v] .vrow .vcol.vctrl{font-size:12px}.v[data-class=v] .vemoji,.v[data-class=v] .emoji{width:26px;height:26px;overflow:hidden;vertical-align:middle;margin:0 1px;display:inline-block}.v[data-class=v] .vwrap{border:1px solid #f0f0f0;border-radius:4px;margin-bottom:10px;overflow:hidden;position:relative;padding:10px}.v[data-class=v] .vwrap input{background:transparent}.v[data-class=v] .vwrap .vedit{position:relative;padding-top:10px}.v[data-class=v] .vwrap .cancel-reply-btn{position:absolute;right:5px;top:5px;cursor:pointer}.v[data-class=v] .vwrap .vemojis{display:none;font-size:18px;max-height:145px;overflow:auto;padding-bottom:10px;-webkit-box-shadow:0px 0 1px #f0f0f0;box-shadow:0px 0 1px #f0f0f0}.v[data-class=v] .vwrap .vemojis i{font-style:normal;padding-top:7px;width:36px;cursor:pointer;text-align:center;display:inline-block;vertical-align:middle}.v[data-class=v] .vwrap .vpreview{padding:7px;-webkit-box-shadow:0px 0 1px #f0f0f0;box-shadow:0px 0 1px #f0f0f0}.v[data-class=v] .vwrap .vheader .vinput{width:33.33%;border-bottom:1px #dedede dashed}.v[data-class=v] .vwrap .vheader.item2 .vinput{width:50%}.v[data-class=v] .vwrap .vheader.item1 .vinput{width:100%}.v[data-class=v] .vwrap .vheader .vinput:focus{border-bottom-color:#eb5055}@media screen and (max-width: 520px){.v[data-class=v] .vwrap .vheader .vinput{width:100%}.v[data-class=v] .vwrap .vheader.item2 .vinput{width:100%}}.v[data-class=v] .vpower{color:#999;font-size:.75em;padding:.5em 0}.v[data-class=v] .vpower a{font-size:.75em}.v[data-class=v] .vcount{padding:5px;font-weight:600;font-size:1.25em}.v[data-class=v] ul,.v[data-class=v] ol{padding:0;margin-left:1.25em}.v[data-class=v] .txt-center{text-align:center}.v[data-class=v] .txt-right{text-align:right}.v[data-class=v] .veditor{width:100%;min-height:8.75em;font-size:.875em;background:transparent;resize:vertical;-webkit-transition:all .25s ease;transition:all .25s ease}.v[data-class=v] .vbtn{-webkit-transition-duration:.4s;transition-duration:.4s;text-align:center;color:#555;border:1px solid #ededed;border-radius:.3em;display:inline-block;background:transparent;margin-bottom:0;font-weight:400;vertical-align:middle;-ms-touch-action:manipulation;touch-action:manipulation;cursor:pointer;white-space:nowrap;padding:.5em 1.25em;font-size:.875em;line-height:1.42857143;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;outline:none}.v[data-class=v] .vbtn+.vbtn{margin-left:1.25em}.v[data-class=v] .vbtn:active,.v[data-class=v] .vbtn:hover{color:#3090e4;border-color:#3090e4}.v[data-class=v] .vbtn:disabled{border-color:#e1e1e1;color:#e1e1e1;background-color:#fdfafa;cursor:not-allowed}.v[data-class=v] .vempty{padding:1.25em;text-align:center;color:#555;overflow:auto}.v[data-class=v] .vsys{display:inline-block;padding:.2em .5em;font-size:.75em;border-radius:.2em;margin-right:.3em}@media screen and (max-width: 520px){.v[data-class=v] .vsys{display:none}}.v[data-class=v] .vcards{width:100%}.v[data-class=v] .vcards .vcard{padding-top:1.25em;position:relative;display:block}.v[data-class=v] .vcards .vcard:after{content:"";clear:both;display:block}.v[data-class=v] .vcards .vcard .vimg{width:3.125em;height:3.125em;float:left;border-radius:50%;margin-right:.7525em;border:1px solid #f5f5f5;padding:.125em}@media screen and (max-width: 720px){.v[data-class=v] .vcards .vcard .vimg{width:2.5em;height:2.5em}}.v[data-class=v] .vcards .vcard .vhead{line-height:1.5;margin-top:0}.v[data-class=v] .vcards .vcard .vhead .vnick{position:relative;font-size:.875em;font-weight:500;margin-right:.875em;cursor:pointer;text-decoration:none;display:inline-block}.v[data-class=v] .vcards .vcard .vhead .vnick:hover{color:#d7191a}.v[data-class=v] .vcards .vcard .vh{overflow:hidden;padding-bottom:.5em;border-bottom:1px dashed #f5f5f5}.v[data-class=v] .vcards .vcard .vh .vtime{font-size:.75em;margin-right:.875em}.v[data-class=v] .vcards .vcard .vh .vmeta{line-height:1;position:relative}.v[data-class=v] .vcards .vcard .vh .vmeta .vat{font-size:.8125em;color:#ef2f11;cursor:pointer;float:right}.v[data-class=v] .vcards .vcard:last-child .vh{border-bottom:none}.v[data-class=v] .vcards .vcard .vcontent{word-wrap:break-word;word-break:break-all;font-size:.875em;line-height:2;position:relative;margin-bottom:.75em;padding-top:.625em}.v[data-class=v] .vcards .vcard .vcontent.expand{cursor:pointer;max-height:8em;overflow:hidden}.v[data-class=v] .vcards .vcard .vcontent.expand::before{display:block;content:"";position:absolute;width:100%;left:0;top:0;bottom:3.15em;background:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0)), to(rgba(255, 255, 255, 0.9)));background:linear-gradient(180deg, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.9));z-index:999}.v[data-class=v] .vcards .vcard .vcontent.expand::after{display:block;content:attr(data-expand);text-align:center;color:#828586;position:absolute;width:100%;height:3.15em;line-height:3.15em;left:0;bottom:0;z-index:999;background:rgba(255,255,255,.9)}.v[data-class=v] .vcards .vcard .vquote{padding-left:1em;border-left:1px dashed rgba(238,238,238,.5)}.v[data-class=v] .vcards .vcard .vquote .vimg{width:2.225em;height:2.225em}.v[data-class=v] .vpage .vmore{margin:1em 0}.v[data-class=v] .clear{content:"";display:block;clear:both}@-webkit-keyframes spin{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes spin{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@-webkit-keyframes pulse{50%{background:#dcdcdc}}@keyframes pulse{50%{background:#dcdcdc}}.v[data-class=v] .vspinner{width:22px;height:22px;display:inline-block;border:6px double #a0a0a0;border-top-color:transparent;border-bottom-color:transparent;border-radius:50%;-webkit-animation:spin 1s infinite linear;animation:spin 1s infinite linear;position:relative;vertical-align:middle;margin:0 5px}[data-theme=dark] .v[data-class=v] .vinput,[data-theme=dark] .v[data-class=v] .veditor,[data-theme=dark] .v[data-class=v] p,[data-theme=dark] .v[data-class=v] pre code,[data-theme=dark] .v[data-class=v] .status-bar,.dark .v[data-class=v] .vinput,.dark .v[data-class=v] .veditor,.dark .v[data-class=v] p,.dark .v[data-class=v] pre code,.dark .v[data-class=v] .status-bar,.theme__dark .v[data-class=v] .vinput,.theme__dark .v[data-class=v] .veditor,.theme__dark .v[data-class=v] p,.theme__dark .v[data-class=v] pre code,.theme__dark .v[data-class=v] .status-bar,.night .v[data-class=v] .vinput,.night .v[data-class=v] .veditor,.night .v[data-class=v] p,.night .v[data-class=v] pre code,.night .v[data-class=v] .status-bar{color:#b2b2b5}[data-theme=dark] .v[data-class=v] .vtime,[data-theme=dark] .v[data-class=v] .vsys,.dark .v[data-class=v] .vtime,.dark .v[data-class=v] .vsys,.theme__dark .v[data-class=v] .vtime,.theme__dark .v[data-class=v] .vsys,.night .v[data-class=v] .vtime,.night .v[data-class=v] .vsys{color:#929298}[data-theme=dark] .v[data-class=v] pre,[data-theme=dark] .v[data-class=v] code,[data-theme=dark] .v[data-class=v] pre code,.dark .v[data-class=v] pre,.dark .v[data-class=v] code,.dark .v[data-class=v] pre code,.theme__dark .v[data-class=v] pre,.theme__dark .v[data-class=v] code,.theme__dark .v[data-class=v] pre code,.night .v[data-class=v] pre,.night .v[data-class=v] code,.night .v[data-class=v] pre code{color:#929298;background-color:#151414}[data-theme=dark] .v[data-class=v] .vwrap,.dark .v[data-class=v] .vwrap,.theme__dark .v[data-class=v] .vwrap,.night .v[data-class=v] .vwrap{border-color:#b2b2b5}[data-theme=dark] .v[data-class=v] .vicon,.dark .v[data-class=v] .vicon,.theme__dark .v[data-class=v] .vicon,.night .v[data-class=v] .vicon{fill:#b2b2b5}[data-theme=dark] .v[data-class=v] .vicon.actived,.dark .v[data-class=v] .vicon.actived,.theme__dark .v[data-class=v] .vicon.actived,.night .v[data-class=v] .vicon.actived{fill:#66b1ff}[data-theme=dark] .v[data-class=v] .vbtn,.dark .v[data-class=v] .vbtn,.theme__dark .v[data-class=v] .vbtn,.night .v[data-class=v] .vbtn{color:#b2b2b5;border-color:#b2b2b5}[data-theme=dark] .v[data-class=v] .vbtn:hover,.dark .v[data-class=v] .vbtn:hover,.theme__dark .v[data-class=v] .vbtn:hover,.night .v[data-class=v] .vbtn:hover{color:#66b1ff;border-color:#66b1ff}[data-theme=dark] .v[data-class=v] a:hover,.dark .v[data-class=v] a:hover,.theme__dark .v[data-class=v] a:hover,.night .v[data-class=v] a:hover{color:#d7191a}[data-theme=dark] .v[data-class=v] .vcards .vcard .vcontent.expand::before,.dark .v[data-class=v] .vcards .vcard .vcontent.expand::before,.theme__dark .v[data-class=v] .vcards .vcard .vcontent.expand::before,.night .v[data-class=v] .vcards .vcard .vcontent.expand::before{background:-webkit-gradient(linear, left top, left bottom, from(rgba(0, 0, 0, 0.3)), to(rgba(0, 0, 0, 0.7)));background:linear-gradient(180deg, rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.7))}[data-theme=dark] .v[data-class=v] .vcards .vcard .vcontent.expand::after,.dark .v[data-class=v] .vcards .vcard .vcontent.expand::after,.theme__dark .v[data-class=v] .vcards .vcard .vcontent.expand::after,.night .v[data-class=v] .vcards .vcard .vcontent.expand::after{background:rgba(0,0,0,.7)}@media(prefers-color-scheme: dark){.v[data-class=v] .vinput,.v[data-class=v] .veditor,.v[data-class=v] p,.v[data-class=v] pre code,.v[data-class=v] .status-bar{color:#b2b2b5}.v[data-class=v] .vtime,.v[data-class=v] .vsys{color:#929298}.v[data-class=v] pre,.v[data-class=v] code,.v[data-class=v] pre code{color:#929298;background-color:#151414}.v[data-class=v] .vwrap{border-color:#b2b2b5}.v[data-class=v] .vicon{fill:#b2b2b5}.v[data-class=v] .vicon.actived{fill:#66b1ff}.v[data-class=v] .vbtn{color:#b2b2b5;border-color:#b2b2b5}.v[data-class=v] .vbtn:hover{color:#66b1ff;border-color:#66b1ff}.v[data-class=v] a:hover{color:#d7191a}.v[data-class=v] .vcards .vcard .vcontent.expand::before{background:-webkit-gradient(linear, left top, left bottom, from(rgba(0, 0, 0, 0.3)), to(rgba(0, 0, 0, 0.7)));background:linear-gradient(180deg, rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.7))}.v[data-class=v] .vcards .vcard .vcontent.expand::after{background:rgba(0,0,0,.7)}}',""]),n.exports=e},function(n,e,t){"use strict";function r(n,e){var t=n[1]||"",r=n[3];if(!r)return t;if(e&&"function"==typeof btoa){var a=function(n){return"/*# ".concat("sourceMappingURL=data:application/json;charset=utf-8;base64,".concat(btoa(unescape(encodeURIComponent(JSON.stringify(n)))))," */")}(r);return[t].concat(r.sources.map((function(n){return"/*# sourceURL=".concat(r.sourceRoot||"").concat(n," */")}))).concat([a]).join("\n")}return[t].join("\n")}n.exports=function(n){var e=[];return e.toString=function(){return this.map((function(e){var t=r(e,n);return e[2]?"@media ".concat(e[2]," {").concat(t,"}"):t})).join("")},e.i=function(n,t,r){"string"==typeof n&&(n=[[null,n,""]]);var a={};if(r)for(var o=0;o<this.length;o++){var i=this[o][0];null!=i&&(a[i]=!0)}for(var s=0;s<n.length;s++){var l=[].concat(n[s]);r&&a[l[0]]||(t&&(l[2]?l[2]="".concat(t," and ").concat(l[2]):l[2]=t),e.push(l))}},e}},function(n,e,t){function r(n,e){for(var t=0;t<n.length;t++){var r=n[t],a=g[r.id];if(a){a.refs++;for(var o=0;o<a.parts.length;o++)a.parts[o](r.parts[o]);for(;o<r.parts.length;o++)a.parts.push(p(r.parts[o],e))}else{var i=[];for(o=0;o<r.parts.length;o++)i.push(p(r.parts[o],e));g[r.id]={id:r.id,refs:1,parts:i}}}}function a(n,e){for(var t=[],r={},a=0;a<n.length;a++){var o=n[a],i=e.base?o[0]+e.base:o[0],s={css:o[1],media:o[2],sourceMap:o[3]};r[i]?r[i].parts.push(s):t.push(r[i]={id:i,parts:[s]})}return t}function o(n,e){var t=h(n.insertInto);if(!t)throw new Error("Couldn't find a style target. This probably means that the value for the 'insertInto' parameter is invalid.");var r=k[k.length-1];if("top"===n.insertAt)r?r.nextSibling?t.insertBefore(e,r.nextSibling):t.appendChild(e):t.insertBefore(e,t.firstChild),k.push(e);else{if("bottom"!==n.insertAt)throw new Error("Invalid value for parameter 'insertAt'. Must be 'top' or 'bottom'.");t.appendChild(e)}}function i(n){if(null===n.parentNode)return!1;n.parentNode.removeChild(n);var e=k.indexOf(n);e>=0&&k.splice(e,1)}function s(n){var e=document.createElement("style");return n.attrs.type="text/css",c(e,n.attrs),o(n,e),e}function l(n){var e=document.createElement("link");return n.attrs.type="text/css",n.attrs.rel="stylesheet",c(e,n.attrs),o(n,e),e}function c(n,e){Object.keys(e).forEach((function(t){n.setAttribute(t,e[t])}))}function p(n,e){var t,r,a,o;if(e.transform&&n.css){if(!(o=e.transform(n.css)))return function(){};n.css=o}if(e.singleton){var c=b++;t=v||(v=s(e)),r=u.bind(null,t,c,!1),a=u.bind(null,t,c,!0)}else n.sourceMap&&"function"==typeof URL&&"function"==typeof URL.createObjectURL&&"function"==typeof URL.revokeObjectURL&&"function"==typeof Blob&&"function"==typeof btoa?(t=l(e),r=m.bind(null,t,e),a=function(){i(t),t.href&&URL.revokeObjectURL(t.href)}):(t=s(e),r=d.bind(null,t),a=function(){i(t)});return r(n),function(e){if(e){if(e.css===n.css&&e.media===n.media&&e.sourceMap===n.sourceMap)return;r(n=e)}else a()}}function u(n,e,t,r){var a=t?"":r.css;if(n.styleSheet)n.styleSheet.cssText=S(e,a);else{var o=document.createTextNode(a),i=n.childNodes;i[e]&&n.removeChild(i[e]),i.length?n.insertBefore(o,i[e]):n.appendChild(o)}}function d(n,e){var t=e.css,r=e.media;if(r&&n.setAttribute("media",r),n.styleSheet)n.styleSheet.cssText=t;else{for(;n.firstChild;)n.removeChild(n.firstChild);n.appendChild(document.createTextNode(t))}}function m(n,e,t){var r=t.css,a=t.sourceMap,o=void 0===e.convertToAbsoluteUrls&&a;(e.convertToAbsoluteUrls||o)&&(r=y(r)),a&&(r+="\n/*# sourceMappingURL=data:application/json;base64,"+btoa(unescape(encodeURIComponent(JSON.stringify(a))))+" */");var i=new Blob([r],{type:"text/css"}),s=n.href;n.href=URL.createObjectURL(i),s&&URL.revokeObjectURL(s)}var g={},f=function(n){var e;return function(){return void 0===e&&(e=n.apply(this,arguments)),e}}((function(){return window&&document&&document.all&&!window.atob})),h=function(n){var e={};return function(t){return void 0===e[t]&&(e[t]=n.call(this,t)),e[t]}}((function(n){return document.querySelector(n)})),v=null,b=0,k=[],y=t(107);n.exports=function(n,e){if("undefined"!=typeof DEBUG&&DEBUG&&"object"!=typeof document)throw new Error("The style-loader cannot be used in a non-browser environment");(e=e||{}).attrs="object"==typeof e.attrs?e.attrs:{},e.singleton||(e.singleton=f()),e.insertInto||(e.insertInto="head"),e.insertAt||(e.insertAt="bottom");var t=a(n,e);return r(t,e),function(n){for(var o=[],i=0;i<t.length;i++){var s=t[i];(l=g[s.id]).refs--,o.push(l)}for(n&&r(a(n,e),e),i=0;i<o.length;i++){var l;if(0===(l=o[i]).refs){for(var c=0;c<l.parts.length;c++)l.parts[c]();delete g[l.id]}}}};var S=function(){var n=[];return function(e,t){return n[e]=t,n.filter(Boolean).join("\n")}}()},function(n,e){n.exports=function(n){var e="undefined"!=typeof window&&window.location;if(!e)throw new Error("fixUrls requires window.location");if(!n||"string"!=typeof n)return n;var t=e.protocol+"//"+e.host,r=t+e.pathname.replace(/\/[^\/]*$/,"/");return n.replace(/url\s*\(((?:[^)(]|\((?:[^)(]+|\([^)(]*\))*\))*)\)/gi,(function(n,e){var a,o=e.trim().replace(/^"(.*)"$/,(function(n,e){return e})).replace(/^'(.*)'$/,(function(n,e){return e}));return/^(#|data:|http:\/\/|https:\/\/|file:\/\/\/)/i.test(o)?n:(a=0===o.indexOf("//")?o:0===o.indexOf("/")?t+o:r+o.replace(/^\.\//,""),"url("+JSON.stringify(a)+")")}))}},function(n,e,t){t(103),n.exports=t(34)}])},function(n,e,t){"use strict";t(113)},function(n,e,t){"use strict";var r=t(289),a=t(114),o=t(63),i=Object.prototype.hasOwnProperty,s={brackets:function(n){return n+"[]"},comma:"comma",indices:function(n,e){return n+"["+e+"]"},repeat:function(n){return n}},l=Array.isArray,c=Array.prototype.push,p=function(n,e){c.apply(n,l(e)?e:[e])},u=Date.prototype.toISOString,d=o.default,m={addQueryPrefix:!1,allowDots:!1,charset:"utf-8",charsetSentinel:!1,delimiter:"&",encode:!0,encoder:a.encode,encodeValuesOnly:!1,format:d,formatter:o.formatters[d],indices:!1,serializeDate:function(n){return u.call(n)},skipNulls:!1,strictNullHandling:!1},g={},f=function n(e,t,o,i,s,c,u,d,f,h,v,b,k,y,S,x){for(var w,E=e,D=x,C=0,I=!1;void 0!==(D=D.get(g))&&!I;){var T=D.get(e);if(C+=1,void 0!==T){if(T===C)throw new RangeError("Cyclic object value");I=!0}void 0===D.get(g)&&(C=0)}if("function"==typeof d?E=d(t,E):E instanceof Date?E=v(E):"comma"===o&&l(E)&&(E=a.maybeMap(E,(function(n){return n instanceof Date?v(n):n}))),null===E){if(s)return u&&!y?u(t,m.encoder,S,"key",b):t;E=""}if("string"==typeof(w=E)||"number"==typeof w||"boolean"==typeof w||"symbol"==typeof w||"bigint"==typeof w||a.isBuffer(E))return u?[k(y?t:u(t,m.encoder,S,"key",b))+"="+k(u(E,m.encoder,S,"value",b))]:[k(t)+"="+k(String(E))];var O,A=[];if(void 0===E)return A;if("comma"===o&&l(E))y&&u&&(E=a.maybeMap(E,u)),O=[{value:E.length>0?E.join(",")||null:void 0}];else if(l(d))O=d;else{var _=Object.keys(E);O=f?_.sort(f):_}for(var R=i&&l(E)&&1===E.length?t+"[]":t,P=0;P<O.length;++P){var F=O[P],B="object"==typeof F&&void 0!==F.value?F.value:E[F];if(!c||null!==B){var M=l(E)?"function"==typeof o?o(R,F):R:R+(h?"."+F:"["+F+"]");x.set(e,C);var j=r();j.set(g,x),p(A,n(B,M,o,i,s,c,"comma"===o&&y&&l(E)?null:u,d,f,h,v,b,k,y,S,j))}}return A};n.exports=function(n,e){var t,a=n,c=function(n){if(!n)return m;if(null!==n.encoder&&void 0!==n.encoder&&"function"!=typeof n.encoder)throw new TypeError("Encoder has to be a function.");var e=n.charset||m.charset;if(void 0!==n.charset&&"utf-8"!==n.charset&&"iso-8859-1"!==n.charset)throw new TypeError("The charset option must be either utf-8, iso-8859-1, or undefined");var t=o.default;if(void 0!==n.format){if(!i.call(o.formatters,n.format))throw new TypeError("Unknown format option provided.");t=n.format}var r=o.formatters[t],a=m.filter;return("function"==typeof n.filter||l(n.filter))&&(a=n.filter),{addQueryPrefix:"boolean"==typeof n.addQueryPrefix?n.addQueryPrefix:m.addQueryPrefix,allowDots:void 0===n.allowDots?m.allowDots:!!n.allowDots,charset:e,charsetSentinel:"boolean"==typeof n.charsetSentinel?n.charsetSentinel:m.charsetSentinel,delimiter:void 0===n.delimiter?m.delimiter:n.delimiter,encode:"boolean"==typeof n.encode?n.encode:m.encode,encoder:"function"==typeof n.encoder?n.encoder:m.encoder,encodeValuesOnly:"boolean"==typeof n.encodeValuesOnly?n.encodeValuesOnly:m.encodeValuesOnly,filter:a,format:t,formatter:r,serializeDate:"function"==typeof n.serializeDate?n.serializeDate:m.serializeDate,skipNulls:"boolean"==typeof n.skipNulls?n.skipNulls:m.skipNulls,sort:"function"==typeof n.sort?n.sort:null,strictNullHandling:"boolean"==typeof n.strictNullHandling?n.strictNullHandling:m.strictNullHandling}}(e);"function"==typeof c.filter?a=(0,c.filter)("",a):l(c.filter)&&(t=c.filter);var u,d=[];if("object"!=typeof a||null===a)return"";u=e&&e.arrayFormat in s?e.arrayFormat:e&&"indices"in e?e.indices?"indices":"repeat":"indices";var g=s[u];if(e&&"commaRoundTrip"in e&&"boolean"!=typeof e.commaRoundTrip)throw new TypeError("`commaRoundTrip` must be a boolean, or absent");var h="comma"===g&&e&&e.commaRoundTrip;t||(t=Object.keys(a)),c.sort&&t.sort(c.sort);for(var v=r(),b=0;b<t.length;++b){var k=t[b];c.skipNulls&&null===a[k]||p(d,f(a[k],k,g,h,c.strictNullHandling,c.skipNulls,c.encode?c.encoder:null,c.filter,c.sort,c.allowDots,c.serializeDate,c.format,c.formatter,c.encodeValuesOnly,c.charset,v))}var y=d.join(c.delimiter),S=!0===c.addQueryPrefix?"?":"";return c.charsetSentinel&&("iso-8859-1"===c.charset?S+="utf8=%26%2310003%3B&":S+="utf8=%E2%9C%93&"),y.length>0?S+y:""}},function(n,e,t){"use strict";var r=t(61),a=t(295),o=t(297),i=r("%TypeError%"),s=r("%WeakMap%",!0),l=r("%Map%",!0),c=a("WeakMap.prototype.get",!0),p=a("WeakMap.prototype.set",!0),u=a("WeakMap.prototype.has",!0),d=a("Map.prototype.get",!0),m=a("Map.prototype.set",!0),g=a("Map.prototype.has",!0),f=function(n,e){for(var t,r=n;null!==(t=r.next);r=t)if(t.key===e)return r.next=t.next,t.next=n.next,n.next=t,t};n.exports=function(){var n,e,t,r={assert:function(n){if(!r.has(n))throw new i("Side channel does not contain "+o(n))},get:function(r){if(s&&r&&("object"==typeof r||"function"==typeof r)){if(n)return c(n,r)}else if(l){if(e)return d(e,r)}else if(t)return function(n,e){var t=f(n,e);return t&&t.value}(t,r)},has:function(r){if(s&&r&&("object"==typeof r||"function"==typeof r)){if(n)return u(n,r)}else if(l){if(e)return g(e,r)}else if(t)return function(n,e){return!!f(n,e)}(t,r);return!1},set:function(r,a){s&&r&&("object"==typeof r||"function"==typeof r)?(n||(n=new s),p(n,r,a)):l?(e||(e=new l),m(e,r,a)):(t||(t={key:{},next:null}),function(n,e,t){var r=f(n,e);r?r.value=t:n.next={key:e,next:n.next,value:t}}(t,r,a))}};return r}},function(n,e,t){"use strict";var r="undefined"!=typeof Symbol&&Symbol,a=t(291);n.exports=function(){return"function"==typeof r&&("function"==typeof Symbol&&("symbol"==typeof r("foo")&&("symbol"==typeof Symbol("bar")&&a())))}},function(n,e,t){"use strict";n.exports=function(){if("function"!=typeof Symbol||"function"!=typeof Object.getOwnPropertySymbols)return!1;if("symbol"==typeof Symbol.iterator)return!0;var n={},e=Symbol("test"),t=Object(e);if("string"==typeof e)return!1;if("[object Symbol]"!==Object.prototype.toString.call(e))return!1;if("[object Symbol]"!==Object.prototype.toString.call(t))return!1;for(e in n[e]=42,n)return!1;if("function"==typeof Object.keys&&0!==Object.keys(n).length)return!1;if("function"==typeof Object.getOwnPropertyNames&&0!==Object.getOwnPropertyNames(n).length)return!1;var r=Object.getOwnPropertySymbols(n);if(1!==r.length||r[0]!==e)return!1;if(!Object.prototype.propertyIsEnumerable.call(n,e))return!1;if("function"==typeof Object.getOwnPropertyDescriptor){var a=Object.getOwnPropertyDescriptor(n,e);if(42!==a.value||!0!==a.enumerable)return!1}return!0}},function(n,e,t){"use strict";var r={foo:{}},a=Object;n.exports=function(){return{__proto__:r}.foo===r.foo&&!({__proto__:null}instanceof a)}},function(n,e,t){"use strict";var r="Function.prototype.bind called on incompatible ",a=Array.prototype.slice,o=Object.prototype.toString;n.exports=function(n){var e=this;if("function"!=typeof e||"[object Function]"!==o.call(e))throw new TypeError(r+e);for(var t,i=a.call(arguments,1),s=function(){if(this instanceof t){var r=e.apply(this,i.concat(a.call(arguments)));return Object(r)===r?r:this}return e.apply(n,i.concat(a.call(arguments)))},l=Math.max(0,e.length-i.length),c=[],p=0;p<l;p++)c.push("$"+p);if(t=Function("binder","return function ("+c.join(",")+"){ return binder.apply(this,arguments); }")(s),e.prototype){var u=function(){};u.prototype=e.prototype,t.prototype=new u,u.prototype=null}return t}},function(n,e,t){"use strict";var r=t(62);n.exports=r.call(Function.call,Object.prototype.hasOwnProperty)},function(n,e,t){"use strict";var r=t(61),a=t(296),o=a(r("String.prototype.indexOf"));n.exports=function(n,e){var t=r(n,!!e);return"function"==typeof t&&o(n,".prototype.")>-1?a(t):t}},function(n,e,t){"use strict";var r=t(62),a=t(61),o=a("%Function.prototype.apply%"),i=a("%Function.prototype.call%"),s=a("%Reflect.apply%",!0)||r.call(i,o),l=a("%Object.getOwnPropertyDescriptor%",!0),c=a("%Object.defineProperty%",!0),p=a("%Math.max%");if(c)try{c({},"a",{value:1})}catch(n){c=null}n.exports=function(n){var e=s(r,i,arguments);if(l&&c){var t=l(e,"length");t.configurable&&c(e,"length",{value:1+p(0,n.length-(arguments.length-1))})}return e};var u=function(){return s(r,o,arguments)};c?c(n.exports,"apply",{value:u}):n.exports.apply=u},function(n,e,t){var r="function"==typeof Map&&Map.prototype,a=Object.getOwnPropertyDescriptor&&r?Object.getOwnPropertyDescriptor(Map.prototype,"size"):null,o=r&&a&&"function"==typeof a.get?a.get:null,i=r&&Map.prototype.forEach,s="function"==typeof Set&&Set.prototype,l=Object.getOwnPropertyDescriptor&&s?Object.getOwnPropertyDescriptor(Set.prototype,"size"):null,c=s&&l&&"function"==typeof l.get?l.get:null,p=s&&Set.prototype.forEach,u="function"==typeof WeakMap&&WeakMap.prototype?WeakMap.prototype.has:null,d="function"==typeof WeakSet&&WeakSet.prototype?WeakSet.prototype.has:null,m="function"==typeof WeakRef&&WeakRef.prototype?WeakRef.prototype.deref:null,g=Boolean.prototype.valueOf,f=Object.prototype.toString,h=Function.prototype.toString,v=String.prototype.match,b=String.prototype.slice,k=String.prototype.replace,y=String.prototype.toUpperCase,S=String.prototype.toLowerCase,x=RegExp.prototype.test,w=Array.prototype.concat,E=Array.prototype.join,D=Array.prototype.slice,C=Math.floor,I="function"==typeof BigInt?BigInt.prototype.valueOf:null,T=Object.getOwnPropertySymbols,O="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?Symbol.prototype.toString:null,A="function"==typeof Symbol&&"object"==typeof Symbol.iterator,_="function"==typeof Symbol&&Symbol.toStringTag&&(typeof Symbol.toStringTag===A||"symbol")?Symbol.toStringTag:null,R=Object.prototype.propertyIsEnumerable,P=("function"==typeof Reflect?Reflect.getPrototypeOf:Object.getPrototypeOf)||([].__proto__===Array.prototype?function(n){return n.__proto__}:null);function F(n,e){if(n===1/0||n===-1/0||n!=n||n&&n>-1e3&&n<1e3||x.call(/e/,e))return e;var t=/[0-9](?=(?:[0-9]{3})+(?![0-9]))/g;if("number"==typeof n){var r=n<0?-C(-n):C(n);if(r!==n){var a=String(r),o=b.call(e,a.length+1);return k.call(a,t,"$&_")+"."+k.call(k.call(o,/([0-9]{3})/g,"$&_"),/_$/,"")}}return k.call(e,t,"$&_")}var B=t(298),M=B.custom,j=z(M)?M:null;function L(n,e,t){var r="double"===(t.quoteStyle||e)?'"':"'";return r+n+r}function N(n){return k.call(String(n),/"/g,"&quot;")}function $(n){return!("[object Array]"!==V(n)||_&&"object"==typeof n&&_ in n)}function U(n){return!("[object RegExp]"!==V(n)||_&&"object"==typeof n&&_ in n)}function z(n){if(A)return n&&"object"==typeof n&&n instanceof Symbol;if("symbol"==typeof n)return!0;if(!n||"object"!=typeof n||!O)return!1;try{return O.call(n),!0}catch(n){}return!1}n.exports=function n(e,t,r,a){var s=t||{};if(q(s,"quoteStyle")&&"single"!==s.quoteStyle&&"double"!==s.quoteStyle)throw new TypeError('option "quoteStyle" must be "single" or "double"');if(q(s,"maxStringLength")&&("number"==typeof s.maxStringLength?s.maxStringLength<0&&s.maxStringLength!==1/0:null!==s.maxStringLength))throw new TypeError('option "maxStringLength", if provided, must be a positive integer, Infinity, or `null`');var l=!q(s,"customInspect")||s.customInspect;if("boolean"!=typeof l&&"symbol"!==l)throw new TypeError("option \"customInspect\", if provided, must be `true`, `false`, or `'symbol'`");if(q(s,"indent")&&null!==s.indent&&"\t"!==s.indent&&!(parseInt(s.indent,10)===s.indent&&s.indent>0))throw new TypeError('option "indent" must be "\\t", an integer > 0, or `null`');if(q(s,"numericSeparator")&&"boolean"!=typeof s.numericSeparator)throw new TypeError('option "numericSeparator", if provided, must be `true` or `false`');var f=s.numericSeparator;if(void 0===e)return"undefined";if(null===e)return"null";if("boolean"==typeof e)return e?"true":"false";if("string"==typeof e)return function n(e,t){if(e.length>t.maxStringLength){var r=e.length-t.maxStringLength,a="... "+r+" more character"+(r>1?"s":"");return n(b.call(e,0,t.maxStringLength),t)+a}return L(k.call(k.call(e,/(['\\])/g,"\\$1"),/[\x00-\x1f]/g,W),"single",t)}(e,s);if("number"==typeof e){if(0===e)return 1/0/e>0?"0":"-0";var y=String(e);return f?F(e,y):y}if("bigint"==typeof e){var x=String(e)+"n";return f?F(e,x):x}var C=void 0===s.depth?5:s.depth;if(void 0===r&&(r=0),r>=C&&C>0&&"object"==typeof e)return $(e)?"[Array]":"[Object]";var T=function(n,e){var t;if("\t"===n.indent)t="\t";else{if(!("number"==typeof n.indent&&n.indent>0))return null;t=E.call(Array(n.indent+1)," ")}return{base:t,prev:E.call(Array(e+1),t)}}(s,r);if(void 0===a)a=[];else if(K(a,e)>=0)return"[Circular]";function M(e,t,o){if(t&&(a=D.call(a)).push(t),o){var i={depth:s.depth};return q(s,"quoteStyle")&&(i.quoteStyle=s.quoteStyle),n(e,i,r+1,a)}return n(e,s,r+1,a)}if("function"==typeof e&&!U(e)){var H=function(n){if(n.name)return n.name;var e=v.call(h.call(n),/^function\s*([\w$]+)/);if(e)return e[1];return null}(e),Z=X(e,M);return"[Function"+(H?": "+H:" (anonymous)")+"]"+(Z.length>0?" { "+E.call(Z,", ")+" }":"")}if(z(e)){var nn=A?k.call(String(e),/^(Symbol\(.*\))_[^)]*$/,"$1"):O.call(e);return"object"!=typeof e||A?nn:G(nn)}if(function(n){if(!n||"object"!=typeof n)return!1;if("undefined"!=typeof HTMLElement&&n instanceof HTMLElement)return!0;return"string"==typeof n.nodeName&&"function"==typeof n.getAttribute}(e)){for(var en="<"+S.call(String(e.nodeName)),tn=e.attributes||[],rn=0;rn<tn.length;rn++)en+=" "+tn[rn].name+"="+L(N(tn[rn].value),"double",s);return en+=">",e.childNodes&&e.childNodes.length&&(en+="..."),en+="</"+S.call(String(e.nodeName))+">"}if($(e)){if(0===e.length)return"[]";var an=X(e,M);return T&&!function(n){for(var e=0;e<n.length;e++)if(K(n[e],"\n")>=0)return!1;return!0}(an)?"["+Q(an,T)+"]":"[ "+E.call(an,", ")+" ]"}if(function(n){return!("[object Error]"!==V(n)||_&&"object"==typeof n&&_ in n)}(e)){var on=X(e,M);return"cause"in Error.prototype||!("cause"in e)||R.call(e,"cause")?0===on.length?"["+String(e)+"]":"{ ["+String(e)+"] "+E.call(on,", ")+" }":"{ ["+String(e)+"] "+E.call(w.call("[cause]: "+M(e.cause),on),", ")+" }"}if("object"==typeof e&&l){if(j&&"function"==typeof e[j]&&B)return B(e,{depth:C-r});if("symbol"!==l&&"function"==typeof e.inspect)return e.inspect()}if(function(n){if(!o||!n||"object"!=typeof n)return!1;try{o.call(n);try{c.call(n)}catch(n){return!0}return n instanceof Map}catch(n){}return!1}(e)){var sn=[];return i&&i.call(e,(function(n,t){sn.push(M(t,e,!0)+" => "+M(n,e))})),Y("Map",o.call(e),sn,T)}if(function(n){if(!c||!n||"object"!=typeof n)return!1;try{c.call(n);try{o.call(n)}catch(n){return!0}return n instanceof Set}catch(n){}return!1}(e)){var ln=[];return p&&p.call(e,(function(n){ln.push(M(n,e))})),Y("Set",c.call(e),ln,T)}if(function(n){if(!u||!n||"object"!=typeof n)return!1;try{u.call(n,u);try{d.call(n,d)}catch(n){return!0}return n instanceof WeakMap}catch(n){}return!1}(e))return J("WeakMap");if(function(n){if(!d||!n||"object"!=typeof n)return!1;try{d.call(n,d);try{u.call(n,u)}catch(n){return!0}return n instanceof WeakSet}catch(n){}return!1}(e))return J("WeakSet");if(function(n){if(!m||!n||"object"!=typeof n)return!1;try{return m.call(n),!0}catch(n){}return!1}(e))return J("WeakRef");if(function(n){return!("[object Number]"!==V(n)||_&&"object"==typeof n&&_ in n)}(e))return G(M(Number(e)));if(function(n){if(!n||"object"!=typeof n||!I)return!1;try{return I.call(n),!0}catch(n){}return!1}(e))return G(M(I.call(e)));if(function(n){return!("[object Boolean]"!==V(n)||_&&"object"==typeof n&&_ in n)}(e))return G(g.call(e));if(function(n){return!("[object String]"!==V(n)||_&&"object"==typeof n&&_ in n)}(e))return G(M(String(e)));if(!function(n){return!("[object Date]"!==V(n)||_&&"object"==typeof n&&_ in n)}(e)&&!U(e)){var cn=X(e,M),pn=P?P(e)===Object.prototype:e instanceof Object||e.constructor===Object,un=e instanceof Object?"":"null prototype",dn=!pn&&_&&Object(e)===e&&_ in e?b.call(V(e),8,-1):un?"Object":"",mn=(pn||"function"!=typeof e.constructor?"":e.constructor.name?e.constructor.name+" ":"")+(dn||un?"["+E.call(w.call([],dn||[],un||[]),": ")+"] ":"");return 0===cn.length?mn+"{}":T?mn+"{"+Q(cn,T)+"}":mn+"{ "+E.call(cn,", ")+" }"}return String(e)};var H=Object.prototype.hasOwnProperty||function(n){return n in this};function q(n,e){return H.call(n,e)}function V(n){return f.call(n)}function K(n,e){if(n.indexOf)return n.indexOf(e);for(var t=0,r=n.length;t<r;t++)if(n[t]===e)return t;return-1}function W(n){var e=n.charCodeAt(0),t={8:"b",9:"t",10:"n",12:"f",13:"r"}[e];return t?"\\"+t:"\\x"+(e<16?"0":"")+y.call(e.toString(16))}function G(n){return"Object("+n+")"}function J(n){return n+" { ? }"}function Y(n,e,t,r){return n+" ("+e+") {"+(r?Q(t,r):E.call(t,", "))+"}"}function Q(n,e){if(0===n.length)return"";var t="\n"+e.prev+e.base;return t+E.call(n,","+t)+"\n"+e.prev}function X(n,e){var t=$(n),r=[];if(t){r.length=n.length;for(var a=0;a<n.length;a++)r[a]=q(n,a)?e(n[a],n):""}var o,i="function"==typeof T?T(n):[];if(A){o={};for(var s=0;s<i.length;s++)o["$"+i[s]]=i[s]}for(var l in n)q(n,l)&&(t&&String(Number(l))===l&&l<n.length||A&&o["$"+l]instanceof Symbol||(x.call(/[^\w$]/,l)?r.push(e(l,n)+": "+e(n[l],n)):r.push(l+": "+e(n[l],n))));if("function"==typeof T)for(var c=0;c<i.length;c++)R.call(n,i[c])&&r.push("["+e(i[c])+"]: "+e(n[i[c]],n));return r}},function(n,e){},function(n,e,t){"use strict";var r=t(114),a=Object.prototype.hasOwnProperty,o=Array.isArray,i={allowDots:!1,allowPrototypes:!1,allowSparse:!1,arrayLimit:20,charset:"utf-8",charsetSentinel:!1,comma:!1,decoder:r.decode,delimiter:"&",depth:5,ignoreQueryPrefix:!1,interpretNumericEntities:!1,parameterLimit:1e3,parseArrays:!0,plainObjects:!1,strictNullHandling:!1},s=function(n){return n.replace(/&#(\d+);/g,(function(n,e){return String.fromCharCode(parseInt(e,10))}))},l=function(n,e){return n&&"string"==typeof n&&e.comma&&n.indexOf(",")>-1?n.split(","):n},c=function(n,e,t,r){if(n){var o=t.allowDots?n.replace(/\.([^.[]+)/g,"[$1]"):n,i=/(\[[^[\]]*])/g,s=t.depth>0&&/(\[[^[\]]*])/.exec(o),c=s?o.slice(0,s.index):o,p=[];if(c){if(!t.plainObjects&&a.call(Object.prototype,c)&&!t.allowPrototypes)return;p.push(c)}for(var u=0;t.depth>0&&null!==(s=i.exec(o))&&u<t.depth;){if(u+=1,!t.plainObjects&&a.call(Object.prototype,s[1].slice(1,-1))&&!t.allowPrototypes)return;p.push(s[1])}return s&&p.push("["+o.slice(s.index)+"]"),function(n,e,t,r){for(var a=r?e:l(e,t),o=n.length-1;o>=0;--o){var i,s=n[o];if("[]"===s&&t.parseArrays)i=[].concat(a);else{i=t.plainObjects?Object.create(null):{};var c="["===s.charAt(0)&&"]"===s.charAt(s.length-1)?s.slice(1,-1):s,p=parseInt(c,10);t.parseArrays||""!==c?!isNaN(p)&&s!==c&&String(p)===c&&p>=0&&t.parseArrays&&p<=t.arrayLimit?(i=[])[p]=a:"__proto__"!==c&&(i[c]=a):i={0:a}}a=i}return a}(p,e,t,r)}};n.exports=function(n,e){var t=function(n){if(!n)return i;if(null!==n.decoder&&void 0!==n.decoder&&"function"!=typeof n.decoder)throw new TypeError("Decoder has to be a function.");if(void 0!==n.charset&&"utf-8"!==n.charset&&"iso-8859-1"!==n.charset)throw new TypeError("The charset option must be either utf-8, iso-8859-1, or undefined");var e=void 0===n.charset?i.charset:n.charset;return{allowDots:void 0===n.allowDots?i.allowDots:!!n.allowDots,allowPrototypes:"boolean"==typeof n.allowPrototypes?n.allowPrototypes:i.allowPrototypes,allowSparse:"boolean"==typeof n.allowSparse?n.allowSparse:i.allowSparse,arrayLimit:"number"==typeof n.arrayLimit?n.arrayLimit:i.arrayLimit,charset:e,charsetSentinel:"boolean"==typeof n.charsetSentinel?n.charsetSentinel:i.charsetSentinel,comma:"boolean"==typeof n.comma?n.comma:i.comma,decoder:"function"==typeof n.decoder?n.decoder:i.decoder,delimiter:"string"==typeof n.delimiter||r.isRegExp(n.delimiter)?n.delimiter:i.delimiter,depth:"number"==typeof n.depth||!1===n.depth?+n.depth:i.depth,ignoreQueryPrefix:!0===n.ignoreQueryPrefix,interpretNumericEntities:"boolean"==typeof n.interpretNumericEntities?n.interpretNumericEntities:i.interpretNumericEntities,parameterLimit:"number"==typeof n.parameterLimit?n.parameterLimit:i.parameterLimit,parseArrays:!1!==n.parseArrays,plainObjects:"boolean"==typeof n.plainObjects?n.plainObjects:i.plainObjects,strictNullHandling:"boolean"==typeof n.strictNullHandling?n.strictNullHandling:i.strictNullHandling}}(e);if(""===n||null==n)return t.plainObjects?Object.create(null):{};for(var p="string"==typeof n?function(n,e){var t,c={__proto__:null},p=e.ignoreQueryPrefix?n.replace(/^\?/,""):n,u=e.parameterLimit===1/0?void 0:e.parameterLimit,d=p.split(e.delimiter,u),m=-1,g=e.charset;if(e.charsetSentinel)for(t=0;t<d.length;++t)0===d[t].indexOf("utf8=")&&("utf8=%E2%9C%93"===d[t]?g="utf-8":"utf8=%26%2310003%3B"===d[t]&&(g="iso-8859-1"),m=t,t=d.length);for(t=0;t<d.length;++t)if(t!==m){var f,h,v=d[t],b=v.indexOf("]="),k=-1===b?v.indexOf("="):b+1;-1===k?(f=e.decoder(v,i.decoder,g,"key"),h=e.strictNullHandling?null:""):(f=e.decoder(v.slice(0,k),i.decoder,g,"key"),h=r.maybeMap(l(v.slice(k+1),e),(function(n){return e.decoder(n,i.decoder,g,"value")}))),h&&e.interpretNumericEntities&&"iso-8859-1"===g&&(h=s(h)),v.indexOf("[]=")>-1&&(h=o(h)?[h]:h),a.call(c,f)?c[f]=r.combine(c[f],h):c[f]=h}return c}(n,t):n,u=t.plainObjects?Object.create(null):{},d=Object.keys(p),m=0;m<d.length;++m){var g=d[m],f=c(g,p[g],t,"string"==typeof n);u=r.merge(u,f,t)}return!0===t.allowSparse?u:r.compact(u)}},function(n,e,t){var r=t(16),a=t(302),o=t(303);n.exports=function(n){var e=r(n);return o(e,a(e))+1}},function(n,e){n.exports=function(n){var e=new Date(n.getTime()),t=e.getTimezoneOffset();return e.setSeconds(0,0),6e4*t+e.getTime()%6e4}},function(n,e,t){var r=t(16);n.exports=function(n){var e=r(n),t=new Date(0);return t.setFullYear(e.getFullYear(),0,1),t.setHours(0,0,0,0),t}},function(n,e,t){var r=t(304);n.exports=function(n,e){var t=r(n),a=r(e),o=t.getTime()-6e4*t.getTimezoneOffset(),i=a.getTime()-6e4*a.getTimezoneOffset();return Math.round((o-i)/864e5)}},function(n,e,t){var r=t(16);n.exports=function(n){var e=r(n);return e.setHours(0,0,0,0),e}},function(n,e,t){var r=t(16),a=t(64),o=t(307);n.exports=function(n){var e=r(n),t=a(e).getTime()-o(e).getTime();return Math.round(t/6048e5)+1}},function(n,e,t){var r=t(16);n.exports=function(n,e){var t=e&&Number(e.weekStartsOn)||0,a=r(n),o=a.getDay(),i=(o<t?7:0)+o-t;return a.setDate(a.getDate()-i),a.setHours(0,0,0,0),a}},function(n,e,t){var r=t(116),a=t(64);n.exports=function(n){var e=r(n),t=new Date(0);return t.setFullYear(e,0,4),t.setHours(0,0,0,0),a(t)}},function(n,e,t){var r=t(115);n.exports=function(n){if(r(n))return!isNaN(n);throw new TypeError(toString.call(n)+" is not an instance of Date")}},function(n,e,t){var r=t(310),a=t(311);n.exports={distanceInWords:r(),format:a()}},function(n,e){n.exports=function(){var n={lessThanXSeconds:{one:"less than a second",other:"less than {{count}} seconds"},xSeconds:{one:"1 second",other:"{{count}} seconds"},halfAMinute:"half a minute",lessThanXMinutes:{one:"less than a minute",other:"less than {{count}} minutes"},xMinutes:{one:"1 minute",other:"{{count}} minutes"},aboutXHours:{one:"about 1 hour",other:"about {{count}} hours"},xHours:{one:"1 hour",other:"{{count}} hours"},xDays:{one:"1 day",other:"{{count}} days"},aboutXMonths:{one:"about 1 month",other:"about {{count}} months"},xMonths:{one:"1 month",other:"{{count}} months"},aboutXYears:{one:"about 1 year",other:"about {{count}} years"},xYears:{one:"1 year",other:"{{count}} years"},overXYears:{one:"over 1 year",other:"over {{count}} years"},almostXYears:{one:"almost 1 year",other:"almost {{count}} years"}};return{localize:function(e,t,r){var a;return r=r||{},a="string"==typeof n[e]?n[e]:1===t?n[e].one:n[e].other.replace("{{count}}",t),r.addSuffix?r.comparison>0?"in "+a:a+" ago":a}}}},function(n,e,t){var r=t(312);n.exports=function(){var n=["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],e=["January","February","March","April","May","June","July","August","September","October","November","December"],t=["Su","Mo","Tu","We","Th","Fr","Sa"],a=["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],o=["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],i=["AM","PM"],s=["am","pm"],l=["a.m.","p.m."],c={MMM:function(e){return n[e.getMonth()]},MMMM:function(n){return e[n.getMonth()]},dd:function(n){return t[n.getDay()]},ddd:function(n){return a[n.getDay()]},dddd:function(n){return o[n.getDay()]},A:function(n){return n.getHours()/12>=1?i[1]:i[0]},a:function(n){return n.getHours()/12>=1?s[1]:s[0]},aa:function(n){return n.getHours()/12>=1?l[1]:l[0]}};return["M","D","DDD","d","Q","W"].forEach((function(n){c[n+"o"]=function(e,t){return function(n){var e=n%100;if(e>20||e<10)switch(e%10){case 1:return n+"st";case 2:return n+"nd";case 3:return n+"rd"}return n+"th"}(t[n](e))}})),{formatters:c,formattingTokensRegExp:r(c)}}},function(n,e){var t=["M","MM","Q","D","DD","DDD","DDDD","d","E","W","WW","YY","YYYY","GG","GGGG","H","HH","h","hh","m","mm","s","ss","S","SS","SSS","Z","ZZ","X","x"];n.exports=function(n){var e=[];for(var r in n)n.hasOwnProperty(r)&&e.push(r);var a=t.concat(e).sort().reverse();return new RegExp("(\\[[^\\[]*\\])|(\\\\)?("+a.join("|")+"|.)","g")}},function(n,e,t){"use strict";var r=t(5),a=t(117),o=t(314),i=t(123);function s(n){var e=new o(n),t=a(o.prototype.request,e);return r.extend(t,o.prototype,e),r.extend(t,e),t}var l=s(t(65));l.Axios=o,l.create=function(n){return s(i(l.defaults,n))},l.Cancel=t(124),l.CancelToken=t(328),l.isCancel=t(122),l.all=function(n){return Promise.all(n)},l.spread=t(329),l.isAxiosError=t(330),n.exports=l,n.exports.default=l},function(n,e,t){"use strict";var r=t(5),a=t(118),o=t(315),i=t(316),s=t(123),l=t(326),c=l.validators;function p(n){this.defaults=n,this.interceptors={request:new o,response:new o}}p.prototype.request=function(n){"string"==typeof n?(n=arguments[1]||{}).url=arguments[0]:n=n||{},(n=s(this.defaults,n)).method?n.method=n.method.toLowerCase():this.defaults.method?n.method=this.defaults.method.toLowerCase():n.method="get";var e=n.transitional;void 0!==e&&l.assertOptions(e,{silentJSONParsing:c.transitional(c.boolean,"1.0.0"),forcedJSONParsing:c.transitional(c.boolean,"1.0.0"),clarifyTimeoutError:c.transitional(c.boolean,"1.0.0")},!1);var t=[],r=!0;this.interceptors.request.forEach((function(e){"function"==typeof e.runWhen&&!1===e.runWhen(n)||(r=r&&e.synchronous,t.unshift(e.fulfilled,e.rejected))}));var a,o=[];if(this.interceptors.response.forEach((function(n){o.push(n.fulfilled,n.rejected)})),!r){var p=[i,void 0];for(Array.prototype.unshift.apply(p,t),p=p.concat(o),a=Promise.resolve(n);p.length;)a=a.then(p.shift(),p.shift());return a}for(var u=n;t.length;){var d=t.shift(),m=t.shift();try{u=d(u)}catch(n){m(n);break}}try{a=i(u)}catch(n){return Promise.reject(n)}for(;o.length;)a=a.then(o.shift(),o.shift());return a},p.prototype.getUri=function(n){return n=s(this.defaults,n),a(n.url,n.params,n.paramsSerializer).replace(/^\?/,"")},r.forEach(["delete","get","head","options"],(function(n){p.prototype[n]=function(e,t){return this.request(s(t||{},{method:n,url:e,data:(t||{}).data}))}})),r.forEach(["post","put","patch"],(function(n){p.prototype[n]=function(e,t,r){return this.request(s(r||{},{method:n,url:e,data:t}))}})),n.exports=p},function(n,e,t){"use strict";var r=t(5);function a(){this.handlers=[]}a.prototype.use=function(n,e,t){return this.handlers.push({fulfilled:n,rejected:e,synchronous:!!t&&t.synchronous,runWhen:t?t.runWhen:null}),this.handlers.length-1},a.prototype.eject=function(n){this.handlers[n]&&(this.handlers[n]=null)},a.prototype.forEach=function(n){r.forEach(this.handlers,(function(e){null!==e&&n(e)}))},n.exports=a},function(n,e,t){"use strict";var r=t(5),a=t(317),o=t(122),i=t(65);function s(n){n.cancelToken&&n.cancelToken.throwIfRequested()}n.exports=function(n){return s(n),n.headers=n.headers||{},n.data=a.call(n,n.data,n.headers,n.transformRequest),n.headers=r.merge(n.headers.common||{},n.headers[n.method]||{},n.headers),r.forEach(["delete","get","head","post","put","patch","common"],(function(e){delete n.headers[e]})),(n.adapter||i.adapter)(n).then((function(e){return s(n),e.data=a.call(n,e.data,e.headers,n.transformResponse),e}),(function(e){return o(e)||(s(n),e&&e.response&&(e.response.data=a.call(n,e.response.data,e.response.headers,n.transformResponse))),Promise.reject(e)}))}},function(n,e,t){"use strict";var r=t(5),a=t(65);n.exports=function(n,e,t){var o=this||a;return r.forEach(t,(function(t){n=t.call(o,n,e)})),n}},function(n,e,t){"use strict";var r=t(5);n.exports=function(n,e){r.forEach(n,(function(t,r){r!==e&&r.toUpperCase()===e.toUpperCase()&&(n[e]=t,delete n[r])}))}},function(n,e,t){"use strict";var r=t(121);n.exports=function(n,e,t){var a=t.config.validateStatus;t.status&&a&&!a(t.status)?e(r("Request failed with status code "+t.status,t.config,null,t.request,t)):n(t)}},function(n,e,t){"use strict";var r=t(5);n.exports=r.isStandardBrowserEnv()?{write:function(n,e,t,a,o,i){var s=[];s.push(n+"="+encodeURIComponent(e)),r.isNumber(t)&&s.push("expires="+new Date(t).toGMTString()),r.isString(a)&&s.push("path="+a),r.isString(o)&&s.push("domain="+o),!0===i&&s.push("secure"),document.cookie=s.join("; ")},read:function(n){var e=document.cookie.match(new RegExp("(^|;\\s*)("+n+")=([^;]*)"));return e?decodeURIComponent(e[3]):null},remove:function(n){this.write(n,"",Date.now()-864e5)}}:{write:function(){},read:function(){return null},remove:function(){}}},function(n,e,t){"use strict";var r=t(322),a=t(323);n.exports=function(n,e){return n&&!r(e)?a(n,e):e}},function(n,e,t){"use strict";n.exports=function(n){return/^([a-z][a-z\d\+\-\.]*:)?\/\//i.test(n)}},function(n,e,t){"use strict";n.exports=function(n,e){return e?n.replace(/\/+$/,"")+"/"+e.replace(/^\/+/,""):n}},function(n,e,t){"use strict";var r=t(5),a=["age","authorization","content-length","content-type","etag","expires","from","host","if-modified-since","if-unmodified-since","last-modified","location","max-forwards","proxy-authorization","referer","retry-after","user-agent"];n.exports=function(n){var e,t,o,i={};return n?(r.forEach(n.split("\n"),(function(n){if(o=n.indexOf(":"),e=r.trim(n.substr(0,o)).toLowerCase(),t=r.trim(n.substr(o+1)),e){if(i[e]&&a.indexOf(e)>=0)return;i[e]="set-cookie"===e?(i[e]?i[e]:[]).concat([t]):i[e]?i[e]+", "+t:t}})),i):i}},function(n,e,t){"use strict";var r=t(5);n.exports=r.isStandardBrowserEnv()?function(){var n,e=/(msie|trident)/i.test(navigator.userAgent),t=document.createElement("a");function a(n){var r=n;return e&&(t.setAttribute("href",r),r=t.href),t.setAttribute("href",r),{href:t.href,protocol:t.protocol?t.protocol.replace(/:$/,""):"",host:t.host,search:t.search?t.search.replace(/^\?/,""):"",hash:t.hash?t.hash.replace(/^#/,""):"",hostname:t.hostname,port:t.port,pathname:"/"===t.pathname.charAt(0)?t.pathname:"/"+t.pathname}}return n=a(window.location.href),function(e){var t=r.isString(e)?a(e):e;return t.protocol===n.protocol&&t.host===n.host}}():function(){return!0}},function(n,e,t){"use strict";var r=t(327),a={};["object","boolean","number","function","string","symbol"].forEach((function(n,e){a[n]=function(t){return typeof t===n||"a"+(e<1?"n ":" ")+n}}));var o={},i=r.version.split(".");function s(n,e){for(var t=e?e.split("."):i,r=n.split("."),a=0;a<3;a++){if(t[a]>r[a])return!0;if(t[a]<r[a])return!1}return!1}a.transitional=function(n,e,t){var a=e&&s(e);function i(n,e){return"[Axios v"+r.version+"] Transitional option '"+n+"'"+e+(t?". "+t:"")}return function(t,r,s){if(!1===n)throw new Error(i(r," has been removed in "+e));return a&&!o[r]&&(o[r]=!0,console.warn(i(r," has been deprecated since v"+e+" and will be removed in the near future"))),!n||n(t,r,s)}},n.exports={isOlderVersion:s,assertOptions:function(n,e,t){if("object"!=typeof n)throw new TypeError("options must be an object");for(var r=Object.keys(n),a=r.length;a-- >0;){var o=r[a],i=e[o];if(i){var s=n[o],l=void 0===s||i(s,o,n);if(!0!==l)throw new TypeError("option "+o+" must be "+l)}else if(!0!==t)throw Error("Unknown option "+o)}},validators:a}},function(n){n.exports=JSON.parse('{"_args":[["axios@0.21.4","E:\\\\BigData\\\\blog"]],"_development":true,"_from":"axios@0.21.4","_id":"axios@0.21.4","_inBundle":false,"_integrity":"sha512-ut5vewkiu8jjGBdqpM44XxjuCjq9LAKeHVmoVfHVzy8eHgxxq8SbAVQNovDA8mVi05kP0Ea/n/UzcSHcTJQfNg==","_location":"/axios","_phantomChildren":{},"_requested":{"type":"version","registry":true,"raw":"axios@0.21.4","name":"axios","escapedName":"axios","rawSpec":"0.21.4","saveSpec":null,"fetchSpec":"0.21.4"},"_requiredBy":["/@vssue/api-bitbucket-v2","/@vssue/api-gitee-v5","/@vssue/api-github-v3","/@vssue/api-github-v4","/@vssue/api-gitlab-v4"],"_resolved":"https://registry.npmmirror.com/axios/-/axios-0.21.4.tgz","_spec":"0.21.4","_where":"E:\\\\BigData\\\\blog","author":{"name":"Matt Zabriskie"},"browser":{"./lib/adapters/http.js":"./lib/adapters/xhr.js"},"bugs":{"url":"https://github.com/axios/axios/issues"},"bundlesize":[{"path":"./dist/axios.min.js","threshold":"5kB"}],"dependencies":{"follow-redirects":"^1.14.0"},"description":"Promise based HTTP client for the browser and node.js","devDependencies":{"coveralls":"^3.0.0","es6-promise":"^4.2.4","grunt":"^1.3.0","grunt-banner":"^0.6.0","grunt-cli":"^1.2.0","grunt-contrib-clean":"^1.1.0","grunt-contrib-watch":"^1.0.0","grunt-eslint":"^23.0.0","grunt-karma":"^4.0.0","grunt-mocha-test":"^0.13.3","grunt-ts":"^6.0.0-beta.19","grunt-webpack":"^4.0.2","istanbul-instrumenter-loader":"^1.0.0","jasmine-core":"^2.4.1","karma":"^6.3.2","karma-chrome-launcher":"^3.1.0","karma-firefox-launcher":"^2.1.0","karma-jasmine":"^1.1.1","karma-jasmine-ajax":"^0.1.13","karma-safari-launcher":"^1.0.0","karma-sauce-launcher":"^4.3.6","karma-sinon":"^1.0.5","karma-sourcemap-loader":"^0.3.8","karma-webpack":"^4.0.2","load-grunt-tasks":"^3.5.2","minimist":"^1.2.0","mocha":"^8.2.1","sinon":"^4.5.0","terser-webpack-plugin":"^4.2.3","typescript":"^4.0.5","url-search-params":"^0.10.0","webpack":"^4.44.2","webpack-dev-server":"^3.11.0"},"homepage":"https://axios-http.com","jsdelivr":"dist/axios.min.js","keywords":["xhr","http","ajax","promise","node"],"license":"MIT","main":"index.js","name":"axios","repository":{"type":"git","url":"git+https://github.com/axios/axios.git"},"scripts":{"build":"NODE_ENV=production grunt build","coveralls":"cat coverage/lcov.info | ./node_modules/coveralls/bin/coveralls.js","examples":"node ./examples/server.js","fix":"eslint --fix lib/**/*.js","postversion":"git push && git push --tags","preversion":"npm test","start":"node ./sandbox/server.js","test":"grunt test","version":"npm run build && grunt version && git add -A dist && git add CHANGELOG.md bower.json package.json"},"typings":"./index.d.ts","unpkg":"dist/axios.min.js","version":"0.21.4"}')},function(n,e,t){"use strict";var r=t(124);function a(n){if("function"!=typeof n)throw new TypeError("executor must be a function.");var e;this.promise=new Promise((function(n){e=n}));var t=this;n((function(n){t.reason||(t.reason=new r(n),e(t.reason))}))}a.prototype.throwIfRequested=function(){if(this.reason)throw this.reason},a.source=function(){var n;return{token:new a((function(e){n=e})),cancel:n}},n.exports=a},function(n,e,t){"use strict";n.exports=function(n){return function(e){return n.apply(null,e)}}},function(n,e,t){"use strict";n.exports=function(n){return"object"==typeof n&&!0===n.isAxiosError}},function(n,e,t){},function(n,e,t){"use strict";t(125)},function(n,e,t){},function(n,e,t){var r;function a(n){function t(){if(t.enabled){var n=t,a=+new Date,o=a-(r||a);n.diff=o,n.prev=r,n.curr=a,r=a;for(var i=new Array(arguments.length),s=0;s<i.length;s++)i[s]=arguments[s];i[0]=e.coerce(i[0]),"string"!=typeof i[0]&&i.unshift("%O");var l=0;i[0]=i[0].replace(/%([a-zA-Z%])/g,(function(t,r){if("%%"===t)return t;l++;var a=e.formatters[r];if("function"==typeof a){var o=i[l];t=a.call(n,o),i.splice(l,1),l--}return t})),e.formatArgs.call(n,i);var c=t.log||e.log||console.log.bind(console);c.apply(n,i)}}return t.namespace=n,t.enabled=e.enabled(n),t.useColors=e.useColors(),t.color=function(n){var t,r=0;for(t in n)r=(r<<5)-r+n.charCodeAt(t),r|=0;return e.colors[Math.abs(r)%e.colors.length]}(n),"function"==typeof e.init&&e.init(t),t}(e=n.exports=a.debug=a.default=a).coerce=function(n){return n instanceof Error?n.stack||n.message:n},e.disable=function(){e.enable("")},e.enable=function(n){e.save(n),e.names=[],e.skips=[];for(var t=("string"==typeof n?n:"").split(/[\s,]+/),r=t.length,a=0;a<r;a++)t[a]&&("-"===(n=t[a].replace(/\*/g,".*?"))[0]?e.skips.push(new RegExp("^"+n.substr(1)+"$")):e.names.push(new RegExp("^"+n+"$")))},e.enabled=function(n){var t,r;for(t=0,r=e.skips.length;t<r;t++)if(e.skips[t].test(n))return!1;for(t=0,r=e.names.length;t<r;t++)if(e.names[t].test(n))return!0;return!1},e.humanize=t(335),e.names=[],e.skips=[],e.formatters={}},function(n,e){var t=1e3,r=6e4,a=60*r,o=24*a;function i(n,e,t){if(!(n<e))return n<1.5*e?Math.floor(n/e)+" "+t:Math.ceil(n/e)+" "+t+"s"}n.exports=function(n,e){e=e||{};var s,l=typeof n;if("string"===l&&n.length>0)return function(n){if((n=String(n)).length>100)return;var e=/^((?:\d+)?\.?\d+) *(milliseconds?|msecs?|ms|seconds?|secs?|s|minutes?|mins?|m|hours?|hrs?|h|days?|d|years?|yrs?|y)?$/i.exec(n);if(!e)return;var i=parseFloat(e[1]);switch((e[2]||"ms").toLowerCase()){case"years":case"year":case"yrs":case"yr":case"y":return 315576e5*i;case"days":case"day":case"d":return i*o;case"hours":case"hour":case"hrs":case"hr":case"h":return i*a;case"minutes":case"minute":case"mins":case"min":case"m":return i*r;case"seconds":case"second":case"secs":case"sec":case"s":return i*t;case"milliseconds":case"millisecond":case"msecs":case"msec":case"ms":return i;default:return}}(n);if("number"===l&&!1===isNaN(n))return e.long?i(s=n,o,"day")||i(s,a,"hour")||i(s,r,"minute")||i(s,t,"second")||s+" ms":function(n){if(n>=o)return Math.round(n/o)+"d";if(n>=a)return Math.round(n/a)+"h";if(n>=r)return Math.round(n/r)+"m";if(n>=t)return Math.round(n/t)+"s";return n+"ms"}(n);throw new Error("val is not a non-empty string or a valid number. val="+JSON.stringify(n))}},function(n,e,t){},function(n,e,t){"use strict";t(127)},function(n,e,t){"use strict";t(128)},function(n,e,t){"use strict";t.r(e);var r=t(1);
/*!
  * vue-router v3.6.5
  * (c) 2022 Evan You
  * @license MIT
  */function a(n,e){for(var t in e)n[t]=e[t];return n}var o=/[!'()*]/g,i=function(n){return"%"+n.charCodeAt(0).toString(16)},s=/%2C/g,l=function(n){return encodeURIComponent(n).replace(o,i).replace(s,",")};function c(n){try{return decodeURIComponent(n)}catch(n){0}return n}var p=function(n){return null==n||"object"==typeof n?n:String(n)};function u(n){var e={};return(n=n.trim().replace(/^(\?|#|&)/,""))?(n.split("&").forEach((function(n){var t=n.replace(/\+/g," ").split("="),r=c(t.shift()),a=t.length>0?c(t.join("=")):null;void 0===e[r]?e[r]=a:Array.isArray(e[r])?e[r].push(a):e[r]=[e[r],a]})),e):e}function d(n){var e=n?Object.keys(n).map((function(e){var t=n[e];if(void 0===t)return"";if(null===t)return l(e);if(Array.isArray(t)){var r=[];return t.forEach((function(n){void 0!==n&&(null===n?r.push(l(e)):r.push(l(e)+"="+l(n)))})),r.join("&")}return l(e)+"="+l(t)})).filter((function(n){return n.length>0})).join("&"):null;return e?"?"+e:""}var m=/\/?$/;function g(n,e,t,r){var a=r&&r.options.stringifyQuery,o=e.query||{};try{o=f(o)}catch(n){}var i={name:e.name||n&&n.name,meta:n&&n.meta||{},path:e.path||"/",hash:e.hash||"",query:o,params:e.params||{},fullPath:b(e,a),matched:n?v(n):[]};return t&&(i.redirectedFrom=b(t,a)),Object.freeze(i)}function f(n){if(Array.isArray(n))return n.map(f);if(n&&"object"==typeof n){var e={};for(var t in n)e[t]=f(n[t]);return e}return n}var h=g(null,{path:"/"});function v(n){for(var e=[];n;)e.unshift(n),n=n.parent;return e}function b(n,e){var t=n.path,r=n.query;void 0===r&&(r={});var a=n.hash;return void 0===a&&(a=""),(t||"/")+(e||d)(r)+a}function k(n,e,t){return e===h?n===e:!!e&&(n.path&&e.path?n.path.replace(m,"")===e.path.replace(m,"")&&(t||n.hash===e.hash&&y(n.query,e.query)):!(!n.name||!e.name)&&(n.name===e.name&&(t||n.hash===e.hash&&y(n.query,e.query)&&y(n.params,e.params))))}function y(n,e){if(void 0===n&&(n={}),void 0===e&&(e={}),!n||!e)return n===e;var t=Object.keys(n).sort(),r=Object.keys(e).sort();return t.length===r.length&&t.every((function(t,a){var o=n[t];if(r[a]!==t)return!1;var i=e[t];return null==o||null==i?o===i:"object"==typeof o&&"object"==typeof i?y(o,i):String(o)===String(i)}))}function S(n){for(var e=0;e<n.matched.length;e++){var t=n.matched[e];for(var r in t.instances){var a=t.instances[r],o=t.enteredCbs[r];if(a&&o){delete t.enteredCbs[r];for(var i=0;i<o.length;i++)a._isBeingDestroyed||o[i](a)}}}}var x={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(n,e){var t=e.props,r=e.children,o=e.parent,i=e.data;i.routerView=!0;for(var s=o.$createElement,l=t.name,c=o.$route,p=o._routerViewCache||(o._routerViewCache={}),u=0,d=!1;o&&o._routerRoot!==o;){var m=o.$vnode?o.$vnode.data:{};m.routerView&&u++,m.keepAlive&&o._directInactive&&o._inactive&&(d=!0),o=o.$parent}if(i.routerViewDepth=u,d){var g=p[l],f=g&&g.component;return f?(g.configProps&&w(f,i,g.route,g.configProps),s(f,i,r)):s()}var h=c.matched[u],v=h&&h.components[l];if(!h||!v)return p[l]=null,s();p[l]={component:v},i.registerRouteInstance=function(n,e){var t=h.instances[l];(e&&t!==n||!e&&t===n)&&(h.instances[l]=e)},(i.hook||(i.hook={})).prepatch=function(n,e){h.instances[l]=e.componentInstance},i.hook.init=function(n){n.data.keepAlive&&n.componentInstance&&n.componentInstance!==h.instances[l]&&(h.instances[l]=n.componentInstance),S(c)};var b=h.props&&h.props[l];return b&&(a(p[l],{route:c,configProps:b}),w(v,i,c,b)),s(v,i,r)}};function w(n,e,t,r){var o=e.props=function(n,e){switch(typeof e){case"undefined":return;case"object":return e;case"function":return e(n);case"boolean":return e?n.params:void 0;default:0}}(t,r);if(o){o=e.props=a({},o);var i=e.attrs=e.attrs||{};for(var s in o)n.props&&s in n.props||(i[s]=o[s],delete o[s])}}function E(n,e,t){var r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;var a=e.split("/");t&&a[a.length-1]||a.pop();for(var o=n.replace(/^\//,"").split("/"),i=0;i<o.length;i++){var s=o[i];".."===s?a.pop():"."!==s&&a.push(s)}return""!==a[0]&&a.unshift(""),a.join("/")}function D(n){return n.replace(/\/(?:\s*\/)+/g,"/")}var C=Array.isArray||function(n){return"[object Array]"==Object.prototype.toString.call(n)},I=U,T=P,O=function(n,e){return B(P(n,e),e)},A=B,_=$,R=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function P(n,e){for(var t,r=[],a=0,o=0,i="",s=e&&e.delimiter||"/";null!=(t=R.exec(n));){var l=t[0],c=t[1],p=t.index;if(i+=n.slice(o,p),o=p+l.length,c)i+=c[1];else{var u=n[o],d=t[2],m=t[3],g=t[4],f=t[5],h=t[6],v=t[7];i&&(r.push(i),i="");var b=null!=d&&null!=u&&u!==d,k="+"===h||"*"===h,y="?"===h||"*"===h,S=t[2]||s,x=g||f;r.push({name:m||a++,prefix:d||"",delimiter:S,optional:y,repeat:k,partial:b,asterisk:!!v,pattern:x?j(x):v?".*":"[^"+M(S)+"]+?"})}}return o<n.length&&(i+=n.substr(o)),i&&r.push(i),r}function F(n){return encodeURI(n).replace(/[\/?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()}))}function B(n,e){for(var t=new Array(n.length),r=0;r<n.length;r++)"object"==typeof n[r]&&(t[r]=new RegExp("^(?:"+n[r].pattern+")$",N(e)));return function(e,r){for(var a="",o=e||{},i=(r||{}).pretty?F:encodeURIComponent,s=0;s<n.length;s++){var l=n[s];if("string"!=typeof l){var c,p=o[l.name];if(null==p){if(l.optional){l.partial&&(a+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(C(p)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(p)+"`");if(0===p.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var u=0;u<p.length;u++){if(c=i(p[u]),!t[s].test(c))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(c)+"`");a+=(0===u?l.prefix:l.delimiter)+c}}else{if(c=l.asterisk?encodeURI(p).replace(/[?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()})):i(p),!t[s].test(c))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+c+'"');a+=l.prefix+c}}else a+=l}return a}}function M(n){return n.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function j(n){return n.replace(/([=!:$\/()])/g,"\\$1")}function L(n,e){return n.keys=e,n}function N(n){return n&&n.sensitive?"":"i"}function $(n,e,t){C(e)||(t=e||t,e=[]);for(var r=(t=t||{}).strict,a=!1!==t.end,o="",i=0;i<n.length;i++){var s=n[i];if("string"==typeof s)o+=M(s);else{var l=M(s.prefix),c="(?:"+s.pattern+")";e.push(s),s.repeat&&(c+="(?:"+l+c+")*"),o+=c=s.optional?s.partial?l+"("+c+")?":"(?:"+l+"("+c+"))?":l+"("+c+")"}}var p=M(t.delimiter||"/"),u=o.slice(-p.length)===p;return r||(o=(u?o.slice(0,-p.length):o)+"(?:"+p+"(?=$))?"),o+=a?"$":r&&u?"":"(?="+p+"|$)",L(new RegExp("^"+o,N(t)),e)}function U(n,e,t){return C(e)||(t=e||t,e=[]),t=t||{},n instanceof RegExp?function(n,e){var t=n.source.match(/\((?!\?)/g);if(t)for(var r=0;r<t.length;r++)e.push({name:r,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return L(n,e)}(n,e):C(n)?function(n,e,t){for(var r=[],a=0;a<n.length;a++)r.push(U(n[a],e,t).source);return L(new RegExp("(?:"+r.join("|")+")",N(t)),e)}(n,e,t):function(n,e,t){return $(P(n,t),e,t)}(n,e,t)}I.parse=T,I.compile=O,I.tokensToFunction=A,I.tokensToRegExp=_;var z=Object.create(null);function H(n,e,t){e=e||{};try{var r=z[n]||(z[n]=I.compile(n));return"string"==typeof e.pathMatch&&(e[0]=e.pathMatch),r(e,{pretty:!0})}catch(n){return""}finally{delete e[0]}}function q(n,e,t,r){var o="string"==typeof n?{path:n}:n;if(o._normalized)return o;if(o.name){var i=(o=a({},n)).params;return i&&"object"==typeof i&&(o.params=a({},i)),o}if(!o.path&&o.params&&e){(o=a({},o))._normalized=!0;var s=a(a({},e.params),o.params);if(e.name)o.name=e.name,o.params=s;else if(e.matched.length){var l=e.matched[e.matched.length-1].path;o.path=H(l,s,e.path)}else 0;return o}var c=function(n){var e="",t="",r=n.indexOf("#");r>=0&&(e=n.slice(r),n=n.slice(0,r));var a=n.indexOf("?");return a>=0&&(t=n.slice(a+1),n=n.slice(0,a)),{path:n,query:t,hash:e}}(o.path||""),d=e&&e.path||"/",m=c.path?E(c.path,d,t||o.append):d,g=function(n,e,t){void 0===e&&(e={});var r,a=t||u;try{r=a(n||"")}catch(n){r={}}for(var o in e){var i=e[o];r[o]=Array.isArray(i)?i.map(p):p(i)}return r}(c.query,o.query,r&&r.options.parseQuery),f=o.hash||c.hash;return f&&"#"!==f.charAt(0)&&(f="#"+f),{_normalized:!0,path:m,query:g,hash:f}}var V,K=function(){},W={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(n){var e=this,t=this.$router,r=this.$route,o=t.resolve(this.to,r,this.append),i=o.location,s=o.route,l=o.href,c={},p=t.options.linkActiveClass,u=t.options.linkExactActiveClass,d=null==p?"router-link-active":p,f=null==u?"router-link-exact-active":u,h=null==this.activeClass?d:this.activeClass,v=null==this.exactActiveClass?f:this.exactActiveClass,b=s.redirectedFrom?g(null,q(s.redirectedFrom),null,t):s;c[v]=k(r,b,this.exactPath),c[h]=this.exact||this.exactPath?c[v]:function(n,e){return 0===n.path.replace(m,"/").indexOf(e.path.replace(m,"/"))&&(!e.hash||n.hash===e.hash)&&function(n,e){for(var t in e)if(!(t in n))return!1;return!0}(n.query,e.query)}(r,b);var y=c[v]?this.ariaCurrentValue:null,S=function(n){G(n)&&(e.replace?t.replace(i,K):t.push(i,K))},x={click:G};Array.isArray(this.event)?this.event.forEach((function(n){x[n]=S})):x[this.event]=S;var w={class:c},E=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:l,route:s,navigate:S,isActive:c[h],isExactActive:c[v]});if(E){if(1===E.length)return E[0];if(E.length>1||!E.length)return 0===E.length?n():n("span",{},E)}if("a"===this.tag)w.on=x,w.attrs={href:l,"aria-current":y};else{var D=function n(e){var t;if(e)for(var r=0;r<e.length;r++){if("a"===(t=e[r]).tag)return t;if(t.children&&(t=n(t.children)))return t}}(this.$slots.default);if(D){D.isStatic=!1;var C=D.data=a({},D.data);for(var I in C.on=C.on||{},C.on){var T=C.on[I];I in x&&(C.on[I]=Array.isArray(T)?T:[T])}for(var O in x)O in C.on?C.on[O].push(x[O]):C.on[O]=S;var A=D.data.attrs=a({},D.data.attrs);A.href=l,A["aria-current"]=y}else w.on=x}return n(this.tag,w,this.$slots.default)}};function G(n){if(!(n.metaKey||n.altKey||n.ctrlKey||n.shiftKey||n.defaultPrevented||void 0!==n.button&&0!==n.button)){if(n.currentTarget&&n.currentTarget.getAttribute){var e=n.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(e))return}return n.preventDefault&&n.preventDefault(),!0}}var J="undefined"!=typeof window;function Y(n,e,t,r,a){var o=e||[],i=t||Object.create(null),s=r||Object.create(null);n.forEach((function(n){!function n(e,t,r,a,o,i){var s=a.path,l=a.name;0;var c=a.pathToRegexpOptions||{},p=function(n,e,t){t||(n=n.replace(/\/$/,""));if("/"===n[0])return n;if(null==e)return n;return D(e.path+"/"+n)}(s,o,c.strict);"boolean"==typeof a.caseSensitive&&(c.sensitive=a.caseSensitive);var u={path:p,regex:Q(p,c),components:a.components||{default:a.component},alias:a.alias?"string"==typeof a.alias?[a.alias]:a.alias:[],instances:{},enteredCbs:{},name:l,parent:o,matchAs:i,redirect:a.redirect,beforeEnter:a.beforeEnter,meta:a.meta||{},props:null==a.props?{}:a.components?a.props:{default:a.props}};a.children&&a.children.forEach((function(a){var o=i?D(i+"/"+a.path):void 0;n(e,t,r,a,u,o)}));t[u.path]||(e.push(u.path),t[u.path]=u);if(void 0!==a.alias)for(var d=Array.isArray(a.alias)?a.alias:[a.alias],m=0;m<d.length;++m){0;var g={path:d[m],children:a.children};n(e,t,r,g,o,u.path||"/")}l&&(r[l]||(r[l]=u))}(o,i,s,n,a)}));for(var l=0,c=o.length;l<c;l++)"*"===o[l]&&(o.push(o.splice(l,1)[0]),c--,l--);return{pathList:o,pathMap:i,nameMap:s}}function Q(n,e){return I(n,[],e)}function X(n,e){var t=Y(n),r=t.pathList,a=t.pathMap,o=t.nameMap;function i(n,t,i){var s=q(n,t,!1,e),c=s.name;if(c){var p=o[c];if(!p)return l(null,s);var u=p.regex.keys.filter((function(n){return!n.optional})).map((function(n){return n.name}));if("object"!=typeof s.params&&(s.params={}),t&&"object"==typeof t.params)for(var d in t.params)!(d in s.params)&&u.indexOf(d)>-1&&(s.params[d]=t.params[d]);return s.path=H(p.path,s.params),l(p,s,i)}if(s.path){s.params={};for(var m=0;m<r.length;m++){var g=r[m],f=a[g];if(Z(f.regex,s.path,s.params))return l(f,s,i)}}return l(null,s)}function s(n,t){var r=n.redirect,a="function"==typeof r?r(g(n,t,null,e)):r;if("string"==typeof a&&(a={path:a}),!a||"object"!=typeof a)return l(null,t);var s=a,c=s.name,p=s.path,u=t.query,d=t.hash,m=t.params;if(u=s.hasOwnProperty("query")?s.query:u,d=s.hasOwnProperty("hash")?s.hash:d,m=s.hasOwnProperty("params")?s.params:m,c){o[c];return i({_normalized:!0,name:c,query:u,hash:d,params:m},void 0,t)}if(p){var f=function(n,e){return E(n,e.parent?e.parent.path:"/",!0)}(p,n);return i({_normalized:!0,path:H(f,m),query:u,hash:d},void 0,t)}return l(null,t)}function l(n,t,r){return n&&n.redirect?s(n,r||t):n&&n.matchAs?function(n,e,t){var r=i({_normalized:!0,path:H(t,e.params)});if(r){var a=r.matched,o=a[a.length-1];return e.params=r.params,l(o,e)}return l(null,e)}(0,t,n.matchAs):g(n,t,r,e)}return{match:i,addRoute:function(n,e){var t="object"!=typeof n?o[n]:void 0;Y([e||n],r,a,o,t),t&&t.alias.length&&Y(t.alias.map((function(n){return{path:n,children:[e]}})),r,a,o,t)},getRoutes:function(){return r.map((function(n){return a[n]}))},addRoutes:function(n){Y(n,r,a,o)}}}function Z(n,e,t){var r=e.match(n);if(!r)return!1;if(!t)return!0;for(var a=1,o=r.length;a<o;++a){var i=n.keys[a-1];i&&(t[i.name||"pathMatch"]="string"==typeof r[a]?c(r[a]):r[a])}return!0}var nn=J&&window.performance&&window.performance.now?window.performance:Date;function en(){return nn.now().toFixed(3)}var tn=en();function rn(){return tn}function an(n){return tn=n}var on=Object.create(null);function sn(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var n=window.location.protocol+"//"+window.location.host,e=window.location.href.replace(n,""),t=a({},window.history.state);return t.key=rn(),window.history.replaceState(t,"",e),window.addEventListener("popstate",pn),function(){window.removeEventListener("popstate",pn)}}function ln(n,e,t,r){if(n.app){var a=n.options.scrollBehavior;a&&n.app.$nextTick((function(){var o=function(){var n=rn();if(n)return on[n]}(),i=a.call(n,e,t,r?o:null);i&&("function"==typeof i.then?i.then((function(n){fn(n,o)})).catch((function(n){0})):fn(i,o))}))}}function cn(){var n=rn();n&&(on[n]={x:window.pageXOffset,y:window.pageYOffset})}function pn(n){cn(),n.state&&n.state.key&&an(n.state.key)}function un(n){return mn(n.x)||mn(n.y)}function dn(n){return{x:mn(n.x)?n.x:window.pageXOffset,y:mn(n.y)?n.y:window.pageYOffset}}function mn(n){return"number"==typeof n}var gn=/^#\d/;function fn(n,e){var t,r="object"==typeof n;if(r&&"string"==typeof n.selector){var a=gn.test(n.selector)?document.getElementById(n.selector.slice(1)):document.querySelector(n.selector);if(a){var o=n.offset&&"object"==typeof n.offset?n.offset:{};e=function(n,e){var t=document.documentElement.getBoundingClientRect(),r=n.getBoundingClientRect();return{x:r.left-t.left-e.x,y:r.top-t.top-e.y}}(a,o={x:mn((t=o).x)?t.x:0,y:mn(t.y)?t.y:0})}else un(n)&&(e=dn(n))}else r&&un(n)&&(e=dn(n));e&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:e.x,top:e.y,behavior:n.behavior}):window.scrollTo(e.x,e.y))}var hn,vn=J&&((-1===(hn=window.navigator.userAgent).indexOf("Android 2.")&&-1===hn.indexOf("Android 4.0")||-1===hn.indexOf("Mobile Safari")||-1!==hn.indexOf("Chrome")||-1!==hn.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function bn(n,e){cn();var t=window.history;try{if(e){var r=a({},t.state);r.key=rn(),t.replaceState(r,"",n)}else t.pushState({key:an(en())},"",n)}catch(t){window.location[e?"replace":"assign"](n)}}function kn(n){bn(n,!0)}var yn={redirected:2,aborted:4,cancelled:8,duplicated:16};function Sn(n,e){return wn(n,e,yn.redirected,'Redirected when going from "'+n.fullPath+'" to "'+function(n){if("string"==typeof n)return n;if("path"in n)return n.path;var e={};return En.forEach((function(t){t in n&&(e[t]=n[t])})),JSON.stringify(e,null,2)}(e)+'" via a navigation guard.')}function xn(n,e){return wn(n,e,yn.cancelled,'Navigation cancelled from "'+n.fullPath+'" to "'+e.fullPath+'" with a new navigation.')}function wn(n,e,t,r){var a=new Error(r);return a._isRouter=!0,a.from=n,a.to=e,a.type=t,a}var En=["params","query","hash"];function Dn(n){return Object.prototype.toString.call(n).indexOf("Error")>-1}function Cn(n,e){return Dn(n)&&n._isRouter&&(null==e||n.type===e)}function In(n,e,t){var r=function(a){a>=n.length?t():n[a]?e(n[a],(function(){r(a+1)})):r(a+1)};r(0)}function Tn(n){return function(e,t,r){var a=!1,o=0,i=null;On(n,(function(n,e,t,s){if("function"==typeof n&&void 0===n.cid){a=!0,o++;var l,c=Rn((function(e){var a;((a=e).__esModule||_n&&"Module"===a[Symbol.toStringTag])&&(e=e.default),n.resolved="function"==typeof e?e:V.extend(e),t.components[s]=e,--o<=0&&r()})),p=Rn((function(n){var e="Failed to resolve async component "+s+": "+n;i||(i=Dn(n)?n:new Error(e),r(i))}));try{l=n(c,p)}catch(n){p(n)}if(l)if("function"==typeof l.then)l.then(c,p);else{var u=l.component;u&&"function"==typeof u.then&&u.then(c,p)}}})),a||r()}}function On(n,e){return An(n.map((function(n){return Object.keys(n.components).map((function(t){return e(n.components[t],n.instances[t],n,t)}))})))}function An(n){return Array.prototype.concat.apply([],n)}var _n="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function Rn(n){var e=!1;return function(){for(var t=[],r=arguments.length;r--;)t[r]=arguments[r];if(!e)return e=!0,n.apply(this,t)}}var Pn=function(n,e){this.router=n,this.base=function(n){if(!n)if(J){var e=document.querySelector("base");n=(n=e&&e.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else n="/";"/"!==n.charAt(0)&&(n="/"+n);return n.replace(/\/$/,"")}(e),this.current=h,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function Fn(n,e,t,r){var a=On(n,(function(n,r,a,o){var i=function(n,e){"function"!=typeof n&&(n=V.extend(n));return n.options[e]}(n,e);if(i)return Array.isArray(i)?i.map((function(n){return t(n,r,a,o)})):t(i,r,a,o)}));return An(r?a.reverse():a)}function Bn(n,e){if(e)return function(){return n.apply(e,arguments)}}Pn.prototype.listen=function(n){this.cb=n},Pn.prototype.onReady=function(n,e){this.ready?n():(this.readyCbs.push(n),e&&this.readyErrorCbs.push(e))},Pn.prototype.onError=function(n){this.errorCbs.push(n)},Pn.prototype.transitionTo=function(n,e,t){var r,a=this;try{r=this.router.match(n,this.current)}catch(n){throw this.errorCbs.forEach((function(e){e(n)})),n}var o=this.current;this.confirmTransition(r,(function(){a.updateRoute(r),e&&e(r),a.ensureURL(),a.router.afterHooks.forEach((function(n){n&&n(r,o)})),a.ready||(a.ready=!0,a.readyCbs.forEach((function(n){n(r)})))}),(function(n){t&&t(n),n&&!a.ready&&(Cn(n,yn.redirected)&&o===h||(a.ready=!0,a.readyErrorCbs.forEach((function(e){e(n)}))))}))},Pn.prototype.confirmTransition=function(n,e,t){var r=this,a=this.current;this.pending=n;var o=function(n){!Cn(n)&&Dn(n)&&(r.errorCbs.length?r.errorCbs.forEach((function(e){e(n)})):console.error(n)),t&&t(n)},i=n.matched.length-1,s=a.matched.length-1;if(k(n,a)&&i===s&&n.matched[i]===a.matched[s])return this.ensureURL(),n.hash&&ln(this.router,a,n,!1),o(function(n,e){var t=wn(n,e,yn.duplicated,'Avoided redundant navigation to current location: "'+n.fullPath+'".');return t.name="NavigationDuplicated",t}(a,n));var l=function(n,e){var t,r=Math.max(n.length,e.length);for(t=0;t<r&&n[t]===e[t];t++);return{updated:e.slice(0,t),activated:e.slice(t),deactivated:n.slice(t)}}(this.current.matched,n.matched),c=l.updated,p=l.deactivated,u=l.activated,d=[].concat(function(n){return Fn(n,"beforeRouteLeave",Bn,!0)}(p),this.router.beforeHooks,function(n){return Fn(n,"beforeRouteUpdate",Bn)}(c),u.map((function(n){return n.beforeEnter})),Tn(u)),m=function(e,t){if(r.pending!==n)return o(xn(a,n));try{e(n,a,(function(e){!1===e?(r.ensureURL(!0),o(function(n,e){return wn(n,e,yn.aborted,'Navigation aborted from "'+n.fullPath+'" to "'+e.fullPath+'" via a navigation guard.')}(a,n))):Dn(e)?(r.ensureURL(!0),o(e)):"string"==typeof e||"object"==typeof e&&("string"==typeof e.path||"string"==typeof e.name)?(o(Sn(a,n)),"object"==typeof e&&e.replace?r.replace(e):r.push(e)):t(e)}))}catch(n){o(n)}};In(d,m,(function(){In(function(n){return Fn(n,"beforeRouteEnter",(function(n,e,t,r){return function(n,e,t){return function(r,a,o){return n(r,a,(function(n){"function"==typeof n&&(e.enteredCbs[t]||(e.enteredCbs[t]=[]),e.enteredCbs[t].push(n)),o(n)}))}}(n,t,r)}))}(u).concat(r.router.resolveHooks),m,(function(){if(r.pending!==n)return o(xn(a,n));r.pending=null,e(n),r.router.app&&r.router.app.$nextTick((function(){S(n)}))}))}))},Pn.prototype.updateRoute=function(n){this.current=n,this.cb&&this.cb(n)},Pn.prototype.setupListeners=function(){},Pn.prototype.teardown=function(){this.listeners.forEach((function(n){n()})),this.listeners=[],this.current=h,this.pending=null};var Mn=function(n){function e(e,t){n.call(this,e,t),this._startLocation=jn(this.base)}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router,t=e.options.scrollBehavior,r=vn&&t;r&&this.listeners.push(sn());var a=function(){var t=n.current,a=jn(n.base);n.current===h&&a===n._startLocation||n.transitionTo(a,(function(n){r&&ln(e,n,t,!0)}))};window.addEventListener("popstate",a),this.listeners.push((function(){window.removeEventListener("popstate",a)}))}},e.prototype.go=function(n){window.history.go(n)},e.prototype.push=function(n,e,t){var r=this,a=this.current;this.transitionTo(n,(function(n){bn(D(r.base+n.fullPath)),ln(r.router,n,a,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,a=this.current;this.transitionTo(n,(function(n){kn(D(r.base+n.fullPath)),ln(r.router,n,a,!1),e&&e(n)}),t)},e.prototype.ensureURL=function(n){if(jn(this.base)!==this.current.fullPath){var e=D(this.base+this.current.fullPath);n?bn(e):kn(e)}},e.prototype.getCurrentLocation=function(){return jn(this.base)},e}(Pn);function jn(n){var e=window.location.pathname,t=e.toLowerCase(),r=n.toLowerCase();return!n||t!==r&&0!==t.indexOf(D(r+"/"))||(e=e.slice(n.length)),(e||"/")+window.location.search+window.location.hash}var Ln=function(n){function e(e,t,r){n.call(this,e,t),r&&function(n){var e=jn(n);if(!/^\/#/.test(e))return window.location.replace(D(n+"/#"+e)),!0}(this.base)||Nn()}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router.options.scrollBehavior,t=vn&&e;t&&this.listeners.push(sn());var r=function(){var e=n.current;Nn()&&n.transitionTo($n(),(function(r){t&&ln(n.router,r,e,!0),vn||Hn(r.fullPath)}))},a=vn?"popstate":"hashchange";window.addEventListener(a,r),this.listeners.push((function(){window.removeEventListener(a,r)}))}},e.prototype.push=function(n,e,t){var r=this,a=this.current;this.transitionTo(n,(function(n){zn(n.fullPath),ln(r.router,n,a,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,a=this.current;this.transitionTo(n,(function(n){Hn(n.fullPath),ln(r.router,n,a,!1),e&&e(n)}),t)},e.prototype.go=function(n){window.history.go(n)},e.prototype.ensureURL=function(n){var e=this.current.fullPath;$n()!==e&&(n?zn(e):Hn(e))},e.prototype.getCurrentLocation=function(){return $n()},e}(Pn);function Nn(){var n=$n();return"/"===n.charAt(0)||(Hn("/"+n),!1)}function $n(){var n=window.location.href,e=n.indexOf("#");return e<0?"":n=n.slice(e+1)}function Un(n){var e=window.location.href,t=e.indexOf("#");return(t>=0?e.slice(0,t):e)+"#"+n}function zn(n){vn?bn(Un(n)):window.location.hash=n}function Hn(n){vn?kn(Un(n)):window.location.replace(Un(n))}var qn=function(n){function e(e,t){n.call(this,e,t),this.stack=[],this.index=-1}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.push=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index+1).concat(n),r.index++,e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index).concat(n),e&&e(n)}),t)},e.prototype.go=function(n){var e=this,t=this.index+n;if(!(t<0||t>=this.stack.length)){var r=this.stack[t];this.confirmTransition(r,(function(){var n=e.current;e.index=t,e.updateRoute(r),e.router.afterHooks.forEach((function(e){e&&e(r,n)}))}),(function(n){Cn(n,yn.duplicated)&&(e.index=t)}))}},e.prototype.getCurrentLocation=function(){var n=this.stack[this.stack.length-1];return n?n.fullPath:"/"},e.prototype.ensureURL=function(){},e}(Pn),Vn=function(n){void 0===n&&(n={}),this.app=null,this.apps=[],this.options=n,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=X(n.routes||[],this);var e=n.mode||"hash";switch(this.fallback="history"===e&&!vn&&!1!==n.fallback,this.fallback&&(e="hash"),J||(e="abstract"),this.mode=e,e){case"history":this.history=new Mn(this,n.base);break;case"hash":this.history=new Ln(this,n.base,this.fallback);break;case"abstract":this.history=new qn(this,n.base);break;default:0}},Kn={currentRoute:{configurable:!0}};Vn.prototype.match=function(n,e,t){return this.matcher.match(n,e,t)},Kn.currentRoute.get=function(){return this.history&&this.history.current},Vn.prototype.init=function(n){var e=this;if(this.apps.push(n),n.$once("hook:destroyed",(function(){var t=e.apps.indexOf(n);t>-1&&e.apps.splice(t,1),e.app===n&&(e.app=e.apps[0]||null),e.app||e.history.teardown()})),!this.app){this.app=n;var t=this.history;if(t instanceof Mn||t instanceof Ln){var r=function(n){t.setupListeners(),function(n){var r=t.current,a=e.options.scrollBehavior;vn&&a&&"fullPath"in n&&ln(e,n,r,!1)}(n)};t.transitionTo(t.getCurrentLocation(),r,r)}t.listen((function(n){e.apps.forEach((function(e){e._route=n}))}))}},Vn.prototype.beforeEach=function(n){return Gn(this.beforeHooks,n)},Vn.prototype.beforeResolve=function(n){return Gn(this.resolveHooks,n)},Vn.prototype.afterEach=function(n){return Gn(this.afterHooks,n)},Vn.prototype.onReady=function(n,e){this.history.onReady(n,e)},Vn.prototype.onError=function(n){this.history.onError(n)},Vn.prototype.push=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.push(n,e,t)}));this.history.push(n,e,t)},Vn.prototype.replace=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.replace(n,e,t)}));this.history.replace(n,e,t)},Vn.prototype.go=function(n){this.history.go(n)},Vn.prototype.back=function(){this.go(-1)},Vn.prototype.forward=function(){this.go(1)},Vn.prototype.getMatchedComponents=function(n){var e=n?n.matched?n:this.resolve(n).route:this.currentRoute;return e?[].concat.apply([],e.matched.map((function(n){return Object.keys(n.components).map((function(e){return n.components[e]}))}))):[]},Vn.prototype.resolve=function(n,e,t){var r=q(n,e=e||this.history.current,t,this),a=this.match(r,e),o=a.redirectedFrom||a.fullPath;return{location:r,route:a,href:function(n,e,t){var r="hash"===t?"#"+e:e;return n?D(n+"/"+r):r}(this.history.base,o,this.mode),normalizedTo:r,resolved:a}},Vn.prototype.getRoutes=function(){return this.matcher.getRoutes()},Vn.prototype.addRoute=function(n,e){this.matcher.addRoute(n,e),this.history.current!==h&&this.history.transitionTo(this.history.getCurrentLocation())},Vn.prototype.addRoutes=function(n){this.matcher.addRoutes(n),this.history.current!==h&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(Vn.prototype,Kn);var Wn=Vn;function Gn(n,e){return n.push(e),function(){var t=n.indexOf(e);t>-1&&n.splice(t,1)}}Vn.install=function n(e){if(!n.installed||V!==e){n.installed=!0,V=e;var t=function(n){return void 0!==n},r=function(n,e){var r=n.$options._parentVnode;t(r)&&t(r=r.data)&&t(r=r.registerRouteInstance)&&r(n,e)};e.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),e.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,r(this,this)},destroyed:function(){r(this)}}),Object.defineProperty(e.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(e.prototype,"$route",{get:function(){return this._routerRoot._route}}),e.component("RouterView",x),e.component("RouterLink",W);var a=e.config.optionMergeStrategies;a.beforeRouteEnter=a.beforeRouteLeave=a.beforeRouteUpdate=a.created}},Vn.version="3.6.5",Vn.isNavigationFailure=Cn,Vn.NavigationFailureType=yn,Vn.START_LOCATION=h,J&&window.Vue&&window.Vue.use(Vn);t(67);var Jn=t(0),Yn=t(129),Qn=t.n(Yn),Xn=t(130),Zn=t.n(Xn),ne={created(){if(this.siteMeta=this.$site.headTags.filter(([n])=>"meta"===n).map(([n,e])=>e),this.$ssrContext){const e=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(n=e)?n.map(n=>{let e="<meta";return Object.keys(n).forEach(t=>{e+=` ${t}="${Zn()(n[t])}"`}),e+">"}).join("\n    "):"",this.$ssrContext.canonicalLink=te(this.$canonicalUrl)}var n},mounted(){this.currentMetaTags=[...document.querySelectorAll("meta")],this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta(){document.title=this.$title,document.documentElement.lang=this.$lang;const n=this.getMergedMetaTags();this.currentMetaTags=re(n,this.currentMetaTags)},getMergedMetaTags(){const n=this.$page.frontmatter.meta||[];return Qn()([{name:"description",content:this.$description}],n,this.siteMeta,ae)},updateCanonicalLink(){ee(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",te(this.$canonicalUrl))}},watch:{$page(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy(){re(null,this.currentMetaTags),ee()}};function ee(){const n=document.querySelector("link[rel='canonical']");n&&n.remove()}function te(n=""){return n?`<link href="${n}" rel="canonical" />`:""}function re(n,e){if(e&&[...e].filter(n=>n.parentNode===document.head).forEach(n=>document.head.removeChild(n)),n)return n.map(n=>{const e=document.createElement("meta");return Object.keys(n).forEach(t=>{e.setAttribute(t,n[t])}),document.head.appendChild(e),e})}function ae(n){for(const e of["name","property","itemprop"])if(n.hasOwnProperty(e))return n[e]+e;return JSON.stringify(n)}var oe=t(131),ie={mounted(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(oe)()((function(){this.setActiveHash()}),300),setActiveHash(){const n=[].slice.call(document.querySelectorAll(".sidebar-link")),e=[].slice.call(document.querySelectorAll(".header-anchor")).filter(e=>n.some(n=>n.hash===e.hash)),t=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),r=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),a=window.innerHeight+t;for(let n=0;n<e.length;n++){const o=e[n],i=e[n+1],s=0===n&&0===t||t>=o.parentElement.offsetTop+10&&(!i||t<i.parentElement.offsetTop-10),l=decodeURIComponent(this.$route.hash);if(s&&l!==decodeURIComponent(o.hash)){const t=o;if(a===r)for(let t=n+1;t<e.length;t++)if(l===decodeURIComponent(e[t].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(t.hash),()=>{this.$nextTick(()=>{this.$vuepress.$set("disableScrollBehavior",!1)})})}}}},beforeDestroy(){window.removeEventListener("scroll",this.onScroll)}},se=(t(272),Object.assign||function(n){for(var e=1;e<arguments.length;e++){var t=arguments[e];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&(n[r]=t[r])}return n}),le=function(n){return"IMG"===n.tagName},ce=function(n){return n&&1===n.nodeType},pe=function(n){return".svg"===(n.currentSrc||n.src).substr(-4).toLowerCase()},ue=function(n){try{return Array.isArray(n)?n.filter(le):function(n){return NodeList.prototype.isPrototypeOf(n)}(n)?[].slice.call(n).filter(le):ce(n)?[n].filter(le):"string"==typeof n?[].slice.call(document.querySelectorAll(n)).filter(le):[]}catch(n){throw new TypeError("The provided selector is invalid.\nExpects a CSS selector, a Node element, a NodeList or an array.\nSee: https://github.com/francoischalifour/medium-zoom")}},de=function(n){var e=document.createElement("div");return e.classList.add("medium-zoom-overlay"),e.style.background=n,e},me=function(n){var e=n.getBoundingClientRect(),t=e.top,r=e.left,a=e.width,o=e.height,i=n.cloneNode(),s=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,l=window.pageXOffset||document.documentElement.scrollLeft||document.body.scrollLeft||0;return i.removeAttribute("id"),i.style.position="absolute",i.style.top=t+s+"px",i.style.left=r+l+"px",i.style.width=a+"px",i.style.height=o+"px",i.style.transform="",i},ge=function(n,e){var t=se({bubbles:!1,cancelable:!1,detail:void 0},e);if("function"==typeof window.CustomEvent)return new CustomEvent(n,t);var r=document.createEvent("CustomEvent");return r.initCustomEvent(n,t.bubbles,t.cancelable,t.detail),r};!function(n,e){void 0===e&&(e={});var t=e.insertAt;if(n&&"undefined"!=typeof document){var r=document.head||document.getElementsByTagName("head")[0],a=document.createElement("style");a.type="text/css","top"===t&&r.firstChild?r.insertBefore(a,r.firstChild):r.appendChild(a),a.styleSheet?a.styleSheet.cssText=n:a.appendChild(document.createTextNode(n))}}(".medium-zoom-overlay{position:fixed;top:0;right:0;bottom:0;left:0;opacity:0;transition:opacity .3s;will-change:opacity}.medium-zoom--opened .medium-zoom-overlay{cursor:pointer;cursor:zoom-out;opacity:1}.medium-zoom-image{cursor:pointer;cursor:zoom-in;transition:transform .3s cubic-bezier(.2,0,.2,1)!important}.medium-zoom-image--hidden{visibility:hidden}.medium-zoom-image--opened{position:relative;cursor:pointer;cursor:zoom-out;will-change:transform}");var fe=function n(e){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},r=window.Promise||function(n){function e(){}n(e,e)},a=function(n){var e=n.target;e!==D?-1!==k.indexOf(e)&&f({target:e}):g()},o=function(){if(!S&&E.original){var n=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0;Math.abs(x-n)>w.scrollOffset&&setTimeout(g,150)}},i=function(n){var e=n.key||n.keyCode;"Escape"!==e&&"Esc"!==e&&27!==e||g()},s=function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},e=n;if(n.background&&(D.style.background=n.background),n.container&&n.container instanceof Object&&(e.container=se({},w.container,n.container)),n.template){var t=ce(n.template)?n.template:document.querySelector(n.template);e.template=t}return w=se({},w,e),k.forEach((function(n){n.dispatchEvent(ge("medium-zoom:update",{detail:{zoom:C}}))})),C},l=function(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};return n(se({},w,e))},c=function(){for(var n=arguments.length,e=Array(n),t=0;t<n;t++)e[t]=arguments[t];var r=e.reduce((function(n,e){return[].concat(n,ue(e))}),[]);return r.filter((function(n){return-1===k.indexOf(n)})).forEach((function(n){k.push(n),n.classList.add("medium-zoom-image")})),y.forEach((function(n){var e=n.type,t=n.listener,a=n.options;r.forEach((function(n){n.addEventListener(e,t,a)}))})),C},p=function(){for(var n=arguments.length,e=Array(n),t=0;t<n;t++)e[t]=arguments[t];E.zoomed&&g();var r=e.length>0?e.reduce((function(n,e){return[].concat(n,ue(e))}),[]):k;return r.forEach((function(n){n.classList.remove("medium-zoom-image"),n.dispatchEvent(ge("medium-zoom:detach",{detail:{zoom:C}}))})),k=k.filter((function(n){return-1===r.indexOf(n)})),C},u=function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};return k.forEach((function(r){r.addEventListener("medium-zoom:"+n,e,t)})),y.push({type:"medium-zoom:"+n,listener:e,options:t}),C},d=function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};return k.forEach((function(r){r.removeEventListener("medium-zoom:"+n,e,t)})),y=y.filter((function(t){return!(t.type==="medium-zoom:"+n&&t.listener.toString()===e.toString())})),C},m=function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},e=n.target,t=function(){var n={width:document.documentElement.clientWidth,height:document.documentElement.clientHeight,left:0,top:0,right:0,bottom:0},e=void 0,t=void 0;if(w.container)if(w.container instanceof Object)e=(n=se({},n,w.container)).width-n.left-n.right-2*w.margin,t=n.height-n.top-n.bottom-2*w.margin;else{var r=(ce(w.container)?w.container:document.querySelector(w.container)).getBoundingClientRect(),a=r.width,o=r.height,i=r.left,s=r.top;n=se({},n,{width:a,height:o,left:i,top:s})}e=e||n.width-2*w.margin,t=t||n.height-2*w.margin;var l=E.zoomedHd||E.original,c=pe(l)?e:l.naturalWidth||e,p=pe(l)?t:l.naturalHeight||t,u=l.getBoundingClientRect(),d=u.top,m=u.left,g=u.width,f=u.height,h=Math.min(Math.max(g,c),e)/g,v=Math.min(Math.max(f,p),t)/f,b=Math.min(h,v),k="scale("+b+") translate3d("+((e-g)/2-m+w.margin+n.left)/b+"px, "+((t-f)/2-d+w.margin+n.top)/b+"px, 0)";E.zoomed.style.transform=k,E.zoomedHd&&(E.zoomedHd.style.transform=k)};return new r((function(n){if(e&&-1===k.indexOf(e))n(C);else{if(E.zoomed)n(C);else{if(e)E.original=e;else{if(!(k.length>0))return void n(C);var r=k;E.original=r[0]}if(E.original.dispatchEvent(ge("medium-zoom:open",{detail:{zoom:C}})),x=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,S=!0,E.zoomed=me(E.original),document.body.appendChild(D),w.template){var a=ce(w.template)?w.template:document.querySelector(w.template);E.template=document.createElement("div"),E.template.appendChild(a.content.cloneNode(!0)),document.body.appendChild(E.template)}if(E.original.parentElement&&"PICTURE"===E.original.parentElement.tagName&&E.original.currentSrc&&(E.zoomed.src=E.original.currentSrc),document.body.appendChild(E.zoomed),window.requestAnimationFrame((function(){document.body.classList.add("medium-zoom--opened")})),E.original.classList.add("medium-zoom-image--hidden"),E.zoomed.classList.add("medium-zoom-image--opened"),E.zoomed.addEventListener("click",g),E.zoomed.addEventListener("transitionend",(function e(){S=!1,E.zoomed.removeEventListener("transitionend",e),E.original.dispatchEvent(ge("medium-zoom:opened",{detail:{zoom:C}})),n(C)})),E.original.getAttribute("data-zoom-src")){E.zoomedHd=E.zoomed.cloneNode(),E.zoomedHd.removeAttribute("srcset"),E.zoomedHd.removeAttribute("sizes"),E.zoomedHd.removeAttribute("loading"),E.zoomedHd.src=E.zoomed.getAttribute("data-zoom-src"),E.zoomedHd.onerror=function(){clearInterval(o),console.warn("Unable to reach the zoom image target "+E.zoomedHd.src),E.zoomedHd=null,t()};var o=setInterval((function(){E.zoomedHd.complete&&(clearInterval(o),E.zoomedHd.classList.add("medium-zoom-image--opened"),E.zoomedHd.addEventListener("click",g),document.body.appendChild(E.zoomedHd),t())}),10)}else if(E.original.hasAttribute("srcset")){E.zoomedHd=E.zoomed.cloneNode(),E.zoomedHd.removeAttribute("sizes"),E.zoomedHd.removeAttribute("loading");var i=E.zoomedHd.addEventListener("load",(function(){E.zoomedHd.removeEventListener("load",i),E.zoomedHd.classList.add("medium-zoom-image--opened"),E.zoomedHd.addEventListener("click",g),document.body.appendChild(E.zoomedHd),t()}))}else t()}}}))},g=function(){return new r((function(n){if(!S&&E.original){S=!0,document.body.classList.remove("medium-zoom--opened"),E.zoomed.style.transform="",E.zoomedHd&&(E.zoomedHd.style.transform=""),E.template&&(E.template.style.transition="opacity 150ms",E.template.style.opacity=0),E.original.dispatchEvent(ge("medium-zoom:close",{detail:{zoom:C}})),E.zoomed.addEventListener("transitionend",(function e(){E.original.classList.remove("medium-zoom-image--hidden"),document.body.removeChild(E.zoomed),E.zoomedHd&&document.body.removeChild(E.zoomedHd),document.body.removeChild(D),E.zoomed.classList.remove("medium-zoom-image--opened"),E.template&&document.body.removeChild(E.template),S=!1,E.zoomed.removeEventListener("transitionend",e),E.original.dispatchEvent(ge("medium-zoom:closed",{detail:{zoom:C}})),E.original=null,E.zoomed=null,E.zoomedHd=null,E.template=null,n(C)}))}else n(C)}))},f=function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},e=n.target;return E.original?g():m({target:e})},h=function(){return w},v=function(){return k},b=function(){return E.original},k=[],y=[],S=!1,x=0,w=t,E={original:null,zoomed:null,zoomedHd:null,template:null};"[object Object]"===Object.prototype.toString.call(e)?w=e:(e||"string"==typeof e)&&c(e),w=se({margin:0,background:"#fff",scrollOffset:40,container:null,template:null},w);var D=de(w.background);document.addEventListener("click",a),document.addEventListener("keyup",i),document.addEventListener("scroll",o),window.addEventListener("resize",g);var C={open:m,close:g,toggle:f,update:s,clone:l,attach:c,detach:p,on:u,off:d,getOptions:h,getImages:v,getZoomedImage:b};return C},he={data:()=>({zoom:null}),mounted(){this.updateZoom()},updated(){this.updateZoom()},methods:{updateZoom(){setTimeout(()=>{this.zoom&&this.zoom.detach(),this.zoom=fe(".theme-reco-content :not(a) > img",void 0)},1e3)}}},ve=t(41),be=t.n(ve),ke={mounted(){be.a.configure({showSpinner:!1}),this.$router.beforeEach((n,e,t)=>{n.path===e.path||r.b.component(n.name)||be.a.start(),t()}),this.$router.afterEach(()=>{be.a.done(),this.isSidebarOpen=!1})}},ye=t(132),Se=t.n(ye),xe={mounted(){Se.a.polyfill()}},we=t(133),Ee={noCopy:!1,noSelect:!1,disabled:!1,minLength:100,authorName:"https://gordonchanfz.github.io/"},De={props:{html:String,lang:String},created(){this.authorName="string"==typeof Ee.authorName?Ee.authorName:this.getI18nValue(Ee.authorName),this.text=this.getI18nValue(we),this.location=String(location).replace(/#.+$/,"")},methods:{getI18nValue(n){return this.lang in n?n[this.lang]:n["en-US"]}}},Ce=t(3),Ie=Object(Ce.a)(De,(function(){var n=this,e=n._self._c;return e("div",[e("p",[n._v(n._s(n.text.beforeAuthor)+n._s(n.authorName||n.text.author)+n._s(n.text.afterAuthor)),e("a",{attrs:{href:n.location}},[n._v(n._s(decodeURIComponent(n.location)))])]),n._v("\n\n"),e("div",{domProps:{innerHTML:n._s(n.html)}})])}),[],!1,null,null,null).exports,Te={data:()=>({isElement:!1}),created(){this.onCopy=n=>{const e=getSelection().getRangeAt(0);if(String(e).length<this.minLength)return;if(n.preventDefault(),this.noCopy)return;const t=document.createElement("div");t.appendChild(getSelection().getRangeAt(0).cloneContents());const a=this.$lang,o=new r.b({render:n=>n(Ie,{props:{html:t.innerHTML,lang:a}})}).$mount(),{innerHTML:i,innerText:s}=o.$el;n.clipboardData?(n.clipboardData.setData("text/html",i),n.clipboardData.setData("text/plain",s)):window.clipboardData&&window.clipboardData.setData("text",s)}},watch:{isElement(n){if(!n)return;let{copyright:e=!Ee.disabled}=this.$frontmatter;if(!e)return;"object"!=typeof e&&(e={});const t=e.noSelect||Ee.noSelect;this.minLength=e.minLength||Ee.minLength,this.noCopy=e.noCopy||Ee.noCopy,t?this.$el.style.userSelect="none":this.$el.addEventListener("copy",this.onCopy)}},updated(){this.isElement="#comment"!==this.$el.nodeName},beforeDestory(){this.$el.removeEventListener("copy",this.onCopy)}},Oe={props:{parent:Object,code:String,options:{align:String,color:String,backgroundTransition:Boolean,backgroundColor:String,successText:String,staticIcon:Boolean}},data:()=>({success:!1,originalBackground:null,originalTransition:null}),computed:{alignStyle(){let n={};return n[this.options.align]="7.5px",n},iconClass(){return this.options.staticIcon?"":"hover"}},mounted(){this.originalTransition=this.parent.style.transition,this.originalBackground=this.parent.style.background},beforeDestroy(){this.parent.style.transition=this.originalTransition,this.parent.style.background=this.originalBackground},methods:{hexToRgb(n){let e=/^#?([a-f\d]{2})([a-f\d]{2})([a-f\d]{2})$/i.exec(n);return e?{r:parseInt(e[1],16),g:parseInt(e[2],16),b:parseInt(e[3],16)}:null},copyToClipboard(n){if(navigator.clipboard)navigator.clipboard.writeText(this.code).then(()=>{this.setSuccessTransitions()},()=>{});else{let n=document.createElement("textarea");document.body.appendChild(n),n.value=this.code,n.select(),document.execCommand("Copy"),n.remove(),this.setSuccessTransitions()}},setSuccessTransitions(){if(clearTimeout(this.successTimeout),this.options.backgroundTransition){this.parent.style.transition="background 350ms";let n=this.hexToRgb(this.options.backgroundColor);this.parent.style.background=`rgba(${n.r}, ${n.g}, ${n.b}, 0.1)`}this.success=!0,this.successTimeout=setTimeout(()=>{this.options.backgroundTransition&&(this.parent.style.background=this.originalBackground,this.parent.style.transition=this.originalTransition),this.success=!1},500)}}},Ae=(t(273),Object(Ce.a)(Oe,(function(){var n=this,e=n._self._c;return e("div",{staticClass:"code-copy"},[e("svg",{class:n.iconClass,style:n.alignStyle,attrs:{xmlns:"http://www.w3.org/2000/svg",width:"24",height:"24",viewBox:"0 0 24 24"},on:{click:n.copyToClipboard}},[e("path",{attrs:{fill:"none",d:"M0 0h24v24H0z"}}),n._v(" "),e("path",{attrs:{fill:n.options.color,d:"M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm-1 4l6 6v10c0 1.1-.9 2-2 2H7.99C6.89 23 6 22.1 6 21l.01-14c0-1.1.89-2 1.99-2h7zm-1 7h5.5L14 6.5V12z"}})]),n._v(" "),e("span",{class:n.success?"success":"",style:n.alignStyle},[n._v("\n        "+n._s(n.options.successText)+"\n    ")])])}),[],!1,null,"49140617",null).exports),_e=(t(274),[ne,ie,he,ke,xe,Te,{updated(){this.update()},methods:{update(){setTimeout(()=>{document.querySelectorAll('div[class*="language-"] pre').forEach(n=>{if(n.classList.contains("code-copy-added"))return;let e=new(r.b.extend(Ae));e.options={align:"bottom",color:"#27b1ff",backgroundTransition:!0,backgroundColor:"#0075b8",successText:"Copied!",staticIcon:!1},e.code=n.innerText,e.parent=n,e.$mount(),n.classList.add("code-copy-added"),n.appendChild(e.$el)})},100)}}}]),Re={name:"GlobalLayout",computed:{layout(){const n=this.getLayout();return Object(Jn.i)("layout",n),r.b.component(n)}},methods:{getLayout(){if(this.$page.path){const n=this.$page.frontmatter.layout;return n&&(this.$vuepress.getLayoutAsyncComponent(n)||this.$vuepress.getVueComponent(n))?n:"Layout"}return"NotFound"}}},Pe=Object(Ce.a)(Re,(function(){return(0,this._self._c)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;Object(Jn.g)(Pe,"mixins",_e);const Fe=[{name:"v-aae13ec4",path:"/2019/01/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-aae13ec4").then(t)}},{path:"/2019/01/01/idea的使用/",redirect:"/2019/01/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8/"},{path:"/2019/01/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8/index.html",redirect:"/2019/01/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8/"},{path:"/tool/IDEA/IDEA的使用.html",redirect:"/2019/01/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8/"},{name:"v-af65573c",path:"/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-af65573c").then(t)}},{path:"/index.html",redirect:"/"},{name:"v-0ea853f1",path:"/2022/03/09/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-0ea853f1").then(t)}},{path:"/2022/03/09/我的常用emoji/",redirect:"/2022/03/09/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji/"},{path:"/2022/03/09/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji/index.html",redirect:"/2022/03/09/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji/"},{path:"/tool/emoji/我的常用emoji.html",redirect:"/2022/03/09/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji/"},{name:"v-0cbdd054",path:"/2022/03/12/%E5%88%A9%E7%94%A8markdown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84emoji/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-0cbdd054").then(t)}},{path:"/2022/03/12/利用markdown收集常用的emoji/",redirect:"/2022/03/12/%E5%88%A9%E7%94%A8markdown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84emoji/"},{path:"/2022/03/12/%E5%88%A9%E7%94%A8markdown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84emoji/index.html",redirect:"/2022/03/12/%E5%88%A9%E7%94%A8markdown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84emoji/"},{path:"/tool/markdown/利用MarkDown收集常用的Emoji.html",redirect:"/2022/03/12/%E5%88%A9%E7%94%A8markdown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84emoji/"},{name:"v-0bf5ebde",path:"/2022/03/09/git%E5%9C%A8-gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-0bf5ebde").then(t)}},{path:"/2022/03/09/git在-gitignore添加忽略文件不起作用/",redirect:"/2022/03/09/git%E5%9C%A8-gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/"},{path:"/2022/03/09/git%E5%9C%A8-gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/index.html",redirect:"/2022/03/09/git%E5%9C%A8-gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/"},{path:"/tool/git/git在.gitignore添加忽略文件不起作用.html",redirect:"/2022/03/09/git%E5%9C%A8-gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/"},{name:"v-46e67ace",path:"/1970/01/01/aboutme/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-46e67ace").then(t)}},{path:"/1970/01/01/aboutme/index.html",redirect:"/1970/01/01/aboutme/"},{path:"/aboutme.html",redirect:"/1970/01/01/aboutme/"},{name:"v-6048fa40",path:"/2022/03/26/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-6048fa40").then(t)}},{path:"/2022/03/26/安利一些电子图书下载网站/",redirect:"/2022/03/26/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99/"},{path:"/2022/03/26/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99/index.html",redirect:"/2022/03/26/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99/"},{path:"/tool/resource/安利一些电子图书下载网站.html",redirect:"/2022/03/26/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99/"},{name:"v-5ed1fdaa",path:"/2022/03/09/github%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-5ed1fdaa").then(t)}},{path:"/2022/03/09/github自定义美化个人主页/",redirect:"/2022/03/09/github%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/"},{path:"/2022/03/09/github%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/index.html",redirect:"/2022/03/09/github%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/"},{path:"/tool/git/GitHub自定义美化个人主页.html",redirect:"/2022/03/09/github%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/"},{name:"v-7d8ca27f",path:"/2022/07/08/kafka/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-7d8ca27f").then(t)}},{path:"/2022/07/08/kafka/index.html",redirect:"/2022/07/08/kafka/"},{path:"/中间件/kafka.html",redirect:"/2022/07/08/kafka/"},{name:"v-dffff514",path:"/2022/03/09/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-dffff514").then(t)}},{path:"/2022/03/09/搭建过程中的问题/",redirect:"/2022/03/09/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/"},{path:"/2022/03/09/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/index.html",redirect:"/2022/03/09/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/"},{path:"/关于本站/搭建过程中的问题.html",redirect:"/2022/03/09/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/"},{name:"v-33a310e8",path:"/2023/06/10/docker/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-33a310e8").then(t)}},{path:"/2023/06/10/docker/index.html",redirect:"/2023/06/10/docker/"},{path:"/云原生/Docker.html",redirect:"/2023/06/10/docker/"},{name:"v-700a68a1",path:"/2022/10/08/rpc/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-700a68a1").then(t)}},{path:"/2022/10/08/rpc/index.html",redirect:"/2022/10/08/rpc/"},{path:"/其他/RPC.html",redirect:"/2022/10/08/rpc/"},{name:"v-2c8e9208",path:"/1970/01/01/k8s/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-2c8e9208").then(t)}},{path:"/1970/01/01/k8s/index.html",redirect:"/1970/01/01/k8s/"},{path:"/云原生/k8s.html",redirect:"/1970/01/01/k8s/"},{name:"v-01787dc2",path:"/2019/09/08/geohash%E7%AE%97%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-01787dc2").then(t)}},{path:"/2019/09/08/geohash算法/",redirect:"/2019/09/08/geohash%E7%AE%97%E6%B3%95/"},{path:"/2019/09/08/geohash%E7%AE%97%E6%B3%95/index.html",redirect:"/2019/09/08/geohash%E7%AE%97%E6%B3%95/"},{path:"/其他/geohash算法.html",redirect:"/2019/09/08/geohash%E7%AE%97%E6%B3%95/"},{name:"v-83d8af48",path:"/2023/06/10/fink-on-k8s/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-83d8af48").then(t)}},{path:"/2023/06/10/fink-on-k8s/index.html",redirect:"/2023/06/10/fink-on-k8s/"},{path:"/云原生/fink-on-k8s.html",redirect:"/2023/06/10/fink-on-k8s/"},{name:"v-c38fec2a",path:"/2023/06/10/helm/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-c38fec2a").then(t)}},{path:"/2023/06/10/helm/index.html",redirect:"/2023/06/10/helm/"},{path:"/其他/helm.html",redirect:"/2023/06/10/helm/"},{name:"v-7e69d236",path:"/2022/05/10/sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-7e69d236").then(t)}},{path:"/2022/05/10/sqoop基本使用/",redirect:"/2022/05/10/sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{path:"/2022/05/10/sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/index.html",redirect:"/2022/05/10/sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{path:"/中间件/Sqoop基本使用.html",redirect:"/2022/05/10/sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{name:"v-cdeeea2e",path:"/2023/06/10/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-cdeeea2e").then(t)}},{path:"/2023/06/10/linux命令总结/",redirect:"/2023/06/10/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/"},{path:"/2023/06/10/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/index.html",redirect:"/2023/06/10/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/"},{path:"/其他/linux命令总结.html",redirect:"/2023/06/10/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/"},{name:"v-10e8b782",path:"/2022/10/08/gitlab%E9%80%9A%E8%BF%87cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-10e8b782").then(t)}},{path:"/2022/10/08/gitlab通过cicd流水线部署/",redirect:"/2022/10/08/gitlab%E9%80%9A%E8%BF%87cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2/"},{path:"/2022/10/08/gitlab%E9%80%9A%E8%BF%87cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2/index.html",redirect:"/2022/10/08/gitlab%E9%80%9A%E8%BF%87cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2/"},{path:"/其他/gitlab通过CICD流水线部署.html",redirect:"/2022/10/08/gitlab%E9%80%9A%E8%BF%87cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2/"},{name:"v-851571e2",path:"/2023/06/10/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-851571e2").then(t)}},{path:"/2023/06/10/一致性hash算法/",redirect:"/2023/06/10/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/"},{path:"/2023/06/10/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/index.html",redirect:"/2023/06/10/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/"},{path:"/其他/一致性hash算法.html",redirect:"/2023/06/10/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/"},{name:"v-7350f07e",path:"/2023/02/08/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-7350f07e").then(t)}},{path:"/2023/02/08/布隆过滤器和布谷鸟过滤器/",redirect:"/2023/02/08/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8/"},{path:"/2023/02/08/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8/index.html",redirect:"/2023/02/08/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8/"},{path:"/其他/布隆过滤器和布谷鸟过滤器.html",redirect:"/2023/02/08/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8/"},{name:"v-22a43ff6",path:"/2023/06/10/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-22a43ff6").then(t)}},{path:"/2023/06/10/分布式一致性算法/",redirect:"/2023/06/10/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/"},{path:"/2023/06/10/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/index.html",redirect:"/2023/06/10/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/"},{path:"/其他/分布式一致性算法.html",redirect:"/2023/06/10/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/"},{name:"v-0ddfb2e2",path:"/2022/10/08/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-0ddfb2e2").then(t)}},{path:"/2022/10/08/差分算法/",redirect:"/2022/10/08/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95/"},{path:"/2022/10/08/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95/index.html",redirect:"/2022/10/08/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95/"},{path:"/其他/差分算法.html",redirect:"/2022/10/08/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95/"},{name:"v-41ccc7e2",path:"/2023/06/10/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-41ccc7e2").then(t)}},{path:"/2023/06/10/常见的序列化方式/",redirect:"/2023/06/10/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F/"},{path:"/2023/06/10/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F/index.html",redirect:"/2023/06/10/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F/"},{path:"/其他/常见的序列化方式.html",redirect:"/2023/06/10/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F/"},{name:"v-29f87cb0",path:"/2023/06/10/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-29f87cb0").then(t)}},{path:"/2023/06/10/本地缓存/",redirect:"/2023/06/10/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98/"},{path:"/2023/06/10/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98/index.html",redirect:"/2023/06/10/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98/"},{path:"/其他/本地缓存.html",redirect:"/2023/06/10/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98/"},{name:"v-2e09059c",path:"/2022/01/08/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-2e09059c").then(t)}},{path:"/2022/01/08/雪花算法/",redirect:"/2022/01/08/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/"},{path:"/2022/01/08/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/index.html",redirect:"/2022/01/08/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/"},{path:"/其他/雪花算法.html",redirect:"/2022/01/08/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/"},{name:"v-7ea9e72a",path:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-7ea9e72a").then(t)}},{path:"/1970/01/01/常见的索引树结构/",redirect:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84/"},{path:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84/index.html",redirect:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84/"},{path:"/其他/常见的索引树结构.html",redirect:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84/"},{name:"v-63f06f0b",path:"/1970/01/01/spring/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-63f06f0b").then(t)}},{path:"/1970/01/01/spring/index.html",redirect:"/1970/01/01/spring/"},{path:"/其他/spring.html",redirect:"/1970/01/01/spring/"},{name:"v-61c5b94b",path:"/2022/10/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-61c5b94b").then(t)}},{path:"/2022/10/08/零拷贝原理/",redirect:"/2022/10/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86/"},{path:"/2022/10/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86/index.html",redirect:"/2022/10/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86/"},{path:"/其他/零拷贝原理.html",redirect:"/2022/10/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86/"},{name:"v-5692179e",path:"/2023/05/08/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-5692179e").then(t)}},{path:"/2023/05/08/美团开源动态线程池/",redirect:"/2023/05/08/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{path:"/2023/05/08/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0/index.html",redirect:"/2023/05/08/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{path:"/其他/美团开源动态线程池.html",redirect:"/2023/05/08/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{name:"v-227baaf0",path:"/2023/06/10/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-227baaf0").then(t)}},{path:"/2023/06/10/网络io模型/",redirect:"/2023/06/10/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/"},{path:"/2023/06/10/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/index.html",redirect:"/2023/06/10/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/"},{path:"/其他/网络IO模型.html",redirect:"/2023/06/10/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/"},{name:"v-e7d27b94",path:"/1970/01/01/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-e7d27b94").then(t)}},{path:"/1970/01/01/git常用命令/",redirect:"/1970/01/01/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{path:"/1970/01/01/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/index.html",redirect:"/1970/01/01/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{path:"/常用命令脚本/git常用命令.html",redirect:"/1970/01/01/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{name:"v-7f30b557",path:"/1970/01/01/redis/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-7f30b557").then(t)}},{path:"/1970/01/01/redis/index.html",redirect:"/1970/01/01/redis/"},{path:"/存储引擎/redis.html",redirect:"/1970/01/01/redis/"},{name:"v-4f7f9be4",path:"/2023/06/10/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-4f7f9be4").then(t)}},{path:"/2023/06/10/k8s常用命令/",redirect:"/2023/06/10/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{path:"/2023/06/10/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/index.html",redirect:"/2023/06/10/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{path:"/常用命令脚本/k8s常用命令.html",redirect:"/2023/06/10/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{name:"v-24ffb3db",path:"/2022/08/08/hbase/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-24ffb3db").then(t)}},{path:"/2022/08/08/hbase/index.html",redirect:"/2022/08/08/hbase/"},{path:"/存储引擎/hbase.html",redirect:"/2022/08/08/hbase/"},{name:"v-e463dc58",path:"/2023/06/10/protobuf3%E8%AF%AD%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-e463dc58").then(t)}},{path:"/2023/06/10/protobuf3语法/",redirect:"/2023/06/10/protobuf3%E8%AF%AD%E6%B3%95/"},{path:"/2023/06/10/protobuf3%E8%AF%AD%E6%B3%95/index.html",redirect:"/2023/06/10/protobuf3%E8%AF%AD%E6%B3%95/"},{path:"/常用命令脚本/protobuf3语法.html",redirect:"/2023/06/10/protobuf3%E8%AF%AD%E6%B3%95/"},{name:"v-007b24d3",path:"/2019/10/08/elasticsearch/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-007b24d3").then(t)}},{path:"/2019/10/08/elasticsearch/index.html",redirect:"/2019/10/08/elasticsearch/"},{path:"/存储引擎/ElasticSearch.html",redirect:"/2019/10/08/elasticsearch/"},{name:"v-305ac8e0",path:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-305ac8e0").then(t)}},{path:"/1970/01/01/常见的限流算法/",redirect:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{path:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/index.html",redirect:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{path:"/其他/常见的限流算法.html",redirect:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{name:"v-4fd15c58",path:"/1970/01/01/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-4fd15c58").then(t)}},{path:"/1970/01/01/redis操作命令锦集/",redirect:"/1970/01/01/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86/"},{path:"/1970/01/01/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86/index.html",redirect:"/1970/01/01/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86/"},{path:"/常用命令脚本/redis操作命令锦集.html",redirect:"/1970/01/01/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86/"},{name:"v-36184f82",path:"/2023/04/08/apache-beam/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-36184f82").then(t)}},{path:"/2023/04/08/apache-beam/index.html",redirect:"/2023/04/08/apache-beam/"},{path:"/计算引擎/Apache-beam.html",redirect:"/2023/04/08/apache-beam/"},{name:"v-49f14a1b",path:"/2019/08/08/flink/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-49f14a1b").then(t)}},{path:"/2019/08/08/flink/index.html",redirect:"/2019/08/08/flink/"},{path:"/计算引擎/flink.html",redirect:"/2019/08/08/flink/"},{name:"v-9f20b3be",path:"/2019/09/08/spark/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-9f20b3be").then(t)}},{path:"/2019/09/08/spark/index.html",redirect:"/2019/09/08/spark/"},{path:"/计算引擎/spark.html",redirect:"/2019/09/08/spark/"},{name:"v-b1564aac",path:"/tag/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tags","v-b1564aac").then(t)},meta:{pid:"tags",id:"tags"}},{path:"/tag/index.html",redirect:"/tag/"},{name:"v-ef9325c4",path:"/categories/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("FrontmatterKey","v-ef9325c4").then(t)},meta:{pid:"categories",id:"categories"}},{path:"/categories/index.html",redirect:"/categories/"},{name:"v-6319eb4e",path:"/timeline/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("TimeLines","v-6319eb4e").then(t)},meta:{pid:"timeline",id:"timeline"}},{path:"/timeline/index.html",redirect:"/timeline/"},{name:"v-3ae5b494",path:"/tag/markdown/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-3ae5b494").then(t)},meta:{pid:"tags",id:"markdown"}},{path:"/tag/markdown/index.html",redirect:"/tag/markdown/"},{name:"v-584666fc",path:"/tag/推荐/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-584666fc").then(t)},meta:{pid:"tags",id:"推荐"}},{path:"/tag/推荐/index.html",redirect:"/tag/推荐/"},{name:"v-23a8b635",path:"/tag/资源分享/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-23a8b635").then(t)},meta:{pid:"tags",id:"资源分享"}},{path:"/tag/资源分享/index.html",redirect:"/tag/资源分享/"},{name:"v-c481210e",path:"/tag/消息队列/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-c481210e").then(t)},meta:{pid:"tags",id:"消息队列"}},{path:"/tag/消息队列/index.html",redirect:"/tag/消息队列/"},{name:"v-598aa1ae",path:"/tag/生产者-消费者/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-598aa1ae").then(t)},meta:{pid:"tags",id:"生产者-消费者"}},{path:"/tag/生产者-消费者/index.html",redirect:"/tag/生产者-消费者/"},{name:"v-6b29cdd0",path:"/tag/云原生/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-6b29cdd0").then(t)},meta:{pid:"tags",id:"云原生"}},{path:"/tag/云原生/index.html",redirect:"/tag/云原生/"},{name:"v-7ed06156",path:"/tag/容器技术/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-7ed06156").then(t)},meta:{pid:"tags",id:"容器技术"}},{path:"/tag/容器技术/index.html",redirect:"/tag/容器技术/"},{name:"v-1f026d94",path:"/tag/远程过程调用/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-1f026d94").then(t)},meta:{pid:"tags",id:"远程过程调用"}},{path:"/tag/远程过程调用/index.html",redirect:"/tag/远程过程调用/"},{name:"v-036115ab",path:"/tag/索引/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-036115ab").then(t)},meta:{pid:"tags",id:"索引"}},{path:"/tag/索引/index.html",redirect:"/tag/索引/"},{name:"v-6dd70f48",path:"/tag/空间索引/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-6dd70f48").then(t)},meta:{pid:"tags",id:"空间索引"}},{path:"/tag/空间索引/index.html",redirect:"/tag/空间索引/"},{name:"v-5f2b2f45",path:"/tag/附近的人/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-5f2b2f45").then(t)},meta:{pid:"tags",id:"附近的人"}},{path:"/tag/附近的人/index.html",redirect:"/tag/附近的人/"},{name:"v-32360c9a",path:"/tag/k8s/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-32360c9a").then(t)},meta:{pid:"tags",id:"k8s"}},{path:"/tag/k8s/index.html",redirect:"/tag/k8s/"},{name:"v-f57983ce",path:"/tag/Flink/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-f57983ce").then(t)},meta:{pid:"tags",id:"Flink"}},{path:"/tag/Flink/index.html",redirect:"/tag/Flink/"},{name:"v-1560bc14",path:"/tag/beam/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-1560bc14").then(t)},meta:{pid:"tags",id:"beam"}},{path:"/tag/beam/index.html",redirect:"/tag/beam/"},{name:"v-53d8f7ba",path:"/tag/k8s部署/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-53d8f7ba").then(t)},meta:{pid:"tags",id:"k8s部署"}},{path:"/tag/k8s部署/index.html",redirect:"/tag/k8s部署/"},{name:"v-b6d24872",path:"/tag/离线同步数据/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-b6d24872").then(t)},meta:{pid:"tags",id:"离线同步数据"}},{path:"/tag/离线同步数据/index.html",redirect:"/tag/离线同步数据/"},{name:"v-70e3a6de",path:"/tag/近实时/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-70e3a6de").then(t)},meta:{pid:"tags",id:"近实时"}},{path:"/tag/近实时/index.html",redirect:"/tag/近实时/"},{name:"v-70980f1d",path:"/tag/linux命令/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-70980f1d").then(t)},meta:{pid:"tags",id:"linux命令"}},{path:"/tag/linux命令/index.html",redirect:"/tag/linux命令/"},{name:"v-18e4cd44",path:"/tag/CICD/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-18e4cd44").then(t)},meta:{pid:"tags",id:"CICD"}},{path:"/tag/CICD/index.html",redirect:"/tag/CICD/"},{name:"v-9f88a2c0",path:"/tag/持续集成部署/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-9f88a2c0").then(t)},meta:{pid:"tags",id:"持续集成部署"}},{path:"/tag/持续集成部署/index.html",redirect:"/tag/持续集成部署/"},{name:"v-6ac82e63",path:"/tag/pipeline/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-6ac82e63").then(t)},meta:{pid:"tags",id:"pipeline"}},{path:"/tag/pipeline/index.html",redirect:"/tag/pipeline/"},{name:"v-60190584",path:"/tag/redis/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-60190584").then(t)},meta:{pid:"tags",id:"redis"}},{path:"/tag/redis/index.html",redirect:"/tag/redis/"},{name:"v-7e42d028",path:"/tag/一致性hash/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-7e42d028").then(t)},meta:{pid:"tags",id:"一致性hash"}},{path:"/tag/一致性hash/index.html",redirect:"/tag/一致性hash/"},{name:"v-257df835",path:"/tag/水平扩容/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-257df835").then(t)},meta:{pid:"tags",id:"水平扩容"}},{path:"/tag/水平扩容/index.html",redirect:"/tag/水平扩容/"},{name:"v-38e037b7",path:"/tag/黑名单过滤/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-38e037b7").then(t)},meta:{pid:"tags",id:"黑名单过滤"}},{path:"/tag/黑名单过滤/index.html",redirect:"/tag/黑名单过滤/"},{name:"v-2ad4af63",path:"/tag/缓存穿透优化/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-2ad4af63").then(t)},meta:{pid:"tags",id:"缓存穿透优化"}},{path:"/tag/缓存穿透优化/index.html",redirect:"/tag/缓存穿透优化/"},{name:"v-08174efe",path:"/tag/分布式/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-08174efe").then(t)},meta:{pid:"tags",id:"分布式"}},{path:"/tag/分布式/index.html",redirect:"/tag/分布式/"},{name:"v-35aa74ba",path:"/tag/一致性/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-35aa74ba").then(t)},meta:{pid:"tags",id:"一致性"}},{path:"/tag/一致性/index.html",redirect:"/tag/一致性/"},{name:"v-1aaea625",path:"/tag/增量更新/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-1aaea625").then(t)},meta:{pid:"tags",id:"增量更新"}},{path:"/tag/增量更新/index.html",redirect:"/tag/增量更新/"},{name:"v-2370020b",path:"/tag/bisdiff/bispatch/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-2370020b").then(t)},meta:{pid:"tags",id:"bisdiff/bispatch"}},{path:"/tag/bisdiff/bispatch/index.html",redirect:"/tag/bisdiff/bispatch/"},{name:"v-84b6fb3e",path:"/tag/序列化/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-84b6fb3e").then(t)},meta:{pid:"tags",id:"序列化"}},{path:"/tag/序列化/index.html",redirect:"/tag/序列化/"},{name:"v-d5dea5f8",path:"/tag/缓存/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-d5dea5f8").then(t)},meta:{pid:"tags",id:"缓存"}},{path:"/tag/缓存/index.html",redirect:"/tag/缓存/"},{name:"v-812754e8",path:"/tag/一级缓存/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-812754e8").then(t)},meta:{pid:"tags",id:"一级缓存"}},{path:"/tag/一级缓存/index.html",redirect:"/tag/一级缓存/"},{name:"v-2a06d83f",path:"/tag/分布式id生成/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-2a06d83f").then(t)},meta:{pid:"tags",id:"分布式id生成"}},{path:"/tag/分布式id生成/index.html",redirect:"/tag/分布式id生成/"},{name:"v-5b26dd53",path:"/tag/kafka的高性能原理/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-5b26dd53").then(t)},meta:{pid:"tags",id:"kafka的高性能原理"}},{path:"/tag/kafka的高性能原理/index.html",redirect:"/tag/kafka的高性能原理/"},{name:"v-0dd50f9b",path:"/tag/服务器小文件传输/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-0dd50f9b").then(t)},meta:{pid:"tags",id:"服务器小文件传输"}},{path:"/tag/服务器小文件传输/index.html",redirect:"/tag/服务器小文件传输/"},{name:"v-19522da4",path:"/tag/线程池设计/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-19522da4").then(t)},meta:{pid:"tags",id:"线程池设计"}},{path:"/tag/线程池设计/index.html",redirect:"/tag/线程池设计/"},{name:"v-76ecf1d8",path:"/tag/spring/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-76ecf1d8").then(t)},meta:{pid:"tags",id:"spring"}},{path:"/tag/spring/index.html",redirect:"/tag/spring/"},{name:"v-7ed250eb",path:"/tag/网络IO/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-7ed250eb").then(t)},meta:{pid:"tags",id:"网络IO"}},{path:"/tag/网络IO/index.html",redirect:"/tag/网络IO/"},{name:"v-6367c596",path:"/tag/列式存储/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-6367c596").then(t)},meta:{pid:"tags",id:"列式存储"}},{path:"/tag/列式存储/index.html",redirect:"/tag/列式存储/"},{name:"v-771b1f98",path:"/tag/搜索/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-771b1f98").then(t)},meta:{pid:"tags",id:"搜索"}},{path:"/tag/搜索/index.html",redirect:"/tag/搜索/"},{name:"v-1fdf3cdf",path:"/tag/倒排索引/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-1fdf3cdf").then(t)},meta:{pid:"tags",id:"倒排索引"}},{path:"/tag/倒排索引/index.html",redirect:"/tag/倒排索引/"},{name:"v-402a1f0e",path:"/tag/流批一体编程框架/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-402a1f0e").then(t)},meta:{pid:"tags",id:"流批一体编程框架"}},{path:"/tag/流批一体编程框架/index.html",redirect:"/tag/流批一体编程框架/"},{name:"v-134dedee",path:"/tag/实时计算/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-134dedee").then(t)},meta:{pid:"tags",id:"实时计算"}},{path:"/tag/实时计算/index.html",redirect:"/tag/实时计算/"},{name:"v-099ad802",path:"/tag/流批一体/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-099ad802").then(t)},meta:{pid:"tags",id:"流批一体"}},{path:"/tag/流批一体/index.html",redirect:"/tag/流批一体/"},{name:"v-dd643e04",path:"/tag/离线计算/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-dd643e04").then(t)},meta:{pid:"tags",id:"离线计算"}},{path:"/tag/离线计算/index.html",redirect:"/tag/离线计算/"},{name:"v-093485d8",path:"/tag/sparksql/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-093485d8").then(t)},meta:{pid:"tags",id:"sparksql"}},{path:"/tag/sparksql/index.html",redirect:"/tag/sparksql/"},{name:"v-638ddf39",path:"/categories/tool/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-638ddf39").then(t)},meta:{pid:"categories",id:"tool"}},{path:"/categories/tool/index.html",redirect:"/categories/tool/"},{name:"v-b571f312",path:"/categories/软件资源/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-b571f312").then(t)},meta:{pid:"categories",id:"软件资源"}},{path:"/categories/软件资源/index.html",redirect:"/categories/软件资源/"},{name:"v-2bf76980",path:"/categories/随笔/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-2bf76980").then(t)},meta:{pid:"categories",id:"随笔"}},{path:"/categories/随笔/index.html",redirect:"/categories/随笔/"},{name:"v-ab31fcde",path:"/categories/中间件/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-ab31fcde").then(t)},meta:{pid:"categories",id:"中间件"}},{path:"/categories/中间件/index.html",redirect:"/categories/中间件/"},{name:"v-c77fd4f6",path:"/categories/消息队列/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-c77fd4f6").then(t)},meta:{pid:"categories",id:"消息队列"}},{path:"/categories/消息队列/index.html",redirect:"/categories/消息队列/"},{name:"v-6f5d94e8",path:"/categories/云原生/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-6f5d94e8").then(t)},meta:{pid:"categories",id:"云原生"}},{path:"/categories/云原生/index.html",redirect:"/categories/云原生/"},{name:"v-7fc479ec",path:"/categories/其他/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-7fc479ec").then(t)},meta:{pid:"categories",id:"其他"}},{path:"/categories/其他/index.html",redirect:"/categories/其他/"},{name:"v-f37f30be",path:"/categories/算法/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-f37f30be").then(t)},meta:{pid:"categories",id:"算法"}},{path:"/categories/算法/index.html",redirect:"/categories/算法/"},{name:"v-5db6d2ed",path:"/categories/数据同步/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-5db6d2ed").then(t)},meta:{pid:"categories",id:"数据同步"}},{path:"/categories/数据同步/index.html",redirect:"/categories/数据同步/"},{name:"v-60c96f6a",path:"/categories/CICD/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-60c96f6a").then(t)},meta:{pid:"categories",id:"CICD"}},{path:"/categories/CICD/index.html",redirect:"/categories/CICD/"},{name:"v-2dd62492",path:"/categories/存储引擎/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-2dd62492").then(t)},meta:{pid:"categories",id:"存储引擎"}},{path:"/categories/存储引擎/index.html",redirect:"/categories/存储引擎/"},{name:"v-03f2e000",path:"/categories/nosql/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-03f2e000").then(t)},meta:{pid:"categories",id:"nosql"}},{path:"/categories/nosql/index.html",redirect:"/categories/nosql/"},{name:"v-c1e9d86a",path:"/categories/计算引擎/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-c1e9d86a").then(t)},meta:{pid:"categories",id:"计算引擎"}},{path:"/categories/计算引擎/index.html",redirect:"/categories/计算引擎/"},{path:"*",component:Pe}],Be={title:"Gordon",description:"会思考的芦苇。",base:"./",headTags:[["link",{rel:"icon",href:"./favicon.ico"}],["meta",{name:"viewport",content:"width=device-width,initial-scale=1,user-scalable=no"}],["meta",{name:"robots",content:"all"}],["meta",{name:"apple-mobile-web-app-capable",content:"yes"}],["script",{},'\n            var _hmt = _hmt || [];\n            (function() {\n              var hm = document.createElement("script");\n              hm.src = "https://hm.baidu.com/hm.js?55943ae09e5901d7a9f5705133737eec";\n              var s = document.getElementsByTagName("script")[0];\n              s.parentNode.insertBefore(hm, s);\n            })();\n           '],["script",{src:"https://hm.baidu.com/hm.js?xxxxxxxxxxx"}]],pages:[{title:"IDEA的使用技巧",frontmatter:{title:"IDEA的使用技巧",date:"2019-01-01T00:00:00.000Z",publish:!1},regularPath:"/tool/IDEA/IDEA%E7%9A%84%E4%BD%BF%E7%94%A8.html",relativePath:"tool/IDEA/IDEA的使用.md",key:"v-aae13ec4",path:"/2019/01/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8/",headers:[{level:2,title:"配置Java方法注释",slug:"配置java方法注释"},{level:2,title:"配置Swagger模板",slug:"配置swagger模板"}],lastUpdated:"2023-6-23 6:36:19 ├F10: PM┤",lastUpdatedTimestamp:1687516579e3,content:" 配置模板 \n 配置Java方法注释 \n \n /**\n * description:\n $params$\n * @return $return$\n *\n * @author $USER$\n * Date: $DATE$ $TIME$\n */ \n \n 1 2 3 4 5 6 7 8 \n params的脚本： \n groovyScript ( \"\n def  result = '' ;  \n def  params = \\\" $ { _1 } \\\" . replaceAll ( '[\\\\\\\\[|\\\\\\\\]|\\\\\\\\s]' ,   '' ) . split ( ',' ) . toList ( ) ;  \n for ( i  =   0 ;  i  <  params . size ( ) ;  i ++ )   { \n    result += '* @param '   +  params [ i ]   +   ( ( i  <  params . size ( )   -   1 )   ?   '\\\\n '   :   '' ) \n } ; \n return  result\" ,   methodParameters ( ) ) methodParameters ( ) ) \n \n 1 2 3 4 5 6 7 快捷键java，即可调用java的JavaDoc注释 \n 配置Swagger模板 \n \n @ApiOperation ( value  =   \"\" ,  notes  =   \"\" ) \n$swaggerParam$\n \n 1 2 \n swaggerParam脚本: \n groovyScript ( \"\ndef result = '' ; \ndef params = \\\"$ { _1 } \\\" . replaceAll ( ' [ \\\\\\\\ [ | \\\\\\\\ ] | \\\\\\\\s ] ', ' ' ) . split ( ',' ) . toList ( ) ;  \ndef paramStr  = '' ; \n for ( i  =   0 ;  i  <  params . size ( ) ;  i ++ ) { \n\tparamStr += ' @ApiImplicitParam ( name  =  \\\"'  +  params [ i ]   +  '\\\"\\ , value  =  \\\"'  +  params [ i ]   +   '\\\"\\)'   +   ( ( i  <  params . size ( )   -   1 )   ?  '\\ , \\\\n         ' : ' \\\\n' ) \n } ; \nresult  += ''  + ( ( params . size ( ) > 1 )    ?  ' @ApiImplicitParams ( { \\\\n        ' + paramStr + '})' :  '' + paramStr + ' ')+' ' ; \n return  result ; \n\" , methodParameters ( ) ) ; \n \n 1 2 3 4 5 6 7 8 9 10 快捷键swf即可生成swagger的接口方法注释 \n"},{title:"Home",frontmatter:{home:!0,heroText:null,tagline:"愿你付出甘之如饴，所得归于欢喜。",bgImageStyle:{height:"450px"},isShowTitleInHome:!1,permalink:"/"},regularPath:"/",relativePath:"README.md",key:"v-af65573c",path:"/",lastUpdated:"2023-6-23 6:36:19 ├F10: PM┤",lastUpdatedTimestamp:1687516579e3,content:" \n"},{title:"我的常用Emoji",frontmatter:{title:"我的常用Emoji",date:"2022-03-09T00:00:00.000Z",publish:!1},regularPath:"/tool/emoji/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji.html",relativePath:"tool/emoji/我的常用emoji.md",key:"v-0ea853f1",path:"/2022/03/09/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji/",headers:[{level:2,title:"参看",slug:"参看"},{level:2,title:"小黄脸",slug:"小黄脸"},{level:2,title:"记录",slug:"记录"}],lastUpdated:"2023-6-23 6:36:19 ├F10: PM┤",lastUpdatedTimestamp:1687516579e3,content:" 参看 \n \n Emoji中文网 \n https://gist.github.com/rxaviers/7360908 \n \n 总结个人常用Emoji \n 小黄脸 \n \n     🤪 滑稽 \n     😀 微笑 \n     😂 笑哭 \n     😳 脸红 \n     😵‍💫 晕 \n     😎 墨镜 \n     😲 震惊 \n     🙄 翻白眼 \n     😏 得意 \n     😱 吓死了 \n     😠 生气 \n 记录 \n \n     ✍️ 写字 \n     📚 书 \n \n"},{title:"利用MarkDown收集常用的Emoji",frontmatter:{title:"利用MarkDown收集常用的Emoji",date:"2022-03-12T00:00:00.000Z",categories:["tool"],tags:["markdown"],subSidebar:!1},regularPath:"/tool/markdown/%E5%88%A9%E7%94%A8MarkDown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84Emoji.html",relativePath:"tool/markdown/利用MarkDown收集常用的Emoji.md",key:"v-0cbdd054",path:"/2022/03/12/%E5%88%A9%E7%94%A8markdown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84emoji/",headers:[{level:2,title:"背景",slug:"背景"},{level:2,title:"emoji合集",slug:"emoji合集"},{level:2,title:"我常用emoji",slug:"我常用emoji"},{level:2,title:"如何收集常用的emoji？",slug:"如何收集常用的emoji"},{level:3,title:"方式一：使用表格（不推荐）",slug:"方式一-使用表格-不推荐"},{level:3,title:"方式二：使用表格布局（推荐）",slug:"方式二-使用表格布局-推荐"},{level:3,title:"方式三：flex布局（极其推荐）",slug:"方式三-flex布局-极其推荐"},{level:2,title:"参看：",slug:"参看"}],excerpt:'<div class="custom-block tip"><p class="title"></p><ol>\n<li>总结常用的emoji</li>\n<li>讲述如何利用 markdown 收集常用的Emoji，并形成网格布局</li>\n</ol>\n</div>',lastUpdated:"2023-6-23 6:36:19 ├F10: PM┤",lastUpdatedTimestamp:1687516579e3,content:' \n 总结常用的emoji \n 讲述如何利用 markdown 收集常用的Emoji，并形成网格布局 \n 背景 \n \n Emoji是一种文本类型的象形符号， 它和图片、表情包不同，它能够在任何文本输入的地方使用，因为它本身就是一种文字 ；最新的emoji 13.0版本包含1814个独立的表情符号（该统计不包含由多符号组成和其他肤色的表情符号），emoji能够在各个不同的操作平台上显示，但显示效果会有一些不同。 \n \n 看到过许多的 MarkDown 文件里都引用格式各样的emoji，也看到过许多的github网站的仓库简介也都使用emoji，如下： \n \n 这样的emoji，要么是  Unicode  格式，要么需要直接复制粘贴去使用。 \n 对于我来说，更多使用的场景是用来 书写MarkDown。 \n 所以我就在思考： 怎么使用 MarkDown 来收集常用的 emoji，并且按照 类似于表格那样排列，然后便于复制粘贴使用？ \n 需求： \n \n \n 使用 MarkDown 收集常用的 emoji \n \n \n 收集的 emoji 采用表格/网格布局 \n \n \n 想要的格式类似于  Emoji中文网  的排列（ 上边一行emoji，下边一行描述 ），如下： \n \n \n \n \n \n 在 MarkDown 文件中，可以随意使用复制的 emoji \n emoji合集 \n \n Emoji中文网 \n https://gist.github.com/rxaviers/7360908 \n 我常用emoji \n 我的常用Emoji \n 如何收集常用的emoji？ \n 在  Emoji中文网  中可以找到各个类型的emoji，重点在于展示的方式。 \n 方式一：使用表格（不推荐） \n 使用表格的话，我们可以直接使用 MarkDown 的语法来做： \n |        |        |        | \n |   ----   |   ----   |   ----   | \n |        |        |        | \n |        |        |        | \n |        |        |        | \n \n 1 2 3 4 5 但是这样的语法， 不能修改 表格内容 在网页中的样式 ，即默认的图标太小，观感体验很不友好。 \n 当然也可以使用 HTML 方式写表格的语法，这样就可以修改表格内容的样式， MarkDown 也天然支持这样的渲染方式。 \n < table   border = " 1px "   align = " center "   bordercolor = " black "   width = " 80% "   height = " 100px " > \n     < tr   align = " center " > \n         < td > 🤪 </ td > \n         < td > 😀 </ td > \n         < td > 😂 </ td > \n     </ tr > \n     < tr   align = " center " > \n         < td > 😳 </ td > \n         < td > 😠 < br > </ td > \n         < td > 😱 </ td > \n     </ tr > \n </ table > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 这样的方式虽然可以实现上班的需求，再配合CSS修改样式。 \n 但是最大的缺点就是，每次不仅要复制  <td>😱</td>  还是要复制 <tr></tr> ， 这样的代码会把行和列定死，需要再次想修改行列数时，非常麻烦 。 \n 方式二：使用表格布局（推荐） \n 话不多说，先亮代码： \n < div   style = " display : grid ; grid-template-columns :   repeat ( auto-fill ,  12.5% ) ; font-size : 30px ; justify-items : center ; align-items : center ; line-height : normal ; text-align : center " > \n     < span > 🤪 < br > 滑稽 </ span > \n     < span > 😀 < br > 微笑 </ span > \n     < span > 😂 < br > 笑哭 </ span > \n     < span > 😳 < br > 脸红 </ span > \n     < span > 😵‍💫 < br > 晕 </ span > \n     < span > 😎 < br > 墨镜 </ span > \n     < span > 😲 < br > 震惊 </ span > \n     < span > 🙄 < br > 翻白眼 </ span > \n     < span > 😏 < br > 得意 </ span > \n     < span > 😱 < br > 吓死了 </ span > \n     < span > 😠 < br > 生气 </ span > \n </ div > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 HTML内部代码自动适应行列，关键代码： \n grid-template-columns :   repeat ( auto-fill ,  10% ) ; \n \n 1 10%  即将内容分为10列。 \n 如果想要5列，那么修改成 20% 即可。 \n 效果图： \n \n 但还是有缺点，如果10列放不下会出现拥挤情况，尤其在手机端显示不友好。 \n 方式三：flex布局（极其推荐） \n < div   style = " display :  flex ; flex-direction :  row ; flex-wrap :  wrap ; justify-content :  flex-start ; text-align :  center ; font-size :  30px ; line-height : normal ; " > \n     < span   style = " flex-basis :  10% ; " > 🤪 < br > 滑稽 </ span > \n     < span   style = " flex-basis :  10% ; " > 😀 < br > 微笑 </ span > \n     < span   style = " flex-basis :  10% ; " > 😂 < br > 笑哭 </ span > \n     < span   style = " flex-basis :  10% ; " > 😳 < br > 脸红 </ span > \n     < span   style = " flex-basis :  10% ; " > 😵‍💫 < br > 晕 </ span > \n     < span   style = " flex-basis :  10% ; " > 😎 < br > 墨镜 </ span > \n     < span   style = " flex-basis :  10% ; " > 😲 < br > 震惊 </ span > \n     < span   style = " flex-basis :  10% ; " > 🙄 < br > 翻白眼 </ span > \n     < span   style = " flex-basis :  10% ; " > 😏 < br > 得意 </ span > \n     < span   style = " flex-basis :  10% ; " > 😱 < br > 吓死了 </ span > \n     < span   style = " flex-basis :  10% ; " > 😠 < br > 生气 </ span > \n </ div > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 #  参看： \n \n 推荐!!! Markdown图标索引网站 \n \n'},{title:"git使用小技巧",frontmatter:{title:"git使用小技巧",date:"2022-03-09T00:00:00.000Z",publish:!1},regularPath:"/tool/git/git%E5%9C%A8.gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8.html",relativePath:"tool/git/git在.gitignore添加忽略文件不起作用.md",key:"v-0bf5ebde",path:"/2022/03/09/git%E5%9C%A8-gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/",headers:[{level:2,title:"问题描述",slug:"问题描述"},{level:2,title:"原因",slug:"原因"},{level:2,title:"解决办法",slug:"解决办法"},{level:3,title:"方式一",slug:"方式一"},{level:3,title:"方式二",slug:"方式二"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' 问题描述 \n 使用git在.gitignore添加忽略文件不起作用，在此commit还是会提交，而且在IDEA中已经添加了的文件或者目录没有变灰，即没有忽略成功。 \n 原因 \n .gitignore文件只对还没有加入版本管理的文件起作用，如果之前已经用git把这些文件纳入了版本库，就不起作用了。说明在git库中已存在了这个文件，之前push提交过该文件。即文件已经被track（追踪）。 \n 解决办法 \n 方式一 \n 将需要忽略的文件先复制出来，然后删除项目目录下的这些需要忽略的文件。（如果是编译后的文件，直接删除就可以）。 \n 在.gitignore文件中添加对应的需要忽略的文件或者目录。 \n 提交所做的修改到本地版本库，然后推送到远程的版本库。 \n 如果文件变为灰色，说明成功 \n 不过这样并不是很稳妥，而且文件多的话，比较麻烦。 \n 方式二 \n 思路：删除本地缓存，重新提交。 \n \n 使用命令工具Git Bash，进入需要修改的工作目录 \n 重置所有缓存（注意后面有个.） \n \n git rm  - r  -- cached  . \n \n 1 \n 将本地代码重新添加（注意后面有个.） \n \n git add  . \n \n 1 \n 提交（让 .gitignore 文件夹生效，读取忽略文件） \n \n git commit  - m  ".gitignore is now working" \n \n 1 此时，发现刚才忽略不掉的文件，已经变灰，说明忽略成功。 \n 注意： \n 注意步骤2也可不删除全部缓存，只删除指定的缓存，例如： \n git rm  - r  -- cached  * / target / \n \n 1 '},{frontmatter:{},regularPath:"/aboutme.html",relativePath:"aboutme.md",key:"v-46e67ace",path:"/1970/01/01/aboutme/",headers:[{level:2,title:"📝关于内容",slug:"关于内容"},{level:2,title:"🙊关于笔者",slug:"关于笔者"},{level:2,title:"🔗个人相关链接",slug:"个人相关链接"},{level:2,title:"☎️联系我",slug:"联系我"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:"  Gordon  \n 思考的芦苇 \n 📝关于内容 \n 大数据相关 \n 🙊 关于笔者 \n 不断学习，保持新鲜。 \n 🔗个人相关链接 \n \n 个人博客 \n ☎️联系我 \n 如对博客内容，知识，排版等有疑问或者建议，欢迎邮件和我联系 \n 邮箱:123456789@qq.com \n"},{title:"安利一些电子图书下载网站",frontmatter:{title:"安利一些电子图书下载网站",date:"2022-03-26T00:00:00.000Z",categories:["软件资源","随笔"],tags:["推荐","资源分享"]},regularPath:"/tool/resource/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99.html",relativePath:"tool/resource/安利一些电子图书下载网站.md",key:"v-6048fa40",path:"/2022/03/26/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99/",headers:[{level:2,title:"电子书需求的背景",slug:"电子书需求的背景"},{level:3,title:"技术需要",slug:"技术需要"},{level:3,title:"小说爱好",slug:"小说爱好"},{level:3,title:"一般下载电子书的不好体验",slug:"一般下载电子书的不好体验"},{level:2,title:"安利一些电子书的下载网站",slug:"安利一些电子书的下载网站"},{level:3,title:"一个综合性电子图书网站——Z-Library",slug:"一个综合性电子图书网站-z-library"},{level:3,title:"鸠摩搜索（远不如上边全）",slug:"鸠摩搜索-远不如上边全"},{level:3,title:"小说下载",slug:"小说下载"},{level:2,title:"特别说明",slug:"特别说明"}],excerpt:'<div class="custom-block tip"><p class="title"></p><p><strong>什么事情不仅香，还能提升自己的幸福感？</strong>  对于大多数人来说，有一点应该毋庸置疑——<strong>白嫖</strong>。</p>\n<p>相信很多人都有看电子书的习惯，但是往往不能在某一个大而全的网站直接下载。今天就安利一下：</p>\n<p>各类电子书籍下载：<a href="https://zh.3lib.net/" target="_blank" rel="noopener noreferrer">https://zh.3lib.net/<OutboundLink/></a></p>\n<p>小说下载：<a href="https://m.ibookben.com/" target="_blank" rel="noopener noreferrer">https://m.ibookben.com/<OutboundLink/></a></p>\n</div>',lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:" 什么事情不仅香，还能提升自己的幸福感？   对于大多数人来说，有一点应该毋庸置疑—— 白嫖 。 \n 相信很多人都有看电子书的习惯，但是往往不能在某一个大而全的网站直接下载。今天就安利一下： \n 各类电子书籍下载： https://zh.3lib.net/ \n 小说下载： https://m.ibookben.com/ \n 电子书需求的背景 \n 技术需要 \n 因为自身是做IT编程相关的，这个行业也是技术更新迭代特别快，激流勇进的一个行业。如果不能持续进行学习，那么很可能就会被慢慢淘汰，所以平时会看会多技术相关的书籍。 \n 纸质书的话，他的阅读体验会很好，有一种莫名的氛围感和也方便用笔写下自己的灵感；但是有一个比较大的缺点， 纸质书不方便将内容和笔记保存在文档中，就像我们平时会写比较多的MarkDown文件 。所以我平时还是看电子书多一些。 \n 小说爱好 \n 我应该算是一个“资深的小说阅读者”了吧。看过非常多的小说： \n \n 玄幻类的像天蚕土豆、唐家三少、风凌天下……他们的小说基本都看完了 \n 网游、都市、修仙、武侠、末世 类的小说也看过很多 \n 有一段时间特别喜欢看雷军和小米相关的小说——《一往无前》《小米之道》《雷军传》 \n 最近特别喜欢看  孑与2  的历史军事类 小说——《明天下》（read，目前我看过小说中觉得最好的）、《大宋的智慧》（reading） \n \n \n 可能未来会专门写一篇文章 回忆和介绍 我之前看过很多不错的小说，敬请期待…… \n \n 正因为非常喜欢看小说，但是往往很多小说又要收费，所以获得一些下载 电子版小说就尤为重要。 \n 一般下载电子书的不好体验 \n 但是下载电子书总会有一个很不好的体验: \n \n 直接百度下载过程繁琐 \n 很容易遇到一些假的书源和广告 \n 下载一本电子书往往非常耗时 \n 很多国内的电子书网站容易被封禁和限制 \n \n 比如这个我以前经常用来下载的网站： \n 安利一些电子书的下载网站 \n 一个综合性电子图书网站—— Z-Library \n 废话不多说，看如下图介绍： \n \n 网站地址 ： \n \n 中文镜像地址： https://zh.3lib.net/ \n 主站（英文）： http://libgen.st/ \n \n 网站好处如下 ： \n \n 收录全球超过  9,945,695的书籍  和  84,837,000的文章 \n 重点： 常用的开发相关的技术书籍，在这里基本都可以找到 \n 推荐理由 ：游客每天免费下载5本，注册用户每天下载10本；次日刷新 \n \n 注意事项： \n \n 大多数情况下，需要翻墙 \n 因为收录全球各种类型的图书，可能存在  黄赌毒  内容； 谨慎、正确使用 \n 鸠摩搜索（远不如上边全） \n 另一个推荐的电子书下载网站： 鸠摩搜索 \n 不用翻墙，但是藏书量远远不如上文的。 \n 小说下载 \n 免费的小说阅读器 \n 软件名字：全本免费小说阅读器 \n 一般长这个样子，或者是绿色的图标（一些应用商店可以下载） \n \n \n 一般来说一些小说都是由版权的，如果软件直接提供了这样的书籍阅读，一般是会被告侵权作下架处理的。 \n \n 但是，这款软件牛在，本身并不提供一些具有版权的小说供读者阅读。 \n 推测软件逻辑 ： \n \n 全网进行搜索 \n 爬取指定网站内容的小说资源（这个软件搜索后，会有很多小说源供你选择） \n 对爬取内容进行转码 \n 阅读优化（这个阅读器的使用总体还不错） \n \n 缺点 ： \n \n 会有一些比较少的广告，但不是很影响。使用体验还是很好地\n \n 一般看一个视频就可以取消广告指定时间 \n 如果你不看视频（15广告），也就是小说也面会有亿点点广告（不影响） \n \n \n 阅读体验还是比不上  微信读书 \n \n 使用场景： \n \n 适合阅读  正在更新的小说 （每天的推更还是非常及时的） \n 小说下载网站（微信读书 阅读） \n 对于全本小说，或者是不追更的小说，我还是非常推荐这样的方式的 。毕竟，微信读书是真的好用，各端导入导出也非常方便。 \n 小说下载地址： https://m.ibookben.com/ \n 这个完站很全，大多数小说都可以搜到。 \n \n 虽然是 手机端的UI，但是实测： 手机、电脑都可正常下载TXT文件 。 \n 然后将下载好的 TXT文件，导入到微信读书里，就可以进行愉快的阅读了。 \n 特别说明 \n 本文推荐的网站，只是用于个人兴趣使用，不负任何责任。 \n"},{title:"githup自定义美化个人主页",frontmatter:{title:"githup自定义美化个人主页",date:"2022-03-09T00:00:00.000Z",publish:!1},regularPath:"/tool/git/GitHub%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5.html",relativePath:"tool/git/GitHub自定义美化个人主页.md",key:"v-5ed1fdaa",path:"/2022/03/09/github%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/",headers:[{level:2,title:"背景",slug:"背景"},{level:2,title:"如何实现GitHub自定义主页",slug:"如何实现github自定义主页"},{level:2,title:"自定义模板",slug:"自定义模板"},{level:3,title:"我的模板",slug:"我的模板"},{level:3,title:"推荐模板仓库",slug:"推荐模板仓库"},{level:3,title:"如何自定义模板",slug:"如何自定义模板"},{level:2,title:"参考",slug:"参考"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:" 背景 \n 偶然间在浏览谷歌浏览器的插件CSDN，里边有个网站商城，其中有个网站叫做 leetcode题解 ，这个一个 大佬 写的非常不错的算法学习的网站，有兴趣可以看看。最近刚好在研究算法，被大佬写的学习方法和题解深深折服，在进入大佬的GitHub主页的时候，发现他的GitHub主页是自定义的，和默认的不一样。于是对此就充满了好奇。 \n 初步看，实现这样的功能，好像是有一个和用户名一样的仓库，里边有一个README.md文件来实现的。于是乎就进行了一番搜索，最终实现了自己的GitHub自定义主页。 \n \n 谷歌浏览器CSDN插件-网站商城 \n \n 大佬的GitHub主页 \n \n 我的自定义GitHub主页 \n 如何实现GitHub自定义主页 \n 其实相当简单，只需要创建一个 同名仓库 。在仓库下的 README.md 文件中进行自定义内容。 \n \n我这里已经创建了同名的仓库所以上面会出现警告，记得下面红框的要勾选，初始化一个README文件，这个文件就可以自定义主页。 \n 自定义模板 \n 内容方面，GitHub 给出了默认的欢迎内容，同时提供了一些建议和提示，这一部分默认被注释掉了。 \n < ! -- \n * * shiwei - Ren / shiwei - Ren * *  is a ✨ _special_ ✨ repository because its ` README . md`  ( this  file )  appears on your  GitHub   profile . \n\nHere  are some ideas  to   get  you started : \n\n -  🔭  I ’m currently working on  . . . \n -  🌱  I ’m currently learning  . . . \n -  👯  I ’m looking  to   collaborate  on  . . . \n -  🤔  I ’m looking  for  help  with   . . . \n -  💬  Ask  me about  . . . \n -  📫  How   to   reach  me :   . . . \n -  😄  Pronouns :   . . . \n -  ⚡  Fun  fact :   . . . \n -- > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #  我的模板 \n https://github.com/duktig666/duktig666 \n 推荐模板仓库 \n \n https://github.com/kautukkundan/Awesome-Profile-README-templates \n https://github.com/rahuldkjain/github-profile-readme-generator \n 如何自定义模板 \n 自定义图标 \n 经常在浏览GitHub时，发现别人项目中的 README.md 文件总能显示一些特殊的图标和统计数据，这些在自定义主页时也可以使用。 \n README.md 中特殊图标的使用，来源于 Simple Icons ，使用的是  Shields IO  语法。可以看到有很多的用法，具体如下： \n \n \n Build : 可视化自定义样式 \n \n \n Code Coverage : 代码覆盖率样式 \n \n \n Analysis : 数据解析样式 \n \n \n Chat : 状态展示样式 \n \n \n Dependencies : 依赖关系样式 \n \n \n Size : 数值大小样式 \n \n \n Downloads : 下载按钮样式 \n \n \n Funding : 项目金额来源样式 \n \n \n Issue Tracking : 问题状态和追踪样式 \n \n \n License : 许可证样式 \n \n \n Rating : 标星样式 \n \n \n Social : 社区信息样式 \n \n \n Version : 版本号信息样式 \n \n \n Platform & Version Support : 平台支持和版本支持样式 \n \n \n Monitoring : 监控状态和方式样式 \n \n \n Activity : 项目活动样式 \n \n \n Other : 其他自定义样式 \n 统计工具 \n 关于GitHub相关数据统计显示可以参考这篇文章 GitHub Readme Stats （写的很详细）。 \n 根据这篇文章可以实现如下图类似的特效： \n \n 个人GitHub统计卡片 \n \n 个人GitHub使用热门语言 \n 代码统计 \n 代码统计需要运行 GitHub Action，GitHub 的相关教程可以先参考阮一峰老师的  GitHub Actions 入门教程 。 \n 后续更新........ \n 统计访问个人主页数量 \n ! [ 描述 ] ( https : / / komarev . com / ghpvc / ? username = your - github - username & color = green ) \n \n 1 \n 自定义表情 \n 可参考： Github表情语法专栏 \n 扩展用法 \n 参考： https://github.com/abhisheknaiidu/awesome-github-profile-readme \n 参考 \n \n GitHub主页美化 \n https://sanii.cn/article/302 \n https://ld246.com/article/1602996971277 \n GitHub 自定义首页，结合 GitHub Action 更香 \n https://juejin.cn/post/6844904035103801352 \n https://juejin.cn/post/6844904114711691272 \n \n"},{title:"kafka",frontmatter:{title:"kafka",date:"2022-07-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["中间件","消息队列"],tags:["消息队列","生产者-消费者"]},regularPath:"/%E4%B8%AD%E9%97%B4%E4%BB%B6/kafka.html",relativePath:"中间件/kafka.md",key:"v-7d8ca27f",path:"/2022/07/08/kafka/",headers:[{level:2,title:"kafka的基本介绍",slug:"kafka的基本介绍"},{level:2,title:"kafka的安装",slug:"kafka的安装"},{level:2,title:"kafka的操作",slug:"kafka的操作"},{level:3,title:"kafka基本shell命令",slug:"kafka基本shell命令"},{level:3,title:"kafka的基准测试",slug:"kafka的基准测试"},{level:3,title:"kafka的 java API的操作",slug:"kafka的-java-api的操作"},{level:2,title:"kafka的核心概念",slug:"kafka的核心概念"},{level:3,title:"Kafka文件存储机制",slug:"kafka文件存储机制"},{level:3,title:"Kafka生产者",slug:"kafka生产者"},{level:3,title:"Broker保存消息",slug:"broker保存消息"},{level:3,title:"Kafka消费者",slug:"kafka消费者"},{level:2,title:"Kafka的优秀架构",slug:"kafka的优秀架构"},{level:3,title:"高可用",slug:"高可用"},{level:3,title:"高并发",slug:"高并发"},{level:3,title:"高性能",slug:"高性能"},{level:2,title:"生产者分区写入策略",slug:"生产者分区写入策略"},{level:2,title:"消费者组Rebalance机制",slug:"消费者组rebalance机制"},{level:2,title:"消费者分区分配策略",slug:"消费者分区分配策略"},{level:2,title:"副本机制",slug:"副本机制"},{level:3,title:"kafka如何保证数据不丢失",slug:"kafka如何保证数据不丢失"},{level:2,title:"kafka的消息存储和查询机制",slug:"kafka的消息存储和查询机制"},{level:2,title:"kafka的消费端数据积压",slug:"kafka的消费端数据积压"},{level:2,title:"Kafka配额限速机制（Quotas）",slug:"kafka配额限速机制-quotas"},{level:3,title:"限制producer端速率",slug:"限制producer端速率"},{level:3,title:"限制consumer端速率",slug:"限制consumer端速率"},{level:3,title:"取消Kafka的Quota配置",slug:"取消kafka的quota配置"},{level:2,title:"Kafka中数据清理",slug:"kafka中数据清理"},{level:3,title:"日志删除",slug:"日志删除"},{level:2,title:"Kafka topic分区数设置和机器数计算",slug:"kafka-topic分区数设置和机器数计算"},{level:2,title:"陌陌项目",slug:"陌陌项目"},{level:3,title:"陌陌案例基本介绍",slug:"陌陌案例基本介绍"},{level:3,title:"陌陌案例数据源介绍",slug:"陌陌案例数据源介绍"},{level:3,title:"陌陌案例架构说明",slug:"陌陌案例架构说明"},{level:3,title:"陌陌案例数据采集操作",slug:"陌陌案例数据采集操作"},{level:3,title:"陌陌案例_接收消息, 写入到HBase",slug:"陌陌案例-接收消息-写入到hbase"},{level:3,title:"陌陌案例_与Phoenix整合完成即席操作",slug:"陌陌案例-与phoenix整合完成即席操作"},{level:3,title:"陌陌案例_与hive整合完成离线查询",slug:"陌陌案例-与hive整合完成离线查询"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:' 前言 \n 消息队列产生的背景 \n 1、解耦合 \n2、异步处理 \n例如电商平台，秒杀活动。 \n一般流程会分为： \n1：风险控制、2：库存锁定、3：生成订单、4：短信通知、5：更新数据 \n通过消息系统将秒杀活动业务拆分开，将不急需处理的业务放在后面慢慢处理； \n流程改为： \n1：风险控制、2：库存锁定、3:消息系统、4:生成订单、5：短信通知、6：更新数据 \n3、流量的控制 \n3.1 网关在接受到请求后，就把请求放入到消息队列里面 \n3.2 后端的服务从消息队列里面获取到请求，完成后续的秒杀处理流程。然后再给用户返回结果。 \n优点：控制了流量 \n缺点：会让流程变慢 \n \n 常见的消息队列的产品 \n \n \n \n activeMQ:  由apache开源的一款消息队列的产品, 出现时间较早, 目前此款使用人群较少, 社区活跃度降低 \n \n \n \n \n RabbitMQ: 目前此款消息队列的产品 在java领域使用非常频繁的 \n \n \n \n \n RocketMQ: 由 阿里出品一款消息队列的产品, 目前仅在阿里范围内广泛使用, 其他人群是较少, 支持的客户端较少, 目前仅支持 java, c不成熟, \n \n \n \n \n kafka :  是一款大数据的消息队列的产品  在大数据领域中一统天下 , kafka存在数据重复消费的问题 \n \n \n \n 消息队列的两种模式 \n ​\t在java中对于消息队列, 专门有一套协议来支撑: JMS(java massage server) \n \n 第一种: 点对点\n \n 一条消息 最终只被一个消费者所消费的模型, 称为点对点 \n 例如: 打电话 .... \n \n \n 第二种: 发布订阅\n \n 一条消息 最终可以被多个消费者所消费的模型, 称为发布订阅 \n 例如: 群发  广播 \n kafka的基本介绍 \n ​\t\t官方网址: http://www.kafka.apache.org \n ​\t    kafka最初是来源于领英公司出品的一款消息队列的产品, 使用Scala语言编写 , kafka仅是对JMS规范实现了一部分, 并不是完整的实现,  kafka可以构建成集群, 可以提供更高并发处理能力, kakfa依赖于zookeeper \n kafka本质上来讲就是一款消息传递的中间件, 负责将数据从一端传递另一端过程\n \n kafka的特点: \n \n \n \n 可靠性: 数据不容易丢失  集群也是非常稳定, 宕机风险较低 \n \n \n \n \n 可扩展性: 非常方便的扩展kafka集群的节点 \n \n \n \n \n 耐用性:  数据都是存储在磁盘上, 持久化存储的 \n \n \n \n \n 性能:  单台可达 10w 并发 还可以通过扩展节点, 提升更高的性能, 保证零停机, 零数据丢失 \n kafka的安装 \n 易错点: \n \n \n \n 最后 server.properties中 路径不要写错了 \n \n \n \n \n 监听地址的前面的注释不要忘记打开 \n \n \n \n \n 分发之后, 不要忘记修改每个server.properties的 id 和 监听地址 \n \n \n \n 如何启动 \n 1)   启动zookeeper: 需要保证zookeeper是启动良好的 \n\n 2)   启动kafka集群:  三个节点都需要执行的 \n    cd   /export/server/kafka_2.12-2.4.1/bin \n   \n    前台启动 : \n    \t\t./kafka-server-start.sh   ../config/server.properties \n    后台启动 : \n    \t\tnohup   ./kafka-server-start.sh ../config/server.properties 2>&1 & \n   \t\t\n    注意 :   在启动的时候, 建议先进行前台启动, 如果没有报出任何错误, 直接挂载在后台 \n   \n 3)   如何查看 kafka是否已经启动:   \n\n 方式一 :    通过JPS  三个节点都得测试 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n 方式二 :   通过zookeeper来检查 \n \n 1 \n \n kafka的一键化启动脚本: \n \n 第一步 :   在Linux中创建一个目录: \n    mkdir   -p /export/onekey \n    cd   /export/onekey \n\n 第二步 :   将资料中 kafka一键化启动和停止脚本中三个文本, 全部上传到此目录中 \n    rz   上传 \n 第三步 :   赋权限 \n    chmod    755 * \n\n 第四步 :   即可测试 \n\n\n 注意 :   使用一键化脚本, 也得需要先启动zookeeper \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 #  kafka的操作 \n kafka基本shell命令 \n \n \n \n 如何创建 topic \n \n \n \n cd  /export/server/kafka_2.12-2.4.1/bin\n执行: \n./kafka-topics.sh   --create   --zookeeper  node1:2181,node2:2181,node3:2181   --topic  test01  --partitions   3  --replication-factor  2 \n\n参数说明:\n --partitions   :  指定topic的分片数量\n--replication-factor: 指定每个分片下有几个副本   ( 包含本身 ) \n \n 1 2 3 4 5 6 7 \n \n \n 通过 shell 模拟一个消费者 \n \n \n \n ./kafka-console-consumer.sh  --bootstrap-server node1:9092,node2:9092,node3:9092  --topic   test01\n \n 1 \n \n \n 通过shell模拟一个生产者 \n \n \n \n ./kafka-console-producer.sh --broker-list node1:9092,node2:9092,node3:9092  --topic  test01\n \n 1 \n \n \n 查看当前所有的topic: \n \n \n \n ./kafka-topics.sh   --list   --zookeeper  node1:2181,node2:2181,node3:2181\n \n 1 \n \n \n 查看某一个topic的详细信息 \n \n \n \n ./kafka-topics.sh   --describe   --zookeeper  node1:2181,node2:2181,node3:2181  --topic  test01\n \n 1 \n \n \n \n 为某一个topic 增加分片的数量 \n \n \n \n ./kafka-topics.sh   --alter   --zookeeper  node1:2181,node2:2181,node3:2181  --topic  test01   --partitions   4 \n\n注意:\n   可以动态添加分片 无法添加副本\n \n 1 2 3 4 \n \n \n 删除一个topic \n \n \n \n ./kafka-topics.sh   --delete   --zookeeper  node1:2181,node2:2181,node3:2181  --topic  test01\n\n注意:\n  在kafka中 删除一个topic 仅仅是逻辑删除操作 ( 默认行为 ) , 原因: 每个topic中都是由消息数据的, kafka担心直接物理删除将需要数据误删\n  \n  如果想将数据删除, 可以手动删除即可:  \n  \t\t数据放置位置: /export/server/kafka_2.12-2.4.1/data\n  \n  如果想直接执行命令的时候 直接删除物理数据:\n     可以在server.properties中配置一个参数:  \n          delete.topic.enable = true\n \n 1 2 3 4 5 6 7 8 9 10 11 #  kafka的基准测试 \n \n \n \n 创建一个topic \n \n \n \n ./kafka-topics.sh   --create   --zookeeper  node1:2181,node2:2181,node3:2181   --topic  test02  --partitions   3  --replication-factor  1 \n \n 1 \n \n \n 测试其 写入的效率 \n \n \n \n ./kafka-producer-perf-test.sh  --topic  test02 --num-records  5000000   --throughput   -1  --record-size  1000  --producer-props  bootstrap.servers = node1.itcast.cn:9092,node2.itcast.cn:9092,node3.itcast.cn:9092  acks = 1 \n \n 1 \n \n 每秒钟: \n    可以处理  37M 数据  数据量为 3.8w的消息数\n\n电脑:\n   CPU: i7  7700k\n   内存:  6.1GB\n   磁盘: 纯固态\n \n 1 2 3 4 5 6 7 \n \n \n 测试读取的效率: \n \n \n \n ./kafka-consumer-perf-test.sh  --topic  test02  --broker-list node1:9092,node2:9092,node3:9092 --fetch-size  1048576    --messages   5000000 \n \n 1 \n \n 总结: \n 1) 写入效率:  副本越多 写入效率约低  将ack调整为 0\n2) 读取效率:  假设broker数量无限,  分区越多 , 效率越高\n \n 1 2 #  kafka的 java API的操作 \n 准备工作: \n \n \n \n 创建项目 导入相关的pom依赖 \n \n \n \n < repositories > \x3c!--代码库--\x3e \n         < repository > \n             < id > aliyun </ id > \n             < url > http://maven.aliyun.com/nexus/content/groups/public/ </ url > \n             < releases > < enabled > true </ enabled > </ releases > \n             < snapshots > \n                 < enabled > false </ enabled > \n                 < updatePolicy > never </ updatePolicy > \n             </ snapshots > \n         </ repository > \n     </ repositories > \n\n     < dependencies > \n\n         < dependency > \n             < groupId > org.apache.kafka </ groupId > \n             < artifactId > kafka-clients </ artifactId > \n             < version > 2.4.1 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.commons </ groupId > \n             < artifactId > commons-io </ artifactId > \n             < version > 1.3.2 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-log4j12 </ artifactId > \n             < version > 1.7.6 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > log4j </ groupId > \n             < artifactId > log4j </ artifactId > \n             < version > 1.2.16 </ version > \n         </ dependency > \n\n     </ dependencies > \n\n     < build > \n         < plugins > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.1 </ version > \n                 < configuration > \n                     < target > 1.8 </ target > \n                     < source > 1.8 </ source > \n                 </ configuration > \n             </ plugin > \n         </ plugins > \n     </ build > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 \n \n \n 创建包: com.itheima.kafka \n \n \n \n 1.1 使用java API 将数据生产到 kafka \n \n \n 代码参考地址: \n http://kafka.apache.org/24/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html \n \n \n 代码实现: \n \n \n // 此类用于将数据生成到kafka中 \n public   class   KafkaProducerTest   { \n\n     public   static   void   main ( String [ ]  args )   { \n\n         //1. 创建kafka生成者核心类对象:   KafkaProducer \n\n         //1.1: 设置相关的配置信息 \n         Properties  props  =   new   Properties ( ) ; \n        \n        props . put ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        props . put ( "acks" ,   "all" ) ;   // ack确认机制, 用于保证生产者在发送数据的不丢失 \n         // 设置 key 和 vlaue 发送数据的序列化类 \n        props . put ( "key.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n        props . put ( "value.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n\n         Producer < String ,   String >  producer  =   new   KafkaProducer < > ( props ) ; \n\n         //2. 发送数据 \n         for   ( int  i  =   0 ;  i  <   100 ;  i ++ )   { \n             //2.1: 发送端 用于承载数据的对象 \n             ProducerRecord < String ,   String >  producerRecord  =   new   ProducerRecord < > ( "test01" ,   Integer . toString ( i ) ) ; \n\n             //2.2: 执行发送 \n            producer . send ( producerRecord ) ; \n         } \n         //3. 释放资源 \n        producer . close ( ) ; \n\n     } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 1.2 使用javaAPI 消费kafka中数据 \n \n \n 代码参考地址: \n http://kafka.apache.org/24/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html \n \n \n 代码实现 \n \n \n // kafka的消费者的代码 \n public   class   KafkaConsumerTest   { \n\n     public   static   void   main ( String [ ]  args )   { \n         //1. 创建kafka的消费者核心类对象:  KafkaConsumer \n\n         //1.1: 设置消费者的配置信息 \n         Properties  props  =   new   Properties ( ) ; \n        \n        props . setProperty ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        props . setProperty ( "group.id" ,   "test" ) ;   // 消费者组 \n        props . setProperty ( "enable.auto.commit" ,   "true" ) ;   // 自动提交 偏移量信息 \n        props . setProperty ( "auto.commit.interval.ms" ,   "1000" ) ;   // 每次提交间隔时间 \n         // 反序列类 \n        props . setProperty ( "key.deserializer" ,   "org.apache.kafka.common.serialization.StringDeserializer" ) ; \n        props . setProperty ( "value.deserializer" ,   "org.apache.kafka.common.serialization.StringDeserializer" ) ; \n\n         KafkaConsumer < String ,   String >  consumer  =   new   KafkaConsumer < > ( props ) ; \n\n         //2. 设置监听那些topic \n        consumer . subscribe ( Arrays . asList ( "test01" ) ) ; \n\n         //3. 获取消息数据 \n         while   ( true )   { \n             //3.1: 通过消费者对象, 获取消息 \n             ConsumerRecords < String ,   String >  records  =  consumer . poll ( Duration . ofMillis ( 100 ) ) ; \n\n             //3.2: 遍历获取每一个数据 \n             for   ( ConsumerRecord < String ,   String >  record  :  records ) { \n                 System . out . println ( "数据为:" + record . value ( )   +   "消息偏移量:" + record . offset ( ) ) ; \n             } \n\n         } \n\n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #  kafka的核心概念 \n \n kafka   cluster: kafka的集群 \n broker :    kakfa中一个个节点 \n producer :   生产者 \n consumer :   消费者 \n topic   :   主题(话题) 类似于一个逻辑容器, 负责数据的存储 \n shard :   分片 \n replicas :   副本 \n zookeeper :   负责元数据的存储和 集群化管理 \n \n 1 2 3 4 5 6 7 8 #  Kafka文件存储机制 \n Topic---Partition—Segment—log+index \n Topic 是逻辑上的概念，而 partition 是物理上的概念. \n 每个 partition 对应于一个 log 文件，该 log 文件中存储的就是 producer 生产的数据。 Producer 生产的数据会被不断追加到该log 文件末端，且每条数据都有自己的 offset 。消费者组中的每个消费者，都会实时记录自己消费到了哪个 offset，以便出错恢复时，从上次的位置继续消费。 \n \n Kafka 采取了 分片和索引 机制，将每个partition 分为多个 segment。每个 segment对应两个文件— “.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名 \n 规则为：topic 名称+分区序号。例如，test这个topic有三个分区，则其对应的文件夹为test-0, test-1, test-2。 \n 如下图： “.index”文件存储大量的索引信息，“.log”文件存储大量的数据 ，索引文件中的元数据指向对应数据文件中 message 的物理偏移地址。 Timemindex根据时间索引判断哪些segment数据，可以定期进行数据清理(TTL) \n Kafka生产者 \n 写入方式 \n producer采用 推（ push ）模式 将消息发布到broker， 每条消息都被追加（ append ）到分区（patition **）中，属于顺序写磁盘**（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。 \n 分区（Partition） \n 消息发送时都被发送到一个topic， 其本质就是一个目录 ，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示： \n \n \n 我们可以看到，每个Partition中的消息都是 有序 的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个 唯一的offset值 。 \n 1）分区的原因 \n （1）方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了； \n （2）可以提高并发，因为可以以 Partition 为单位读写了。 \n 2）分区的原则 \n （1）指定了patition，则直接使用； \n （2）未指定patition但指定key，通过对key的hash值与topic的patition数取余得到partition值；(后面讲解) \n （3）patition和key都未指定，使用轮询选出一个patition，第一次随机生成一个整数，该数值在每次调用会自增，将这个值与topic可用的partition总数取余得到partition值，也就是roundrobin算法(后面讲解) \n 副本（Replication） \n 同一个partition可能会有多个replication（对应 server.properties 配置中的 default.replication.factor=N）。没有replication的情况下，一旦broker 宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication， 而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互 ，其它replication作为follower从leader 中复制数据。 \n 写入流程分析 \n \n 如下kafkatool记录state \n \n 1）producer先从zookeeper的 "/brokers/.../state"节点找到该partition的leader \n 2）producer将消息发送给该leader \n 3）leader将消息写入本地log \n 4）followers从leader pull消息，写入本地log后向leader发送ACK \n 5）leader收到所有 ISR同步副本中的replication的ACK 后， 增加HW（high watermark ，最后commit 的offset）并向producer发送ACK \n 数据可靠性保证 \n 上述的案例我们能看到ACK机制，如何理解？ \n 为保证 producer 发送的数据，能可靠的发送到指定的 topic，topic 的每个 partition 收到 \n producer 发送的数据后，都需要向 producer 发送 ack（acknowledgement 确认收到），如果 \n producer 收到 ack，就会进行下一轮的发送，否则重新发送数据。 \n ​    这里需要理解何时发送ack以及 多少个follwers同步完成后发送ack ? \n (1)  确保有follower与leader同步完成，leader再发送ack，这样才能保证leader挂掉之后，能在follower中选举出新的leader. \n (2)  两种方案：全部followers同步完成以及板书以上followers同步完成。 \n \n Kafka 选择了第二种方案，原因如下： \n 1.同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本(如果有n个机器挂了，半数以上指的是n+1)，而第二种方案只需要 n+1个副本，而 Kafka 的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。 \n 2.虽然第二种方案的*网络延迟会比较高*，但网络延迟对 Kafka 的影响较小。 \n 2）ISR \n 采用第二种方案之后，设想以下情景： leader 收到数据，所有 follower 都开始同步数据， \n 但有一个 follower，因为某种故障，迟迟不能与 leader 进行同步，那 leader 就要一直等下去， \n 直到它完成同步，才能发送 ack。这个问题怎么解决呢？ \n 答案：Leader 维护了一个动态的 in-sync replica set (ISR)，意为和 leader 保持同步的 follower 集 \n 合。当  ISR 中的 follower 完成数据的同步之后，leader 就会给 follower 发送 ack 。如果 follower \n 长时间 未 向 leader 同 步 数 据 ， 则 该 follower 将 被 踢 出 ISR ， 该 时 间 阈 值 由 replica.lag.time.max.ms(心跳时间，默认10s)  参数设定。Leader 发生故障之后，就会从 ISR 中选举新的 leader。 \n 3）ack 应答机制 \n 对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失， \n 所以没必要等 ISR 中的 follower 全部接收成功。 \n 所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡 ， \n 选择以下的配置。 \n acks 参数配置： \n acks： 0：producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟，broker 一接收到还没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据； \n 1：producer 等待 broker 的 ack，partition 的 leader 落盘成功后返回 ack， 如果在 follower同步成功之前 leader 故障，那么将会丢失数据 ； \n \n -1（all）：producer 等待 broker 的 ack，partition 的 leader 和 follower 全部落盘成功后才返回 ack。 但是如果在 follower 同步完成后，broker 发送 ack 之前 ，leader 发生故障，那么会 \n 造成数据重复。 \n 故障处理细节 \n LEO：指的是每个副本最大的 offset； (消费者的最大offset，保证消费者消费一致性) \n HW：指的是消费者能见到的最大的 offset，ISR 队列中最小的 LEO。 \n 思考：假设没有HW，会出现什么情况？ \n 如果没有HW，消费者在leader获取数据可以获取offset为19的数据，如果此时leader挂了，消费者是可以获取16-19之间的数据的，但是下面两个follower都没有16-19的offset的值，所以这里设置HW消费者端可以看到HW=12，能够达到无论谁挂掉获取Offset偏移量都统一。 \n \n （1）follower 故障 \n follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后，follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。等该 follower 的 LEO 大于等于该 Partition 的 HW， 即 follower 追上 leader 之后，就可以重新加入 ISR 了。 \n （2）leader 故障 \n leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader \n 同步数据。 \n 注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 \n Exactly-Once语义 \n 将服务器的ACK级别设置为-1，可以保证Producer到Server之间的不会丢失数据，也就是At LeasetOnce，相对的，如果将服务器ACK级别设置为0，可以保证生产者每条数据只会被发送一次，会出现丢失，就是AtMostOnce。0.11版本kafka引入幂等性。无论Producer向Server发送多少重复数据，Server都只会持久化一条数据。幂等性结合AtLeastOnce实现精确一致。 \n AtLeastOnce+幂等性=ExactlyOnce \n 开启幂等性，只需要将Producer的参数中ennable.idompotence设置为true即可。Kafka的幂等性就是将原来下游的需要做的去重放到了数据上游，开启幂等性的Producer在初始化的时候会分配一个PID，发往同一个Partition的消息会附带SequenceNumber。而Broker端会对<PID, Partition,SequenceNumber>做缓存，当具有相同主键提交的时候，Broker会持久化一条数据。 \n Broker保存消息 \n 存储方式 \n 物理上把topic分成一个或多个patition（对应 server.properties 中的num.partitions=3配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件），如下： \n 存储策略 \n 无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据： \n 1）基于时间：log.retention.hours=168 \n 2）基于大小：log.retention.bytes=1073741824 \n 需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。 \n Zk存储结构 \n \n 注意：producer不在zk中注册，消费者在zk中注册。 \n 注意：在Kafka 0.9 版本之前，consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始，consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中，该 topic 为__consumer_offsets。 \n Kafka消费者 \n 消费的流程分析 \n \n l 每个consumer都可以根据分配策略（默认RangeAssignor），获得要消费的分区 \n l  获取到consumer对应的offset（默认从ZK中获取上一次消费的offset） \n l   找到该分区的leader，拉取数据 \n l  消费者提交offset \n API介绍 \n kafka提供了两套consumer API：高级Consumer API和低级Consumer API。 \n 高级API \n 1）高级API优点 \n 高级API 写起来简单 \n 不需要自行去管理offset，系统通过zookeeper自行管理。 \n 不需要管理分区，副本等情况，.系统自动管理。 \n 消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（ 默认设置1分钟更新一下zookeeper中存的offset） \n 可以使用 group来区分对同一个topic 的不同程序访问分离 开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响） \n 2）高级API缺点 \n 不能自行控制offset（对于某些特殊需求来说） \n 不能细化控制如分区、副本、zk等 \n 低级API \n 1）低级 API 优点 \n 能够让开发者自己控制offset，想从哪里读取就从哪里读取。 \n 自行控制连接分区，对分区自定义进行负载均衡 \n 对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中） \n 2）低级API缺点 \n 太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等。 \n 消费者组 \n \n 消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition。在图中， 有一个由三个消费者组成的group ，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者。 \n 在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区。 \n 消费方式 \n consumer采用pull（拉）模式从broker中读取数据。 \n push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。 \n 对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。 \n pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）。 \n 分区策略 \n 一个 consumer group 中有多个 consumer，一个 topic 有多个 partition，所以必然会涉及 \n 到 partition 的分配问题，即确定那个 partition 由哪个 consumer 来消费。 \n Kafka 有两种分配策略，一是 RoundRobin，一是 Range。 \n RoundRobin： \n \n Range： \n \n 消费者组案例后置： \n 需求：测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费。 \n Kafka的优秀架构 \n Kafka — 高并发、高可用、高性能 \n  高可用 \n 多副本机制 \n  高并发 \n 网络架构设计( 网络IO模型 ) \n三层架构：多selector -> 多线程 -> 队列的设计 \n Reactor网络设计模式1： \n \n (1) 首先服务端创建了 ServerSocketChannel 对象并在 Selector 上注册了 OP_ACCEPT 事件，ServerSocketChannel 负责监听指定端口上的连接。\n（2）当客户端发起到服务端的网络连接请求时，服务端的 Selector 监听到 OP_ACCEPT 事件，会触发 Acceptor 来处理 OP_ACCEPT 事件.\n（3）当 Acceptor 接收到来自客户端的 socket 请求时会为这个连接创建对应的 SocketChannel，将这个 SocketChannel 设置为非阻塞模式，并在 Selector 上注册它关注的 I/O 事件。如：OP_WRITER,OP_READ 事件。此时客户端与服务端的 socket 连接正式建立完成。\n（4）当客户端通过上面建立好的 socket 连接向服务端发送请求时，服务端的 Selector 会监听到 OP_READ 事件，并触发对应的处理逻辑（read handler）。服务端像客户端发送响应时，服务端的 Selector 可以监听到 OP_WRITER 事件，并触发对应的处理逻辑（writer handler）。\n\n我们看到这种单线程的reactor设计就是将所有的事件处理都在同一个线程中完成。这样的设计适合用在客户端这种并发比较小的场景。如果并发量比较大，或者有个请求处理逻辑要较为复杂，耗时较长，那么就会影响到后续所有的请求，接着就会导致大量的任务超时。\n \n 1 2 3 4 5 6 Reactor网络设计模式2： \n \n Accept 单独运行在一个线程中，这个线程使用 ExecutorService 实现，因为这样的话，当 Accept 线程异常退出的时候，ExecutorService 也会创建新的线程进行补偿。Read handler 里面也是一个线程池，这个里面所有的线程都注册了 OP_READ 事件，负责接收客户端传过来的请求，当然也是一个线程对应了多个 socket 连接。Read handler 里的线程接收到请求以后把请求都存入到 MessageQueue 里面。Handler Poll 线程池中的线程就会从 MessageQueue 队列里面获取请求，然后对请求进行处理。这样设计的话，即使某个请求需要大量耗时，Handler Poll 线程池里的其它线程也会去处理后面的请求，避免了整个服务端的阻塞。当请求处理完了以后 handler Pool 中的线程注册 OP_WRITER 事件，实现往客户端发送响应的功能。\n通过使用线程池多线程解决了单线程性能瓶颈的问题，另外增加了MessageQueue 的队列组件（根据实际业务权衡设计，默认500），如果请求量非常大，直接交给 write ThreadPool 线程池进行处理， 可能会出现该线程池处理不过来的情况发生，如果处理不过来，也会出现阻塞瓶颈。所以消息队列 起到一个缓冲的作用但是如果突然发生了大量的网络 I/O。单个 Selector 可能会在分发事件的时候成为性能瓶颈。所以我们很容易想的到应该将上面的单独的 Selector 扩展为多个\n \n 1 2 Reactor网络设计模式3： \n \n Accepetor 启动了以后接收连接请求，接收到了请求以后把请求发送给一个线程池（Processor）线程池里的每个线程获取到请求以后，把请求封装为一个个 SocketChannel 缓存在自己的队列里面。接下来给这些 SocketChannel 注册上 OP_READ 事件，这样就可以接收客户端发送过来的请求了，Processor 线程就把接收到的请求封装成 Request 对象存入到 RequestChannel 的 RequestQueue 队列。接下来启动了一个线程池，默认是 8 个线程来对队列里面的请求进行处理。处理完了以后把对应的响应放入到对应 ReponseQueue 里面。每个 Processor 线程从其对应的 ReponseQueue 里面获取响应，注册 OP_WRITER 事件，最终把响应发送给客户端。\n \n 1 Kafka超高并发网络设计： \n  高性能 \n 写数据： \n \n 把数据先写入到OS Cache \n 写到磁盘上面是顺序写，性能很高 \n \n 读数据： \n \n 根据跳表（ConcurrentSkipListMap,key就是baseOffSet，value就是logSegment）、稀疏索引，快速定位到要消费的数据 \n 零拷贝机制 \n减少数据的拷贝 \n减少了应用程序与操作系统上下文切换 \n \n Kafka磁盘顺序写保证写数据性能 \n kafka写数据： \n顺序写， 往磁盘上写数据时，就是追加数据，减少寻址的时间 。 \n经验： \n如果一个服务器磁盘达到一定的个数，磁盘也达到一定转数，往磁盘里面顺序写（追加写）数据的速度和写内存的速度差不多。 \n \n 稀疏索引：（一个存储块设置一个指针对） \nKafka中 采用了稀疏索引的方式读取索引 ，kafka每当写入了 4k 大小的日志（.log），就往index里写入一个记录索引。 \n log.segment.bytes Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分成块的大小，默认值 1G。 log.index.interval.bytes 默认 4kb，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引。 稀疏索引。 \n \n Kafka零拷贝机制保证读数据高性能 \n 消费者未使用零拷贝读取数据流程： \n1.消费者发送请求给kafka服务 \n2.kafka服务去os cache缓存读取数据（缓存没有就去磁盘读取数据） \n3.从磁盘读取了数据到os cache缓存中 \n4.os cache 复制 数据到kafka应用程序中 \n5.kafka将数据（ 复制 ）发送到socket cache中 \n6.socket cache通过网卡传输给消费者 \n \n Kafka的Zero-Copy零拷贝技术采用的是Java 类库java.nio.channels.FileChannel 中的 transferTo() 方法 \n 传统的文件传输 \n \n 零拷贝小文件传输 \n \n 通常情况下，kafka的消息会有多个订阅者，生产者发布的消息会被不同的消费者多次消费。 \n 如果有10个消费者，传统方式下，数据复制次数为4 * 10 = 40 次，而是用零拷贝技术只需要复制1 + 10 = 11 次，其中一次为从磁盘复制到页面缓存，10 次表示10个消费者各自读取一次页面缓存，由此可以看出kafka的效率是非常高的。 \n 生产者分区写入策略 \n \n \n 分发策略: \n \n kafka支持 数据分发策略有几种:   \n   1)  轮询方案(2.4版本以下) --\x3e 粘性分发策略\n   2)  hash随机方案 \n   3)  指定分发策略  \n   4)  自定义分发策略\n       \n \n 1 2 3 4 5 6 \n 如果使用不同的分发策略: \n \n 1 )   默认分区类\n    用于支持数据分发的分区类 :   DefaultPartitioner \n\n 2 )  在生产者传递数据选择方式不同 ,  其分发措施也不同\n     // 如果选择这个构建进行数据生产,底层会通过 DefaultPartitioner随机选择一个分区, 然后尽可能黏住这个分区, 进行数据生产操作 \n     public   ProducerRecord ( String  topic ,   V  value )   { \n          this ( topic ,   null ,   null ,   null ,  value ,   null ) ; \n     } \n     // 如果选择这个构建进行数据生产, 底层会调用 DefaultPartitioner中分发类基于hash进行数据分发操作 \n     public   ProducerRecord ( String  topic ,   K  key ,   V  value )   { \n          this ( topic ,   null ,   null ,  key ,  value ,   null ) ; \n      } \n     // 如果选择是这个构造来进行数据生产, 指定了要将数据发生给那个分区. 直接发送即可, 不会执行 DefaultPartitioner \n      public   ProducerRecord ( String  topic ,   Integer  partition ,   K  key ,   V  value )   { \n           this ( topic ,  partition ,   null ,  key ,  value ,   null ) ; \n       } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \n 如何自定义分发策略: \n \n 如何进行自定义分发策略 :    抄  DefaultPartitioner \n      1)   创建一个类  实现 Partitioner接口 \n      2)   重写接口中方法:  partition(参数 .....) \n              参数1 :   topic名 \n              参数2 :   key值 \n              参数3 :   key的字节数组 \n              参数4 :    value的值 \n              参数5 :   valye的值的字节数组 \n              参数6 :    集群对象(用于获取topic的分片的数量) \n     3)   在partition的方法中, 自定义分发策略(根据业务定义) \n              返回值为   分区的编号  编号从 0开始 \n     4)   将自定义分区类设置到生产者中: 在properties配置 \n               partitioner.class :   自定义分区类 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 生产者写入消息到topic，Kafka将依据不同的策略将数据分配到不同的分区中 \n \n \n 轮询分区策略--2.4版本后默认改为粘性 \n \n \n 随机分区策略（不用） \n \n \n 按key分区分配策略 \n \n \n 自定义分区策略 \n 轮询策略 \n \n l   默认的策略，也是使用最多的策略 ，可以最大限度保证所有消息平均分配到一个分区 \n l  如果在生产消息时，key为null，则使用轮询算法均衡地分配分区 \n 随机策略（不用） \n 随机策略，每次都随机地将消息分配到每个分区。在较早的版本，默认的分区策略就是随机策略，也是为了将消息均衡地写入到每个分区。但后续轮询策略表现更佳，所以基本上很少会使用随机策略。 \n 按key分配策略 \n \n 按key分配策略，有可能会出现「数据倾斜」，例如：某个key包含了大量的数据，因为key值一样，所有所有的数据将都分配到一个分区中，造成该分区的消息数量远大于其他的分区。 \n 乱序问题 \n 轮询策略、随机策略都会导致一个问题，生产到Kafka中的数据是乱序存储的。而按key分区可以一定程度上实现数据有序存储——也就是局部有序，但这又可能会导致数据倾斜，所以在实际生产环境中要结合实际情况来做取舍。 \n 自定义分区策略 \n 实现步骤： \n \n 创建自定义分区器 \n \n public   class   KeyWithRandomPartitioner   implements   Partitioner   { \n\n     private   Random  r ; \n\n     @Override \n     public   void   configure ( Map < String ,   ? >  configs )   { \n        r  =   new   Random ( ) ; \n     } \n\n     @Override \n     public   int   partition ( String  topic ,   Object  key ,   byte [ ]  keyBytes ,   Object  value ,   byte [ ]  valueBytes ,   Cluster  cluster )   { \n         // cluster.partitionCountForTopic 表示获取指定topic的分区数量 \n         return  r . nextInt ( 1000 )   %  cluster . partitionCountForTopic ( topic ) ; \n     } \n\n     @Override \n     public   void   close ( )   { \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \n 在Kafka生产者配置中，自定使用自定义分区器的类名 \n \n props . put ( ProducerConfig . PARTITIONER_CLASS_CONFIG , KeyWithRandomPartitioner . class . getName ( ) ) ; \n \n 1 #  消费者组Rebalance机制 \n Rebalance再均衡 \n Kafka中的Rebalance称之为再均衡，是Kafka中确保Consumer group下所有的consumer如何达成一致，分配订阅的topic的每个分区的机制。 \n Rebalance触发的时机有： \n \n \n 消费者组中consumer的个数发生变化。例如：有新的consumer加入到消费者组，或者是某个consumer停止了。 \n \n \n 订阅的topic个数发生变化 \n \n \n 消费者可以订阅多个主题，假设当前的消费者组订阅了三个主题，但有一个主题突然被删除了，此时也需要发生再均衡。 \n \n 订阅的topic分区数发生变化 \n Rebalance的不良影响 \n l 发生Rebalance时，consumer group下的所有consumer都会协调在一起共同参与， Kafka使用分配策略尽可能达到最公平的分配 \n l Rebalance过程会对consumer group产生非常严重的影响，Rebalance的过程中所有的消费者都将停止工作，直到Rebalance完成 \n 消费者分区分配策略 \n Range范围分配策略 \n **Range范围分配策略是Kafka默认的分配策略，**它可以确保每个消费者消费的分区数量是均衡的。 \n 注意：Rangle范围分配策略是针对每个Topic的。 \n 配置 \n 配置消费者的partition.assignment.strategy为org.apache.kafka.clients.consumer.RangeAssignor。 \n 算法公式 \n n = 分区数量 / 消费者数量 \n m = 分区数量 % 消费者数量 \n 前m个消费者消费n+1个 \n 剩余消费者消费n个 \n RoundRobin轮询策略 \n RoundRobinAssignor轮询策略是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序（topic和分区的hashcode进行排序），然后通过轮询方式逐个将分区以此分配给每个消费者。 \n 配置 \n 配置消费者的partition.assignment.strategy为org.apache.kafka.clients.consumer.RoundRobinAssignor。 \n Stricky粘性分配策略 \n 从Kafka 0.11.x开始，引入此类分配策略。主要目的： \n \n \n 分区分配尽可能均匀 \n \n \n 在发生rebalance的时候，分区的分配尽可能与上一次分配保持相同 \n \n \n 没有发生rebalance时，Striky粘性分配策略和RoundRobin分配策略类似。 \n \n 上面如果consumer2崩溃了，此时需要进行rebalance。如果是Range分配和轮询分配都会重新进行分配，例如： \n 通过上图 ，我们发现，consumer0和consumer1原来消费的分区大多发生了改变。接下来我们再来看下粘性分配策略。 \n \n 我们发现，Striky粘性分配策略，保留rebalance之前的分配结果。这样，只是将原先consumer2负责的两个分区再均匀分配给consumer0、consumer1。这样可以明显减少系统资源的浪费，例如：之前consumer0、consumer1之前正在消费某几个分区，但由于rebalance发生，导致consumer0、consumer1需要重新消费之前正在处理的分区，导致不必要的系统开销。（例如：某个事务正在进行就必须要取消了） \n 副本机制 \n 副本的目的就是冗余备份，当某个Broker上的分区数据丢失时，依然可以保障数据可用。因为在其他的Broker上的副本是可用的。 \n producer的ACKs参数 \n 对副本关系较大的就是，producer配置的acks参数了,acks参数表示当生产者生产消息的时候，写入到副本的要求严格程度。它决定了生产者如何在性能和可靠性之间做取舍。 \n     0: 当ack的确认机制为0的时候, 表示生产者只管发送数据即可, 并不关心此数据是否已经被kafka的接收完成了, 不去接收返回检验码\n    \n    1: 当ack的确认机制为1的时候, 表示生产者需要等待对应接收分片上的主副本(leader)返回ack确认信息, 认为数据发送成功\n    \n  -1(all): 当ack的确认机制为-1的时候, 表示生产者需要保证分片上的每一个副本(同步给follwer)都要接收到消息, 并返回确认码 认为数据发送成功\n \n 配置： \n Properties  props  =   new   Properties ( ) ; \nprops . put ( "bootstrap.servers" ,   "node1.itcast.cn:9092" ) ; \nprops . put ( "acks" ,   "all" ) ; \nprops . put ( "key.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \nprops . put ( "value.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n\n \n 1 2 3 4 5 6 l  Kafka****中的leader负责处理读写操作，而follower只负责副本数据的同步 \n l  如果leader出现故障，其他follower会被重新选举为leader \n l  follower****像一个consumer一样，拉取leader对应分区的数据，并保存到日志数据文件中 \n 在实际环境中，leader有可能会出现一些故障，所以Kafka一定会选举出新的leader。在讲解leader选举之前，我们先要明确几个概念。Kafka中，把follower可以按照不同状态分为三类——AR、ISR、OSR。 \n l 分区的所有副本称为 「AR」（Assigned Replicas——已分配的副本） \n l 所有与leader副本保持一定程度同步的副本（包括 leader 副本在内）组成 「ISR」（In-Sync Replicas——在同步中的副本） \n l 由于follower副本同步滞后过多的副本（不包括 leader 副本）组成 「OSR」（Out-of-Sync Replias） \n l AR = ISR + OSR \n l 正常情况下，所有的follower副本都应该与leader副本保持同步，即AR = ISR，OSR集合为空。 \n 查看分区的ISR \n \n 使用Kafka Eagle查看某个Topic的partition的ISR有哪几个节点。 \n \n \n \n 尝试关闭id为0的broker（杀掉该broker的进程），参看topic的ISR情况。 \n kafka如何保证数据不丢失 \n 生产端如何保证数据不丢失的 \n \n \n 基于ack校验: \n \n 生产者如何保证数据不丢失 :    ack的校验确认机制 \n        \n         0 :   当ack的确认机制为0的时候, 表示生产者只管发送数据即可, 并不关心此数据是否已经被kafka的接收完成了, 不去接收返回检验码 \n        \n         1 :   当ack的确认机制为1的时候, 表示生产者需要等待对应接收分片上的主副本(leader)返回ack确认信息, 认为数据发送成功 \n        \n       -1(all) :   当ack的确认机制为-1的时候, 表示生产者需要保证分片上的每一个副本(同步给follwer)都要接收到消息, 并返回确认码 认为数据发送成功 \n\n\n 从安全角度 :   \n         -1   > 1 > 0 \n 从效率角度 : \n         0    >  1 > -1 \n\n 在实际生产中,   一般会设置多少呢? 三种都有可能 \n        需要根据消息数据的重要程度选择不同的ack\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n 如何设置ack校验方案呢? \n \n 设置方式 :  在生产者的javaAPI的properties对象中进行设置即可\n    props . put ( "acks" ,   "all" ) ; \n \n 1 2 \n 相关的高频面试题 \n \n 1)   目前发送一条数据, broker就要给与ACK校验信息, 如果broker迟迟没有给与ack的校验信息, 如何解决呢? \n 解决方案 : \n      先等待,   设置一个等待时间(超时时间),如果超时之后, 依然没有ack返回呢? \n      接下来进行重试策略,   重新进行发送, 重试之后, 依然无法接收, 可以设置一个重试次数 , 依然无法接收 \n      最后   程序之间报错, 并基于告警系统, 通知相关的人员进行维护 \n\n 2)   发送一条数据, broker就要给与一次ack的校验, 发一次, 校验码返回一次, 这样是否会对网络带宽产生影响, 如果产生, 如何解决呢?  \n 解决方案 : \n      引入缓冲池方案,   采用异步发送方式, 先将数据生产到缓存池当中, 当池子中数据达到一定的阈值后, 一次性写出到broker端, 此时broker端只需要针对这一批数据(同一分片上)来一次响应即可 \n      注意事项 : \n          一批数据,   必须只能属于同一个分片, 不能是多个分片, 否则无法实现 \n          如果一批数据中存在多个分片的数据,   此时在发生的时候, 内部底层需要将其分成好几批数据发生到对应分片上 \n\n 3)   假设采用缓存池进行批量数据发送, 如果在发生一批数据到broker端 broker端又没有给与ack的响应, 但是此时缓存池子数据已经满了, 如何解决? \n 解决方案 : \n     程序员可以选择,   直接清空掉缓存池, 以保证生产者可以继续写入到缓存池(会导致数据丢失) \n     直接报错 :   先将缓存池中数据可以报错到临时容器中, 然后程序报错, 通知相关的管理人员即可 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 额外内容: \n 1) 数据发送方式出现两种: 同步发送  和 异步发送\n2) 默认等待超时时间:  120000  (2分钟)\n\t\tdelivery.timeout.ms: 120000\n3) 默认重试的次数:  2147483647 一般设置为 3\n      retries: 2147483647\n4) 默认缓存池子大小是: 33554432 (32M)\n\t  buffer.memory: 33554432\n5) 达到一批阈值是多少: 时间阈值(默认为0)  大小阈值(16384(16kb))\n\tbatch.size: 16384\n\tlinger.ms : 0\n\n如何配置呢?  在生产者的javaAPI的properties对象中进行设置即可\n \n 1 2 3 4 5 6 7 8 9 10 11 12 \n 如何模拟同步发送数据 \n \n // 此类用于将数据生成到kafka中 -- 同步 \n public   class   KafkaProducerTest_sync   { \n\n     public   static   void   main ( String [ ]  args )   { \n\n         //1. 创建kafka生成者核心类对象:   KafkaProducer \n\n         //1.1: 设置相关的配置信息 \n         Properties  props  =   new   Properties ( ) ; \n        props . put ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        props . put ( "acks" ,   "all" ) ;   // ack确认机制, 用于保证生产者在发送数据的不丢失 \n\n         // 设置 key 和 vlaue 发送数据的序列化类 \n        props . put ( "key.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n        props . put ( "value.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n\n         Producer < String ,   String >  producer  =   new   KafkaProducer < > ( props ) ; \n\n         //2. 发送数据 \n         for   ( int  i  =   0 ;  i  <   100 ;  i ++ )   { \n             //2.1: 发送端 用于承载数据的对象 \n             ProducerRecord < String ,   String >  producerRecord  =   new   ProducerRecord < > ( "test01" ,   Integer . toString ( i ) ) ; \n\n             //2.2: 执行发送 \n             try   { \n                producer . send ( producerRecord ) . get ( ) ; \n             }   catch   ( Exception  e )   { \n                 // 如果出现异常, 描述数据以及重试完成, 并重试依然失败后, 弹出错误 \n                 // 发送失败后业务 .... \n             } \n         } \n         //3. 释放资源 \n        producer . close ( ) ; \n     } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 \n 如何模拟异步发送方式 \n \n // 此类用于将数据生成到kafka中  -- 异步 \n public   class   KafkaProducerTest_asyn   { \n\n     public   static   void   main ( String [ ]  args )   { \n\n         //1. 创建kafka生成者核心类对象:   KafkaProducer \n\n         //1.1: 设置相关的配置信息 \n         Properties  props  =   new   Properties ( ) ; \n        props . put ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        props . put ( "acks" ,   "all" ) ;   // ack确认机制, 用于保证生产者在发送数据的不丢失 \n\n         // 设置 key 和 vlaue 发送数据的序列化类 \n        props . put ( "key.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n        props . put ( "value.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n\n         Producer < String ,   String >  producer  =   new   KafkaProducer < > ( props ) ; \n\n         //2. 发送数据 \n         for   ( int  i  =   0 ;  i  <   100 ;  i ++ )   { \n             //2.1: 发送端 用于承载数据的对象 \n             ProducerRecord < String ,   String >  producerRecord  =   new   ProducerRecord < > ( "test01" ,   Integer . toString ( i ) ) ; \n\n             //2.2: 执行发送 \n\n             //producer.send(producerRecord); // 异步方式, 只不过不需要关心是否接受成功, 不成功直接报错 \n\n            producer . send ( producerRecord ,   new   Callback ( )   { \n                 // 异步发送的每一次, 底层都会回调这个函数, 如果成功的, exception为null \n                 @Override \n                 public   void   onCompletion ( RecordMetadata  metadata ,   Exception  exception )   { \n\n                     if ( exception != null ) { \n                         // 如果不为null 说明 当下消息发生失败了, 此时执行失败后业务 \n\n\n\n                     } \n                 } \n             } ) ; \n         } \n         //3. 释放资源 \n        producer . close ( ) ; \n     } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 broker端如何保证数据不丢失 \n 解决方案 \n \n \n \n 采用副本机制来保证, 副 本越多, 数据越安全 \n \n \n \n \n 生产端ack的校验码为 -1 \n \n \n \n 消费端如何保证数据不丢失 \n \n \n 消费数据流程 \n \n 消费者消费流程 : \n       1)   当消费端启动后, 首先会先询问broker端, 对应监听topic上次消费到那个偏移量 \n       2)   broker端会根据消费者对应消费者组编号, 去查询对应这个组上次消费这个topic到了那个偏移量 \n       3)   如果没有找到偏移量,认为第一次来, 让其从 当前位置(默认)开始消费, 设置偏移量为 0 , 如果找到偏移量, 从这个偏移量开始往后进行消费即可 \n        4)   消费者开始消费数据, 在消费过程中, 消费端可以选择自动或者手动方式提交消费到那个偏移量, 然后broker负责记录即可 \n\n\n        通过这套流程可以发现,   数据绝对不会出现丢失的问题, 但是会存在重复消费的问题, 因为消费端如果提交偏移量不及时, 而且宕机之后, 可能会发生 \n \n 1 2 3 4 5 6 7 8 \n 偏移量数据存储位置: \n \n 注意 : \n     偏移量信息最新版本(0.8.x后)   是由 broker端来记录每一组消费者的偏移量信息, 将这个信息记录在一个 __consumer_offsets topics中 \n\n     在0.8.x版本以前,   偏移量信息是记录在zookeeper中 \n    \n    \n __consumer_offset   : \n     有50个分区   每个分区下有1个副本 \n \n 1 2 3 4 5 6 7 8 \n 如何实现手动提交偏移量的方案 \n \n // kafka的消费者的代码-- 手动提交 \n public   class   KafkaConsumerTest_Manual   { \n\n     public   static   void   main ( String [ ]  args )   { \n         //1. 创建kafka的消费者核心类对象:  KafkaConsumer \n\n         //1.1: 设置消费者的配置信息 \n         Properties  props  =   new   Properties ( ) ; \n        props . setProperty ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        props . setProperty ( "group.id" ,   "test" ) ;   // 消费者组 \n        props . setProperty ( "enable.auto.commit" ,   "false" ) ;   // 自动提交 偏移量信息 \n         //props.setProperty("auto.commit.interval.ms", "1000"); // 每次提交间隔时间 \n         // 反序列类 \n        props . setProperty ( "key.deserializer" ,   "org.apache.kafka.common.serialization.StringDeserializer" ) ; \n        props . setProperty ( "value.deserializer" ,   "org.apache.kafka.common.serialization.StringDeserializer" ) ; \n\n         KafkaConsumer < String ,   String >  consumer  =   new   KafkaConsumer < > ( props ) ; \n\n         //2. 设置监听那些topic \n        consumer . subscribe ( Arrays . asList ( "test01" ) ) ; \n\n         //3. 获取消息数据 \n         while   ( true )   { \n             //3.1: 通过消费者对象, 获取消息 \n             ConsumerRecords < String ,   String >  records  =  consumer . poll ( Duration . ofMillis ( 100 ) ) ; \n\n             //3.2: 遍历获取每一个数据 \n             for   ( ConsumerRecord < String ,   String >  record  :  records ) { \n                 System . out . println ( "数据为:" + record . value ( )   +   "消息偏移量:" + record . offset ( ) ) ; \n                \n                 //消费完成后, 提交偏移量 \n                consumer . commitSync ( ) ; // 同步 \n                 //consumer.commitAsync();//异步方式 \n             } \n\n         } \n\n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #  kafka的消息存储和查询机制 \n 4.1 kafka的消息存储 \n \n \n 过期时间配置以及segment大小的设置: server.properties \n \n \n 4.2 kafka的数据查询机制 \n \n 查询数据流程 : \n 1)   确定数据在哪个segment段当中 \n 2)   然后去查询这个段中index文件, 确定368776在log的具体物理偏移量 \n 3)   最后查询log文件,根据index文件确定的物理偏移量, 顺序查询到具体消息数据即可 \n \n 1 2 3 4 顺序读写和 随机读写 \n 顺序读写的效率要远远大于随机读写操作\n \n 1 #  kafka的消费端数据积压 \n \n 思考: \n 1)   请思考: 如果模拟点对点消费模式呢? \n      方案一 :   只有一个消费者来消费这个topic \n      方案二 :   将消费者多个topic的消费者全部放置在一个组内 \n\n 2)   请思考: 如何模拟发布订阅消费模式呢? \n      将多个消费者放置在不同的消费者组\n \n 1 2 3 4 5 6 #  Kafka配额限速机制（Quotas） \n 生产者和消费者以极高的速度生产/消费大量数据或产生请求，从而占用broker上的全部资源，造成网络IO饱和。 有了配额（Quotas）就可以避免这些问题 。Kafka支持配额管理，从而可以对Producer和Consumer的produce&fetch操作进行流量限制，防止个别业务压爆服务器。 \n 限制producer端速率 \n 为所有client id设置默认值，以下为 所有producer程序设置其TPS不超过1MB/s ，即1048576/s，命令如下： \n  bin/kafka-configs.sh  --zookeeper  node1.itcast.cn:2181   --alter  --add-config  \'producer_byte_rate=1048576\'  --entity-type  clients --entity-default  \n \n 1 运行基准测试，观察生产消息的速率 \n bin/kafka-producer-perf-test.sh  --topic   test  --num-records   500000   --throughput   -1  --record-size  1000  --producer-props  bootstrap.servers = node1.itcast.cn:9092,node2.itcast.cn:9092,node3.itcast.cn:9092   acks = 1  \n \n 1 结果： \n 50000 records sent, 1108.156028 records/sec (1.06 MB/sec) \n 限制consumer端速率 \n 对consumer限速与producer类似，只不过参数名不一样。 \n 为指定的topic进行限速，以下为所有consumer程序设置topic速率不超过1MB/s，即1048576/s。命令如下： \n bin/kafka-configs.sh  --zookeeper   node1.itcast.cn:2181  --alter  --add-config  \'consumer_byte_rate=1048576\'  --entity-type clients --entity-default  \n \n 1 运行基准测试： \n bin/kafka-consumer-perf-test.sh --broker-list  node1.itcast.cn:9092,node2.itcast.cn:9092,node3.itcast.cn:9092  --topic   test   --fetch-size  1048576   --messages   500000     \n \n 1 结果为： \n MB.sec：1.0743 \n 取消Kafka的Quota配置 \n 使用以下命令，删除Kafka的Quota配置 \n bin/kafka-configs.sh  --zookeeper   node1.itcast.cn:2181  --alter  --delete-config  \'producer_byte_rate\'  --entity-type  clients --entity-default  \n\n bin/kafka-configs.sh  --zookeeper   node1.itcast.cn:2181  --alter  --delete-config  \'consumer_byte_rate\'  --entity-type  clients --entity-default  \n \n 1 2 3 #  Kafka中数据清理 \n Kafka的消息存储在磁盘中，为了控制磁盘占用空间，Kafka需要不断地对过去的一些消息进行清理工作。Kafka的每个分区都有很多的日志文件，这样也是为了方便进行日志的清理。在Kafka中，提供两种日志清理方式： \n l  日志删除（Log Deletion）：按照指定的策略 直接删除 不符合条件的日志。 \n l  日志压缩（Log Compaction）：按照消息的key进行整合，有相同key的但有不同value值，只保留最后一个版本。 \n 在Kafka的broker或topic配置中： \n \n \n \n 配置项 \n 配置值 \n 说明 \n \n \n \n \n log.cleaner.enable \n true（默认） \n 开启自动清理日志功能 \n \n \n log.cleanup.policy \n delete（默认） \n 删除日志 \n \n \n log.cleanup.policy \n compaction \n 压缩日志 \n \n \n log.cleanup.policy \n delete,compact \n 同时支持删除、压缩 \n 日志删除 \n 日志删除是以段（segment日志）为单位来进行定期清理的。 \n 定时日志删除任务 \n Kafka日志管理器中会有一个专门的日志删除任务来定期检测和删除不符合保留条件的日志分段文件，**这个周期可以通过broker端参数log.retention.check.interval.ms来配置，默认值为300,000，即5分钟。**当前日志分段的保留策略有3种： \n \\1. 基于时间的保留策略 \n \\2. 基于日志大小的保留策略 \n \\3. 基于日志起始偏移量的保留策略 \n 基于时间的保留策略 \n 以下三种配置可以指定如果Kafka中的消息超过指定的阈值，就会将日志进行自动清理： \n l  log.retention.hours \n l  log.retention.minutes \n l  log.retention.ms \n 其中，优先级为 log.retention.ms > log.retention.minutes > log.retention.hours。默认情况，在broker中，配置如下： \n log.retention.hours=168 \n 也就是，默认日志的保留时间为168小时，相当于保留7天。 \n 删除日志分段时: \n \\1. 从日志文件对象中所维护日志分段的跳跃表中移除待删除的日志分段，以保证没有线程对这些日志分段进行读取操作 \n \\2. 将日志分段文件添加上“.deleted”的后缀（也包括日志分段对应的索引文件） \n 3.   Kafka的后台定时任务会定期删除这些“.deleted”为后缀的文件，这个任务的延迟执行时间可以通过file.delete.delay.ms参数来设置，默认值为60000，即1分钟。 \n 基于日志大小的保留策略 \n 日志删除任务会检查当前日志的大小是否超过设定的阈值来寻找可删除的日志分段的文件集合 。可以通过broker端参数  log.retention.bytes  来配置，默认值为-1，表示无穷大。如果超过该大小，会自动将超出部分删除。 \n 注意: \n log.retention.bytes 配置的是日志文件的总大小，而不是单个的日志分段的大小，一个日志文件包含多个日志分段。 \n 基于日志起始偏移量保留策略 \n 每个segment日志都有它的起始偏移量，如果起始偏移量小于 logStartOffset，那么这些日志文件将会标记为删除。 \n Kafka topic分区数设置和机器数计算 \n 一、分区副本数设置 \n由于分区副本仅提供数据冗余的功能，且分区副本数量与集群吐吞量负相关，故冗余度在满足安全要求基础上设置为最小即可。 \n故我们不妨将分区副本数设置为2. \n 二、kafka分区数设置 \n通过对单个分区的topic进行消费者和生产者的压力测试，得出单个分区所能提供的消费和生产的最大峰值吐吞量。 \n1、创建只有一个分区的topic。 \n kafka-topics.sh  --create   \\ \n --zookeeper  Linux001:2181    \\ \n --partitions   1     \\ \n--replication-factor  2   \\ \n --topic   test  \n \n 1 2 3 4 5 2、测试该topic的producer峰值吞吐量Tp \n kafka-producer-pref-test.sh  \\ \n--producer-props  bootstrap.servers = ${bootstrap.servers}   \\ \n --producer.config   ${KAFKA_HOME} /config/producer.properties  \\ \n --topic   test   \\ \n--record-size  900000   \\    ( 根据你们的业务进行配置 ) \n--num-records  10000000   \\ \n --throughput   -1  \n\t--producer-prop参数下还可以额外指定其他键值对，如 "acks=1" \n\t --throughput  代表每秒发多少条消息，-1代表不限流\n测试环境测得约为Tp = 80MB/s。  \n \n 1 2 3 4 5 6 7 8 9 10 3、测试该topic的consumer峰值吞吐量Tc \n kafka-consumer-pref-test.sh  \\ \n--broker-list  ${bootstrap.servers}   \\ \n --consumer.config = ${KAFKA_HOME} /config/consumer.properties  \\ \n --topic   test   \\ \n --messages   10000000   \\ \n--reporting-interval  1000   \\ \n--show-detailed-stats\n 测试环境测得约为Tc = 90MB/s。\n \n 1 2 3 4 5 6 7 8 4、计算所需要的topic分区数 \n假设你需要的目标吞吐量为T，则分区数num=T/min(Tp,Tc)。 \n分母取最小值表明即使取消费者额生产者的最小吞吐量，只要设置为num的分区数，仍能达到目标吐吞量要求。 \n 假设我们需要的吐吞量为100MB/s，则分区数num=100/min(80,90)=2 \n 由于消费者Flink数量为3台，为保证充分利用flink计算资源，避免flink消费者空转，我们不妨将Kafka的topic分区设置为3。 \n后续若业务升级再动态扩容topic分区到6个、9个、12个...保持与flink消费者数n:1的关系。 \n动态扩容topic分区命令： \n kafka-topics.sh  --alter   \\ \n  --zookeeper   ${zookeeper.url}    \\ \n  --partitions   6 \t \\ \n  --topic  t001 \t\n \n 1 2 3 4 三、kafka生产者性能优化测试 \n我们设置好topic的分区数、分区副本数后，可以使用控制变量的方式来测试，寻找公司集群上各参数的最佳值。 \n会影响生产端吞吐量的参数：batch.size\\longer.ms\\acks,这些参数都可以在--producer-props参数后。 \n其他参数还有：压缩格式、网络带宽。 \n 四、kafka消费者性能优化测试 \n我们设置好topic的分区数、分区副本数后，可以使用控制变量的方式来测试，寻找公司集群上各参数的最佳值。 \n会影响消费端吞吐量的参数：--fetch-size--threads \n其他参数还有：压缩格式、网络带宽。 \n 五、kafka机器节点数量设置 \n n=2*(峰值生产速度m/s*副本数/100m/s)+1\n更关注峰值生产速度是为了避免数据在业务层积压。\n \n 1 2 #  陌陌项目 \n 陌陌案例基本介绍 \n \n 数据的特点:  多   写的操作远远大于读的操作 \n 需求说明: \n 1)   选择合适的存储容器来存储消息数据, 并且能够支持即席查询 和 离线数据分析操作 \n\n 2)   进行实时统计消息总量 \n 3)   实时统计各个地区 收  发 消息的数量 \n 4)   实时统计每一位客户 发送 和 接收消息总量 \n \n 1 2 3 4 5 #  陌陌案例数据源介绍 \n 2.1 数据源介绍 \n 业务端 消息数据的结构信息描述: \n \n 2.2 模拟数据源 \n \n \n \n 在Linux中创建一个目录, 用于放置生产数据的jar包和原始数据集 \n \n \n \n mkdir   -p  /export/data/momo_init\n cd  /export/data/momo_init\n \n 1 2 \n \n \n 将资料中   生产数据工具 目录下的两个文件上传到 /export/data/momo_init目录下 \n \n \n \n rz 上传即可\n \n 1 \n \n \n \n 创建输出数据的路径 \n \n \n \n mkdir   -p  /export/data/momo_data\n \n 1 \n \n \n 启动jar包, 查是否数据写入到此目录中 \n \n \n \n 格式:\n     java   -jar   jar包路径  初始数据集路径  输出数据目录  最大数据间隔时间\n \n 1 2 \n 到 /export/data/momo_data目录检测是否有数据生成 \n \n 如果能够在这个 MOMO_DATA.dat文件中, 查看到数据在源源不断的生产中, 说明 业务端搞定结束 \n jar包特点:  数据会源源不断的生产到 MOMO_DATA.dat文件中 \n 陌陌案例架构说明 \n 陌陌案例数据采集操作 \n 4.1: apache flume基本介绍 \n ​\t  apache flume 本质上就是一款数据采集的工作,来源于cloudera公司, 基于java编写,  后期贡献给apache, 成为apache的顶级项目 \n ​\t域名: http://flume.apache.org \n ​    版本说明: 在 0.9x版本之前, 称为 flume OG  在 1.0以后 称为 flume ng, 目前只会接触到flume NG \n ​    flume这款软件主要是由三大组件来构成的: \n \n \n \n source组件  : 数据源   在flume中提前定义多种数据源组件, 以满足不同场景数据采集操作 \n \n \n \n \n channel组件 : 管道  主要是用于数据缓存的操作, 数据是先从source到达channel 然后才到sink组件中, flume提供多种管道 ( 内存 , 文件) \n \n \n \n \n sink组件  : 下沉组件(目的地组件)  主要是用于将数据送往指定的目的地 flume也提供多种下沉组件, 用于满足不同业务场景要求 \n \n \n \n 使用操作: 一般都是通过配置采集文件, 来完成数据采集要求 \n flume一般安装的位置:  要采集数据在哪里, 请将flume安装在哪里去 \n flume最基本架构单元: 一个flume示例 称为 一个agent , 一条数据, 被flume封装为一个个event对象 \n \n 4.2 apache flume安装 \n ​\t直接参考课件即可, 安装非常的方便 \n 4.3 apache flume的入门操作 \n 4.3.1 入门案例的流程分析 \n \n 说明:\n   首先要确认使用三大组件\n   接着到flume官网中,找到这个组件的案例配置\n   最后, 修改这些配置满足采集要求即可\n \n 1 2 3 4 4.3.2 实现入门案例 \n \n \n \n 组装配置文件 \n \n \n \n cd   /export/server/apache-flume-1.9.0-bin/conf \n vim    01_netcatSource_loggerSink.conf \n\n 内容如下 : \n1) 配置三大组件的名称 \n a1.sources   =   r1 \n a1.channels   =   c1 \n a1.sinks   =   k1 \n2) 细化配置三大组件的详细内容 \n2.1) 配置 source组件 \n a1.sources.r1.type   =   netcat \n a1.sources.r1.bind   =   node1 \n a1.sources.r1.port   =   44444 \n2.2) 配置 channel组件 \n a1.channels.c1.type   =   memory \n a1.channels.c1.capacity   =   1000 \n a1.channels.c1.transactionCapacity   =   100 \n\n #2.3) 配置 sink组件 \n a1.sinks.k1.type   =   logger \n\n #3) 配置连接操作 \n a1.sources.r1.channels   =   c1 \n a1.sinks.k1.channel   =   c1 \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \n \n \n 启动flume 进行数据采集 \n \n \n \n 启动的格式 : \n     cd   /export/server/apache-flume-1.9.0-bin \n    \n     bin/flume-ng   agent  -c conf -f conf/01_netcatSource_loggerSink.conf -n a1 -Dflume.root.logger=INFO,console \n    \n 属性说明 : \n    -c   :    指定flume的配置文件的在那个路径下 \n    -f   :    指定采集文件的路径 \n    -n   :    指定 agent的名称 \n    -Dflume.root.logger = INFO,console :  指定日志数据打印方案 \n    \n \n 1 2 3 4 5 6 7 8 9 10 11 \n \n \n \n 向44444端口号, 写数据, 查看flume是否可以采集到 \n \n \n \n 格式 : \n    telnet   主机名 端口号 \n   \n 案例 : \n    telnet    node1 44444 \n\n 此时执行的时候,   可能会报出一个错误: \n   -bash :   telnet: 未找到命令 \n\n 如果出现此错误,   说明当前节点未安装 netcat的命令, 执行以下命令进行安装 \n yum   -y install telnet \n \n 1 2 3 4 5 6 7 8 9 10 11 发送数据: 可以在任意的节点 \n \n flume自动采集数据: \n \n 4.4 基于flume实现陌陌消息数据采集 \n 数据源特点: \n ​      消息数据持续不断的写入到 某一个文件中 \n 功能要求: \n ​       监听某一个文件, 当文件中有了新的内容后, 立即进行采集, 将采集到数据写入到 Kafka中, 请兼顾目录操作 \n 实现操作: \n 1)   确定三大组件 \n      source :   既能监听某一个文件  又能监听某个目录下多个文件 \n         组件名称 :   Taildir Source \n         相关配置信息 : \n             a1.sources.r1.type   =   TAILDIR \npositionFile 为了支持 断点续传功能 \n             a1.sources.r1.positionFile   =   /export/data/flume/taildir_position.json \n             a1.sources.r1.filegroups   =   f1 f2 \n             a1.sources.r1.filegroups.f1   =   /export/data/momo_data/MOMO_DATA.dat \n             a1.sources.r1.filegroups.f2   =   /export/data/momo_data/.*log.* \n      \n       channel :    内存管道 \n          组件名称 :   Memory channel \n          相关配置信息 : \n              a1.channels.c1.type   =   memory \n \t\t\t a1.channels.c1.capacity   =   1000 \n \t\t\t a1.channels.c1.transactionCapacity   =   100 \n      sink组件 :    下沉到kafka的sink组件 \n         组件名称 :   kafka sink \n         相关的配置内容 : \n             a1.sinks.k1.type   =   org.apache.flume.sink.kafka.KafkaSink \n             a1.sinks.k1.kafka.topic   =   MOMO_MSG \n             a1.sinks.k1.kafka.bootstrap.servers   =   node1:9092,node2:9092,node3:9092 \n             a1.sinks.k1.kafka.flumeBatchSize   =   10 \n             a1.sinks.k1.kafka.producer.acks   =   1 \n             a1.sinks.k1.kafka.producer.linger.ms   =   1 \n             a1.sinks.k1.kafka.producer.compression.type   =   snappy \n\n     \n 2)   编写配置文件 \n #2.1: 定义三大组件名称: \n a1.sources   =   r1 \n a1.channels   =   c1 \n a1.sinks   =   k1 \nsource 组件配置 \n a1.sources.r1.type   =   TAILDIR \n a1.sources.r1.positionFile   =   /export/data/flume/taildir_position.json \n a1.sources.r1.filegroups   =   f1 f2 \n a1.sources.r1.filegroups.f1   =   /export/data/momo_data/MOMO_DATA.dat \n a1.sources.r1.filegroups.f2   =   /export/data/momo_data/.*log.* \nchannel组件配置 \n a1.channels.c1.type   =   memory \n a1.channels.c1.capacity   =   1000 \n a1.channels.c1.transactionCapacity   =   100 \nsink组件配置 \n a1.sinks.k1.type   =   org.apache.flume.sink.kafka.KafkaSink \n a1.sinks.k1.kafka.topic   =   MOMO_MSG \n a1.sinks.k1.kafka.bootstrap.servers   =   node1:9092,node2:9092,node3:9092 \n a1.sinks.k1.kafka.flumeBatchSize   =   10 \n a1.sinks.k1.kafka.producer.acks   =   1 \n a1.sinks.k1.kafka.producer.linger.ms   =   1 \n a1.sinks.k1.kafka.producer.compression.type   =   snappy \n\n\n #2.3. 定义连接组件 \n a1.sources.r1.channels   =   c1 \n a1.sinks.k1.channel   =   c1 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 具体实现: \n \n \n \n 组装flume采集配置文件 \n \n \n \n cd    /export/server/apache-flume-1.9.0-bin/conf \n vim   MOMO_tailDirSource_kafkaSink.conf \n\n 内容如下 : \n #1) 构建三大组件的名称 \n a1.sources   =   r1 \n a1.channels   =   c1 \n a1.sinks   =   k1 \n\n #2) 设置source组件相关配置 \n a1.sources.r1.type   =   TAILDIR \n a1.sources.r1.positionFile   =   /export/data/flume/taildir_position.json \n a1.sources.r1.filegroups   =   f1 f2 \n a1.sources.r1.filegroups.f1   =   /export/data/momo_data/MOMO_DATA.dat \n a1.sources.r1.filegroups.f2   =   /export/data/momo_data/.*log.* \n\n #3) 设置channel组件相关配置 \n a1.channels.c1.type   =   memory \n a1.channels.c1.capacity   =   1000 \n a1.channels.c1.transactionCapacity   =   100 \n\n #4) 设置sink组件相关配置 \n a1.sinks.k1.type   =   org.apache.flume.sink.kafka.KafkaSink \n a1.sinks.k1.kafka.topic   =   MOMO_MSG \n a1.sinks.k1.kafka.bootstrap.servers   =   node1:9092,node2:9092,node3:9092 \n a1.sinks.k1.kafka.flumeBatchSize   =   10 \n a1.sinks.k1.kafka.producer.acks   =   1 \n a1.sinks.k1.kafka.producer.linger.ms   =   1 \n a1.sinks.k1.kafka.producer.compression.type   =   snappy \n\n #5) 设置连接信息 \n a1.sources.r1.channels   =   c1 \n a1.sinks.k1.channel   =   c1 \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \n \n \n 启动flume的对应相关的软件:  kafka集群 \n \n \n 先启动zookeeper \n 接着启动kafka \n \n \n \n \n 在kafka中构建 MOMO_MSG的 topic :   3分片 2副本 \n \n \n \n cd   /export/server/kafka_2.12-2.4.1/bin/ \n\n ./kafka-topics.sh    --create --zookeeper node1:2181,node2:2181,node3:2181 --topic MOMO_MSG --partitions 3 --replication-factor 2 \n \n 1 2 3 \n \n \n \n 在任意的一个节点上, 启动一个消费者, 监听 MOMO_MSG 消息数据 \n \n \n \n [ root@node3 bin ] # ./kafka-console-consumer.sh  --bootstrap-server node1:9092,node2:9092,node3:9092 --topic MOMO_MSG \n \n 1 \n \n \n \n 启动flume \n \n \n \n   cd  /export/server/apache-flume-1.9.0-bin\n    \nbin/flume-ng agent   -c  conf  -f  conf/MOMO_tailDirSource_kafkaSink.conf  -n  a1  -Dflume.root.logger = INFO,console\n \n 1 2 3 \n \n \n \n 启动陌陌数据源, 生产数据, 查看 消费端是否有数据消费 \n \n \n \n java   -jar  MoMo_DataGen.jar  MoMo_Data.xlsx  /export/data/momo_data/  1000 \n \n 1 \n 陌陌案例_接收消息, 写入到HBase \n 5.1 写入到HBASE_准备工作 \n \n \n \n 启动HBase \n \n \n \n \n 创建一个名称空间: MOMO_CHAT \n \n \n \n create_namespace   \'MOMO_CHAT\'  \n \n 1 \n \n \n 在此空间下创建目标表: MOMO_MSG \n \n \n \n 创建表 :   \n     列族的设计   1个(C1)  预分区操作: 6个  压缩方案:GZ   \n\n 建表语句 : \n    create   \'MOMO_CHAT:MOMO_MSG\',{NAME=> \'C1\',COMPRESSION=>\'GZ\'} , {NUMREGIONS=>6,SPLITALGO =>\'HexStringSplit\'} \n \n 1 2 3 4 5 \n 5.2 陌陌案例中rowkey设计 \n \n 回顾: rowkey的设计原则 \n \n 1) 不要使用递增/时序的键作为rowkey前缀\n2) 建议 rowkey 和列族的长度 越短越好\n3) 用LONG要比string要更好一些, 前提rowkey都是数字\n4) 保证rowkey唯一性\n \n 1 2 3 4 \n 思考rowkey如何设计 \n \n 假设后续的即席查询的需求 :   \n     根据   发件人账户 和 收件人账户 以及发送消息时间 进行查询操作 \n\n ROWKEY   :   \n     hash(发件人账户_收件人账户)_发件人账户_收件人账户_时间戳\n     \n 此rowkey可以满足的内容 : \n 1)   不要使用递增/时序的键作为rowkey前缀 \n 2)   建议 rowkey 和列族的长度 越短越好 \n 3)   保证rowkey唯一性 \n 4)   将关联性比较大的数据放置在一起, 放置后续查询 \n\n 存在问题 :   \n    如果整个消息体系中,   同一个发件人和收件人的消息远远大于其他的人员 会出现某一个region管理高于其他的region \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 5.3 构建一个消费者完成数据写入到HBASE操作 \n \n \n \n 创建一个项目 加入相关的依赖 \n \n \n \n      < repositories > \x3c!--代码库--\x3e \n         < repository > \n             < id > aliyun </ id > \n             < url > http://maven.aliyun.com/nexus/content/groups/public/ </ url > \n             < releases > < enabled > true </ enabled > </ releases > \n             < snapshots > \n                 < enabled > false </ enabled > \n                 < updatePolicy > never </ updatePolicy > \n             </ snapshots > \n         </ repository > \n     </ repositories > \n\n\n     < dependencies > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-java </ artifactId > \n             < version > 1.10.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-java_2.11 </ artifactId > \n             < version > 1.10.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-runtime-web_2.11 </ artifactId > \n             < version > 1.10.0 </ version > \n         </ dependency > \n         \x3c!-- flink操作hdfs，所需要导入该包--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-shaded-hadoop-2-uber </ artifactId > \n             < version > 2.7.5-10.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-connector-kafka_2.11 </ artifactId > \n             < version > 1.10.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.bahir </ groupId > \n             < artifactId > flink-connector-redis_2.11 </ artifactId > \n             < version > 1.0 </ version > \n         </ dependency > \n         \x3c!--Hbase 客户端--\x3e \n         < dependency > \n             < groupId > org.apache.hbase </ groupId > \n             < artifactId > hbase-client </ artifactId > \n             < version > 2.1.0 </ version > \n         </ dependency > \n         \x3c!--kafka 客户端--\x3e \n         < dependency > \n             < groupId > org.apache.kafka </ groupId > \n             < artifactId > kafka-clients </ artifactId > \n             < version > 2.4.1 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.httpcomponents </ groupId > \n             < artifactId > httpclient </ artifactId > \n             < version > 4.5.4 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > com.alibaba </ groupId > \n             < artifactId > fastjson </ artifactId > \n             < version > 1.2.62 </ version > \n         </ dependency > \n\n\n     </ dependencies > \n\n\n\n\n     < build > \n         < plugins > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.1 </ version > \n                 < configuration > \n                     < target > 1.8 </ target > \n                     < source > 1.8 </ source > \n                 </ configuration > \n             </ plugin > \n         </ plugins > \n     </ build > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 \n \n \n 创建包结构: com.itheima.momo.consumertoHBase; \n \n \n \n \n 编写代码: \n \n \n \n package   com . itheima . momo . consumertoHBase ; \n\n import   org . apache . hadoop . conf . Configuration ; \n import   org . apache . hadoop . hbase . HBaseConfiguration ; \n import   org . apache . hadoop . hbase . TableName ; \n import   org . apache . hadoop . hbase . client . Connection ; \n import   org . apache . hadoop . hbase . client . ConnectionFactory ; \n import   org . apache . hadoop . hbase . client . Put ; \n import   org . apache . hadoop . hbase . client . Table ; \n import   org . apache . hadoop . hbase . util . MD5Hash ; \n import   org . apache . kafka . clients . consumer . ConsumerRecord ; \n import   org . apache . kafka . clients . consumer . ConsumerRecords ; \n import   org . apache . kafka . clients . consumer . KafkaConsumer ; \n\n import   java . io . IOException ; \n import   java . text . SimpleDateFormat ; \n import   java . time . Duration ; \n import   java . util . Arrays ; \n import   java . util . Properties ; \n\n public   class   MOMOConsumerToHBase   { \n\n     private     static   Connection  hBConn  ; \n     private    static   Table  table ; \n\n     static   {   // 随着类的加载而加载 一般只会加载一次 \n         try   { \n             //4.1 获取连接对象 \n             Configuration  conf  =   HBaseConfiguration . create ( ) ; \n            conf . set ( "hbase.zookeeper.quorum" , "node1:2181,node2:2181,node3:2181" ) ; \n\n            hBConn  =   ConnectionFactory . createConnection ( conf ) ; \n\n             //4.2 构建表对象 \n            table  =  hBConn . getTable ( TableName . valueOf ( "MOMO_CHAT:MOMO_MSG" ) ) ; \n\n\n         }   catch   ( IOException  e )   { \n            e . printStackTrace ( ) ; \n         } \n\n     } \n\n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         //1. 创建 kafka消费者核心类对象 \n\n         //1.1: 设置相关的配置 \n         Properties  props  =   new   Properties ( ) ; \n\n        props . setProperty ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        props . setProperty ( "group.id" ,   "MOMO_G1" ) ;   // 消费者组 \n        props . setProperty ( "enable.auto.commit" ,   "true" ) ;   // 自动提交 偏移量信息 \n        props . setProperty ( "auto.commit.interval.ms" ,   "1000" ) ;   // 每次提交间隔时间 \n         // 反序列类 \n        props . setProperty ( "key.deserializer" ,   "org.apache.kafka.common.serialization.StringDeserializer" ) ; \n        props . setProperty ( "value.deserializer" ,   "org.apache.kafka.common.serialization.StringDeserializer" ) ; \n\n\n         KafkaConsumer < String , String >  kafkaConsumer  =   new   KafkaConsumer < String ,   String > ( props ) ; \n\n\n         //2. 设置监听的topic \n        kafkaConsumer . subscribe ( Arrays . asList ( "MOMO_MSG" ) ) ; \n\n\n         //3. 循环获取消息数据 \n         while ( true ) { \n             //3.1: 通过消费者从kafka中获取数据: \n             ConsumerRecords < String ,   String >  records  =  kafkaConsumer . poll ( Duration . ofSeconds ( 1 ) ) ; \n\n             for   ( ConsumerRecord < String ,   String >  record  :  records )   { \n                 //3.2: 获取消息数据 \n                 String  message  =  record . value ( ) ; \n\n                 //----------------获取到数据后, 将消息数据写入到HBASE中-------------------- \n\n                 if ( message != null   &&   ! "" . equals ( message . trim ( ) ) ) { \n\n                     String [ ]  fields  =  message . split ( "\\001" ) ; \n                     if ( fields . length  ==   20 ) { \n\n                         //4. 写入到HBASE \n                         //4.3: 执行相关的操作: 写入数据 \n                         String  rowKey  =   getRowKey ( message ) ; \n                         Put  put  =   new   Put ( rowKey . getBytes ( ) ) ; \n\n                        put . addColumn ( "C1" . getBytes ( ) , "msg_time" . getBytes ( ) , fields [ 0 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_nickyname" . getBytes ( ) , fields [ 1 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_account" . getBytes ( ) , fields [ 2 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_sex" . getBytes ( ) , fields [ 3 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_ip" . getBytes ( ) , fields [ 4 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_os" . getBytes ( ) , fields [ 5 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_phone_type" . getBytes ( ) , fields [ 6 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_network" . getBytes ( ) , fields [ 7 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_gps" . getBytes ( ) , fields [ 8 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_nickyname" . getBytes ( ) , fields [ 9 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_ip" . getBytes ( ) , fields [ 10 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_account" . getBytes ( ) , fields [ 11 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_os" . getBytes ( ) , fields [ 12 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_phone_type" . getBytes ( ) , fields [ 13 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_network" . getBytes ( ) , fields [ 14 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_gps" . getBytes ( ) , fields [ 15 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_sex" . getBytes ( ) , fields [ 16 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "msg_type" . getBytes ( ) , fields [ 17 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "distance" . getBytes ( ) , fields [ 18 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "message" . getBytes ( ) , fields [ 19 ] . getBytes ( ) ) ; \n\n\n                        table . put ( put ) ; \n                     } \n\n                 } \n\n             } \n         } \n\n\n     } \n\n     // 用于生成rowkey的方法   hash(发件人账户_收件人账户)_发件人账户_收件人账户_时间戳 \n     public   static   String   getRowKey ( String  message )   throws   Exception { \n\n         // 对消息进行切割操作 \n\n         String [ ]  fields  =  message . split ( "\\001" ) ; \n\n         // 时间  2021-03-17 09:22:27 \n         String  dateStr  =  fields [ 0 ] ; \n\n         SimpleDateFormat  format1  =   new   SimpleDateFormat ( "yyyy-MM-dd HH:mm:ss" ) ; \n         long  timestamp  =  format1 . parse ( dateStr ) . getTime ( ) ; \n\n\n         // 发件人账户 \n         String  sender_account  =  fields [ 2 ] ; \n\n         // 收件人的账户 \n         String  receiver_account  =  fields [ 11 ] ; \n\n         // 组装rowkey \n         StringBuffer  stringBuffer  =   new   StringBuffer ( ) ; \n        stringBuffer . append ( sender_account + "_" ) ; \n        stringBuffer . append ( receiver_account + "_" ) ; \n\n         String  md5Hash  =   MD5Hash . getMD5AsHex ( stringBuffer . toString ( ) . getBytes ( ) ) . substring ( 0 , 8 ) ; \n\n         String  rowkey  =  md5Hash  + "_" + stringBuffer . toString ( ) + timestamp ; \n\n         return   rowkey ; \n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 5.4 测试操作 \n \n \n \n 启动所有相关的软件 \n \n \n \n zookeeper  hadoop  hbase  kafka \n \n 1 \n \n \n 启动消费者 \n \n \n \n \n \n \n \n 启动flume进行数据采集到kafka操作 \n \n \n \n  bin/flume-ng   agent  -c conf -f conf/MOMO_tailDirSource_kafkaSink.conf -n a1 -Dflume.root.logger=INFO,console \n \n 1 \n \n \n 启动 数据源 jar包 \n \n \n \n \n 查询 hbase对应 MOMO_CHAT:MOMO_MSG表 \n 陌陌案例_与Phoenix整合完成即席操作 \n \n \n \n 启动进入Phoenix \n \n \n \n cd /export/server/apache-phoenix-5.0.0-HBase-2.0-bin/\n./sqlline.py node1:2181\n \n 1 2 \n \n \n \n 创建和hbase的映射表 \n \n \n \n create   view  MOMO_CHAT . MOMO_MSG ( \n    id  varchar   primary   key , \n    C1 . "msg_time"   varchar , \n    C1 . "sender_nickyname"   varchar , \n    C1 . "sender_account"   varchar , \n    C1 . "sender_sex"   varchar , \n    C1 . "sender_ip"   varchar , \n    C1 . "sender_os"   varchar , \n    C1 . "sender_phone_type"   varchar , \n    C1 . "sender_network"   varchar , \n    C1 . "sender_gps"   varchar , \n    C1 . "receiver_nickyname"   varchar , \n    C1 . "receiver_ip"   varchar , \n    C1 . "receiver_account"   varchar , \n    C1 . "receiver_os"   varchar , \n    C1 . "receiver_phone_type"   varchar , \n    C1 . "receiver_network"   varchar , \n    C1 . "receiver_gps"   varchar , \n    C1 . "receiver_sex"   varchar , \n    C1 . "msg_type"   varchar , \n    C1 . "distance"   varchar , \n    C1 . "message"   varchar \n ) ; \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \n \n \n 测试即可 \n 陌陌案例_与hive整合完成离线查询 \n \n \n \n 启动hive, 然后进入hive客户端 \n \n \n \n cd /export/server/hive-2.1.0/bin\nnohup ./hive --service metastore &\nnohup ./hive --service hiveserver2 &\n\n\n \n 1 2 3 4 5 \n \n \n \n \n 创建和hbase的映射表 \n \n \n \n create   database  MOMO_CHAT ; \n USE  MOMO_CHAT ; \n create   external  table   MOMO_MSG  ( \n    id string , \n    msg_time string , \n    sender_nickyname string , \n    sender_account string , \n    sender_sex string , \n    sender_ip string , \n    sender_os string , \n    sender_phone_type string , \n    sender_network string , \n    sender_gps string , \n    receiver_nickyname string , \n    receiver_ip string , \n    receiver_account string , \n    receiver_os string , \n    receiver_phone_type string , \n    receiver_network string , \n    receiver_gps string , \n    receiver_sex string , \n    msg_type string , \n    distance string , \n    message string\n )  stored  by   \'org.apache.hadoop.hive.hbase.HBaseStorageHandler\'   with  serdeproperties ( \'hbase.columns.mapping\' = \':key,C1:msg_time,\nC1:sender_nickyname,\nC1:sender_account,\nC1:sender_sex,\nC1:sender_ip,\nC1:sender_os,\nC1:sender_phone_type,\nC1:sender_network,\nC1:sender_gps,\nC1:receiver_nickyname,\nC1:receiver_ip,\nC1:receiver_account,\nC1:receiver_os,\nC1:receiver_phone_type,\nC1:receiver_network,\nC1:receiver_gps,\nC1:receiver_sex,\nC1:msg_type,\nC1:distance,\nC1:message\' )  tblproperties ( \'hbase.table.name\' = \'MOMO_CHAT:MOMO_MSG\' ) ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 \n \n \n 测试即可 \n \n \n \n select * from momo_msg limit 1;\nselect count(1) from momo_msg;\n \n 1 2 \n \n'},{title:"搭建过程中遇到的问题",frontmatter:{title:"搭建过程中遇到的问题",date:"2022-03-09T00:00:00.000Z",publish:!1},regularPath:"/%E5%85%B3%E4%BA%8E%E6%9C%AC%E7%AB%99/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98.html",relativePath:"关于本站/搭建过程中的问题.md",key:"v-dffff514",path:"/2022/03/09/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/",headers:[{level:2,title:"1.build异常",slug:"_1-build异常"},{level:2,title:"2.文章中的图片引用问题",slug:"_2-文章中的图片引用问题"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' 1.build异常 \n 资源加载失败，基本都是路径出了问题。查看dist/index.html文件可以看到，资源路径都写的是根目录下 / ： \n \n 这里的根目录 / 路径是我们之前在/docs/.vuepress/config.js中配置的base字段： \n \n 把base更改为：  base"./" \n \n 本地开发时，把base注释掉，不然会出现 cannot get \n \n \n 然后最重要的一步 \n最重要是在node_modules找到这个文件 \n 在node_modules/@vuepress/core/lib/client/app.js这个文件注释掉mode: \'history\', \n \n \n 最后再打包一下就可以了。 \n 2.文章中的图片引用问题 \n Vuepress中静态资源放在public下，但是typora编辑的时候使用相对文档，使用偏好是在哪个目录下编辑，图片就生成在哪个目录下。 \n 解决办法 \n下载解析插件 \n npm   install  --save-dev markdown-it-disable-url-encode\n \n 1 配置 .vuepress/config.js \n module . exports  =   { \n\t ...  \n     // 处理路径问题 \n     markdown :   { \n         extendMarkdown :   md   =>   { \n            md . set ( { breaks :   true } ) \n            md . use ( require ( "markdown-it-disable-url-encode" ) ,   "./" ) \n         } \n     } \n     ... \n } ; \n \n 1 2 3 4 5 6 7 8 9 10 11 3.打包的js文件超过500kb报错 \n 下载包 \n npm   install  increase-memory-limit  -D \n \n 1 调大参数 \nwindows  \n set   NODE_OPTIONS = --max_old_space_size = 10240 \n #linux \n export   NODE_OPTIONS = --max_old_space_size = 10240 \n \n 1 2 3 4 4.格式异常，特别是网络连接图片,不接受 > 开头 \n \n 解决：修改为小写png格式 \n'},{title:"Docker",frontmatter:{title:"Docker",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["云原生"],tags:["云原生","容器技术"]},regularPath:"/%E4%BA%91%E5%8E%9F%E7%94%9F/Docker.html",relativePath:"云原生/Docker.md",key:"v-33a310e8",path:"/2023/06/10/docker/",headers:[{level:2,title:"1. Docker 的介绍和安装",slug:"_1-docker-的介绍和安装"},{level:3,title:"容器技术的介绍",slug:"容器技术的介绍"},{level:3,title:"在 Linux 系统上安装 Docker",slug:"在-linux-系统上安装-docker"},{level:2,title:"2.容器的快速上手",slug:"_2-容器的快速上手"},{level:3,title:"docker命令行的认识",slug:"docker命令行的认识"},{level:3,title:"理解Image vs Container 镜像 vs 容器",slug:"理解image-vs-container-镜像-vs-容器"},{level:3,title:"容器的基本操作",slug:"容器的基本操作"},{level:3,title:"docker container 命令小技巧",slug:"docker-container-命令小技巧"},{level:2,title:"批量停止",slug:"批量停止"},{level:2,title:"批量删除",slug:"批量删除"},{level:3,title:"Container Mode 容器运行的各种模式",slug:"container-mode-容器运行的各种模式"},{level:3,title:"连接容器的 shell",slug:"连接容器的-shell"},{level:3,title:"容器和虚拟机 Container vs VM",slug:"容器和虚拟机-container-vs-vm"},{level:3,title:"docker container run 背后发生了什么？",slug:"docker-container-run-背后发生了什么"},{level:2,title:"3. 镜像的获取的三种方式",slug:"_3-镜像的获取的三种方式"},{level:3,title:"从registry拉取pull",slug:"从registry拉取pull"},{level:3,title:"从离线导入load",slug:"从离线导入load"},{level:3,title:"从dockerfile build",slug:"从dockerfile-build"},{level:2,title:"4.Dockerfile命令详解",slug:"_4-dockerfile命令详解"},{level:3,title:"基础镜像的选择 (FROM)",slug:"基础镜像的选择-from"},{level:3,title:"在image里RUN 执行指令",slug:"在image里run-执行指令"},{level:3,title:"文件复制 (ADD,COPY)",slug:"文件复制-add-copy"},{level:3,title:"构建参数和环境变量 (ARG↑ vs ENV)",slug:"构建参数和环境变量-arg↑-vs-env"},{level:3,title:"容器启动命令 CMD",slug:"容器启动命令-cmd"},{level:3,title:"容器启动命令 ENTRYPOINT",slug:"容器启动命令-entrypoint"},{level:3,title:"Shell 格式和 Exec 格式",slug:"shell-格式和-exec-格式"},{level:3,title:"实战：构建一个 Python Flask 镜像",slug:"实战-构建一个-python-flask-镜像"},{level:3,title:"Dockerfile 技巧——镜像的多阶段构建",slug:"dockerfile-技巧-镜像的多阶段构建"},{level:3,title:"Dockerfile 技巧——尽量使用非root用户",slug:"dockerfile-技巧-尽量使用非root用户"},{level:2,title:"5.Docker的存储",slug:"_5-docker的存储"},{level:3,title:"Data Volume",slug:"data-volume"},{level:3,title:"Data Volume 练习 MySQL",slug:"data-volume-练习-mysql"},{level:3,title:"多个机器之间的容器共享数据",slug:"多个机器之间的容器共享数据"},{level:2,title:"6.Docker网络",slug:"_6-docker网络"},{level:3,title:"容器间通信",slug:"容器间通信"},{level:3,title:"外部访问容器：端口转发",slug:"外部访问容器-端口转发"},{level:3,title:"同一host中的不同bridge通信",slug:"同一host中的不同bridge通信"},{level:3,title:"容器通信的底层原理：",slug:"容器通信的底层原理"},{level:2,title:"7.Docker Compose",slug:"_7-docker-compose"},{level:3,title:"docker compose 介绍",slug:"docker-compose-介绍"},{level:3,title:"docker compose 的安装",slug:"docker-compose-的安装"},{level:3,title:"docker-compose基本使用",slug:"docker-compose基本使用"},{level:3,title:"docker-compose 服务的构建、拉取与更新",slug:"docker-compose-服务的构建、拉取与更新"},{level:3,title:"docker-compose 网络",slug:"docker-compose-网络"},{level:3,title:"水平扩展 scale",slug:"水平扩展-scale"},{level:3,title:"docker compose 环境变量",slug:"docker-compose-环境变量"},{level:3,title:"docker compose 健康检查",slug:"docker-compose-健康检查"},{level:3,title:"一站式hadoop集群便于测试开发",slug:"一站式hadoop集群便于测试开发"},{level:2,title:"8.Docker Swarm",slug:"_8-docker-swarm"},{level:3,title:"docker swarm 介绍",slug:"docker-swarm-介绍"},{level:3,title:"Swarm 单节点快速上手",slug:"swarm-单节点快速上手"},{level:3,title:"Docker Stack",slug:"docker-stack"},{level:2,title:"9.Docker整合gitlab CICD",slug:"_9-docker整合gitlab-cicd"},{level:3,title:"单模块下部署",slug:"单模块下部署"},{level:3,title:"多模块合并部署",slug:"多模块合并部署"},{level:3,title:"部署过程中遇到的问题",slug:"部署过程中遇到的问题"},{level:2,title:"10.Docker 整合Sonar进行代码质量检查,统计单元测试覆盖率",slug:"_10-docker-整合sonar进行代码质量检查-统计单元测试覆盖率"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' 1. Docker 的介绍和安装 \n 容器技术的介绍 \n 注意我们这里所说的容器container是指的一种技术，而Docker只是一个容器技术的实现，或者说让容器技术普及开来的最成功的实现 \n 容器正在引领基础架构的一场新的革命 \n \n 90年代的PC \n 00年代的虚拟化 \n 10年代的cloud \n 11年代的container \n \n 什么是container(容器）？ \n 容器是一种快速的打包技术 \n Package Software into Standardized Units for Development, Shipment and Deployment \n \n 标准化 \n 轻量级 \n 易移植 \n \n 为什么容器技术会出现？ \n 容器技术出现之前 \n \n 容器技术出现之后 \n \n 容器 vs 虚拟机 \n \n Linux Container容器技术的诞生于2008年（Docker诞生于2013年），解决了IT世界里“集装箱运输”的问题。Linux Container（简称LXC）它是一种内核轻量级的操作系统层虚拟化技术。Linux Container主要由Namespace 和Cgroups  两大机制来保证实现 \n \n Namespace命名空间主要用于资源的隔离（诞生于2002年） \n Cgroups(Control Groups)就负责资源管理控制作用，比如进程组使用CPU/MEM的限制，进程组的优先级控制，进程组的挂起和恢复等等。（由Google贡献，2008年合并到了Linux Kernel） \n \n 容器的标准化 \n docker != container\n \n 1 在2015年，由Google，Docker、红帽等厂商联合发起了OCI（Open Container Initiative）组织，致力于容器技术的标准化 \n 容器运行时标准 （runtime spec） \n 简单来讲就是规定了容器的基本操作规范，比如如何下载镜像，创建容器，启动容器等。 \n 容器镜像标准（image spec） \n 主要定义镜像的基本格式。 \n 容器是关乎“速度” \n \n 容器会加速你的软件开发 \n 容器会加速你的程序编译和构建 \n 容器会加速你的测试 \n 容器会速度你的部署 \n 容器会加速你的更新 \n 容器会速度你的故障恢复 \n \n 容器的快速发展和普及 \n 到2020年，全球超过50%的公司将在生产环境中使用container —— Gartner \n 在 Linux 系统上安装 Docker \n 使用脚本快速安装 \n #下载安装脚本到get-dcoker.sh,version为可选 \n curl   -fsSL  get.docker.com  -o  get-docker.sh  \n sh  get-docker.sh  --version   < VERSION > \n #确认是否安装成功 \n sudo  systemctl start  docker \n sudo   docker  version\n \n 1 2 3 4 5 6 \n 2.容器的快速上手 \n docker命令行的认识 \n docker + 管理的对象（比如容器，镜像） + 具体操作（比如创建，启动，停止，删除） \n 例如 \n \n docker image pull nginx  拉取一个叫nginx的docker image镜像 \n docker container stop web  停止一个叫web的docker container容器 \n \n docker  version 或docker info  #查看版本 \n docker   help   #查看docker的命令 \n docker  container  --help \n docker  container  ps   #当前运行的容器 \n docker  container  ps   -a   #当前所有的容器 \n docker  container  ls   #与ps效果一样，ps是老版的命令 \n docker  image  ls \n docker  image  rm  \n \n 1 2 3 4 5 6 7 8 #  理解Image vs Container 镜像 vs 容器 \n image镜像 \n \n Docker image是一个  read-only  文件 \n 这个文件包含文件系统，源码，库文件，依赖，工具等一些运行application所需要的文件 \n 可以理解成一个模板 \n docker image具有分层的概念 \n \n container容器 \n \n “一个运行中的docker image” \n 实质是复制image并在image最上层加上一层  read-write  的层 （称之为  container layer  ,容器层） \n 基于同一个image可以创建多个container \n \n \n docker image的获取 \n \n 自己制作 \n 从registry拉取（比如docker hub） \n \n 配置国内镜像仓库加速拉取，跟maven配置镜像源一个道理 \n \n \n 为docker配置国内镜像地址，用于在pull镜像下载加速 \n \n 创建配置文件daemon.json \n \n 在目录/etc/docker/daemon.json下，如果没，则创建该文件 \n \n 按如下格式化添加镜像地址 \n \n { \n   "registry-mirrors" :   [ \n     "https://registry.docker-cn.com" ,\n     "http://hub-mirror.c.163.com" ,\n     "https://docker.mirrors.ustc.edu.cn" \n   ] \n } \n #docker中国镜像地址：    "https://registry.docker-cn.com", \n #网络docker镜像地址  "http://hub-mirror.c.163.com", \n #ustc大学镜像地址 "https://docker.mirrors.ustc.edu.cn" \n \n 1 2 3 4 5 6 7 8 9 10 \n 添加完重启docker，并使用docker info 命令查看 \n \n sudo  systemctl daemon-reload\n sudo  systemctl restart  docker \n sudo   docker  info\n \n 1 2 3 docker信息: \n 容器的基本操作 \n docker container 命令小技巧 \n 批量停止 \n \n \n $  docker  container  ps \nCONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS     NAMES\ncd3a825fedeb   nginx      "/docker-entrypoint.…"     7  seconds ago    Up  6  seconds     80 /tcp    mystifying_leakey\n269494fe89fa   nginx      "/docker-entrypoint.…"     9  seconds ago    Up  8  seconds     80 /tcp    funny_gauss\n34b68af9deef   nginx      "/docker-entrypoint.…"     12  seconds ago   Up  10  seconds    80 /tcp    interesting_mahavira\n7513949674fc   nginx      "/docker-entrypoint.…"     13  seconds ago   Up  12  seconds    80 /tcp    kind_nobel\n \n 1 2 3 4 5 6 方法1 \n $  docker  container stop cd3  269  34b  751 \n \n 1 方法2 \n $  docker  container stop  $( docker  container  ps   -aq ) \ncd3a825fedeb\n269494fe89fa\n34b68af9deef\n7513949674fc\n$  docker  container  ps   -a \nCONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS                     PORTS     NAMES\ncd3a825fedeb   nginx      "/docker-entrypoint.…"     30  seconds ago   Exited  ( 0 )   2  seconds ago             mystifying_leakey\n269494fe89fa   nginx      "/docker-entrypoint.…"     32  seconds ago   Exited  ( 0 )   2  seconds ago             funny_gauss\n34b68af9deef   nginx      "/docker-entrypoint.…"     35  seconds ago   Exited  ( 0 )   2  seconds ago             interesting_mahavira\n7513949674fc   nginx      "/docker-entrypoint.…"     36  seconds ago   Exited  ( 0 )   2  seconds ago             kind_nobel\n$\n \n 1 2 3 4 5 6 7 8 9 10 11 12 \n 批量删除 \n \n \n 和批量停止类似，可以使用  docker container rm $(docker container ps -aq) \n \n docker system prune -a -f  可以快速对系统进行清理，删除停止的容器，不用的image，等等 \n Container Mode 容器运行的各种模式 \n attach 模式 \n 范例指令: docker container run -p 80:80 nginx \n \n 透过这种方式创建容器的话，容器在前台执行 \n 容器的输入输出结果会反映到本地端，本地端的输入输出也会反映到容器，例如能在终端机看到网页浏览器的 log，ctrl + c 会让容器停止执行 \n 一般情况不推荐使用 \n detach 模式(一般生产中使用) \n 范例指令: \n docker  container run  -d   -p   80 :80 nginx\n \n 1 \n \n 容器会在后台执行 \n \n \n 想进入查看log 可通过 \n \n \n #这种方式进入 ctl+c退出，容器也会stop \n docker  container attach  id \n #故查看日志，推荐docker logs \n docker  container logs  id \n \n 1 2 3 4 \n 连接容器的 shell \n 创建一个容器并进入交互式模式 \n docker  container run  -it \n \n 1 ~  docker  container run  -it  busybox  sh \n/  # \n/  # \n/  # ls \nbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/  # ps \nPID    USER      TIME  COMMAND\n     1  root       0 :00  sh \n     8  root       0 :00  ps \n/  # exit \n \n 1 2 3 4 5 6 7 8 9 10 在一个已经运行的容器里执行一个额外的command \n docker  container  exec   -it \n \n 1 ➜  ~ docker container run -d nginx\n33d2ee50cfc46b5ee0b290f6ad75d724551be50217f691e68d15722328f11ef6\n➜  ~\n➜  ~ docker container exec -it 33d sh\nls\nbin  boot  dev  docker-entrypoint.d  docker-entrypoint.sh  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\nexit\n➜  ~\n \n 1 2 3 4 5 6 7 8 9 10 11 #  容器和虚拟机 Container vs VM \n \n \n \n 容器不是Mini虚拟机 \n \n \n 容器其实是进程Containers are just processes \n \n \n 容器中的进程被限制了对CPU内存等资源的访问 \n \n \n 当进程停止后，容器就退出了 \n docker container run 背后发生了什么？ \n $ docker container run -d --publish 80:80 --name webhost nginx\n \n 1 \n \n \n 在本地查找是否有nginx这个image镜像，但是没有发现 \n \n \n \n \n 去远程的image registry查找nginx镜像（默认的registry是Docker Hub) \n \n \n \n \n 下载最新版本的nginx镜像 （nginx:latest 默认) \n \n \n \n \n 基于nginx镜像来创建一个新的容器，并且准备运行 \n \n \n \n \n docker engine分配给这个容器一个虚拟IP地址 \n \n \n \n \n 在宿主机上打开80端口并把容器的80端口转发到宿主机上 \n \n \n \n \n 启动容器，运行指定的命令（这里是一个shell脚本去启动nginx） \n 3. 镜像的获取的三种方式 \n \n pull from  registry  (online) 从registry拉取\n \n public（公有） \n private（私有） \n \n \n build from  Dockerfile  (online) 从Dockerfile构建 \n load from  file  (offline) 文件导入 （离线） \n \n \n 镜像的命令查询 \n $  docker  image\n\nUsage:   docker  image COMMAND\n\nManage images\n\nCommands:\nbuild       Build an image from a Dockerfile\n history      Show the  history  of an image\n import       Import the contents from a tarball to create a filesystem image\ninspect     Display detailed information on one or  more  images\nload        Load an image from a  tar  archive or STDIN\n ls           List images\nprune       Remove unused images\npull        Pull an image or a repository from a registry\npush        Push an image or a repository to a registry\n rm           Remove one or  more  images\nsave        Save one or  more  images to a  tar  archive  ( streamed to STDOUT by default ) \ntag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE\n\nRun  \'docker image COMMAND --help\'   for   more  information on a command.\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #  从registry拉取pull \n docker  pull nginx  #默认从Docker Hub拉取，如果不指定版本，会拉取最新版 \n docker  pull nginx:1.20.0  # 指定版本 \n docker  pull quay.io/bitnami/nginx  #从指定仓库Quay上拉取镜像 \n docker  image  ls   #镜像的查看 \n docker  image  rm   id   #镜像的删除 \n \n 1 2 3 4 5 #  从离线导入load \n 模拟生成一个离线的镜像 \n docker  image save nginx:1.20.0  -o  nginx.myself\n \n 1 \n docker  image load  -i  nginx.myself\n \n 1 \n 从dockerfile build \n Dockerfile 介绍 \n Docker can build images automatically by reading the instructions from a  Dockerfile . A Dockerfile is a  text  document that contains all the commands a user could call on the command line to assemble an image. Using docker build users can create an automated build that executes several command-line instructions in succession. \n https://docs.docker.com/engine/reference/builder/ \n \n Dockerfile是用于构建docker镜像的文件 \n Dockerfile里包含了构建镜像所需的“指令” \n Dockerfile有其特定的语法规则 \n Dockerfile的构建 \n 示例：使用centos 脚本打印"hello docker !" \n hello.sh \n echo   "hello docker !" \n \n 1 Dockerfile \n FROM  centos:7 \n ADD  hello.sh / \n CMD  [ "sh" ,  "/hello.sh" ] \n \n 1 2 3 \n 示例：部署一个Hello Docker java项目 \n 新建HelloWorld,并将项目打包 \n public   class   HelloDocker   { \n     public   static   void   main ( String [ ]  args )   { \n         System . out . println ( "Hello Docker !" ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 新建一个Dockerfile文件 \n FROM  openjdk:8-jdk-alpine \n ADD  *.jar app.jar \n ENTRYPOINT  [ "java" , "-Djava.security.egd=file:/dev/./urandom" , "-jar" , "/app.jar" ] \n \n 1 2 3 将文件上传到linux服务器上 \n \n 构建镜像 \n docker  image build  -t  docker-demo-java  . \n \n 1 \n 重新打标签 \n docker  image tag docker-demo-java docker-demo-java:2.0\n #实际应按照远程仓库的规则打标签 \n docker  tag  [ ImageId ]  registry.cn-shenzhen.aliyuncs.com/gordonchan/quickstart: [ 镜像版本号 ] \n \n 1 2 3 \n 推送镜像到远程仓库进行共享 \n docker  login  --username = XXXXXXX registry.cn-shenzhen.aliyuncs.com\n docker  push registry.cn-shenzhen.aliyuncs.com/gordonchan/quickstart: [ 镜像版本号 ] \n \n 1 2 \n \n 拉取镜像并构建容器运行 \n \n \n 示例：执行一个Python程序 \n 容器及进程，所以镜像就是一个运行这个进程所需要的环境。 \n 假如我们要在centos:7上运行下面这个hello.py的Python程序 \n hello.py的文件内容： \n print("hello docker")\n \n 1 第一步， 准备Python环境 \n yum install -y python3.9 python3-pip python3.9-dev\n \n 1 第二步， 运行hello.py \n $ python3 hello.py\nhello docker\n \n 1 2 \n 把上面的程序构建成一个Dockerfile \n Dockerfile \n ARG  image=centos:7 \n FROM   ${image} \n RUN   yum install -y python3.9 python3-pip python3.9-dev \n ADD  hello.py / \n CMD  [ "python3" ,  "/hello.py" ] \n \n 1 2 3 4 5 #  4.Dockerfile命令详解 \n 基础镜像的选择 (FROM) \n 基本原则 \n \n 官方镜像优于非官方的镜像，如果没有官方镜像，则尽量选择Dockerfile开源的 \n 固定版本tag而不是每次都使用latest \n 尽量选择体积小的镜像 \n \n $  docker  image  ls \nREPOSITORY      TAG             IMAGE ID       CREATED          SIZE\nbitnami/nginx    1.18 .0          dfe237636dde    28  minutes ago    89 .3MB\nnginx            1.21 .0-alpine   a6eb2a334a9f    2  days ago        22 .6MB\nnginx            1.21 .0          d1a364dc548d    2  days ago       133MB\n \n 1 2 3 4 5 Build一个Nginx镜像 \n 假如我们有一个  index.html  文件 \n < h1 > Hello Docker </ h1 > \n \n 1 准备一个Dockerfile \n FROM  nginx:1.21.0-alpine \n\n ADD  ../其他/index.html /usr/share/nginx/html/index.html \n \n 1 2 3 #  在image里RUN 执行指令 \n RUN  主要用于在build Image里执行指令，比如安装软件，下载文件等。 \n $  apt-get  update\n$  apt-get   install   wget \n$  wget  https://github.com/ipinfo/cli/releases/download/ipinfo-2.0.1/ipinfo_2.0.1_linux_amd64.tar.gz\n$  tar  zxf ipinfo_2.0.1_linux_amd64.tar.gz\n$  mv  ipinfo_2.0.1_linux_amd64 /usr/bin/ipinfo\n$  rm   -rf  ipinfo_2.0.1_linux_amd64.tar.gz\n \n 1 2 3 4 5 6 Dockerfile \n FROM  ubuntu:20.04 \n RUN  apt-get update \n RUN  apt-get install -y wget \n RUN  wget https://github.com/ipinfo/cli/releases/download/ipinfo-2.0.1/ipinfo_2.0.1_linux_amd64.tar.gz \n RUN  tar zxf ipinfo_2.0.1_linux_amd64.tar.gz \n RUN  mv ipinfo_2.0.1_linux_amd64 /usr/bin/ipinfo \n RUN  rm -rf ipinfo_2.0.1_linux_amd64.tar.gz \n \n 1 2 3 4 5 6 7 镜像的大小和分层 \n $  docker  image  ls \nREPOSITORY   TAG       IMAGE ID       CREATED         SIZE\nipinfo       latest    97bb429363fb    4  minutes ago   138MB\nubuntu        21.04      478aa0080b60    4  days ago       74 .1MB\n$  docker  image  history  97b\nIMAGE          CREATED         CREATED BY                                      SIZE      COMMENT\n97bb429363fb    4  minutes ago   RUN /bin/sh  -c   rm   -rf  ipinfo_2.0.1_linux_amd…   0B        buildkit.dockerfile.v0\n < missing >        4  minutes ago   RUN /bin/sh  -c   mv  ipinfo_2.0.1_linux_amd64 /…    9 .36MB    buildkit.dockerfile.v0\n < missing >        4  minutes ago   RUN /bin/sh  -c   tar  zxf ipinfo_2.0.1_linux_am…    9 .36MB    buildkit.dockerfile.v0\n < missing >        4  minutes ago   RUN /bin/sh  -c   wget  https://github.com/ipinf…    4 .85MB    buildkit.dockerfile.v0\n < missing >        4  minutes ago   RUN /bin/sh  -c   apt-get   install   -y   wget   # bui…   7.58MB    buildkit.dockerfile.v0 \n < missing >        4  minutes ago   RUN /bin/sh  -c   apt-get  update  # buildkit        33MB      buildkit.dockerfile.v0 \n < missing >        4  days ago      /bin/sh  -c   #(nop)  CMD ["/bin/bash"]            0B \n < missing >        4  days ago      /bin/sh  -c   mkdir   -p  /run/systemd  &&   echo   \'do…   7B\n<missing>      4 days ago      /bin/sh -c [ -z "$(apt-get indextargets)" ]     0B\n<missing>      4 days ago      /bin/sh -c set -xe   && echo \' #!/bin/sh\' > /…   811B \n < missing >        4  days ago      /bin/sh  -c   #(nop) ADD file:d6b6ba642344138dc…   74.1MB \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 每一行的RUN命令都会产生一层image layer, 导致镜像的臃肿。 \n 改进版Dockerfile \n FROM  ubuntu:20.04 \n RUN  apt-get update &&  \\ \n    apt-get install -y wget &&  \\ \n    wget https://github.com/ipinfo/cli/releases/download/ipinfo-2.0.1/ipinfo_2.0.1_linux_amd64.tar.gz &&  \\ \n    tar zxf ipinfo_2.0.1_linux_amd64.tar.gz &&  \\ \n    mv ipinfo_2.0.1_linux_amd64 /usr/bin/ipinfo &&  \\ \n    rm -rf ipinfo_2.0.1_linux_amd64.tar.gz \n \n 1 2 3 4 5 6 7 $  docker  image  ls \nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\nipinfo-new   latest    fe551bc26b92    5  seconds ago    124MB\nipinfo       latest    97bb429363fb    16  minutes ago   138MB\nubuntu        21.04      478aa0080b60    4  days ago        74 .1MB\n$  docker  image  history  fe5\nIMAGE          CREATED          CREATED BY                                      SIZE      COMMENT\nfe551bc26b92    16  seconds ago   RUN /bin/sh  -c   apt-get  update  &&      apt-get…    49 .9MB    buildkit.dockerfile.v0\n < missing >        4  days ago       /bin/sh  -c   #(nop)  CMD ["/bin/bash"]            0B \n < missing >        4  days ago       /bin/sh  -c   mkdir   -p  /run/systemd  &&   echo   \'do…   7B\n<missing>      4 days ago       /bin/sh -c [ -z "$(apt-get indextargets)" ]     0B\n<missing>      4 days ago       /bin/sh -c set -xe   && echo \' #!/bin/sh\' > /…   811B \n < missing >        4  days ago       /bin/sh  -c   #(nop) ADD file:d6b6ba642344138dc…   74.1MB \n$\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #  文件复制 (ADD,COPY) \n 往镜像里复制文件有两种方式， COPY  和  ADD  , 我们来看一下两者的不同。 \n 复制普通文件 \n COPY  和  ADD  都可以 把local的一个文件复制到镜像里 ，如果目标目录不存在，则会自动创建 \n FROM  python:3.9.5-alpine3.13 \n COPY  hello.py /app/hello.py \n \n 1 2 比如把本地的 hello.py 复制到 /app 目录下。 /app这个folder不存在，则会自动创建 \n 复制压缩文件 \n ADD  比 COPY高级一点的地方就是，如果复制的是一个gzip等压缩文件时，ADD会帮助我们自动去解压缩文件。 \n FROM  python:3.9.5-alpine3.13 \n ADD  hello.tar.gz /app/ \n \n 1 2 如何选择 \n 因此在 COPY 和 ADD 指令中选择的时候，可以遵循这样的原则，所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD。 \n 构建参数和环境变量 (ARG↑ vs ENV) \n ARG  和  ENV  是经常容易被混淆的两个Dockerfile的语法，都可以用来设置一个“变量”。 但实际上两者有很多的不同。 \n FROM  ubuntu:20.04 \n RUN  apt-get update &&  \\ \n    apt-get install -y wget &&  \\ \n    wget https://github.com/ipinfo/cli/releases/download/ipinfo-2.0.1/ipinfo_2.0.1_linux_amd64.tar.gz &&  \\ \n    tar zxf ipinfo_2.0.1_linux_amd64.tar.gz &&  \\ \n    mv ipinfo_2.0.1_linux_amd64 /usr/bin/ipinfo &&  \\ \n    rm -rf ipinfo_2.0.1_linux_amd64.tar.gz \n \n 1 2 3 4 5 6 7 ENV \n FROM  ubuntu:20.04 \n ENV  VERSION=2.0.1 \n RUN  apt-get update &&  \\ \n    apt-get install -y wget &&  \\ \n    wget https://github.com/ipinfo/cli/releases/download/ipinfo- ${VERSION} /ipinfo_ ${VERSION} _linux_amd64.tar.gz &&  \\ \n    tar zxf ipinfo_ ${VERSION} _linux_amd64.tar.gz &&  \\ \n    mv ipinfo_ ${VERSION} _linux_amd64 /usr/bin/ipinfo &&  \\ \n    rm -rf ipinfo_ ${VERSION} _linux_amd64.tar.gz \n \n 1 2 3 4 5 6 7 8 ARG \n FROM  ubuntu:20.04 \n ARG  VERSION=2.0.1 \n RUN  apt-get update &&  \\ \n    apt-get install -y wget &&  \\ \n    wget https://github.com/ipinfo/cli/releases/download/ipinfo- ${VERSION} /ipinfo_ ${VERSION} _linux_amd64.tar.gz &&  \\ \n    tar zxf ipinfo_ ${VERSION} _linux_amd64.tar.gz &&  \\ \n    mv ipinfo_ ${VERSION} _linux_amd64 /usr/bin/ipinfo &&  \\ \n    rm -rf ipinfo_ ${VERSION} _linux_amd64.tar.gz \n \n 1 2 3 4 5 6 7 8 区别 \n \n ARG 可以在镜像build的时候动态修改value, 通过  --build-arg \n $ docker image build -f .\\Dockerfile-arg -t ipinfo-arg-2.0.0 --build-arg VERSION=2.0.0 .\n$ docker image ls\nREPOSITORY         TAG       IMAGE ID       CREATED          SIZE\nipinfo-arg-2.0.0   latest    0d9c964947e2   6 seconds ago    124MB\n$ docker container run -it ipinfo-arg-2.0.0\nroot@b64285579756:/#\nroot@b64285579756:/# ipinfo version\n2.0.0\nroot@b64285579756:/#\n \n 1 2 3 4 5 6 7 8 9 ENV 设置的变量可以在Image中保持，并在容器中的环境变量里 \n 容器启动命令 CMD \n CMD可以用来设置容器启动时默认会执行的命令。 \n \n 容器启动时默认执行的命令 \n 如果docker container run启动容器时指定了其它命令，则CMD命令会被忽略 \n 如果定义了多个CMD，只有最后一个会被执行。 \n \n FROM ubuntu:20.04\nENV VERSION=2.0.1\nRUN apt-get update && \\\n    apt-get install -y wget && \\\n    wget https://github.com/ipinfo/cli/releases/download/ipinfo-${VERSION}/ipinfo_${VERSION}_linux_amd64.tar.gz && \\\n    tar zxf ipinfo_${VERSION}_linux_amd64.tar.gz && \\\n    mv ipinfo_${VERSION}_linux_amd64 /usr/bin/ipinfo && \\\n    rm -rf ipinfo_${VERSION}_linux_amd64.tar.gz\n \n 1 2 3 4 5 6 7 8 $ docker image build -t ipinfo .\n$ docker container run -it ipinfo\nroot@8cea7e5e8da8:/#\nroot@8cea7e5e8da8:/#\nroot@8cea7e5e8da8:/#\nroot@8cea7e5e8da8:/# pwd\n/\nroot@8cea7e5e8da8:/#\n \n 1 2 3 4 5 6 7 8 默认进入到shell是因为在ubuntu的基础镜像里有定义CMD \n $docker image history ipinfo\nIMAGE          CREATED        CREATED BY                                      SIZE      COMMENT\ndb75bff5e3ad   24 hours ago   RUN /bin/sh -c apt-get update &&     apt-get…   50MB      buildkit.dockerfile.v0\n<missing>      24 hours ago   ENV VERSION=2.0.1                               0B        buildkit.dockerfile.v0\n<missing>      7 days ago     /bin/sh -c #(nop)  CMD ["/bin/bash"]            0B\n<missing>      7 days ago     /bin/sh -c mkdir -p /run/systemd && echo \'do…   7B\n<missing>      7 days ago     /bin/sh -c [ -z "$(apt-get indextargets)" ]     0B\n<missing>      7 days ago     /bin/sh -c set -xe   && echo \'#!/bin/sh\' > /…   811B\n<missing>      7 days ago     /bin/sh -c #(nop) ADD file:d6b6ba642344138dc…   74.1MB\n \n 1 2 3 4 5 6 7 8 9 #  容器启动命令 ENTRYPOINT \n ENTRYPOINT 也可以设置容器启动时要执行的命令，但是和CMD是有区别的。 \n \n CMD  设置的命令，可以在docker container run 时传入其它命令，覆盖掉  CMD  的命令，但是  ENTRYPOINT  所设置的命令是一定会被执行的。 \n ENTRYPOINT  和  CMD  可以联合使用， ENTRYPOINT  设置执行的命令，CMD传递参数 \n \n FROM ubuntu:20.04\nCMD ["echo", "hello docker"]\n \n 1 2 把上面的Dockerfile build成一个叫  demo-cmd  的镜象 \n $ docker image ls\nREPOSITORY        TAG       IMAGE ID       CREATED      SIZE\ndemo-cmd          latest    5bb63bb9b365   8 days ago   74.1MB\n \n 1 2 3 FROM ubuntu:20.04\nENTRYPOINT ["echo", "hello docker"]\n \n 1 2 build成一个叫  demo-entrypoint  的镜像 \n $ docker image ls\nREPOSITORY        TAG       IMAGE ID       CREATED      SIZE\ndemo-entrypoint   latest    b1693a62d67a   8 days ago   74.1MB\n \n 1 2 3 CMD的镜像，如果执行创建容器，不指定运行时的命令，则会默认执行CMD所定义的命令，打印出hello docker \n $ docker container run -it --rm demo-cmd\nhello docker\n \n 1 2 但是如果我们docker container run的时候指定命令，则该命令会覆盖掉CMD的命令，如： \n $ docker container run -it --rm demo-cmd echo "hello world"\nhello world\n \n 1 2 但是ENTRYPOINT的容器里ENTRYPOINT所定义的命令则无法覆盖，一定会执行 \n $ docker container run -it --rm demo-entrypoint\nhello docker\n$ docker container run -it --rm demo-entrypoint echo "hello world"\nhello docker echo hello world\n$\n \n 1 2 3 4 5 #  Shell 格式和 Exec 格式 \n 容器的启动命令中带有参数传递的处理方式 \n CMD和ENTRYPOINT同时支持shell格式和Exec格式。 \n Shell格式 \n CMD echo "hello docker"\n \n 1 ENTRYPOINT echo "hello docker"\n \n 1 Exec格式 \n 以可执行命令的方式 \n ENTRYPOINT ["echo", "hello docker"]\n \n 1 CMD ["echo", "hello docker"]\n \n 1 注意shell脚本的问题 \n FROM ubuntu:20.04\nENV NAME=docker\nCMD echo "hello $NAME"\n \n 1 2 3 假如我们要把上面的CMD改成Exec格式，下面这样改是不行的, 大家可以试试。 \n FROM ubuntu:20.04\nENV NAME=docker\nCMD ["echo", "hello $NAME"]\n \n 1 2 3 它会打印出  hello $NAME  , 而不是  hello docker  ,那么需要怎么写呢？ 我们需要以shell脚本的方式去执行 \n FROM ubuntu:20.04\nENV NAME=docker\nCMD ["sh", "-c", "echo hello $NAME"]\n \n 1 2 3 #  实战：构建一个 Python Flask 镜像 \n Python 程序 \n from  flask  import  Flask\n\napp  =  Flask ( __name__ ) \n\n\n @app . route ( \'/\' ) \n def   hello_world ( ) : \n     return   \'Hello, World!\' \n \n 1 2 3 4 5 6 7 8 Dockerfile \n FROM  python:3.9.5-slim \n\n COPY  app.py /src/app.py \n\n RUN  pip install flask \n\n WORKDIR  /src \n ENV  FLASK_APP=app.py \n\n EXPOSE  5000 \n\n CMD  [ "flask" ,  "run" ,  "-h" ,  "0.0.0.0" ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 docker  image build  -t  flask  . \n docker  run  -d   -p   5000 :5000 flask\n \n 1 2 \n Dockerfile 技巧——合理使用缓存 \n 如果按照上述Dockerfile，修改app.py后，之后的操作比如install flask都不会使用cache，这样影响构建效率，因此，把经常改变的内容放在后面，尽可能的使用缓存，提高构建速度。 \n FROM  python:3.9.5-slim \n RUN  pip install flask \n WORKDIR  /src \n ENV  FLASK_APP=app.py \n EXPOSE  5000 \n COPY  app.py /src/app.py \n CMD  [ "flask" ,  "run" ,  "-h" ,  "0.0.0.0" ] \n \n 1 2 3 4 5 6 7 \n Dockerfile 技巧——合理使用 .dockerignore \n 什么是Docker build context \n Docker是client-server架构，理论上Client和Server可以不在一台机器上。 \n 在构建docker镜像的时候，需要把所需要的文件由CLI（client）发给Server，这些文件实际上就是build context \n 加入文件夹下有些文件不是必须的，可以过滤掉。 \n Dockerfile \n FROM  python:3.9.5-slim \n RUN  pip install flask \n WORKDIR  /src \n ENV  FLASK_APP=app.py \n EXPOSE  5000 \n COPY  ./ /src/ \n CMD  [ "flask" ,  "run" ,  "-h" ,  "0.0.0.0" ] \n \n 1 2 3 4 5 6 7 \n \n 增加vim .dockerignore \n ./hello.image\n \n 1 有了.dockerignore文件后，我们再build, build context就小了很多 \n Dockerfile 技巧——镜像的多阶段构建 \n 这一节来聊聊多阶段构建，以及为什么要使用它。 \n \n C语言例子 \n \n 假如有一个C的程序，我们想用Docker去做编译，然后执行可执行文件。 \n #include <stdio.h>\n\nvoid main(int argc, char *argv[])\n{\n    printf("hello %s\\n", argv[argc - 1]);\n}\n \n 1 2 3 4 5 6 本地测试（如果你本地有C环境） \n $ gcc --static -o hello hello.c\n$ ls\nhello  hello.c\n$ ./hello docker\nhello docker\n$ ./hello world\nhello world\n$ ./hello friends\nhello friends\n$\n \n 1 2 3 4 5 6 7 8 9 10 构建一个Docker镜像，因为要有C的环境，所以我们选择gcc这个image \n FROM gcc:9.4\n\nCOPY hello.c /src/hello.c\n\nWORKDIR /src\n\nRUN gcc --static -o hello hello.c\n\nENTRYPOINT [ "/src/hello" ]\n\nCMD []\n \n 1 2 3 4 5 6 7 8 9 10 11 build和测试 \n $ docker build -t hello .\nSending build context to Docker daemon   5.12kB\nStep 1/6 : FROM gcc:9.4\n---\x3e be1d0d9ce039\nStep 2/6 : COPY hello.c /src/hello.c\n---\x3e Using cache\n---\x3e 70a624e3749b\nStep 3/6 : WORKDIR /src\n---\x3e Using cache\n---\x3e 24e248c6b27c\nStep 4/6 : RUN gcc --static -o hello hello.c\n---\x3e Using cache\n---\x3e db8ae7b42aff\nStep 5/6 : ENTRYPOINT [ "/src/hello" ]\n---\x3e Using cache\n---\x3e 7f307354ee45\nStep 6/6 : CMD []\n---\x3e Using cache\n---\x3e 7cfa0cbe4e2a\nSuccessfully built 7cfa0cbe4e2a\nSuccessfully tagged hello:latest\n$ docker image ls\nREPOSITORY     TAG          IMAGE ID       CREATED       SIZE\nhello          latest       7cfa0cbe4e2a   2 hours ago   1.14GB\ngcc            9.4          be1d0d9ce039   9 days ago    1.14GB\n$ docker run --rm -it hello docker\nhello docker\n$ docker run --rm -it hello world\nhello world\n$ docker run --rm -it hello friends\nhello friends\n$\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 可以看到镜像非常的大，1.14GB \n 实际上当我们把hello.c编译完以后，并不需要这样一个大的GCC环境，一个小的alpine镜像就可以了。 \n 这时候我们就可以使用多阶段构建了。 \n FROM gcc:9.4 AS builder\n\nCOPY hello.c /src/hello.c\n\nWORKDIR /src\n\nRUN gcc --static -o hello hello.c\n\n\n\nFROM alpine:3.13.5\n\nCOPY --from=builder /src/hello /src/hello\n\nENTRYPOINT [ "/src/hello" ]\n\nCMD []\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 测试 \n $ docker build -t hello-apline -f Dockerfile-new .\nSending build context to Docker daemon   5.12kB\nStep 1/8 : FROM gcc:9.4 AS builder\n---\x3e be1d0d9ce039\nStep 2/8 : COPY hello.c /src/hello.c\n---\x3e Using cache\n---\x3e 70a624e3749b\nStep 3/8 : WORKDIR /src\n---\x3e Using cache\n---\x3e 24e248c6b27c\nStep 4/8 : RUN gcc --static -o hello hello.c\n---\x3e Using cache\n---\x3e db8ae7b42aff\nStep 5/8 : FROM alpine:3.13.5\n---\x3e 6dbb9cc54074\nStep 6/8 : COPY --from=builder /src/hello /src/hello\n---\x3e Using cache\n---\x3e 18c2bce629fb\nStep 7/8 : ENTRYPOINT [ "/src/hello" ]\n---\x3e Using cache\n---\x3e 8dfb9d9d6010\nStep 8/8 : CMD []\n---\x3e Using cache\n---\x3e 446baf852214\nSuccessfully built 446baf852214\nSuccessfully tagged hello-apline:latest\n$ docker image ls\nREPOSITORY     TAG          IMAGE ID       CREATED       SIZE\nhello-alpine   latest       446baf852214   2 hours ago   6.55MB\nhello          latest       7cfa0cbe4e2a   2 hours ago   1.14GB\ndemo           latest       079bae887a47   2 hours ago   125MB\ngcc            9.4          be1d0d9ce039   9 days ago    1.14GB\n$ docker run --rm -it hello-alpine docker\nhello docker\n$ docker run --rm -it hello-alpine world\nhello world\n$ docker run --rm -it hello-alpine friends\nhello friends\n$\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 可以看到这个镜像非常小，只有6.55MB \n \n Java 语言例子 \n \n 使用的openjdk:8-jdk-alpine就是瘦身过的镜像，不过要注意时区问题，要自己设置。 \n Dockerfile 技巧——尽量使用非root用户 \n Root的危险性 \n docker的root权限一直是其遭受诟病的地方，docker的root权限有那么危险么？我们举个例子。 \n 假如我们有一个用户，叫demo，它本身不具有sudo的权限，所以就有很多文件无法进行读写操作，比如/root目录它是无法查看的。 \n [demo@docker-host ~]$ sudo ls /root\n[sudo] password for demo:\ndemo is not in the sudoers file.  This incident will be reported.\n[demo@docker-host ~]$\n \n 1 2 3 4 但是这个用户有执行docker的权限，也就是它在docker这个group里。 \n [demo@docker-host ~]$ groups\ndemo docker\n[demo@docker-host ~]$ docker image ls\nREPOSITORY   TAG       IMAGE ID       CREATED      SIZE\nbusybox      latest    a9d583973f65   2 days ago   1.23MB\n[demo@docker-host ~]$\n \n 1 2 3 4 5 6 这时，我们就可以通过Docker做很多越权的事情了，比如，我们可以把这个无法查看的/root目录映射到docker container里，你就可以自由进行查看了。 \n [demo@docker-host vagrant]$ docker run -it -v /root/:/root/tmp busybox sh\n/ # cd /root/tmp\n~/tmp # ls\nanaconda-ks.cfg  original-ks.cfg\n~/tmp # ls -l\ntotal 16\n-rw-------    1 root     root          5570 Apr 30  2020 anaconda-ks.cfg\n-rw-------    1 root     root          5300 Apr 30  2020 original-ks.cfg\n~/tmp #\n \n 1 2 3 4 5 6 7 8 9 更甚至我们可以给我们自己加sudo权限。我们现在没有sudo权限 \n [demo@docker-host ~]$ sudo vim /etc/sudoers\n[sudo] password for demo:\ndemo is not in the sudoers file.  This incident will be reported.\n[demo@docker-host ~]$\n \n 1 2 3 4 但是我可以给自己添加。 \n [demo@docker-host ~]$ docker run -it -v /etc/sudoers:/root/sudoers busybox sh\n/ # echo "demo    ALL=(ALL)       ALL" >> /root/sudoers\n/ # more /root/sudoers | grep demo\ndemo    ALL=(ALL)       ALL\n \n 1 2 3 4 然后退出container，bingo，我们有sudo权限了。 \n [demo@docker-host ~]$ sudo more /etc/sudoers | grep demo\ndemo    ALL=(ALL)       ALL\n[demo@docker-host ~]$\n \n 1 2 3 如何使用非root用户 \n 我们准备两个Dockerfile，第一个Dockerfile如下， \n FROM python:3.9.5-slim\n\nRUN pip install flask\n\nCOPY app.py /src/app.py\n\nWORKDIR /src\nENV FLASK_APP=app.py\n\nEXPOSE 5000\n\nCMD ["flask", "run", "-h", "0.0.0.0"]\n \n 1 2 3 4 5 6 7 8 9 10 11 12 假设构建的镜像名字为  flask-demo \n 第二个Dockerfile，使用非root用户来构建这个镜像，名字叫  flask-no-root  Dockerfile如下： \n \n 通过groupadd和useradd创建一个flask的组和用户 \n 通过USER指定后面的命令要以flask这个用户的身份运行 \n \n FROM python:3.9.5-slim\n\nRUN pip install flask && \\\n    groupadd -r flask && useradd -r -g flask flask && \\\n    mkdir /src && \\\n    chown -R flask:flask /src\n\nUSER flask\n\nCOPY app.py /src/app.py\n\nWORKDIR /src\nENV FLASK_APP=app.py\n\nEXPOSE 5000\n\nCMD ["flask", "run", "-h", "0.0.0.0"]\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ docker image ls\nREPOSITORY      TAG          IMAGE ID       CREATED          SIZE\nflask-no-root   latest       80996843356e   41 minutes ago   126MB\nflask-demo      latest       2696c68b51ce   49 minutes ago   125MB\npython          3.9.5-slim   609da079b03a   2 weeks ago      115MB\n \n 1 2 3 4 5 分别使用这两个镜像创建两个容器 \n $ docker run -d --name flask-root flask-demo\nb31588bae216951e7981ce14290d74d377eef477f71e1506b17ee505d7994774\n$ docker run -d --name flask-no-root flask-no-root\n83aaa4a116608ec98afff2a142392119b7efe53617db213e8c7276ab0ae0aaa0\n$ docker container ps\nCONTAINER ID   IMAGE           COMMAND                  CREATED          STATUS          PORTS      NAMES\n83aaa4a11660   flask-no-root   "flask run -h 0.0.0.0"   4 seconds ago    Up 3 seconds    5000/tcp   flask-no-root\nb31588bae216   flask-demo      "flask run -h 0.0.0.0"   16 seconds ago   Up 15 seconds   5000/tcp   f\n \n 1 2 3 4 5 6 7 8 #  5.Docker的存储 \n Data Volume \n 示例：保存数据了解数据是如何持久化？ \n 环境准备 \n 准备一个Dockerfile 和一个 my-cron的文件 \n $ ls\nDockerfile  my-cron\n$ more Dockerfile\nFROM alpine:latest\nRUN apk update\nRUN apk --no-cache add curl\nENV SUPERCRONIC_URL=https://github.com/aptible/supercronic/releases/download/v0.1.12/supercronic-linux-amd64 \\\n    SUPERCRONIC=supercronic-linux-amd64 \\\n    SUPERCRONIC_SHA1SUM=048b95b48b708983effb2e5c935a1ef8483d9e3e\nRUN curl -fsSLO "$SUPERCRONIC_URL" \\\n    && echo "${SUPERCRONIC_SHA1SUM}  ${SUPERCRONIC}" | sha1sum -c - \\\n    && chmod +x "$SUPERCRONIC" \\\n    && mv "$SUPERCRONIC" "/usr/local/bin/${SUPERCRONIC}" \\\n    && ln -s "/usr/local/bin/${SUPERCRONIC}" /usr/local/bin/supercronic\nCOPY my-cron /app/my-cron\nWORKDIR /app\n\nVOLUME ["/app"]\nRUN cron job\nCMD ["/usr/local/bin/supercronic", "/app/my-cron"]\n$\n$ more my-cron\n*/1 * * * * date >> /app/test.txt\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 工具supercronic：https://github.com/aptible/supercronic/ 这个专为容器而生的计划任务工具。 \n my-cron的意思是使用的my-cron就是一个crontab格式的计划任务，比如, 每隔一分钟输出时间到一个文件里 \n 构建镜像 \n $  docker  image build  -t  my-cron  . \n$  docker  image  ls \nREPOSITORY   TAG       IMAGE ID       CREATED         SIZE\nmy-cron      latest    e9fbd9a562c9    4  seconds ago    24 .7MB\n \n 1 2 3 4 创建容器(不指定-v参数)，在这个Volume的mountpoint可以发现容器创建的文件。此时Docker会自动创建一个随机名字的volume，去存储我们在Dockerfile定义的volume  VOLUME ["/app"] \n $  docker  run  -d  my-cron\n9a8fa93f03c42427a498b21ac520660752122e20bcdbf939661646f71d277f8f\n$  docker  volume  ls \nDRIVER    VOLUME NAME\n local      043a196c21202c484c69f2098b6b9ec22b9a9e4e4bb8d4f55a4c3dce13c15264\n$  docker  volume inspect 043a196c21202c484c69f2098b6b9ec22b9a9e4e4bb8d4f55a4c3dce13c15264\n [ \n     { \n         "CreatedAt" :   "2021-06-22T23:06:13+02:00" ,\n         "Driver" :   "local" ,\n         "Labels" :  null,\n         "Mountpoint" :   "/var/lib/docker/volumes/043a196c21202c484c69f2098b6b9ec22b9a9e4e4bb8d4f55a4c3dce13c15264/_data" ,\n         "Name" :   "043a196c21202c484c69f2098b6b9ec22b9a9e4e4bb8d4f55a4c3dce13c15264" ,\n         "Options" :  null,\n         "Scope" :   "local" \n     } \n ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 创建容器(指定-v参数) \n 在创建容器的时候通过  -v  参数我们可以手动的指定需要创建Volume的名字，以及对应于容器内的路径，这个路径是可以任意的，不必需要在Dockerfile里通过VOLUME定义 \n 比如我们把上面的Dockerfile里的VOLUME删除 \n FROM alpine:latest\nRUN apk update\nRUN apk --no-cache add curl\nENV SUPERCRONIC_URL=https://github.com/aptible/supercronic/releases/download/v0.1.12/supercronic-linux-amd64 \\\n    SUPERCRONIC=supercronic-linux-amd64 \\\n    SUPERCRONIC_SHA1SUM=048b95b48b708983effb2e5c935a1ef8483d9e3e\nRUN curl -fsSLO "$SUPERCRONIC_URL" \\\n    && echo "${SUPERCRONIC_SHA1SUM}  ${SUPERCRONIC}" | sha1sum -c - \\\n    && chmod +x "$SUPERCRONIC" \\\n    && mv "$SUPERCRONIC" "/usr/local/bin/${SUPERCRONIC}" \\\n    && ln -s "/usr/local/bin/${SUPERCRONIC}" /usr/local/bin/supercronic\nCOPY my-cron /app/my-cron\nWORKDIR /app\nRUN cron job\nCMD ["/usr/local/bin/supercronic", "/app/my-cron"]\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 重新build镜像，然后创建容器，加-v参数 容器外路径:容器内路径 \n docker  image build  -t  my-cron  . \n docker  container run  -d   -v  cron-data:/app my-cron\n \n 1 2 43c6d0357b0893861092a752c61ab01bdfa62ea766d01d2fcb8b3ecb6c88b3de\n$ docker volume ls\nDRIVER    VOLUME NAME\nlocal     cron-data\n$ docker volume inspect cron-data\n[\n    {\n        "CreatedAt": "2021-06-22T23:25:02+02:00",\n        "Driver": "local",\n        "Labels": null,\n        "Mountpoint": "/var/lib/docker/volumes/cron-data/_data",\n        "Name": "cron-data",\n        "Options": null,\n        "Scope": "local"\n    }\n]\n$ ls /var/lib/docker/volumes/cron-data/_data\nmy-cron\n$ ls /var/lib/docker/volumes/cron-data/_data\nmy-cron  test.txt\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 环境清理 \n 强制删除所有容器，系统清理和volume清理 \n $  docker   rm   -f   $( docker  container  ps   -aq )   #强制删除所有容器 \n$  docker  system prune  -f   #删除不在运行的contianer cache \n$  docker  volume prune  -f   #Remove all unused local volumes \n \n 1 2 3 #  Data Volume 练习 MySQL \n 使用MySQL官方镜像，tag版本5.7 \n Dockerfile可以在这里查看 https://github.com/docker-library/mysql/tree/master/5.7 \n 准备镜像 \n docker  pull mysql:5.7\n \n 1 创建容器 \n 关于MySQL的镜像使用，可以参考dockerhub https://hub.docker.com/_/mysql?tab=description&page=1&ordering=last_updated \n 关于Dockerfile Volume的定义，可以参考 https://github.com/docker-library/mysql/tree/master/5.7 \n #--name 容器名docker-mysql  -e 编辑密码 -v磁盘映射到docker下的mysql-data目录下 \n docker  container run  --name  docker-mysql  -e   MYSQL_ROOT_PASSWORD = 123456   -d   -v  mysql-data:/var/lib/mysql mysql:5.7\n \n 1 2 \n$ docker volume ls\nDRIVER    VOLUME NAME\nlocal     mysql-data\n$ docker volume inspect mysql-data\n[\n    {\n        "CreatedAt": "2021-06-21T23:55:23+02:00",\n        "Driver": "local",\n        "Labels": null,\n        "Mountpoint": "/var/lib/docker/volumes/mysql-data/_data",\n        "Name": "mysql-data",\n        "Options": null,\n        "Scope": "local"\n    }\n]\n$\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 数据库写入数据 \n 进入MySQL的shell \n docker  container  exec   -it  022  sh \nmysql  -u  root  -p \n \n 1 2 Enter password:\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 2\nServer version: 5.7.34 MySQL Community Server (GPL)\n\nCopyright (c) 2000, 2021, Oracle and/or its affiliates.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType \'help;\' or \'\\h\' for help. Type \'\\c\' to clear the current input statement.\n\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n4 rows in set (0.00 sec)\n\nmysql> create database demo;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| demo               |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n\nmysql> exit\nBye\nexit\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 创建了一个叫 demo的数据库 \n 查看data volume \n $  docker  volume inspect mysql-data\n [ \n     { \n         "CreatedAt" :   "2021-06-22T00:01:34+02:00" ,\n         "Driver" :   "local" ,\n         "Labels" :  null,\n         "Mountpoint" :   "/var/lib/docker/volumes/mysql-data/_data" ,\n         "Name" :   "mysql-data" ,\n         "Options" :  null,\n         "Scope" :   "local" \n     } \n ] \n$  ls   /var/lib/docker/volumes/mysql-data/_data\nauto.cnf    client-cert.pem  ib_buffer_pool  ibdata1  performance_schema  server-cert.pem\nca-key.pem  client-key.pem   ib_logfile0     ibtmp1   private_key.pem     server-key.pem\nca.pem      demo             ib_logfile1     mysql    public_key.pem      sys\n$\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #  多个机器之间的容器共享数据 \n \n 官方参考链接 https://docs.docker.com/storage/volumes/#share-data-among-machines \n Docker的volume支持多种driver。默认创建的volume driver都是local \n $ docker volume inspect vscode\n[\n    {\n        "CreatedAt": "2021-06-23T21:33:57Z",\n        "Driver": "local",\n        "Labels": null,\n        "Mountpoint": "/var/lib/docker/volumes/vscode/_data",\n        "Name": "vscode",\n        "Options": null,\n        "Scope": "local"\n    }\n]\n \n 1 2 3 4 5 6 7 8 9 10 11 12 这一节我们看看一个叫sshfs的driver，如何让docker使用不在同一台机器上的文件系统做volume \n 环境准备 \n 准备三台Linux机器，之间可以通过SSH相互通信。 \n \n \n \n hostname \n ip \n ssh username \n ssh password \n \n \n \n \n docker-host1 \n 192.168.200.10 \n vagrant \n vagrant \n \n \n docker-host2 \n 192.168.200.11 \n vagrant \n vagrant \n \n \n docker-host3 \n 192.168.200.12 \n vagrant \n vagrant \n \n \n \n 安装plugin \n 在其中两台机器上安装一个plugin  vieux/sshfs \n [vagrant@docker-host1 ~]$ docker plugin install --grant-all-permissions vieux/sshfs\nlatest: Pulling from vieux/sshfs\nDigest: sha256:1d3c3e42c12138da5ef7873b97f7f32cf99fb6edde75fa4f0bcf9ed277855811\n52d435ada6a4: Complete\nInstalled plugin vieux/sshfs\n \n 1 2 3 4 5 [vagrant@docker-host2 ~]$ docker plugin install --grant-all-permissions vieux/sshfs\nlatest: Pulling from vieux/sshfs\nDigest: sha256:1d3c3e42c12138da5ef7873b97f7f32cf99fb6edde75fa4f0bcf9ed277855811\n52d435ada6a4: Complete\nInstalled plugin vieux/sshfs\n \n 1 2 3 4 5 创建volume \n [vagrant@docker-host1 ~]$ docker volume create --driver vieux/sshfs \\\n                          -o sshcmd=vagrant@192.168.200.12:/home/vagrant \\\n                          -o password=vagrant \\\n                          sshvolume\n \n 1 2 3 4 查看 \n [vagrant@docker-host1 ~]$ docker volume ls\nDRIVER               VOLUME NAME\nvieux/sshfs:latest   sshvolume\n[vagrant@docker-host1 ~]$ docker volume inspect sshvolume\n[\n    {\n        "CreatedAt": "0001-01-01T00:00:00Z",\n        "Driver": "vieux/sshfs:latest",\n        "Labels": {},\n        "Mountpoint": "/mnt/volumes/f59e848643f73d73a21b881486d55b33",\n        "Name": "sshvolume",\n        "Options": {\n            "password": "vagrant",\n            "sshcmd": "vagrant@192.168.200.12:/home/vagrant"\n        },\n        "Scope": "local"\n    }\n]\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 创建容器挂载Volume \n 创建容器，挂载sshvolume到/app目录，然后进入容器的shell，在/app目录创建一个test.txt文件 \n [vagrant@docker-host1 ~]$ docker run -it -v sshvolume:/app busybox sh\nUnable to find image \'busybox:latest\' locally\nlatest: Pulling from library/busybox\nb71f96345d44: Pull complete\nDigest: sha256:930490f97e5b921535c153e0e7110d251134cc4b72bbb8133c6a5065cc68580d\nStatus: Downloaded newer image for busybox:latest\n/ #\n/ # ls\napp   bin   dev   etc   home  proc  root  sys   tmp   usr   var\n/ # cd /app\n/app # ls\n/app # echo "this is ssh volume"> test.txt\n/app # ls\ntest.txt\n/app # more test.txt\nthis is ssh volume\n/app #\n/app #\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 这个文件我们可以在docker-host3上看到 \n [vagrant@docker-host3 ~]$ pwd\n/home/vagrant\n[vagrant@docker-host3 ~]$ ls\ntest.txt\n[vagrant@docker-host3 ~]$ more test.txt\nthis is ssh volume\n \n 1 2 3 4 5 6 如何给已创建的容器额外挂载共享文件夹 \n 一、背景 \n 在使用docker过程中，有时候创建容器时候没有设置挂载本地数据卷进行文件夹共享，但已经在容器中配置完了环境，此时再重新创建一个容器非常麻烦，因此需要对已有的容器挂载数据卷。 \n 二、挂载原理 \n Docker中所有的容器的配置，如挂载点、运行方式等都是以json文件进行配置，修改对应的json文件参数即可挂载指定文件夹。 \n 配置容器的json文件 \n /var/lib/docker/containers/<容器ID>/config.v2.json \n /var/lib/docker/containers/<容器ID>/hostconfig.json \n 三、打开文件 \n \n \n 使用  docker ps -a 拿到需要更改的container的12位ID，然后 docker inspect id ，拿到64位ID(终端最上面的那个ID) \n \n \n 停止所有container 并使用 service docker stop 关闭docker服务（ 必须关闭Docker服务，否则无法修改成功 ） \n \n \n 到目录 /var/lib/docker/containers/<64位容器ID>/ 中复制  config.v2.json 和  hostconfig.json 两个文件到任意不用root权限的目录下，同时对原文件进行备份。 \n 4.在 ~/ 下新建两个同名文件(避免权限问题) \n cd ~/ \n touch config.v2.json hostconfig.json \n \n \n 5.新开一个终端，进入到容器目录下，打开文件 \n sudo -i  ，提升权限 \n cd /var/lib/docker/containers/<64位容器ID>/ \n 修改前一定要先备份下，否则改错了就GG！ \n cp config.v2.json config.v2.json.back \n cp hostconfig.json hostconfig.back \n 四、添加共享文件夹挂载信息 \n 打开** ~/ 目录**下的这两个文件 \n 1.修改 config.v2.json 文件 \n \n 在 MountPoints 参数下按照相应的格式进行添加相应的字段， 注意，必须是绝对路径，且不能是/root，必须是/root/的二级子目录 。 \n 2.修改 hostconfig.json 文件 \n 在 hostconfig.json 文件中的 Binds 参数添加宿主机和容器共享文件夹目录（ 注意，必须是绝对路径，且不能是/root，必须是/root/dataset这样的二级子目录 ） \n \n 五、修改容器配置 \n 把 ～/ 目录下的 config.v2.json 和 hostconfig.json 两个文件内容，对应复制到以下文件中。 \n /var/lib/docker/containers/<容器ID>/config.v2.json \n /var/lib/docker/containers/<容器ID>/hostconfig.json \n 六、启动docker 服务 \n service docker start\n \n 1 #  6.Docker网络 \n Bridge 网络 \n  容器间通信 \n 两个容器都连接到了一个叫 docker0 的Linux bridge上 \n 创建两个容器 \n docker  container run  -d    --name  box1 busybox /bin/sh  -c   "while true; do sleep 3600; done" \n\n docker  container run  -d    --name  box2 busybox /bin/sh  -c   "while true; do sleep 3600; done" \n \n 1 2 3 docker  network  ls \n docker  network inspect bridge\n #查看网络 \nbrctl show\n \n 1 2 3 4 NETWORK ID     NAME      DRIVER    SCOPE\n1847e179a316   bridge    bridge     local \na647a4ad0b4f    host        host        local \nfbd81b56c009   none      null       local \n$  docker  network inspect bridge\n [ \n     { \n         "Name" :   "bridge" ,\n         "Id" :   "1847e179a316ee5219c951c2c21cf2c787d431d1ffb3ef621b8f0d1edd197b24" ,\n         "Created" :   "2021-07-01T15:28:09.265408946Z" ,\n         "Scope" :   "local" ,\n         "Driver" :   "bridge" ,\n         "EnableIPv6" :  false,\n         "IPAM" :   { \n             "Driver" :   "default" ,\n             "Options" :  null,\n             "Config" :   [ \n                 { \n                     "Subnet" :   "172.17.0.0/16" ,\n                     "Gateway" :   "172.17.0.1" \n                 } \n             ] \n         } ,\n         "Internal" :  false,\n         "Attachable" :  false,\n         "Ingress" :  false,\n         "ConfigFrom" :   { \n             "Network" :   "" \n         } ,\n         "ConfigOnly" :  false,\n         "Containers" :   { \n             "03494b034694982fa085cc4052b6c7b8b9c046f9d5f85f30e3a9e716fad20741" :   { \n                 "Name" :   "box1" ,\n                 "EndpointID" :   "072160448becebb7c9c333dce9bbdf7601a92b1d3e7a5820b8b35976cf4fd6ff" ,\n                 "MacAddress" :   "02:42:ac:11:00:02" ,\n                 "IPv4Address" :   "172.17.0.2/16" ,\n                 "IPv6Address" :   "" \n             } ,\n             "4f3303c84e5391ea37db664fd08683b01decdadae636aaa1bfd7bb9669cbd8de" :   { \n                 "Name" :   "box2" ,\n                 "EndpointID" :   "4cf0f635d4273066acd3075ec775e6fa405034f94b88c1bcacdaae847612f2c5" ,\n                 "MacAddress" :   "02:42:ac:11:00:03" ,\n                 "IPv4Address" :   "172.17.0.3/16" ,\n                 "IPv6Address" :   "" \n             } \n         } ,\n         "Options" :   { \n             "com.docker.network.bridge.default_bridge" :   "true" ,\n             "com.docker.network.bridge.enable_icc" :   "true" ,\n             "com.docker.network.bridge.enable_ip_masquerade" :   "true" ,\n             "com.docker.network.bridge.host_binding_ipv4" :   "0.0.0.0" ,\n             "com.docker.network.bridge.name" :   "docker0" ,\n             "com.docker.network.driver.mtu" :   "1500" \n         } ,\n         "Labels" :   { } \n     } \n ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 brctl` 使用前需要安装, 对于CentOS, 可以通过 `sudo yum install -y bridge-utils` 安装. 对于Ubuntu, 可以通过 `sudo apt-get install -y bridge-utils\n$ brctl show\nbridge name     bridge id               STP enabled     interfaces\ndocker0         8000.0242759468cf       no              veth8c9bb82\n                                                        vethd8f9afb\n \n 1 2 3 4 5 查看路由 \n $ ip route\ndefault via 10.0.2.2 dev eth0 proto dhcp metric 100\n10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100\n172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1\n192.168.200.0/24 dev eth1 proto kernel scope link src 192.168.200.10 metric 101\n \n 1 2 3 4 5 #   外部访问容器：端口转发 \n 不同host主机之间的容器通信 \n 创建容器 \n docker  container run  -d   --rm   --name  web  -p   8080 :80 nginx:1.20.0\n #暴露端口，外部访问容器 \nfirewall-cmd  --zone = public --add-port = 80 /tcp  --permanent \n #重启防火墙 \nsystemctl restart firewalld\n #查看已经对外暴露的端口，确认端口是否已经暴露 \nfirewall-cmd  --zone = public --list-ports\n \n 1 2 3 4 5 6 7 \n 测试容器间是否通信,让box1去请求web，通信返回一个index文件。 \n docker  container  exec   -it  box1  wget  http://172.17.0.5\n \n 1 \n 测试外部网络是否能访问容器，访问web暴露出来的8080端口 \n \n \n 同一host暴露出来的端口需唯一，不然后起的服务报错，端口被占用 。这样就保证外部所访问的容器是唯一的。 \n docker  container run  -d   --rm   --name  web2  -p   8080 :80 nginx:1.20.0\n \n 1 \n 如果network采用host模式，暴露出来是默认的80端口 \n docker  container run  -d   --rm   --name  web1  --network   host  nginx:1.20.0\n \n 1 \n 同一host中的不同bridge通信 \n 创建和使用 bridge \n 在上面的启动的容器中，断开web的连接 \n docker  network disconnect bridge web\n \n 1 \n 创建一个自己的mybridge \n docker  network create mybridge\n \n 1 \n web连接mybridge，要实现box1能通信web，box1也得连接mybridge,而box2是ping不同web。 \n docker  network connect mybridge web\n docker  container  exec   -it  box1  ping  web\n docker  network connect mybridge box1\n \n 1 2 3 \n \n 创建时可指定路由和ip范围 \n docker  network create  -d  bridge  --gateway   172.200 .0.1  --subnet   172.200 .0.0/16 detailbridge\n \n 1 \n 容器通信的底层原理： \n -----------------------------------------网络命名空间和虚拟网络设备对 \n Linux的Namespace（命名空间）技术是一种隔离技术，常用的Namespace有 user namespace, process namespace, network namespace等 \n 网络命名空间 是 Linux 内核用来隔离不同容器间的网络资源（每个 Docker 容器都拥有一个独立的网络命名空间），网络命名空间主要隔离的资源包括： \n 1.iptables规则表 \n 2.路由规则表 \n 3.网络设备列表 \n 如下图所示，当系统中拥有 3 个网络命名空间： \n \n 由于不同的网络命名空间之间是相互隔离的，所以不同的网络命名空间之间并不能直接通信。比如在 网络命名空间A 配置了一个 IP 地址为 172.17.42.1 的设备，但在 网络命名空间B 里却不能访问，如下图所示： \n \n 就好比两台电脑，如果没有任何网线连接，它们之间是不能通信的。所以，Linux 内核提供了 虚拟网络设备对（veth） 这个功能，用于解决不同网络命名空间之间的通信。 \n Docker 就是使用 虚拟网络设备对 来实现不同容器之间的通信，其原理如下图： \n \n \n 以下模拟的是容器的通信原理，即不同命名空间的通信 \n 新建增加网络命名空间脚本 \n vim  add-ns-to-br.sh\n \n 1 #!/bin/bash \n\n bridge = $1 \n namespace = $2 \n addr = $3 \n\n #虚拟网络设备对 \n #在docker0上 \n vethA = veth- $namespace \n #在容器内 \n vethB = eth00- $namespace \n #新建命名空间 \n sudo   ip  netns  add   $namespace \n #创建一个veth pair \n sudo   ip   link   add   $vethA   type  veth peer name  $vethB \n #设置虚拟网络设备所属命名空间 \n sudo   ip   link   set   $vethB  netns  $namespace \n #给虚拟网络设备添加ip \n sudo   ip  netns  exec   $namespace   ip  addr  add   $addr  dev  $vethB \n #开启网络命令空间 namespace vethB 端口 \n sudo   ip  netns  exec   $namespace   ip   link   set   $vethB  up\n\n sudo   ip   link   set   $vethA  up\n\n sudo  brctl addif  $bridge   $vethA \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 brctl --help查看使用命令 \n\nUsage: brctl  [ commands ] \ncommands:\n   addbr            < bridge >                  add  bridge\n   delbr            < bridge >                 delete bridge\n   addif            < bridge >   < device >         add  interface to bridge\n   delif            < bridge >   < device >        delete interface from bridge\n   hairpin          < bridge >   < port >   { on | off }         turn hairpin on/off\n   setageing        < bridge >   < time >           set  ageing  time \n   setbridgeprio    < bridge >   < prio >           set  bridge priority\n   setfd            < bridge >   < time >           set  bridge forward delay\n   sethello         < bridge >   < time >           set  hello  time \n   setmaxage        < bridge >   < time >           set  max message age\n   setpathcost      < bridge >   < port >   < cost >    set  path cost\n   setportprio      < bridge >   < port >   < prio >    set  port priority\n   show             [   < bridge >   ]             show a list of bridges\n   showmacs         < bridge >                 show a list of mac addrs\n   showstp          < bridge >                 show bridge stp info\n   stp              < bridge >   { on | off }        turn stp on/off\n\n [ root@container1 docker ] # ip netns help \nUsage:  ip  netns list\n   ip  netns  add  NAME\n   ip  netns  set  NAME NETNSID\n   ip   [ -all ]  netns delete  [ NAME ] \n   ip  netns identify  [ PID ] \n   ip  netns pids NAME\n   ip   [ -all ]  netns  exec   [ NAME ]  cmd  .. .\n   ip  netns monitor\n   ip  netns list-id\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 启动两个容器 \n docker  container run  -d   --name  box3  --network  mybridge  busybox /bin/sh  -c   "while true; do sleep 3600; done" \n docker  container run  -d   --name  box4  --network  mybridge  busybox /bin/sh  -c   "while true; do sleep 3600; done" \n\n \n 1 2 3 \n \n 获取bridge name \n brctl show\n \n 1 \n 设置命名空间 \n sh  add-ns-to-br.sh br-1379584d9670 ns1  172.19 .0.2/16\n sh  add-ns-to-br.sh br-1379584d9670 ns2  172.19 .0.3/16\n #查看网络命名空间list \n sudo   ip  netns  ls \n #进入命令空间查看ip信息 \n sudo   ip  netns  exec  ns2 \n ip  a\n ping   172.19 .0.2\n \n 1 2 3 4 5 6 7 8 \n 以上就是模拟实现了不同网络命名空间之间的通信。 \n 断开虚拟网络设备，不同的网络命名空间就ping不同，实现隔离。 \n sudo   ip   link   set  veth-ns1 down\n \n 1 \n #重启网卡 \n service  network restart\n #查看配置信息 \n ifconfig \n #错误日志查看 \n cat  /var/log/messages  |   grep  network\n #查看docker的情况 \nsystemctl status docker.service\n #关掉网桥 \n ifconfig  docker0 down\n #删除网桥 \nbrctl delbr docker0\n \n 1 2 3 4 5 6 7 8 9 10 11 12 #  7.Docker Compose \n docker compose 介绍 \n 要启动一些容器需进行一系列操作，一般我们使用shell脚本进行一键式操作。而基于这个需求，docker compose就是专门用于解决这种组合的操作。通过配置yml文件来实现。 \n docker compose 的安装 \n Windows和Mac在默认安装了docker desktop以后，docker-compose随之自动安装 \n PS C:\\Users\\Peng Xiao\\docker.tips> docker-compose --version\ndocker-compose version 1.29.2, build 5becea4c\n \n 1 2 Linux用户需要自行安装 \n 最新版本号可以在这里查询 https://github.com/docker/compose/releases \n $ sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose\n$ sudo chmod +x /usr/local/bin/docker-compose\n$ docker-compose --version\ndocker-compose version 1.29.2, build 5becea4c\n \n 1 2 3 4 熟悉python的朋友，可以使用pip去安装docker-Compose \n #快速安装对应版本pip \n wget  https://bootstrap.pypa.io/pip/3.6/get-pip.py\npython3 get-pip.py\n \n 1 2 3 pip install docker - compose\n \n 1 #  docker-compose基本使用 \n compose 文件的结构和版本 \n docker compose文件的语法说明 https://docs.docker.com/compose/compose-file/ \n 基本语法结构 \n version: "3.8"\n\nservices: # 容器\n  servicename: # 服务名字，这个名字也是内部 bridge网络可以使用的 DNS name\n    image: # 镜像的名字\n    command: # 可选，如果设置，则会覆盖默认镜像里的 CMD命令\n    environment: # 可选，相当于 docker run里的 --env\n    volumes: # 可选，相当于docker run里的 -v\n    networks: # 可选，相当于 docker run里的 --network\n    ports: # 可选，相当于 docker run里的 -p\n  servicename2:\n\nvolumes: # 可选，相当于 docker volume create\n\nnetworks: # 可选，相当于 docker network create\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 以 Python Flask + Redis练习：为例子，改造成一个docker-compose文件 \n 自定义的flask-demo \n from  flask  import  Flask\n from  redis  import  Redis\n import  os\n import  socket\n\napp  =  Flask ( __name__ ) \nredis  =  Redis ( host = os . environ . get ( \'REDIS_HOST\' ,   \'127.0.0.1\' ) ,  port = 6379 ) \n\n\n @app . route ( \'/\' ) \n def   hello ( ) : \n    redis . incr ( \'hits\' ) \n     return   f"Hello Container World! I have been seen  { redis . get ( \'hits\' ) . decode ( \'utf-8\' ) }  times and my hostname is  { socket . gethostname ( ) } .\\n" \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 Dockerfile \n FROM  python:3.9.5-slim \n\n RUN  python -m pip install --upgrade pip &&  \\ \n    pip install flask redis &&  \\ \n    groupadd -r flask && useradd -r -g flask flask &&  \\ \n    mkdir /src &&  \\ \n    chown -R flask:flask /src \n\n USER  flask \n\n WORKDIR  /src \n\n ENV  FLASK_APP=app.py REDIS_HOST=redis \n\n COPY  app.py /src/app.py \n EXPOSE  5000 \n\n CMD  [ "flask" ,  "run" ,  "-h" ,  "0.0.0.0" ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 未使用docker-compose之前的操作 \n docker image pull redis\ndocker image build -t flask-demo .\ncreate network\ndocker network create -d bridge demo-network\ncreate container\ndocker container run -d --name redis-server --network demo-network redis\ndocker container run -d --network demo-network --name flask-demo --env REDIS_HOST=redis-server -p 5000:5000 flask-demo\n \n 1 2 3 4 5 6 7 8 9 只构建flask-demo镜像，后面可以直接拉取 \n docker  image build  -t  flask-demo  . \n \n 1 docker-compose.yml 文件如下 \n version :   "3.8" \n\n services : \n   flask-demo : \n     image :  flask - demo : latest\n     environment : \n       -  REDIS_HOST=redis - server\n     networks : \n       -  demo - network\n     ports : \n       -  8080 : 5000 \n\n   redis-server : \n     image :  redis : latest\n     networks : \n       -  demo - network\n\n networks : \n   demo-network : \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 基本操作 \n Usage:\n docker-compose   [ -f  < arg > .. . ]   [ --profile  < name > .. . ]   [ options ]   [ -- ]   [ COMMAND ]   [ ARGS .. . ] \n docker-compose  -h | --help\n\nCommands:\nbuild              Build or rebuild services\nconfig             Validate and view the Compose  file \ncreate             Create services\ndown               Stop and remove resources\nevents             Receive real  time  events from containers\n exec                Execute a  command   in  a running container\n help                Get  help  on a  command \nimages             List images\n kill                Kill containers\nlogs               View output from containers\npause              Pause services\nport               Print the public port  for  a port binding\n ps                  List containers\npull               Pull  service  images\npush               Push  service  images\nrestart            Restart services\n rm                  Remove stopped containers\nrun                Run a one-off  command \nscale              Set number of containers  for  a  service \nstart              Start services\nstop               Stop services\n top                 Display the running processes\nunpause            Unpause services\nup                 Create and start containers\nversion            Show version information and quit\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #确认配置 \n docker-compose  config\n从配置build的Dockerfile构建镜像，并不会启动容器，可以用于构建镜像或者更新镜像，没配置直接拉取镜像 \n docker-compose  build \n #获取镜像启动服务，镜像不存在，按情况从仓库拉取或build \n docker-compose  up  -d \n \n 1 2 3 4 5 6 \n \n #查看compose中的相关镜像，只显示该compose服务包含的镜像 \n docker-compose  images\n #查看容器的运行情况 \n docker-compose   ps \n \n 1 2 3 4 \n #对容器的操作就是docker的操作 \n docker   ps \n docker   exec   -it  0b  sh \n #对redis进行读写数据 \n set  k1 v1\nget k1\n \n 1 2 3 4 5 6 \n #停止服务 \n docker-compose  stop\n #移除停止container \n docker-compose   rm \n \n 1 2 3 4 \n docker-compose 服务的构建、拉取与更新 \n 带有build的yml，会视情况构建镜像 \n version :   "3.8" \n\n services : \n   flask-demo : \n     image :  flask - demo : latest\n     build : \n       context :  .\n       dockerfile :  Dockerfile\n     environment : \n       -  REDIS_HOST=redis - server\n     networks : \n       -  demo - network\n     ports : \n       -  8080 : 5000 \n\n   redis-server : \n     image :  redis : latest\n     networks : \n       -  demo - network\n\n networks : \n   demo-network : \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \n #根据Dockerfile重新下载需要的镜像并构建容器,并启动 \n docker-compose  up  --build   -d \n #重新启动单个容器 \n docker-compose  up  -d   --build  flask\n \n 1 2 3 4 \n 根据Dockerfile重新下载需要的镜像并构建容器，也就是说这句相当于是 docker-compose build --no-cache 和 docker-compose up -d 的集合体， \n意味着构建镜像的时候是根据Dockerfile的最新内容来的，而不会使用缓存，这样就避免了构建镜像时由于缓存造成的影响。 \n docker-compose 网络 \n 如果配置没指定网络，则创建默认的网络docker-compose_default（文件目录_default） \n version :   "3.8" \n\n services : \n   box1 : \n     image :  busybox\n \n 1 2 3 4 5 \n 多网络的配置 \n version :   "3.8" \n\n services : \n   box1 : \n     image :  busybox\n     command :  /bin/sh  - c "while true; do sleep 3600; done"\n     networks : \n       -  demo - network1\n\n   box2 : \n     image :  busybox\n     command :  /bin/sh  - c "while true; do sleep 3600; done"\n     networks : \n       -  demo - network2\n   box3 : \n     image :  busybox\n     command :  /bin/sh  - c "while true; do sleep 3600; done"\n     networks : \n       -  demo - network1\n       -  demo - network2\n\n networks : \n   demo-network1 : \n     ipam : \n       driver :  defaule\n       config : \n         -   subnet :  172.28.0.0/16\n   demo-network2 : \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \n 按照配置，box1和box2 ping不通，都能和box3通信 \n 水平扩展 scale \n 去掉端口指定，不然端口占用，启动失败 \n version:  "3.8" \n\nservices:\n  flask:\n    build:\n      context:  .. /flask\n      dockerfile: Dockerfile\n    image: flask-demo:latest\n    environment:\n      -  REDIS_HOST = redis-server\n    networks:\n      - demo-network\n\n  redis-server:\n    image: redis:latest\n    networks:\n      - demo-network\n  client:\n    image: busybox\n    command: /bin/sh  -c   "while true; do sleep 3600; done" \n    networks:\n      - demo-network\n\nnetworks:\n  demo-network:\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 docker-compose  up  -d   --build   --scale   flask = 3  \n \n 1 \n 与服务器通信 \n ping  flask\n #busybox没有curl命令使用wget \n wget  -O- flask:5000\n #也可以用封装好的image \nxiaopeng163/net-box:latest\n \n 1 2 3 4 5 \n wget -O- 以\'-\'作为file参数，那么数据将会被打印到标准输出，通常为控制台。 \n \n \n \n 外部如何访问，flask端口不能写死暴露？ \n 通过nginx反向代理 \n nginx.conf 配置文件 \n server {\n  listen  80 default_server;\n  location / {\n    proxy_pass http://flask:5000;\n  }\n}\n \n 1 2 3 4 5 6 yml配置 \n version :   "3.8" \n\n services : \n   flask : \n     build : \n       context :  ../flask\n       dockerfile :  Dockerfile\n     image :  flask - demo : latest\n     environment : \n       -  REDIS_HOST=redis - server\n     depends_on : \n       -  redis - server\n     networks : \n       -  frontend\n       -  backend\n\n   redis-server : \n     image :  redis : latest\n     networks : \n       -  backend\n   nginx : \n     image :  nginx : stable - alpine\n     ports : \n       -  8000 : 80 \n     depends_on : \n       -  flask\n     volumes : \n       -  ../nginx/nginx.conf : /etc/nginx/conf.d/default.conf : ro\n       -  ../log/nginx : /var/log/nginx\n     networks : \n       -  frontend\n\n networks : \n     backend : \n     frontend : \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \n 服务的依赖：nginx依赖flask，flask依赖redis，通过depend_on配置来定义程序启动顺序 \n 和 ports 的区别是， expose 不会将端口暴露给主机，主机无法访问 expose 的端口。 \n \n .dockerignore \n .env\n.dockerignore\nDockerfile\ndocker-compose.yaml\nnginx.conf\nvar\n \n 1 2 3 4 5 6 docker-compose  config\n docker-compose  up  -d   --build   --scale   flask = 3 \n \n 1 2 \n docker compose 环境变量 \n 上述涉及的redis若添加密码，无论写到代码里还是配置里都是不安全的，故可以写到.env里面，.env文件.dockerignore文件不上传。 \n .env \n REDIS_PASSWORD = ABC123\n \n 1 yml文件 \n version :   "3.8" \n\n services : \n   flask : \n     build : \n       context :  ../flask\n       dockerfile :  Dockerfile\n     image :  flask - demo : latest\n     environment : \n       -  REDIS_HOST=redis - server\n       -  REDIS_PASS=$ { REDIS_PASSWORD } \n     networks : \n       -  frontend\n       -  backend\n\n   redis-server : \n     image :  redis : latest\n     command :  redis - server  - - requirepass $ { REDIS_PASSWORD } \n     networks : \n       -  backend\n   nginx : \n     image :  nginx : stable - alpine\n     ports : \n       -  8000 : 80 \n     depends_on : \n       -  flask\n     volumes : \n       -  ../nginx/nginx.conf : /etc/nginx/conf.d/default.conf : ro\n       -  ../log/nginx : /var/log/nginx\n     networks : \n       -  frontend\n\n networks : \n     backend : \n     frontend : \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 app.py增加获取该变量 \n from  flask  import  Flask\n from  redis  import  StrictRedis\n import  os\n import  socket\n\napp  =  Flask ( __name__ ) \nredis  =  StrictRedis ( host = os . environ . get ( \'REDIS_HOST\' ,   \'127.0.0.1\' ) , \n                    port = 6379 ,  password = os . environ . get ( \'REDIS_PASS\' ) ) \n\n\n @app . route ( \'/\' ) \n def   hello ( ) : \n    redis . incr ( \'hits\' ) \n     return   f"Hello Container World! I have been seen  { redis . get ( \'hits\' ) . decode ( \'utf-8\' ) }  times and my hostname is  { socket . gethostname ( ) } .\\n" \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n docker compose 健康检查 \n Dockerfile healthcheck https://docs.docker.com/engine/reference/builder/#healthcheck \n docker compose https://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck \n 健康检查是容器运行状态的高级检查，主要是检查容器所运行的进程是否能正常的对外提供“服务”，比如一个数据库容器，我们不光 需要这个容器是up的状态，我们还要求这个容器的数据库进程能够正常对外提供服务，这就是所谓的健康检查。 \n 容器的健康检查 \n 容器本身有一个健康检查的功能，但是需要在Dockerfile里定义，或者在执行docker container run 的时候，通过下面的一些参数指定 \n --health-cmd string              Command to run to check health\n--health-interval duration       Time between running the check\n                                (ms|s|m|h) (default 0s)\n--health-retries int             Consecutive failures needed to\n                                report unhealthy\n--health-start-period duration   Start period for the container to\n                                initialize before starting\n                                health-retries countdown\n                                (ms|s|m|h) (default 0s)\n--health-timeout duration        Maximum time to allow one check to\n \n 1 2 3 4 5 6 7 8 9 10 Dockerfile healthcheck示例源码 \n 我们以下面的这个flask容器为例，Dockerfile如下 \n FROM  python:3.9.5-slim \n\n RUN  pip install flask redis &&  \\ \n    apt-get update &&  \\ \n    apt-get install -y curl &&  \\ \n    groupadd -r flask && useradd -r -g flask flask &&  \\ \n    mkdir /src &&  \\ \n    chown -R flask:flask /src \n\n USER  flask \n\n COPY  app.py /src/app.py \n\n WORKDIR  /src \n\n ENV  FLASK_APP=app.py REDIS_HOST=redis \n\n EXPOSE  5000 \n\n HEALTHCHECK   --interval = 30s   --timeout = 3s   \\ \n     CMD  curl -f http://localhost:5000/ || exit 1 \n\n CMD  [ "flask" ,  "run" ,  "-h" ,  "0.0.0.0" ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 上面Dockerfili里的HEALTHCHECK 就是定义了一个健康检查。 会每隔30秒检查一次，如果失败就会退出，退出代码是1 \n 查看容器状态 \n $ docker container ls\nCONTAINER ID   IMAGE        COMMAND                  CREATED       STATUS                            PORTS      NAMES\n059c12486019   flask-demo   "flask run -h 0.0.0.0"   4 hours ago   Up 8 seconds (health: starting)   5000/tcp   dazzling_tereshkova\n \n 1 2 3 也可以通过docker container inspect 059 查看详情， 其中有有关health的 \n "Health": {\n"Status": "starting",\n"FailingStreak": 1,\n"Log": [\n    {\n        "Start": "2021-07-14T19:04:46.4054004Z",\n        "End": "2021-07-14T19:04:49.4055393Z",\n        "ExitCode": -1,\n        "Output": "Health check exceeded timeout (3s)"\n    }\n]\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 模拟实现故障，把redis停掉之后，经过3次检查，一直是不通的，然后health的状态会从starting变为 unhealthy \n \n 重启redis，经过几秒钟，我们的flask 变成了healthy \n \n docker compose health示例 \n 除了在Dockerfile中配置，还可以在docker-compose.yml文件中配置 \n version :   "3.8" \n\n services : \n   flask : \n     build : \n       context :  ../flask\n       dockerfile :  Dockerfile\n     image :  flask - demo : latest\n     environment : \n       -  REDIS_HOST=redis - server\n     healthcheck : \n       test :   [ "CMD" ,   "curl" ,   "-f" ,   "http://localhost:5000" ] \n       interval :  30s\n       timeout :  3s\n       retries :   3 \n       start_period :  40s\n     depends_on : \n       redis-server : \n         condition :  service_healthy\n     networks : \n       -  frontend\n       -  backend\n\n   redis-server : \n     image :  redis : latest\n     healthcheck : \n       test :   [ "CMD" ,   "redis-cli" ,   "ping" ] \n       interval :  1s\n       timeout :  3s\n       retries :   30 \n     networks : \n       -  backend\n   nginx : \n     image :  nginx : stable - alpine\n     ports : \n       -  8000 : 80 \n     depends_on : \n       flask : \n         condition :  service_healthy\n     volumes : \n       -  ../nginx/nginx.conf : /etc/nginx/conf.d/default.conf : ro\n       -  ../log/nginx : /var/log/nginx\n     networks : \n       -  frontend\n\n networks : \n     backend : \n     frontend : \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 \n 一站式hadoop集群便于测试开发 \n 通过 docker-compose 快速部署 Hadoop 集群详细教程 - 大数据老司机 - 博客园 (cnblogs.com) \n 包含了zookeeper 3.8.0，mysql5.7，hadoop3.3.5，hive3.1.3，spark 3.3.2，Flink1.17.0 \n 1、下载 \n git clone https://gitee.com/hadoop-bigdata/docker-compose-hadoop.git\n \n 1 #  2、创建网络 \n创建，注意不能使用hadoop_network，要不然启动hs2服务的时候会有问题！！！\ndocker network create hadoop-network\n查看\ndocker network ls\n \n 1 2 3 4 5 #  3、部署 mysql5.7 \n cd docker-compose-hadoop/mysql\n\ndocker-compose -f mysql-compose.yaml up -d\n\ndocker-compose -f mysql-compose.yaml ps\n\n#root 密码：123456，以下是登录命令，注意一般在公司不能直接在命令行明文输入密码，要不然容易被安全抓，切记，切记！！！\ndocker exec -it mysql mysql -uroot -p123456\n \n 1 2 3 4 5 6 7 8 #  4、部署 Hadoop Hive \n cd docker-compose-hadoop/hadoop_hive\n\ndocker-compose -f docker-compose.yaml up -d\n查看\ndocker-compose -f docker-compose.yaml ps\n查看指定列\ndocker ps --format "table {{.ID}}\\t{{.Names}}\\t{{.Ports}}"\nhive\ndocker exec -it hive-hiveserver2 hive -e "show databases";\nhiveserver2\ndocker exec -it hive-hiveserver2 beeline -u jdbc:hive2://hive-hiveserver2:10000  -n hadoop -e "show databases;"\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 5.测试访问hdfs UI 和 yarn UI ，该项目设置的是http: //ip:30070 和http: //ip:30889 \n 6.yarn ui地址在镜像中写死了，如果没有进行转发处理，宿主机无法访问，可以进入hadoop-yarn-nm进行修改，写自己的域名，docker-compose.yaml中的暴露端口与之对应，然后重启。 \n < property > \n                 < name > yarn.web-proxy.address </ name > \n                 < value > container1:9111 </ value > \n         </ property > \n\n \n 1 2 3 4 5 \n 8.Docker Swarm \n docker swarm 介绍 \n 为什么不建议在生产环境中使用docker-compose \n \n 多机器如何管理？ \n 如果跨机器做scale横向扩展？ \n 容器失败退出时如何新建容器确保服务正常运行？ \n 如何确保零宕机时间？ \n 如何管理密码，Key等敏感数据？ \n 其它 \n \n 容器编排 swarm \n \n Swarm的基本架构 \n Swarm 单节点快速上手 \n 初始化 \n 这个命令可以查看我们的docker engine有没有激活swarm模式， 默认是没有的，我们会看到 \n docker  info\n \n 1 \n 激活swarm，有两个方法： \n \n 初始化一个swarm集群，自己成为manager \n 加入一个已经存在的swarm集群 \n \n Swarm Commands:\nconfig      Manage Swarm configs\n node         Manage Swarm nodes\nsecret      Manage Swarm secrets\n service      Manage Swarm services\nstack       Manage Swarm stacks\nswarm       Manage Swarm\n\n\n\nUsage:   docker  swarm COMMAND\n\nManage Swarm\n\nCommands:\nca          Display and rotate the root CA\ninit        Initialize a swarm\n join         Join a swarm as a  node  and/or manager\njoin-token  Manage  join  tokens\nleave       Leave the swarm\nunlock      Unlock swarm\nunlock-key  Manage the unlock key\nupdate      Update the swarm\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n #初始化一个集群 \n docker  swarm init\n #查看集群节点 \n docker   node   ls \n #查看node的信息 \n docker   node  inspect  < node id > \n \n 1 2 3 4 5 6 \n docker swarm init 背后发生了什么 \n 主要是PKI和安全相关的自动化 \n \n 创建swarm集群的根证书 \n manager节点的证书 \n 其它节点加入集群需要的tokens \n \n 创建Raft数据库用于存储证书，配置，密码等数据 \n RAFT相关资料 \n \n http://thesecretlivesofdata.com/raft/ \n https://raft.github.io/ \n https://docs.docker.com/engine/swarm/raft/ \n \n 看动画学会 Raft 算法 \n https://mp.weixin.qq.com/s/p8qBcIhM04REuQ-uG4gnbw \n swarm join-token ：可以查看或更换join token。\n docker  swarm join-token worker：查看加入woker的命令。\n docker  swarm join-token manager：查看加入manager的命令\n docker  swarm join-token  --rotate  worker：重置woker的Token。\n docker  swarm join-token  -q  worker：仅打印Token。\n \n 1 2 3 4 5 #打开另外的虚拟机作为worker 加入，执行以下命令 \n docker  swarm  join   --token  SWMTKN-1-3hocsrayh278lv0lz17g0ty5bxedekofilq3pbiaj9xskqcjmh-331n58myelyc6bej36wosd69v  192.168 .8.10:2377\n \n 1 2 \n #打开manager所在的虚拟机，查看node情况 \n docker   node   ls \n \n 1 2 \n #创建服务 \n docker   service  create  --name  nginx  -d   --replicas   3  nginx:stable-alpine\n docker   service   ps  nginx\n \n 1 2 3 调节service的副本数在docker swarm中可以在创建service时，通过设置--replicas 来设置副本数，但是当服务运行起来后应该如何处理呢？docker 提供了scale命令来实现这个功能 \n #scale 动态扩缩容 \n docker   service  scale  nginx = 2 \n \n 1 2 \n #动态更新服务 \n docker   service  update  --image  nginx:latest nginx\n #动态回滚服务 \n docker   service  update   --rollback  nginx\n \n 1 2 3 4 \n Docker Stack \n docker 与docker-compose的区别是后者可以部署多个容器； \n docker swarm与docker stack的区别是stack可以部署多个服务； \n docker stack与docker-compose的区别，stack是集群式，compose是单机模式，多用于开发测试。 \n stack获取不到.env，找到githup上的一种方法： \n docker stack deploy  in 1.13 doesn\'t load  .env  file as  docker-compose up  does · Issue #29133 · moby/moby (github.com) \n env   $( cat  .env  |   grep  ^ [ A-Z ]   |   xargs )   docker  stack deploy  -c  stack.yaml STACK\n \n 1 任务的编排可以进阶学习k8s。 \n 9.Docker整合gitlab CICD \n 单模块下部署 \n Dockerfile配置 \n FROM  openjdk:8-jre-alpine \n VOLUME  /tmp \n ###复制文件到容器app-springboot \n COPY  ./target/cicd-0.0.1-SNAPSHOT.jar app.jar \n\n ENTRYPOINT  [ "java" , "-Djava.security.egd=file:/dev/./urandom" , "-jar" , "/app.jar" ] \n \n 1 2 3 4 5 6 .gitlab-ci.yml配置 \n 单环境 \n #1：需要用到的镜像 \n #2：必须配置的一些环境变量。如果本地可不配置 DOCKER_HOST。 \n #3：配置缓存，配置后，maven 下载的依赖可以被缓存起来，下次不需要重复去下载了。 \n #4：配置需要用到的额外的服务。docker:dind，这个貌似是用于在 docker 中运行 docker 的一种东西，在项目的构建中需要。 \n   #有时需要在容器内执行 docker 命令，比如：在 jenkins 容器内运行 docker 命令执行构建镜像 \n   #直接在 docker 容器内嵌套安装 docker 未免太过臃肿 \n   #更好的办法是：容器内仅部署 docker 命令行工具（作为客户端），实际执行交由宿主机内的 docker-engine（服务器） \n   #先启动一个docker:dind容器A，再启动一个docker容器B，容器B指定host为A容器内的docker daemon。 \n\n #5：stages，这是 Gitlab CI 中的概念，Stages 表示构建阶段，就是一些按序执行的流程，具体执行是依赖于 Jobs 的。 \n #6 ：定义的 Jobs 之一，用于构建 jar 包。内部又引入 maven 镜像来处理，负责执行 package 这一流程。script 为具体执行的脚本。 \n #7：定义的 Jobs 之一，用于构建 Docker 镜像。负责执行 deploy 这一流程。具体执行 build 和 run。only 节点表示只监控 master 分支。 \n\n image :  docker : latest   #1 \n variables :    #2 \n   DOCKER_DRIVER :  overlay2\n   DOCKER_HOST :  tcp : //192.168.8.10 : 2375    # docker host，本地可不写 \n   DOCKER_TLS_CERTDIR :   \'\' \n   TAG :  root/hello - spring : v0.1   # 镜像名称 \n cache :    #3 \n   key :  m2 - repo\n   paths : \n     -  .m2/repository\n services :    #4 \n   -  docker : dind\n stages :    #5 \n   -  package\n   -  deploy\n maven-package :    #6 \n   image :  maven : 3.5.0 - jdk - 8 \n   tags : \n     -  maven\n   stage :  package\n   script : \n     -  mvn clean package  - Dmaven.test.skip=true\n   artifacts : \n     paths : \n       -  target/ *.jar \n build-master :    #7 \n   tags : \n     -  docker\n   stage :  deploy\n   script : \n     -  docker build  - t $TAG .\n     -  docker rm  - f test  | |  true\n     -  docker run  - d  - - name test  - p 8888 : 8888 $TAG\n   only : \n     -  master\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 多环境 \n #1：需要用到的镜像 \n #2：必须配置的一些环境变量。如果本地可不配置 DOCKER_HOST。 \n #3：配置缓存，配置后，maven 下载的依赖可以被缓存起来，下次不需要重复去下载了。 \n #4：配置需要用到的额外的服务。docker:dind，这个貌似是用于在 docker 中运行 docker 的一种东西，在项目的构建中需要。 \n   #有时需要在容器内执行 docker 命令，比如：在 jenkins 容器内运行 docker 命令执行构建镜像 \n   #直接在 docker 容器内嵌套安装 docker 未免太过臃肿 \n   #更好的办法是：容器内仅部署 docker 命令行工具（作为客户端），实际执行交由宿主机内的 docker-engine（服务器） \n   #先启动一个docker:dind容器A，再启动一个docker容器B，容器B指定host为A容器内的docker daemon。 \n\n #5：stages，这是 Gitlab CI 中的概念，Stages 表示构建阶段，就是一些按序执行的流程，具体执行是依赖于 Jobs 的。 \n #6 ：定义的 Jobs 之一，用于构建 jar 包。内部又引入 maven 镜像来处理，负责执行 package 这一流程。script 为具体执行的脚本。 \n #7：定义的 Jobs 之一，用于构建 Docker 镜像。负责执行 deploy 这一流程。具体执行 build 和 run。only 节点表示只监控 master 分支。 \n\nimage: docker:latest   #1 \nvariables:   #2 \n  DOCKER_DRIVER: overlay2\n  DOCKER_HOST: tcp://192.168.8.10:2375   # docker host，本地可不写 \n  DOCKER_TLS_CERTDIR:  \'\' \n  USERSERVICE_TAG:  \':1.0\'    # 镜像版本号 \n  USERSERVICE_NAME: cicd  #镜像名称 \n  USERSERVICE_RPORT:  8888   #镜像端口 容器内的端口与其不一致 \n  USERSERVICE_DIRECTORY: cicd  #模块目录 \n  PROFILE_ACTIVE: prd\ncache:   #3 \n  key: m2-repo\n  paths:\n    - .m2/repository\nservices:   #4 \n  - docker:dind\nstages:   #5 \n  - package\n  - deploy-dev\n  - deploy-prd\nmaven-package:   #6 \n  image: maven:3.5.0-jdk-8\n  tags:\n    - maven\n  stage: package\n  script:\n    - mvn clean package  -P   $PROFILE_ACTIVE   -Dmaven.test.skip = true\n  artifacts:\n    paths:\n      - target/*.jar\nbuild-dev:\n  tags:\n    -  docker \n  stage: deploy-dev\n  script:\n    -  docker  login  $CI_REGISTRY   -u   $CI_REGISTRY_USER   -p   $CI_REGISTRY_PASSWORD \n    -  docker   rm   -f   $USERSERVICE_NAME   ||   true \n    -  docker  rmi  -f   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG   ||   true \n    -  docker  build  -f  Dockerfile  -t   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG   . \n    -  docker  push  $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG \n    -  docker  run  -d   --name   $USERSERVICE_NAME   -v   $USERSERVICE_NAME :/apps  --privileged = true  -p   $USERSERVICE_RPORT : $USERSERVICE_RPORT   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG \n    -  docker  image prune  -f \n  only:  #限制作业在什么时候创建，定义了哪些分支或标签(branches and tags)的作业会运行 \n    refs:  #分支 \n      - dev\n     #changes: # 下面的文件中任一文件发生改变 \n - .gitlab-ci.yml \nbuild-prd:   #7 \n  tags:\n    -  docker \n  stage: deploy-prd\n  before_script:\n    -  docker  login  $CI_REGISTRY   -u   $CI_REGISTRY_USER   -p   $CI_REGISTRY_PASSWORD \n    -  docker  rmi  -f   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG   ||   true \n    -  docker  build  -f  Dockerfile  -t   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG   . \n    -  docker  push  $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG \n    -  docker  image prune  -f \n    -  \'which  ssh-agent || ( yum update -y  && yum install openssh-client git -y )\' \n    -  eval   $( ssh-agent  -s ) \n    -  echo   " $SSH_PRIVATE_KEY "   |   tr   -d   \'\\r\'   |  ssh-add -\n    -  mkdir   -p  ~/.ssh\n    -  chmod   700  ~/.ssh\n    - ssh-keyscan  192.168 .8.103  >>  ~/.ssh/known_hosts  #ssh-keyscan 从服务器收集 SSH公钥。 ssh-keyscan 是一个收集多个主机的 SSH 公共密钥的实用工具。它被设计用来帮助构建和验证 ssh_known_hosts 文件 \n    -  chmod   644  ~/.ssh/known_hosts\n  script:\n-tq 打印台静默不打印 \n    -  ssh   -tq   192.168 .8.103  <<  EOF\n    -  docker  login  $CI_REGISTRY   -u   $CI_REGISTRY_USER   -p   $CI_REGISTRY_PASSWORD \n    -  docker   rm   -f   $USERSERVICE_NAME   ||   true \n    -  docker  rmi  -f   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG   ||   true \n    -  docker  run  -d   --name   $USERSERVICE_NAME   -v   $USERSERVICE_NAME :/apps  --privileged = true  -p   $USERSERVICE_RPORT : $USERSERVICE_RPORT   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG \n    -  docker  image prune  -f \n    -  exit \n    - EOF\n  only:\n    variables:  [   $PROFILE_ACTIVE   ==   "prd"   ] \n    refs:\n      - master\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 #  多模块合并部署 \n 根目录.gitlab-ci.yml配置 \n #stages: \n - triggers \n - triggers1 \n #trigger_a: \n stage: triggers \n trigger: \n   include: szbxl-service/szbxl-user-service/.gitlab-ci.yml \n rules: \n   - changes: \n       - szbxl-service/szbxl-user-service/* \n #trigger_b: \n stage: triggers1 \n trigger: \n   include: szbxl-getway/.gitlab-ci.yml \n rules: \n   - changes: \n       - szbxl-getway/* \n\n variables :   #2 \n   DOCKER_DRIVER :  overlay2\n   DOCKER_HOST :  tcp : //192.168.8.10 : 2375    # docker host，本地可不写 \n   DOCKER_TLS_CERTDIR :   \'\' \n   PROFILE_ACTIVE :  prd  #激活属性,dev/prod,从Gitlab的变量设置，这样定时部署可以动态指定 \n   GIT_STRATEGY :  clone  #Initialized empty Git repository as it did in the first run of the first job, it now says Reinitialized existing Git repository. \n   #https://stackoverflow.com/questions/57779750/does-gitlab-runner-or-docker-cache-the-builds-directory-by-default \n多模块.gitlab-ci.yml \n stages : \n   -  service_first - package\n   -  service_first - dev - deploy\n   -  service_first - prd - deploy\n   -  service_second - package\n   -  service_second - dev - deploy\n   -  service_second - prd - deploy\n include : \n   -   local :  service_first/.gitlab - ci.yml\n   -   local :  service_second/.gitlab - ci.yml\n\n #include \n   #全局关键字。可以在本地的gitlab-ci.yml文件中引入其他地方的gitlab-ci.yml的配置文件。 \n\n   #include:local \n   #可以使用include:local来引入同一个仓库同一个分支中的其他配置文件（其他配置文件必须是.yml 或者.ymal扩展名） \n   #include:project \n       #可以通过include:project 来引入同一个gitlab 服务中其他项目中的gitlab-ci.yml配置 \n       #主要有下面几部分构成， \n\n       #project：表在该gitlab服务器中的项目地址 \n       #file：一个数组，表引入该项目中的哪些文件 \n       #ref：非必须，表该项目中的那个分支。没有就是master(main) \n   #include:remote \n     #使用include:remote 可以通过一个完整的URL地址加载远端的gitlab-ci.yml的配置文件。 \n     #远端的配置文件使用HTTP/GTTPS 的GET请求可以获取的到 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 子模块一目录 Dockerfile配置 \n基础镜像 \n不能使用openjdk:8-jre-alpine，原因如下： \n1、会导致easy-captcha验证码生成失败。 \n2、 Docker启动会报错：ERROR: failed to launch: exec.d: failed to execute exec.d file at path \'/layers/paketo-buildpacks_bellsoft-liberica/helper/exec.d/memory-calculator\': exit status 1 \n FROM  openjdk:8-jre-alpine \n镜像作者 \n LABEL  author= "gordon" \n指定容器内的时区 \n RUN  echo  "Asia/Shanghai"  > /etc/timezone \n环境变量 \n ENV  MYPATH /apps/service_first/ \n创建项目存放路径 \n RUN  mkdir -p  ${MYPATH} \n指定工作目录 \n WORKDIR   ${MYPATH} \n尽量采用指定路径挂载,不然会生成很多临时挂载垃圾文件 \n匿名挂载目录(宿主机位置：/var/lib/docker/volumes/xxxxxxxxxxxxxxxx) \n查看挂载目录：docker inspect szbxl-user-service \n查看所有挂载卷：docker volume ls \n #VOLUME ${MYPATH} \nTomcat 默认使用/tmp作为工作目录。这个命令的效果是：在宿主机的/var/lib/docker目录下创建一个临时文件并把它链接到容器中的/tmp目录 \n #VOLUME /tmp \n #VOLUME /log \n添加jar包到指定路径,当前目录是WORKDIR \n COPY  service_first/target/*.jar app.jar \n暴露端口 \n #EXPOSE 802 \n启动容器执行命令 \n #ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","app.jar"] \n #ENTRYPOINT ["sh", "-c", "java --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/java.lang.reflect=ALL-UNNAMED -Djava.security.egd=file:/dev/./urandom -jar app.jar"] \n ENTRYPOINT  [ "java" , "-Djava.security.egd=file:/dev/./urandom" , "-jar" , "app.jar" ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 子模块一目录.gitlab-ci.yml配置 \n #1：需要用到的镜像 \n #2：必须配置的一些环境变量。如果本地可不配置 DOCKER_HOST。 \n #3：配置缓存，配置后，maven 下载的依赖可以被缓存起来，下次不需要重复去下载了。 \n #4：配置需要用到的额外的服务。docker:dind，这个貌似是用于在 docker 中运行 docker 的一种东西，在项目的构建中需要。 \n   #有时需要在容器内执行 docker 命令，比如：在 jenkins 容器内运行 docker 命令执行构建镜像 \n   #直接在 docker 容器内嵌套安装 docker 未免太过臃肿 \n   #更好的办法是：容器内仅部署 docker 命令行工具（作为客户端），实际执行交由宿主机内的 docker-engine（服务器） \n   #先启动一个docker:dind容器A，再启动一个docker容器B，容器B指定host为A容器内的docker daemon。 \n\n #5：stages，这是 Gitlab CI 中的概念，Stages 表示构建阶段，就是一些按序执行的流程，具体执行是依赖于 Jobs 的。 \n #6 ：定义的 Jobs 之一，用于构建 jar 包。内部又引入 maven 镜像来处理，负责执行 package 这一流程。script 为具体执行的脚本。 \n #7：定义的 Jobs 之一，用于构建 Docker 镜像。负责执行 deploy 这一流程。具体执行 build 和 run。only 节点表示只监控 master 分支。 \n\n image :  docker : latest   #1 \n variables :    #2 \n   #根目录已提供 \n   #DOCKER_DRIVER: overlay2 \n   #DOCKER_HOST: tcp://192.168.8.10:2375  # docker host，本地可不写 \n   #DOCKER_TLS_CERTDIR: \'\' \n   #PROFILE_ACTIVE: prd \n   FIRST_TAG :   \':1.0\'    # 镜像版本号 \n   FIRST_NAME :  service_first  #镜像名称 \n   FIRST_RPORT :   8888   #镜像端口 容器内的端口与其不一致 \n   FIRST_DIRECTORY :  service_first  #模块目录 \n\n cache :    #3 \n   key :  m2 - repo\n   paths : \n     -  .m2/repository\n services :    #4 \n   -  docker : dind\n stages :    #5 \n   -  service_first - package\n   -  service_first - dev - deploy\n   -  service_first - prd - deploy\n maven-package:service_first :    #6 \n   image :  maven : 3.5.0 - jdk - 8 \n   tags : \n     -  maven\n   stage :  service_first - package\n   script : \n     #mvn clean package -pl service_first -am -Dmaven.test.skip=true -pl --projects <arg> 构建制定的模块，模块间用逗号分隔；-am --also-make 同时构建所列模块的依赖模块； \n     -  mvn clean package  - P $PROFILE_ACTIVE  - Dmaven.test.skip=true  - pl $FIRST_DIRECTORY  - am\n   artifacts : \n     paths : \n       -  $FIRST_DIRECTORY/target/ *.jar \n build-dev:service_first : \n   tags : \n     -  docker\n   stage :  service_first - dev - deploy\n   script : \n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rm  - f $FIRST_NAME  | |  true\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG  | |  true\n     -  docker build  - f $FIRST_DIRECTORY/Dockerfile  - t $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG .\n     -  docker push $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG\n     -  docker run  - d  - - name $FIRST_NAME  - v /apps/$FIRST_DIRECTORY : /apps/$FIRST_DIRECTORY/log  - - privileged=true  - p $FIRST_RPORT : $FIRST_RPORT $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG\n     -  docker image prune  - f\n   only :   #限制作业在什么时候创建，定义了哪些分支或标签(branches and tags)的作业会运行 \n     refs :   #分支 \n       -  dev\n     #changes: # 下面的文件中任一文件发生改变 \n - .gitlab-ci.yml \n build-prd:service_first :   #7 \n   tags : \n     -  docker\n   stage :  service_first - prd - deploy\n   before_script : \n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG  | |  true\n     -  docker build  - f $FIRST_DIRECTORY/Dockerfile  - t $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG .\n     -  docker push $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG\n     -  docker image prune  - f\n     -   \'which  ssh-agent || ( yum update -y  && yum install openssh-client git -y )\' \n     -  eval $(ssh - agent  - s)\n     -  echo "$SSH_PRIVATE_KEY"  |  tr  - d \'\\r\'  |  ssh - add  - \n     -  mkdir  - p ~/.ssh\n     -  chmod 700 ~/.ssh\n     -  ssh - keyscan 192.168.8.103  > >  ~/.ssh/known_hosts  #ssh-keyscan 从服务器收集 SSH公钥。 ssh-keyscan 是一个收集多个主机的 SSH 公共密钥的实用工具。它被设计用来帮助构建和验证 ssh_known_hosts 文件 \n     -  chmod 644 ~/.ssh/known_hosts\n   script : \n-tq 打印台静默不打印 \n     -  ssh  - tq 192.168.8.103 << EOF\n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rm  - f $FIRST_NAME  | |  true\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG  | |  true\n     -  docker run  - d  - - name $FIRST_NAME  - v /apps/$FIRST_DIRECTORY : /apps/$FIRST_DIRECTORY/log  - - privileged=true  - p $FIRST_RPORT : $FIRST_RPORT $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG\n     -  docker image prune  - f\n     -  exit\n     -  EOF\n   only : \n     variables :   [  $PROFILE_ACTIVE == "prd"  ] \n     refs : \n       -  main\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 子模块二目录 Dockerfile配置 \n基础镜像 \n不能使用openjdk:8-jre-alpine，原因如下： \n1、会导致easy-captcha验证码生成失败。 \n2、 Docker启动会报错：ERROR: failed to launch: exec.d: failed to execute exec.d file at path \'/layers/paketo-buildpacks_bellsoft-liberica/helper/exec.d/memory-calculator\': exit status 1 \n FROM  openjdk:8-jre-alpine \n镜像作者 \n LABEL  author= "gordon" \n指定容器内的时区 \n RUN  echo  "Asia/Shanghai"  > /etc/timezone \n环境变量 \n ENV  MYPATH /apps/service_second/ \n创建项目存放路径 \n RUN  mkdir -p  ${MYPATH} \n指定工作目录 \n WORKDIR   ${MYPATH} \n尽量采用指定路径挂载,不然会生成很多临时挂载垃圾文件 \n匿名挂载目录(宿主机位置：/var/lib/docker/volumes/xxxxxxxxxxxxxxxx) \n查看挂载目录：docker inspect szbxl-user-service \n查看所有挂载卷：docker volume ls \n #VOLUME ${MYPATH} \nTomcat 默认使用/tmp作为工作目录。这个命令的效果是：在宿主机的/var/lib/docker目录下创建一个临时文件并把它链接到容器中的/tmp目录 \n #VOLUME /tmp \n #VOLUME /log \n添加jar包到指定路径 \n COPY  service_second/target/*.jar app.jar \n暴露端口 \n #EXPOSE 802 \n启动容器执行命令 \n #ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","app.jar"] \n #ENTRYPOINT ["sh", "-c", "java --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/java.lang.reflect=ALL-UNNAMED -Djava.security.egd=file:/dev/./urandom -jar app.jar"] \n ENTRYPOINT  [ "java" , "-Djava.security.egd=file:/dev/./urandom" , "-jar" , "app.jar" ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 子模块一目录.gitlab-ci.yml配置 \n #1：需要用到的镜像 \n #2：必须配置的一些环境变量。如果本地可不配置 DOCKER_HOST。 \n #3：配置缓存，配置后，maven 下载的依赖可以被缓存起来，下次不需要重复去下载了。 \n #4：配置需要用到的额外的服务。docker:dind，这个貌似是用于在 docker 中运行 docker 的一种东西，在项目的构建中需要。 \n   #有时需要在容器内执行 docker 命令，比如：在 jenkins 容器内运行 docker 命令执行构建镜像 \n   #直接在 docker 容器内嵌套安装 docker 未免太过臃肿 \n   #更好的办法是：容器内仅部署 docker 命令行工具（作为客户端），实际执行交由宿主机内的 docker-engine（服务器） \n   #先启动一个docker:dind容器A，再启动一个docker容器B，容器B指定host为A容器内的docker daemon。 \n\n #5：stages，这是 Gitlab CI 中的概念，Stages 表示构建阶段，就是一些按序执行的流程，具体执行是依赖于 Jobs 的。 \n #6 ：定义的 Jobs 之一，用于构建 jar 包。内部又引入 maven 镜像来处理，负责执行 package 这一流程。script 为具体执行的脚本。 \n #7：定义的 Jobs 之一，用于构建 Docker 镜像。负责执行 deploy 这一流程。具体执行 build 和 run。only 节点表示只监控 master 分支。 \n\n image :  docker : latest   #1 \n variables :    #2 \n   #根目录已提供 \n   #DOCKER_DRIVER: overlay2 \n   #DOCKER_HOST: tcp://192.168.8.10:2375  # docker host，本地可不写 \n   #DOCKER_TLS_CERTDIR: \'\' \n   #PROFILE_ACTIVE: prd \n   SECOND_TAG :   \':1.0\'    # 镜像版本号 \n   SECOND_NAME :  service_second  #镜像名称 \n   SECOND_RPORT :   9999   #镜像端口 容器内的端口与其不一致 \n   SECOND_DIRECTORY :  service_second  #模块目录 \n\n cache :    #3 \n   key :  m2 - repo\n   paths : \n     -  .m2/repository\n services :    #4 \n   -  docker : dind\n stages :    #5 \n   -  service_second - package\n   -  service_second - dev - deploy\n   -  service_second - prd - deploy\n maven-package:service_second :    #6 \n   image :  maven : 3.5.0 - jdk - 8 \n   tags : \n     -  maven\n   stage :  service_second - package\n   script : \n     #mvn clean package -pl service_first -am -Dmaven.test.skip=true -pl --projects <arg> 构建制定的模块，模块间用逗号分隔；-am --also-make 同时构建所列模块的依赖模块； \n     -  mvn clean package  - P $PROFILE_ACTIVE  - Dmaven.test.skip=true  - pl $SECOND_DIRECTORY  - am\n   artifacts : \n     paths : \n       -  $SECOND_DIRECTORY/target/ *.jar \n build-dev:service_second : \n   tags : \n     -  docker\n   stage :  service_second - dev - deploy\n   script : \n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rm  - f $SECOND_NAME  | |  true\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG  | |  true\n     -  docker build  - f $SECOND_DIRECTORY/Dockerfile  - t $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG .\n     -  docker push $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG\n     -  docker run  - d  - - name $SECOND_NAME  - v /apps/$SECOND_DIRECTORY : /apps/$SECOND_DIRECTORY/log  - - privileged=true  - p $SECOND_RPORT : $SECOND_RPORT $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG\n     -  docker image prune  - f\n   only :   #限制作业在什么时候创建，定义了哪些分支或标签(branches and tags)的作业会运行 \n     refs :   #分支 \n       -  dev\n     #changes: # 下面的文件中任一文件发生改变 \n - .gitlab-ci.yml \n build-prd:service_second :   #7 \n   tags : \n     -  docker\n   stage :  service_second - prd - deploy\n   before_script : \n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG  | |  true\n     -  docker build  - f $SECOND_DIRECTORY/Dockerfile  - t $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG .\n     -  docker push $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG\n     -  docker image prune  - f\n     -   \'which  ssh-agent || ( yum update -y  && yum install openssh-client git -y )\' \n     -  eval $(ssh - agent  - s)\n     -  echo "$SSH_PRIVATE_KEY"  |  tr  - d \'\\r\'  |  ssh - add  - \n     -  mkdir  - p ~/.ssh\n     -  chmod 700 ~/.ssh\n     -  ssh - keyscan 192.168.8.103  > >  ~/.ssh/known_hosts  #ssh-keyscan 从服务器收集 SSH公钥。 ssh-keyscan 是一个收集多个主机的 SSH 公共密钥的实用工具。它被设计用来帮助构建和验证 ssh_known_hosts 文件 \n     -  chmod 644 ~/.ssh/known_hosts\n   script : \n-tq 打印台静默不打印 \n     -  ssh  - tq 192.168.8.103 << EOF\n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rm  - f $SECOND_NAME  | |  true\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG  | |  true\n     -  docker run  - d  - - name $SECOND_NAME  - v /apps/$SECOND_DIRECTORY : /apps/$SECOND_DIRECTORY/log  - - privileged=true  - p $SECOND_RPORT : $SECOND_RPORT $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG\n     -  docker image prune  - f\n     -  exit\n     -  EOF\n   only : \n     variables :   [  $PROFILE_ACTIVE == "prd"  ] \n     refs : \n       -  main\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 多模块 \n 部署过程中遇到的问题 \n \n #重新加载daemon \nsystemctl daemon-reload\n \n 1 2 下载的镜像重复拉取 \n The image pull policy: never, if-not-present or always (default). \n never是从不从远端拉镜像，只用本地。if-not-present 是优先本地，然后是从网络拉取镜像。always 是从远端拉取镜像。 \n 解决办法：配置先从本地拉取。\n vi  /etc/gitlab-runner/config.toml 添加：    \npull_policy  =   "if-not-present"  \n \n 1 2 3 \n \n maven下载慢 \n maven安装到宿主机，在runner里面进行挂载，这样可以重复使用，也可以使用国内的镜像源 \n \n [ [ runners ] ] \n  name  =   "Docker Maven Runner" \n  url  =   "http://192.168.8.10:2280" \n  token  =   "mPciztaPrpZsYSsWuRAX" \n  executor  =   "docker" \n   [ runners.custom_build_dir ] \n   [ runners.cache ] \n     [ runners.cache.s3 ] \n     [ runners.cache.gcs ] \n     [ runners.cache.azure ] \n   [ runners.docker ] \n    tls_verify  =   false \n    image  =   "maven:3.5.0-jdk-8" \n    privileged  =   true \n    pull_policy  =   "if-not-present" \n    disable_entrypoint_overwrite  =   false \n    oom_kill_disable  =   false \n    disable_cache  =   false \n    volumes  =   [ "/cache" ,  "/export/server/bigdata-stack/maven/apache-maven-3.9.2:/root/.m2" ] \n    shm_size  =   0 \n\n [ [ runners ] ] \n  name  =   "Docker Build Runner" \n  url  =   "http://192.168.8.10:2280" \n  token  =   "pRuA3NL6tsJsBgq-Ps65" \n  executor  =   "docker" \n   [ runners.custom_build_dir ] \n   [ runners.cache ] \n     [ runners.cache.s3 ] \n     [ runners.cache.gcs ] \n     [ runners.cache.azure ] \n   [ runners.docker ] \n    tls_verify  =   false \n    image  =   "docker:latest" \n    privileged  =   true \n    pull_policy  =   "if-not-present" \n    disable_entrypoint_overwrite  =   false \n    oom_kill_disable  =   false \n    disable_cache  =   false \n    volumes  =   [ "/cache" ,  "/export/server/bigdata-stack/maven/apache-maven-3.9.2:/root/.m2" ] \n    shm_size  =   0 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 \n 10.Docker 整合Sonar进行代码质量检查,统计单元测试覆盖率 \n 单模块的.yml配置修改如下： \n #1：需要用到的镜像 \n #2：必须配置的一些环境变量。如果本地可不配置 DOCKER_HOST。 \n #3：配置缓存，配置后，maven 下载的依赖可以被缓存起来，下次不需要重复去下载了。 \n #4：配置需要用到的额外的服务。docker:dind，这个貌似是用于在 docker 中运行 docker 的一种东西，在项目的构建中需要。 \n #有时需要在容器内执行 docker 命令，比如：在 jenkins 容器内运行 docker 命令执行构建镜像 \n #直接在 docker 容器内嵌套安装 docker 未免太过臃肿 \n #更好的办法是：容器内仅部署 docker 命令行工具（作为客户端），实际执行交由宿主机内的 docker-engine（服务器） \n #先启动一个docker:dind容器A，再启动一个docker容器B，容器B指定host为A容器内的docker daemon。 \n\n #5：stages，这是 Gitlab CI 中的概念，Stages 表示构建阶段，就是一些按序执行的流程，具体执行是依赖于 Jobs 的。 \n #6 ：定义的 Jobs 之一，用于构建 jar 包。内部又引入 maven 镜像来处理，负责执行 package 这一流程。script 为具体执行的脚本。 \n #7：定义的 Jobs 之一，用于构建 Docker 镜像。负责执行 deploy 这一流程。具体执行 build 和 run。only 节点表示只监控 master 分支。 \n\n image :  docker : latest   #1 \n variables :    #2 \n   DOCKER_DRIVER :  overlay2\n   DOCKER_HOST :  tcp : //192.168.8.10 : 2375    # docker host，本地可不写 \n   DOCKER_TLS_CERTDIR :   \'\' \n   USERSERVICE_TAG :   \':1.0\'    # 镜像版本号 \n   USERSERVICE_NAME :  cicd  #镜像名称 \n   USERSERVICE_RPORT :   6666   #镜像端口 容器内的端口与其不一致 \n   USERSERVICE_DIRECTORY :  cicd  #模块目录 \n   PROFILE_ACTIVE :  prd\nsonner scanner 安装目录 \n   SCANNER_HOME :   "/export/server/sonar-scanner" \n扫描代码路径 \n   SCAN_DIR :   "src" \n cache :    #3 \n   key :  m2 - repo\n   paths : \n     -  .m2/repository\n services :    #4 \n   -  docker : dind\n stages :    #5 \n   -  package\n   -  sonarqube - check\n   -  deploy - dev\n   -  deploy - prd\n maven-package :    #6 \n   image :  maven : 3.5.0 - jdk - 8 \n   tags : \n     -  maven\n   stage :  package\n   script : \n     -  mvn clean org.jacoco : jacoco - maven - plugin : prepare - agent install dependency : copy - dependencies\n       - Dmaven.test.failure.ignore=true package  - P $PROFILE_ACTIVE\nMaven编译，所以会有Jar包产物，这里定义产物的过期时间 \n   artifacts : \n     expire_in :  1 days\n     paths : \n       -  target/\n sonarqube-check : \n   image : \n     name :  sonarsource/sonar - scanner - cli : latest\n     entrypoint :   [ "" ] \n   tags : \n     -  sonar\n   stage :  sonarqube - check\n   needs :   [ maven - package ] \n   dependencies : \n     -  maven - package\n   variables : \n     SONAR_USER_HOME :   "${CI_PROJECT_DIR}/.sonar"    # Defines the location of the analysis task cache \n     GIT_DEPTH :   "0"    # Tells git to fetch all the branches of the project, required by the analysis task \n   cache : \n     key :   "${CI_JOB_NAME}" \n     paths : \n       -  .sonar/cache\n   script : \n     -  sonar - scanner\n       - Dsonar.projectKey=sonarqube - check\n       - Dsonar.sources=src/main/\n       - Dsonar.tests=src/test/\n       - Dsonar.coverage.exclusions=src/main/java/com/gordon/cicd/CicdApplication.java\n       - Dsonar.scm.disabled=true\n       - Dsonar.language=java\n       - Dsonar.sourceEncoding=UTF - 8 \n       - Dsonar.host.url=http : //192.168.8.10 : 9000 \n       - Dsonar.login=b95cb41cb57230c28a79a177a9f0fc3bddf02a79\n       - Dsonar.java.binaries=target/classes\n       - Dsonar.java.test.binaries=target/test - classes\n       - Dsonar.java.libraries=target/dependency\n       - Dsonar.java.test.libraries=target/dependency\n       - Dsonar.java.surefire.report=target/surefire - reports\n       - Dsonar.coverage.jacoco.xmlReportPaths=target/site/jacoco/jacoco.xml\n     #统计结果输出 \n     -  awk  - F" , " \' {  instructions += $4 + $5; covered += $5  }  END  {  print 100  *covered/instructions ,   "% covered"   } \' target/site/jacoco/jacoco.csv\n\n   allow_failure :   false \n   only : \n     -  merge_requests\n     -  master  # or the name of your main branch \n     -  develop\n   artifacts : \n     paths : \n       -  target/\n\n build-dev : \n   tags : \n     -  docker\n   stage :  deploy - dev\n   needs :   [ sonarqube - check ] \n   script : \n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rm  - f $USERSERVICE_NAME  | |  true\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG  | |  true\n     -  docker build  - f Dockerfile  - t $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG .\n     -  docker push $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG\n     -  docker run  - d  - - name $USERSERVICE_NAME  - v $USERSERVICE_NAME : /apps  - - privileged=true  - p $USERSERVICE_RPORT : $USERSERVICE_RPORT $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG\n     -  docker image prune  - f\n   only :   #限制作业在什么时候创建，定义了哪些分支或标签(branches and tags)的作业会运行 \n     refs :   #分支 \n       -  dev\n     #changes: # 下面的文件中任一文件发生改变 \n - .gitlab-ci.yml \n build-prd :    #7 \n   tags : \n     -  docker\n   stage :  deploy - prd\n   needs :   [ sonarqube - check ] \n   before_script : \n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG  | |  true\n     -  docker build  - f Dockerfile  - t $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG .\n     -  docker push $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG\n     -  docker image prune  - f\n     -   \'which  ssh-agent || ( yum update -y  && yum install openssh-client git -y )\' \n     -  eval $(ssh - agent  - s)\n     -  echo "$SSH_PRIVATE_KEY"  |  tr  - d \'\\r\'  |  ssh - add  - \n     -  mkdir  - p ~/.ssh\n     -  chmod 700 ~/.ssh\n     -  ssh - keyscan 192.168.8.103  > >  ~/.ssh/known_hosts  #ssh-keyscan 从服务器收集 SSH公钥。 ssh-keyscan 是一个收集多个主机的 SSH 公共密钥的实用工具。它被设计用来帮助构建和验证 ssh_known_hosts 文件 \n     -  chmod 644 ~/.ssh/known_hosts\n   script : \n-tq 打印台静默不打印 \n     -  ssh  - tq 192.168.8.103 << EOF\n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rm  - f $USERSERVICE_NAME  | |  true\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG  | |  true\n     -  docker run  - d  - - name $USERSERVICE_NAME  - v $USERSERVICE_NAME : /apps  - - privileged=true  - p $USERSERVICE_RPORT : $USERSERVICE_RPORT $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG\n     -  docker image prune  - f\n     -  exit\n     -  EOF\n   only : \n     variables :   [  $PROFILE_ACTIVE == "prd"  ] \n     refs : \n       -  master\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 \n \n 为了readme能查看结果，pom中jacoco做了exclude \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n     < parent > \n         < groupId > org.springframework.boot </ groupId > \n         < artifactId > spring-boot-starter-parent </ artifactId > \n         < version > 2.0.1.RELEASE </ version > \n         < relativePath />   \x3c!-- lookup parent from repository --\x3e \n     </ parent > \n\n     < groupId > com.gordon </ groupId > \n     < artifactId > cicd </ artifactId > \n     < version > 0.0.1-SNAPSHOT </ version > \n     < packaging > jar </ packaging > \n     < name > cicd </ name > \n     < description > Demo project for Spring Boot </ description > \n\n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n     </ properties > \n\n     < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-web </ artifactId > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n\n     </ dependencies > \n\n\n     < build > \n         < pluginManagement > \n             < plugins > \n                 < plugin > \n                     < groupId > org.sonarsource.scanner.maven </ groupId > \n                     < artifactId > sonar-maven-plugin </ artifactId > \n                     < version > 3.7.0.1746 </ version > \n                 </ plugin > \n             </ plugins > \n         </ pluginManagement > \n         < plugins > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.1 </ version > \n                 < configuration > \n                     < source > ${java.version} </ source > \n                     < target > ${java.version} </ target > \n                 </ configuration > \n             </ plugin > \n                 \x3c!--覆盖率测试--\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-surefire-plugin </ artifactId > \n                 < version > 2.21.0 </ version > \n                 < configuration > \n                     < testFailureIgnore > true </ testFailureIgnore > \n                     < argLine > @{argLine} -Dfile.encoding=utf-8 </ argLine > \n                 </ configuration > \n             </ plugin > \n             < plugin > \n                 < groupId > org.jacoco </ groupId > \n                 < artifactId > jacoco-maven-plugin </ artifactId > \n                 < version > 0.8.7 </ version > \n                 < configuration > \n                     < excludes > \n                         < exclude > **/CicdApplication.class </ exclude > \n                     </ excludes > \n                 </ configuration > \n                 < executions > \n                     < execution > \n                         < id > prepare-agent </ id > \n                         < goals > \n                             < goal > prepare-agent </ goal > \n                         </ goals > \n                     </ execution > \n                     < execution > \n                         < id > report </ id > \n                         < goals > \n                             < goal > report </ goal > \n                         </ goals > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n\n\n         </ plugins > \n     </ build > \n\n </ project > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 覆盖率展示格式化 \n \\ d+. \\ d+  \\ % covered\n \n 1 \n \n 多模块的子模块配置更新如下： \n 这里提供一种思路，新建一个模块把其他模块作为依赖引入，然后计算总的覆盖率。 \n |    .gitlab-ci.yml\n |    pom.xml\n |    README.md\n+---report-aggregate\n |        .gitlab-ci.yml\n |        pom.xml\n | \n+---service_first\n |     |    .gitlab-ci.yml\n |     |    Dockerfile\n |     |    pom.xml\n |     | \n |     \\ ---src\n |        +---main\n |         |    +---java\n |         |     |     \\ ---com\n |         |     |         \\ ---gordon\n |         |     |             |    FirstApplication.java\n |         |     |             | \n |         |     |             \\ ---controller\n |         |     |                    TestController.java\n |         |     | \n |         |     \\ ---resources\n |         |            application.properties\n |         | \n |         \\ ---test\n |             \\ ---java\n |                 \\ ---com\n |                     \\ ---gordon\n |                         \\ ---controller\n |                                TestControllerTest.java\n | \n \\ ---service_second\n     |    .gitlab-ci.yml\n     |    Dockerfile\n     |    pom.xml\n     | \n     \\ ---src\n        +---main\n         |    +---java\n         |     |     \\ ---com\n         |     |         \\ ---gordon\n         |     |             |    SecondApplication.java\n         |     |             | \n         |     |             \\ ---controller\n         |     |                    TestController.java\n         |     | \n         |     \\ ---resources\n         |            application.properties\n         | \n         \\ ---test\n             \\ ---java\n                 \\ ---com\n                     \\ ---gordon\n                         \\ ---controller\n                                TestControllerTest.java\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 创建第三模块 \n pom \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 " \n          xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < parent > \n         < artifactId > cicd_muil </ artifactId > \n         < groupId > com.gordon </ groupId > \n         < version > 1.0-SNAPSHOT </ version > \n     </ parent > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     < artifactId > report-aggregate </ artifactId > \n     < version > 1.0-SNAPSHOT </ version > \n\n     < dependencies > \n         < dependency > \n             < groupId > com.gordon </ groupId > \n             < artifactId > service_first </ artifactId > \n             < version > 1.0-SNAPSHOT </ version > \n         </ dependency > \n         < dependency > \n             < groupId > com.gordon </ groupId > \n             < artifactId > service_second </ artifactId > \n             < version > 1.0-SNAPSHOT </ version > \n         </ dependency > \n     </ dependencies > \n\n     < build > \n         < plugins > \n             < plugin > \n                 < groupId > org.jacoco </ groupId > \n                 < artifactId > jacoco-maven-plugin </ artifactId > \n                 < version > 0.8.7 </ version > \n                 < configuration > \n                     < excludes > \n                         < exclude > **/FirstApplication.class </ exclude > \n                         < exclude > **/SecondApplication.class </ exclude > \n                     </ excludes > \n                 </ configuration > \n                 < executions > \n                     < execution > \n                         < id > jacoco-report-aggregate </ id > \n                         \x3c!-- 触发的maven 命令 --\x3e \n                         < phase > install </ phase > \n                         < goals > \n                             < goal > report-aggregate </ goal > \n                         </ goals > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n         </ plugins > \n     </ build > \n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 .gitlab-ci.yml \n image :  docker : latest   #1 \n variables :    #2 \n   #根目录已提供 \n   #DOCKER_DRIVER: overlay2 \n   #DOCKER_HOST: tcp://192.168.8.10:2375  # docker host，本地可不写 \n   #DOCKER_TLS_CERTDIR: \'\' \n   #PROFILE_ACTIVE: prd \n   REPORT_NAME :  report - aggregate  #镜像名称 \n   REPORT_DIRECTORY :  report - aggregate  #模块目录 \n\n cache :    #3 \n   key :  m2 - repo\n   paths : \n     -  .m2/repository\n services :    #4 \n   -  docker : dind\n stages :    #5 \n   -  report - aggregate\n\n maven-package:report-aggregate :    #6 \n   image :  maven : 3.5.0 - jdk - 8 \n   tags : \n     -  maven\n   stage :  report - aggregate\n   script : \n     #mvn clean package -pl service_first -am -Dmaven.test.skip=true -pl --projects <arg> 构建制定的模块，模块间用逗号分隔；-am --also-make 同时构建所列模块的依赖模块； \n     -  mvn clean install dependency : copy - dependencies  - Dmaven.test.failure.ignore=true  - pl $REPORT_DIRECTORY   - am\n     -  awk  - F" , " \' {  instructions += $4 + $5; covered += $5  }  END  {  print 100  *covered/instructions ,   "% covered"   } \' $REPORT_DIRECTORY/target/site/jacoco - aggregate/jacoco.csv\n   artifacts : \n     paths : \n       -  $REPORT_DIRECTORY/target/\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 查看pipeline运行情况以及扫描结果 \n \n test \n \n'},{title:"RPC",frontmatter:{title:"RPC",date:"2022-10-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["其他"],tags:["远程过程调用"]},regularPath:"/%E5%85%B6%E4%BB%96/RPC.html",relativePath:"其他/RPC.md",key:"v-700a68a1",path:"/2022/10/08/rpc/",headers:[{level:2,title:"1.由来（是什么）",slug:"_1-由来-是什么"},{level:2,title:"2.入门",slug:"_2-入门"},{level:2,title:"3.常见的 RPC 框架",slug:"_3-常见的-rpc-框架"},{level:3,title:"3.1 Dubbo",slug:"_3-1-dubbo"},{level:3,title:"https://github.com/YClimb/redis-demo",slug:"https-github-com-yclimb-redis-demo"},{level:3,title:"3.2 BRPC",slug:"_3-2-brpc"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:' 1.由来（是什么） \n 架构演变 \n 单一应用框架(ORM) \n 当网站流量很小时，只需一个应用，将所有功能如下单支付等都部署在一起，以减少部署节点和成本。 缺点：单一的系统架构，使得在开发过程中，占用的资源越来越多，而且随着流量的增加越来越难以维护。 \n 垂直应用框架(MVC) \n 垂直应用架构解决了单一应用架构所面临的扩容问题，流量能够分散到各个子系统当中，且系统的体积可控，一定程度上降低了开发人员之间协同以及维护的成本，提升了开发效率。 缺点：但是在垂直架构中相同逻辑代码需要不断的复制，不能复用。 \n 分布式应用架构(RPC) \n 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心。 \n 流动计算架构(SOA) \n 随着服务化的进一步发展，服务越来越多，服务之间的调用和依赖关系也越来越复杂，诞生了面向服务的架构体系(SOA)，也因此衍生出了一系列相应的技术，如对服务提供、服务调用、连接处理、通信协议、序列化方式、服务发现、服务路由、日志输出等行为进行封装的服务框架。 \n 总结 \n 单一应用架构 当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。 此时，用于简化增删改查工作量的 数据访问框架(ORM) 是关键。 垂直应用架构 当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。 此时，用于加速前端页面开发的 Web框架(MVC) 是关键。 分布式服务架构 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。 此时，用于提高业务复用及整合的 分布式服务框架(RPC) 是关键。 流动计算架构 当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。 此时，用于提高机器利用率的 资源调度和治理中心(SOA) 是关键。 \n \n RPC就是从一台机器(客户端)上通过参数传递的方式调用另一台机器(服务器)上的一个函数或方法(可以统称为服务)并得到返回的结果。目的，RPC使得程序能够像访问本地系统资源一样，去访问远端系统资源。 \n 2.入门 \n RPC框架简易实现及其实例分析 \n (1).服务端 \n　　服务端提供客户端所期待的服务， 一般包括三个部分：服务接口，服务实现以及服务的注册暴露三部分 . \n 服务接口 \n public   interface   HelloService   { \n     String   hello ( String  name ) ; \n     String   hi ( String  msg ) ; \n } \n \n 1 2 3 4 服务实现 \n public   class   HelloServiceImpl   implements   HelloService { \n     @Override \n     public   String   hello ( String  name )   { \n         return   "Hello "   +  name ; \n     } \n \n     @Override \n     public   String   hi ( String  msg )   { \n         return   "Hi, "   +  msg ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 服务暴露：只有把服务暴露出来，才能让客户端进行调用，这是RPC框架功能之一。 \n public   class   RpcProvider   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         HelloService  service  =   new   HelloServiceImpl ( ) ; \n         // RPC框架将服务暴露出来，供客户端消费 \n         RpcFramework . export ( service ,   1234 ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 (2).客户端 \n　　客户端消费服务端所提供的服务，一般包括两个部分：服务接口和服务引用两个部分 \n 服务接口：与服务端共享同一个服务接口 \n public   interface   HelloService   { \n   String   hello ( String  name ) ; \n   String   hi ( String  msg ) ; \n } \n \n 1 2 3 4 服务引用：消费端通过RPC框架进行远程调用，这也是RPC框架功能之一 \n  \n public   class   RpcConsumer   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         // 由RpcFramework生成的HelloService的代理 \n         HelloService  service  =   RpcFramework . refer ( HelloService . class ,   "127.0.0.1" ,   1234 ) ; \n         String  hello  =  service . hello ( "World" ) ; \n         System . out . println ( "客户端收到远程调用的结果 ： "   +  hello ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 (3).RPC框架原型实现 \n RPC框架主要包括两大功能：一个用于服务端暴露服务，一个用于客户端引用服务。 \n public   class   RpcFramework   { \n     /**\n     * 暴露服务\n     *\n     * @param service 服务实现\n     * @param port    服务端口\n     * @throws Exception\n     */ \n     public   static   void   export ( final   Object  service ,   int  port )   throws   Exception   { \n         if   ( service  ==   null )   { \n             throw   new   IllegalArgumentException ( "service instance == null" ) ; \n         } \n         if   ( port  <=   0   ||  port  >   65535 )   { \n             throw   new   IllegalArgumentException ( "Invalid port "   +  port ) ; \n         } \n         System . out . println ( "Export service "   +  service . getClass ( ) . getName ( )   +   " on port "   +  port ) ; \n         // 建立Socket服务端 \n         ServerSocket  server  =   new   ServerSocket ( port ) ; \n         for   ( ;   ;   )   { \n             try   { \n                 // 监听Socket请求 \n                 final   Socket  socket  =  server . accept ( ) ; \n                 new   Thread ( new   Runnable ( )   { \n                     @Override \n                     public   void   run ( )   { \n                         try   { \n                             try   { \n                                 /* 获取请求流，Server解析并获取请求*/ \n                                 // 构建对象输入流，从源中读取对象到程序中 \n                                 ObjectInputStream  input  =   new   ObjectInputStream ( \n                                    socket . getInputStream ( ) ) ; \n                                 try   { \n \n                                     System . out . println ( "\\nServer解析请求 ： " ) ; \n                                     String  methodName  =  input . readUTF ( ) ; \n                                     System . out . println ( "methodName : "   +  methodName ) ; \n                                     // 泛型与数组是不兼容的，除了通配符作泛型参数以外 \n                                     Class < ? > [ ]  parameterTypes  =   ( Class < ? > [ ] ) input . readObject ( ) ; \n                                     System . out . println ( \n                                         "parameterTypes : "   +   Arrays . toString ( parameterTypes ) ) ; \n                                     Object [ ]  arguments  =   ( Object [ ] ) input . readObject ( ) ; \n                                     System . out . println ( "arguments : "   +   Arrays . toString ( arguments ) ) ; \n \n \n                                     /* Server 处理请求，进行响应*/ \n                                     ObjectOutputStream  output  =   new   ObjectOutputStream ( \n                                        socket . getOutputStream ( ) ) ; \n                                     try   { \n                                         // service类型为Object的(可以发布任何服务)，故只能通过反射调用处理请求 \n                                         // 反射调用，处理请求 \n                                         Method  method  =  service . getClass ( ) . getMethod ( methodName , \n                                            parameterTypes ) ; \n                                         Object  result  =  method . invoke ( service ,  arguments ) ; \n                                         System . out . println ( "\\nServer 处理并生成响应 ：" ) ; \n                                         System . out . println ( "result : "   +  result ) ; \n                                        output . writeObject ( result ) ; \n                                     }   catch   ( Throwable  t )   { \n                                        output . writeObject ( t ) ; \n                                     }   finally   { \n                                        output . close ( ) ; \n                                     } \n                                 }   finally   { \n                                    input . close ( ) ; \n                                 } \n                             }   finally   { \n                                socket . close ( ) ; \n                             } \n                         }   catch   ( Exception  e )   { \n                            e . printStackTrace ( ) ; \n                         } \n                     } \n                 } ) . start ( ) ; \n             }   catch   ( Exception  e )   { \n                e . printStackTrace ( ) ; \n             } \n         } \n     } \n \n \n     /**\n     * 引用服务\n     *\n     * @param <T>            接口泛型\n     * @param interfaceClass 接口类型\n     * @param host           服务器主机名\n     * @param port           服务器端口\n     * @return 远程服务，返回代理对象\n     * @throws Exception\n     */ \n     @SuppressWarnings ( "unchecked" ) \n     public   static   < T >   T   refer ( final   Class < T >  interfaceClass ,   final   String  host ,   final   int  port ) \n         throws   Exception   { \n         if   ( interfaceClass  ==   null )   { \n             throw   new   IllegalArgumentException ( "Interface class == null" ) ; \n         } \n         // JDK 动态代理的约束，只能实现对接口的代理 \n         if   ( ! interfaceClass . isInterface ( ) )   { \n             throw   new   IllegalArgumentException ( \n                 "The "   +  interfaceClass . getName ( )   +   " must be interface class!" ) ; \n         } \n         if   ( host  ==   null   ||  host . length ( )   ==   0 )   { \n             throw   new   IllegalArgumentException ( "Host == null!" ) ; \n         } \n         if   ( port  <=   0   ||  port  >   65535 )   { \n             throw   new   IllegalArgumentException ( "Invalid port "   +  port ) ; \n         } \n         System . out . println ( \n             "Get remote service "   +  interfaceClass . getName ( )   +   " from server "   +  host  +   ":"   +  port ) ; \n \n         // JDK 动态代理 \n         T  proxy  =   ( T ) Proxy . newProxyInstance ( interfaceClass . getClassLoader ( ) , \n             new   Class < ? > [ ]   { interfaceClass } ,   new   InvocationHandler ( )   { \n                 // invoke方法本意是对目标方法的增强，在这里用于发送RPC请求和接收响应 \n                 @Override \n                 public   Object   invoke ( Object  proxy ,   Method  method ,   Object [ ]  arguments ) \n                     throws   Throwable   { \n                     // 创建Socket客户端，并与服务端建立链接 \n                     Socket  socket  =   new   Socket ( host ,  port ) ; \n                     try   { \n                         /* 客户端像服务端进行请求，并将请求参数写入流中*/ \n                         // 将对象写入到对象输出流，并将其发送到Socket流中去 \n                         ObjectOutputStream  output  =   new   ObjectOutputStream ( \n                            socket . getOutputStream ( ) ) ; \n                         try   { \n                             // 发送请求 \n                             System . out . println ( "\\nClient发送请求 ： " ) ; \n                            output . writeUTF ( method . getName ( ) ) ; \n                             System . out . println ( "methodName : "   +  method . getName ( ) ) ; \n                            output . writeObject ( method . getParameterTypes ( ) ) ; \n                             System . out . println ( "parameterTypes : "   +   Arrays . toString ( method\n                                 . getParameterTypes ( ) ) ) ; \n                            output . writeObject ( arguments ) ; \n                             System . out . println ( "arguments : "   +   Arrays . toString ( arguments ) ) ; \n \n \n                             /* 客户端读取并返回服务端的响应*/ \n                             ObjectInputStream  input  =   new   ObjectInputStream ( \n                                socket . getInputStream ( ) ) ; \n                             try   { \n                                 Object  result  =  input . readObject ( ) ; \n                                 if   ( result  instanceof   Throwable )   { \n                                     throw   ( Throwable ) result ; \n                                 } \n                                 System . out . println ( "\\nClient收到响应 ： " ) ; \n                                 System . out . println ( "result : "   +  result ) ; \n                                 return  result ; \n                             }   finally   { \n                                input . close ( ) ; \n                             } \n                         }   finally   { \n                            output . close ( ) ; \n                         } \n                     }   finally   { \n                        socket . close ( ) ; \n                     } \n                 } \n             } ) ; \n         return  proxy ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 RPC服务端逻辑是：首先创建ServerSocket负责监听特定端口并接收客户连接请求，然后使用Java原生的序列化/反序列化机制来解析得到请求，包括所调用方法的名称、参数列表和实参，最后反射调用服务端对服务接口的具体实现并将得到的结果回传至客户端。至此，一次简单PRC调用的服务端流程执行完毕。 \nRPC客户端逻辑是：首先创建Socket客户端并与服务端建立链接，然后使用Java原生的序列化/反序列化机制将调用请求发送给客户端，包括所调用方法的名称、参数列表将服务端的响应返回给用户即可。至此，一次简单PRC调用的客户端流程执行完毕。特别地，从代码实现来看，实现透明的PRC调用的关键就是 动态代理，这是RPC框架实现的灵魂所在。 \n 特性(深层原理) \n  3.常见的 RPC 框架 \n 目前有很多常见的 RPC 框架，可以分为两类：一类是跟某种特定语言平台绑定的，另一类是与语言无关即跨语言平台的⁵。 \n 跟语言平台绑定的开源 RPC 框架主要有下面几种⁵⁶。 \n Dubbo：国内最早开源的 RPC 框架，由阿里巴巴公司开发并于 2011 年末对外开源，仅支持 Java 语言⁵⁶。它的架构主要包含四个角色：服务消费者、服务提供者、注册中心和监控系统⁵。 \nMotan：微博内部使用的 RPC 框架，于 2016 年对外开源，仅支持 Java 语言⁵⁶。它的架构主要包含几个功能模块：注册、协议、序列化、通信和集群⁵。 \nTars：腾讯内部使用的 RPC 框架，于 2017 年对外开源，仅支持 C++ 语言⁵⁶。它的架构主要包含几个流程：服务发布、管理命令、心跳上报、信息上报和客户端访问⁵。 \nSpring Cloud：国外 Pivotal 公司 2014 年对外开源的 RPC 框架，仅支持 Java 语言⁵⁶。它的架构主要包含多个组件，如 API 网关、注册中心、熔断器、监控系统等⁵。 \n而跨语言平台的开源 RPC 框架主要有以下几种⁵⁶。 \n gRPC：Google 于 2015 年对外开源的跨语言 RPC 框架，支持多种语言⁵⁶。它的原理是通过 IDL 文件定义服务接口和数据类型，然后通过代码生成器生成客户端和服务端的代码⁵。它的特性包括通信协议采用 HTTP/2、IDL 使用 ProtoBuf 和多语言支持⁵。 \nThrift：最初是由 Facebook 开发的内部系统跨语言的 RPC 框架，2007 年贡献给了 Apache 基金，成为 Apache 开源项目之一，支持多种语言⁵⁶。它的原理也是通过 IDL 文件定义服务接口和数据类型，然后通过代码生成器生成客户端和服务端的代码⁵。它的特性包括支持多种序列化格式和传输协议⁵。 \n  3.1 Dubbo \n 官网： Apache Dubbo \n 3.1.1 Dubbo入门案例 \n 服务端 \n 首先，我们先把服务端的接口写好，因为其实dubbo的作用简单来说就是给消费端提供接口。 \n 接口定义 \n /** \n* xml方式服务提供者接口 \n*/  \n public   interface   ProviderService   {  \n     String   SayHello ( String  word ) ;  \n } \n \n 1 2 3 4 5 6 这个接口非常简单，只是包含一个 SayHello 的方法。 \n 接着，定义它的 实现类 \n /** \n* xml方式服务提供者实现类 \n*/ \n     public   class   ProviderServiceImpl   implements   ProviderService {    \n         public   String   SayHello ( String  word )   {    \n             return  word ;     \n         }  \n     } \n \n 1 2 3 4 5 6 7 8 这样我们就把我们的接口写好了，那么我们应该怎么将我们的服务暴露出去呢？ \n 导入 maven 依赖 \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 " \n          xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     < groupId > com.ouyangsihai </ groupId > \n     < artifactId > dubbo-provider </ artifactId > \n     < version > 1.0-SNAPSHOT </ version > \n\n     < dependencies > \n         < dependency > \n             < groupId > junit </ groupId > \n             < artifactId > junit </ artifactId > \n             < version > 3.8.1 </ version > \n             < scope > test </ scope > \n         </ dependency > \n         \x3c!-- https://mvnrepository.com/artifact/com.alibaba/dubbo --\x3e \n         < dependency > \n             < groupId > com.alibaba </ groupId > \n             < artifactId > dubbo </ artifactId > \n             < version > 2.6.6 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.zookeeper </ groupId > \n             < artifactId > zookeeper </ artifactId > \n             < version > 3.4.10 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > com.101tec </ groupId > \n             < artifactId > zkclient </ artifactId > \n             < version > 0.5 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > io.netty </ groupId > \n             < artifactId > netty-all </ artifactId > \n             < version > 4.1.32.Final </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.curator </ groupId > \n             < artifactId > curator-framework </ artifactId > \n             < version > 2.8.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.curator </ groupId > \n             < artifactId > curator-recipes </ artifactId > \n             < version > 2.8.0 </ version > \n         </ dependency > \n\n     </ dependencies > \n </ project > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 这里使用的dubbo的版本是2.6.6，需要注意的是，如果你只导入dubbo的包的时候是会报错的，找不到netty和curator的依赖，所以，在这里我们需要把这两个的依赖加上，就不会报错了。 \n 另外，这里我们使用 zookeeper作为注册中心。 \n 到目前为止，dubbo 需要的环境就已经可以了，下面，我们就把上面刚刚定义的接口暴露出去。 \n **暴露接口（** xml   配置方法） \n首先，我们在我们项目的resource目录下创建 META-INF.spring 包，然后再创建 provider.xml 文件，名字可以任取哦，如下图所示 \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 120318.png) \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xmlns: dubbo = " http://code.alibabatech.com/schema/dubbo " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://code.alibabatech.com/schema/dubbo        http://code.alibabatech.com/schema/dubbo/dubbo.xsd " > \n\n     \x3c!--当前项目在整个分布式架构里面的唯一名称，计算依赖关系的标签--\x3e \n     < dubbo: application   name = " provider "   owner = " sihai " > \n         < dubbo: parameter   key = " qos.enable "   value = " true " /> \n         < dubbo: parameter   key = " qos.accept.foreign.ip "   value = " false " /> \n         < dubbo: parameter   key = " qos.port "   value = " 55555 " /> \n     </ dubbo: application > \n\n     < dubbo: monitor   protocol = " registry " /> \n\n     \x3c!--dubbo这个服务所要暴露的服务地址所对应的注册中心--\x3e \n     \x3c!--<dubbo:registry address="N/A"/>--\x3e \n     < dubbo: registry   address = " N/A "   /> \n\n     \x3c!--当前服务发布所依赖的协议；webservice、Thrift、Hessain、http--\x3e \n     < dubbo: protocol   name = " dubbo "   port = " 20880 " /> \n\n     \x3c!--服务发布的配置，需要暴露的服务接口--\x3e \n     < dubbo: service \n             interface = " com.sihai.dubbo.provider.service.ProviderService " \n             ref = " providerService " /> \n\n     \x3c!--Bean bean定义--\x3e \n     < bean   id = " providerService "   class = " com.sihai.dubbo.provider.service.ProviderServiceImpl " /> \n\n </ beans > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 说明： \n 1、上面的文件其实就是类似 spring的配置文件，而且，dubbo 底层就是 spring。 \n2、节点：dubbo:application \n就是整个项目在分布式架构中的唯一名称，可以在name属性中配置，另外还可以配置owner字段，表示属于谁。 \n下面的参数是可以不配置的，这里配置是因为出现了端口的冲突，所以配置。 \n3、节点：dubbo:monitor \n监控中心配置， 用于配置连接监控中心相关信息，可以不配置，不是必须的参数。 \n4、节点：dubbo:registry \n配置注册中心的信息，比如，这里我们可以配置 zookeeper 作为我们的注册中心。address 是注册中心的地址，这里我们配置的是 N/A 表示由 dubbo自动分配地址。或者说是一种直连的方式，不通过注册中心。 \n5、节点：dubbo:protocol \n服务发布的时候 dubbo 依赖什么协议，可以配置dubbo、webservice、http等协议。 \n6、节点：dubbo:service \n这个节点就是我们的重点了，当我们服务发布的时候，我们就是通过这个配置将我们的服务发布出去的。interface是接口的包路径，ref是第 ⑦ 点配置的接口的bean。 \n7、最后，我们需要像配置spring的接口一样，配置接口的 bean。 \n 到这一步，关于服务端的配置就完成了，下面我们通过 main 方法将接口发布出去。 \n 发布接口 \n package   com . sihai . dubbo . provider ; \n\n import   com . alibaba . dubbo . config . ApplicationConfig ; \n import   com . alibaba . dubbo . config . ProtocolConfig ; \n import   com . alibaba . dubbo . config . RegistryConfig ; \n import   com . alibaba . dubbo . config . ServiceConfig ; \n import   com . alibaba . dubbo . container . Main ; \n import   com . sihai . dubbo . provider . service . ProviderService ; \n import   com . sihai . dubbo . provider . service . ProviderServiceImpl ; \n import   org . springframework . context . support . ClassPathXmlApplicationContext ; \n\n import   java . io . IOException ; \n\n /**\n * xml方式启动\n *\n */ \n public   class   App  \n { \n     public   static   void   main (   String [ ]  args  )   throws   IOException   { \n         //加载xml配置文件启动 \n         ClassPathXmlApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "META-INF/spring/provider.xml" ) ; \n        context . start ( ) ; \n         System . in . read ( ) ;   // 按任意键退出 \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 发布接口非常简单，因为dubbo底层就是依赖 spring 的，所以，我们只需要通过 ClassPathXmlApplicationContext拿到我们刚刚配置好的 xml，然后调用 context.start()方法就启动了。 \n 看到下面的截图，就算是启动成功了，接口也就发布出去了。 \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 120339.png) \n 你以为到这里就结束了了，并不是的，我们拿到 dubbo 暴露出去的 url分析分析。 \n Dubbo 暴露的 URL \n dubbo://192.168.234.1:20880/com.sihai.dubbo.provider.service.ProviderService?anyhost=true&application=provider&bean.name=com.sihai.dubbo.provider.service.ProviderService&bind.ip=192.168.234.1&bind.port=20880&dubbo=2.0.2&generic=false&interface=com.sihai.dubbo.provider.service.ProviderService&methods=SayHello&owner=sihai&pid=8412&qos.accept.foreign.ip=false&qos.enable=true&qos.port=55555&side=provider&timestamp=1562077289380\n \n 1 分析如下： \n 1、首先，在形式上我们发现，其实这么牛逼的 dubbo也是用类似于 http 的协议发布自己的服务的，只是这里我们用的是dubbo协议。 \n2、dubbo://192.168.234.1:20880/com.sihai.dubbo.provider.service.ProviderService \n上面这段链接就是 ?之前的链接，构成： 协议://ip:端口/接口 。发现是不是也没有什么神秘的。 \n3、 \n anyhost=true&application=provider&bean.name=com.sihai.dubbo.provider.service.ProviderService&bind.ip=192.168.234.1&bind.port=20880&dubbo=2.0.2&generic=false&interface=com.sihai.dubbo.provider.service.ProviderService&methods=SayHello&owner=sihai&pid=8412&qos.accept.foreign.ip=false&qos.enable=true&qos.port=55555&side=provider&timestamp=1562077289380\n \n 1 ?之后的字符串，分析后你发现，这些都是刚刚在provider.xml中配置的字段，然后通过& 拼接而成的，闻到了 http 的香味了吗？ \n 终于，dubbo 服务端入门了。下面我们看看拿到了 url后，怎么消费呢？ \n 消费端 \n 上面提到，我们在服务端提供的只是点对点的方式提供服务，并没有使用注册中心，所以，下面的配置也是会有一些不一样的。 \n 消费端环境配置 \n首先，我们在消费端的 resource下建立配置文件 consumer.xml \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 120310.png) \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xmlns: dubbo = " http://code.alibabatech.com/schema/dubbo " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://code.alibabatech.com/schema/dubbo        http://code.alibabatech.com/schema/dubbo/dubbo.xsd " > \n\n     \x3c!--当前项目在整个分布式架构里面的唯一名称，计算依赖关系的标签--\x3e \n     < dubbo: application   name = " consumer "   owner = " sihai " /> \n\n     \x3c!--dubbo这个服务所要暴露的服务地址所对应的注册中心--\x3e \n     \x3c!--点对点的方式--\x3e \n     < dubbo: registry   address = " N/A "   /> \n     \x3c!--<dubbo:registry address="zookeeper://localhost:2181" check="false"/>--\x3e \n\n     \x3c!--生成一个远程服务的调用代理--\x3e \n     \x3c!--点对点方式--\x3e \n     < dubbo: reference   id = " providerService " \n interface = " com.sihai.dubbo.provider.service.ProviderService " \n url = " dubbo://192.168.234.1:20880/com.sihai.dubbo.provider.service.ProviderService " /> \n\n     \x3c!--<dubbo:reference id="providerService"  \n                    interface="com.sihai.dubbo.provider.service.ProviderService"/>--\x3e \n </ beans > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 分析如下所示： \n 1、发现这里的 dubbo:application 和 dubbo:registry 是一致的 \n2、dubbo:reference ：我们这里采用点对点的方式，所以，需要配置在服务端暴露的 url \n maven 依赖 \n和服务端一样 \n 调用服务 \n package   com . sihai . dubbo . consumer ; \n\n import   com . alibaba . dubbo . config . ApplicationConfig ; \n import   com . alibaba . dubbo . config . ReferenceConfig ; \n import   com . alibaba . dubbo . config . RegistryConfig ; \n import   com . sihai . dubbo . provider . service . ProviderService ; \n import   org . springframework . context . support . ClassPathXmlApplicationContext ; \n\n import   java . io . IOException ; \n\n /**\n * xml的方式调用\n *\n */ \n public   class   App  \n { \n     public   static   void   main (   String [ ]  args  )   throws   IOException   { \n\n         ClassPathXmlApplicationContext  context = new   ClassPathXmlApplicationContext ( "consumer.xml" ) ; \n        context . start ( ) ; \n         ProviderService  providerService  =   ( ProviderService )  context . getBean ( "providerService" ) ; \n         String  str  =   providerService . SayHello ( "hello" ) ; \n         System . out . println ( str ) ; \n         System . in . read ( ) ; \n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 这里和服务端的发布如出一辙 \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 120305.png) \n 如此，我们就成功调用接口了。 \n 加入zookeeper作为注册中心 \n 在前面的案例中，我们没有使用任何的注册中心，而是用一种直连的方式进行的。但是，实际上很多时候，我们都是使用dubbo + zookeeper的方式，使用 zookeeper 作为注册中心，这里，我们就介绍一下 zookeeper作为注册中心的使用方法。 \n 需要提前安装zk并启动 \n 这里，我们在前面的入门实例中进行改造。 \n 服务端 \n 在服务端中，我们只需要修改provider.xml 即可。 \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xmlns: dubbo = " http://code.alibabatech.com/schema/dubbo " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://code.alibabatech.com/schema/dubbo        http://code.alibabatech.com/schema/dubbo/dubbo.xsd " > \n\n     \x3c!--当前项目在整个分布式架构里面的唯一名称，计算依赖关系的标签--\x3e \n     < dubbo: application   name = " provider "   owner = " sihai " > \n         < dubbo: parameter   key = " qos.enable "   value = " true " /> \n         < dubbo: parameter   key = " qos.accept.foreign.ip "   value = " false " /> \n         < dubbo: parameter   key = " qos.port "   value = " 55555 " /> \n     </ dubbo: application > \n\n     < dubbo: monitor   protocol = " registry " /> \n\n     \x3c!--dubbo这个服务所要暴露的服务地址所对应的注册中心--\x3e \n     \x3c!--<dubbo:registry address="N/A"/>--\x3e \n     < dubbo: registry   address = " zookeeper://localhost:2181 "   check = " false " /> \n\n     \x3c!--当前服务发布所依赖的协议；webservice、Thrift、Hessain、http--\x3e \n     < dubbo: protocol   name = " dubbo "   port = " 20880 " /> \n\n     \x3c!--服务发布的配置，需要暴露的服务接口--\x3e \n     < dubbo: service \n             interface = " com.sihai.dubbo.provider.service.ProviderService " \n             ref = " providerService " /> \n\n     \x3c!--Bean bean定义--\x3e \n     < bean   id = " providerService "   class = " com.sihai.dubbo.provider.service.ProviderServiceImpl " /> \n\n </ beans > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 重点关注这句话 \n < dubbo: registry   address = " zookeeper://localhost:2181 "   /> \n \n 1 在 address 中，使用我们的 zookeeper 的地址。 \n 如果是zookeeper集群的话，使用下面的方式。 \n < dubbo: registry    protocol = " zookeeper "   address = " 192.168.11.129:2181,192.168.11.137:2181,192.168.11.138:2181 " /> \n \n 1 服务端的配置就好了，其他的跟 入门案例 一样。 \n 消费端 \n 跟服务端一样，在消费端，我们也只需要修改 consumer.xml 即可。 \n 1、注册中心配置跟服务端一样 \n < dubbo: registry   address = " zookeeper://localhost:2181 " /> \n \n 1 2、dubbo:reference \n由于我们这里使用 zookeeper 作为注册中心，所以，跟点对点的方式是不一样的，这里不再需要 dubbo 服务端提供的 url 了，只需要直接引用服务端提供的接口即可 \n < dubbo: reference   id = " providerService "                       interface = " com.sihai.dubbo.provider.service.ProviderService " /> \n \n 1 好了，消费端也配置好了，这样就可以使用修改的入门案例，重新启动运行了。 \n 同样成功了。 \n 这时候的区别在于， 将   dubbo   发布的 url注册到了zookeeper**，消费端从**  zookeeper 消费，zookeeper  相当于一个中介，给消费者提供服务 。 \n 你以为这就完了？不，好戏才刚刚开始呢。 \n  多种配置方式 \n 在入门实例的时候，我们使用的是 xml 配置的方式，对 dubbo 的环境进行了配置，但是，官方还提供了其他的配置方式，这里我们也一一分解。 \n API配置方式 \n 这种方式其实官方是不太推荐的，官方推荐使用xml配置的方式，但是，在有的时候测试的时候，还是可以用的到的，另外，为了保证完整性，这些内容还是有必要讲讲的。 \n 首先还是回到服务端工程。 \n 服务端 \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 120401.png) \n 这里我们使用 api的方式配置，所以，provider.xml这个配置文件就暂时不需要了，我们只需要在上面的 AppApi 这个类中的 main方法中用api配置及启动即可。 \n package   com . sihai . dubbo . provider ; \n\n import   com . alibaba . dubbo . config . ApplicationConfig ; \n import   com . alibaba . dubbo . config . ProtocolConfig ; \n import   com . alibaba . dubbo . config . RegistryConfig ; \n import   com . alibaba . dubbo . config . ServiceConfig ; \n import   com . sihai . dubbo . provider . service . ProviderService ; \n import   com . sihai . dubbo . provider . service . ProviderServiceImpl ; \n\n import   java . io . IOException ; \n\n /**\n * Api方式启动\n * api的方式调用不需要其他的配置，只需要下面的代码即可。\n * 但是需要注意，官方建议：\n * Api方式用于测试用例使用，推荐xml的方式\n */ \n public   class   AppApi \n { \n     public   static   void   main (   String [ ]  args  )   throws   IOException   { \n\n         // 服务实现 \n         ProviderService  providerService  =   new   ProviderServiceImpl ( ) ; \n\n         // 当前应用配置 \n         ApplicationConfig  application  =   new   ApplicationConfig ( ) ; \n        application . setName ( "provider" ) ; \n        application . setOwner ( "sihai" ) ; \n\n         // 连接注册中心配置 \n         RegistryConfig  registry  =   new   RegistryConfig ( ) ; \n        registry . setAddress ( "zookeeper://localhost:2181" ) ; \n //        registry.setUsername("aaa"); \n //        registry.setPassword("bbb"); \n\n         // 服务提供者协议配置 \n         ProtocolConfig  protocol  =   new   ProtocolConfig ( ) ; \n        protocol . setName ( "dubbo" ) ; \n        protocol . setPort ( 20880 ) ; \n         //protocol.setThreads(200); \n\n         // 注意：ServiceConfig为重对象，内部封装了与注册中心的连接， \n         //以及开启服务端口 \n\n         // 服务提供者暴露服务配置 \n         // 此实例很重，封装了与注册中心的连接，请自行缓存， \n         //否则可能造成内存和连接泄漏 \n         ServiceConfig < ProviderService >  service  =   new   ServiceConfig < ProviderService > ( ) ;  \n        service . setApplication ( application ) ; \n         // 多个注册中心可以用setRegistries() \n        service . setRegistry ( registry ) ;  \n         // 多个协议可以用setProtocols() \n        service . setProtocol ( protocol ) ;  \n        service . setInterface ( ProviderService . class ) ; \n        service . setRef ( providerService ) ; \n        service . setVersion ( "1.0.0" ) ; \n\n         // 暴露及注册服务 \n        service . export ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 分析说明如下所示： \n 看到上面的代码是不是云里雾里，不要慌，我们通过对照 xml的方式分析一下。 \n registry 的 xml方式 \n < dubbo: registry   protocol = " zookeeper "   address = " localhost:2181 " /> \n \n 1 API 的方式 \n RegistryConfig  registry  =   new   RegistryConfig ( ) ;  registry . setAddress ( "zookeeper://localhost:2181" ) ; \n \n 1 dubbo:registry节点对应RegistryConfig ，xml的属性对应 API方式用 set方法就可以了。对比之下，你就会发现，如果 API的方式不熟悉，可以对照xml 配置方式就可以。 \n 其他 API \n org . apache . dubbo . config . ServiceConfig   org . apache . dubbo . config . ReferenceConfig   org . apache . dubbo . config . ProtocolConfig   org . apache . dubbo . config . RegistryConfig   org . apache . dubbo . config . MonitorConfig   org . apache . dubbo . config . ApplicationConfig   org . apache . dubbo . config . ModuleConfig   org . apache . dubbo . config . ProviderConfig   org . apache . dubbo . config . ConsumerConfig   org . apache . dubbo . config . MethodConfig   org . apache . dubbo . config . ArgumentConfig \n \n 1 更详细的可以查看官方文档： \n http:// dubbo .apache.org/zh-cn… \n 我们再看看我配置的消费端的Api方式。 \n 消费端 \n同样，我们不需要 consumer.xml 配置文件了，只需要在 main方法中启动即可。 \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 120157.png) \n package   com . sihai . dubbo . consumer ; \n\n import   com . alibaba . dubbo . config . ApplicationConfig ; \n import   com . alibaba . dubbo . config . ReferenceConfig ; \n import   com . alibaba . dubbo . config . RegistryConfig ; \n import   com . sihai . dubbo . provider . service . ProviderService ; \n\n /**\n * api的方式调用\n * api的方式调用不需要其他的配置，只需要下面的代码即可。\n * 但是需要注意，官方建议：\n * Api方式用于测试用例使用，推荐xml的方式\n */ \n public   class   AppApi   { \n\n     public   static   void   main ( String [ ]  args )   { \n         // 当前应用配置 \n         ApplicationConfig  application  =   new   ApplicationConfig ( ) ; \n        application . setName ( "consumer" ) ; \n        application . setOwner ( "sihai" ) ; \n\n         // 连接注册中心配置 \n         RegistryConfig  registry  =   new   RegistryConfig ( ) ; \n        registry . setAddress ( "zookeeper://localhost:2181" ) ; \n\n         // 注意：ReferenceConfig为重对象，内部封装了与注册中心的连接， \n         //以及与服务提供方的连接 \n\n         // 引用远程服务 \n         ReferenceConfig < ProviderService >  reference  =   new   ReferenceConfig < ProviderService > ( ) ;   // 此实例很重，封装了与注册中心的连接以及与提供者的连接，请自行缓存，否则可能造成内存和连接泄漏 \n        reference . setApplication ( application ) ; \n        reference . setRegistry ( registry ) ;   // 多个注册中心可以用setRegistries() \n        reference . setInterface ( ProviderService . class ) ; \n\n         // 和本地bean一样使用xxxService \n         ProviderService  providerService  =  reference . get ( ) ;   // 注意：此代理对象内部封装了所有通讯细节，对象较重，请缓存复用 \n         providerService . SayHello ( "hello dubbo! I am sihai!" ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 这部分的 API配置的方式就到这了，注意：官方推荐 xml 的配置方法 \n 注解配置方式 \n 注解配置方式还是需要了解一下的，现在微服务都倾向于这种方式，这也是以后发展的趋势， 注解配置应该是这几年的趋势。 \n 那么如何对dubbo 使用注解的方式呢？我们先看服务端。 \n 服务端 \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 120207.png) \n 第一步：定义接口及实现类，在上面的截图中的   annotation   包下 \n package   com . sihai . dubbo . provider . service . annotation ; \n\n /**\n * 注解方式接口\n */ \n public   interface   ProviderServiceAnnotation   { \n     String   SayHelloAnnotation ( String  word ) ; \n } \n\n \n 1 2 3 4 5 6 7 8 9 package   com . sihai . dubbo . provider . service . annotation ; \n\n import   com . alibaba . dubbo . config . annotation . Service ; \n\n /**\n * 注解方式实现类\n */ \n @Service ( timeout  =   5000 ) \n public   class   ProviderServiceImplAnnotation   implements   ProviderServiceAnnotation { \n\n     public   String   SayHelloAnnotation ( String  word )   { \n         return  word ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Service \n @Service 用来配置 Dubbo 的服务提供方。 \n 第二步：组装服务提供方 。通过 Spring 中 Java Config 的技术（@Configuration）和 annotation 扫描（@EnableDubbo）来发现、组装、并向外提供Dubbo的服务。 \n package   com . sihai . dubbo . provider . configuration ; \n\n import   com . alibaba . dubbo . config . ApplicationConfig ; \n import   com . alibaba . dubbo . config . ProtocolConfig ; \n import   com . alibaba . dubbo . config . ProviderConfig ; \n import   com . alibaba . dubbo . config . RegistryConfig ; \n import   com . alibaba . dubbo . config . spring . context . annotation . EnableDubbo ; \n import   org . springframework . context . annotation . Bean ; \n import   org . springframework . context . annotation . Configuration ; \n\n /**\n * 注解方式配置\n */ \n @Configuration \n @EnableDubbo ( scanBasePackages  =   "com.sihai.dubbo.provider.service.annotation" ) \n public   class   DubboConfiguration   { \n\n     @Bean   // #1 服务提供者信息配置 \n     public   ProviderConfig   providerConfig ( )   { \n         ProviderConfig  providerConfig  =   new   ProviderConfig ( ) ; \n        providerConfig . setTimeout ( 1000 ) ; \n         return  providerConfig ; \n     } \n\n     @Bean   // #2 分布式应用信息配置 \n     public   ApplicationConfig   applicationConfig ( )   { \n         ApplicationConfig  applicationConfig  =   new   ApplicationConfig ( ) ; \n        applicationConfig . setName ( "dubbo-annotation-provider" ) ; \n         return  applicationConfig ; \n     } \n\n     @Bean   // #3 注册中心信息配置 \n     public   RegistryConfig   registryConfig ( )   { \n         RegistryConfig  registryConfig  =   new   RegistryConfig ( ) ; \n        registryConfig . setProtocol ( "zookeeper" ) ; \n        registryConfig . setAddress ( "localhost" ) ; \n        registryConfig . setPort ( 2181 ) ; \n         return  registryConfig ; \n     } \n\n     @Bean   // #4 使用协议配置，这里使用 dubbo \n     public   ProtocolConfig   protocolConfig ( )   { \n         ProtocolConfig  protocolConfig  =   new   ProtocolConfig ( ) ; \n        protocolConfig . setName ( "dubbo" ) ; \n        protocolConfig . setPort ( 20880 ) ; \n         return  protocolConfig ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 分析说明如下： \n1、通过 @EnableDubbo指定在com.sihai.dubbo.provider.service.annotation下扫描所有标注有 @Service 的类 \n 2、通过 @Configuration将 DubboConfiguration中所有的 @Bean通过Java Config的方式组装出来并注入给 Dubbo服务，也就是标注有@Service 的类。 \n 这其中就包括了： \n ProviderConfig：服务提供方配置 \n\nApplicationConfig：应用配置\n\n RegistryConfig：注册中心配置 \n\nProtocolConfig：协议配置\n \n 1 2 3 4 5 6 7 第三步：启动服务 \n package   com . sihai . dubbo . provider ; \n\n import   com . alibaba . dubbo . config . spring . context . annotation . DubboComponentScan ; \n import   com . sihai . dubbo . provider . configuration . DubboConfiguration ; \n import   org . springframework . context . annotation . AnnotationConfigApplicationContext ; \n import   sun . applet . Main ; \n\n import   java . io . IOException ; \n\n /**\n * 注解启动方式\n */ \n public   class   AppAnnotation   { \n\n     public   static   void   main ( String [ ]  args )   throws   IOException   { \n         AnnotationConfigApplicationContext  context  =   new   AnnotationConfigApplicationContext ( DubboConfiguration . class ) ;  \n        context . start ( ) ; \n         System . in . read ( ) ;  \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 发现输出下面信息就表示 success了 \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 120958.png) \n 消费端 \n 同样我们下看看消费端的工程，有一个感性认识。 \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 121020.png) \n 第一步：引用服务 \n package   com . sihai . dubbo . consumer . Annotation ; \n\n import   com . alibaba . dubbo . config . annotation . Reference ; \n import   com . sihai . dubbo . provider . service . annotation . ProviderServiceAnnotation ; \n import   org . springframework . stereotype . Component ; \n\n /**\n * 注解方式的service\n */ \n @Component ( "annotatedConsumer" ) \n public   class   ConsumerAnnotationService   { \n\n     @Reference \n     private   ProviderServiceAnnotation  providerServiceAnnotation ; \n\n     public   String   doSayHello ( String  name )   { \n         return   providerServiceAnnotation . SayHelloAnnotation ( name ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 在 ConsumerAnnotationService 类中，通过 @Reference 引用服务端提供的类，然后通过方法调用这个类的方式，给消费端提供接口。 \n 注意：如果这里找不到 ProviderServiceAnnotation 类，请在服务端先把服务端工程用 Maven intall一下，然后将服务端的依赖放到消费端的pom 中。如下： \n < dependency > \n           < groupId > com.ouyangsihai </ groupId > \n             < artifactId > dubbo-provider </ artifactId > \n             < version > 1.0-SNAPSHOT </ version > \n </ dependency > \n\n \n 1 2 3 4 5 6 解释一下：引入的jar包里面只是未被实现的接口，rpc需要在客户端服务端定义一套统一的接口，然后在服务端实现接口，实际上还是网络通信，只不过长得像本地实现 \n 第二步：组装服务消费者 \n 这一步和服务端是类似的，这里就不在重复了。 \n package   com . sihai . dubbo . consumer . configuration ; \n\n import   com . alibaba . dubbo . config . ApplicationConfig ; \n import   com . alibaba . dubbo . config . ConsumerConfig ; \n import   com . alibaba . dubbo . config . RegistryConfig ; \n import   com . alibaba . dubbo . config . spring . context . annotation . EnableDubbo ; \n import   org . springframework . context . annotation . Bean ; \n import   org . springframework . context . annotation . ComponentScan ; \n import   org . springframework . context . annotation . Configuration ; \n\n import   java . util . HashMap ; \n import   java . util . Map ; \n\n /**\n * 注解配置类\n */ \n @Configuration \n @EnableDubbo ( scanBasePackages  =   "com.sihai.dubbo.consumer.Annotation" ) \n @ComponentScan ( value  =   { "com.sihai.dubbo.consumer.Annotation" } ) \n public   class   ConsumerConfiguration   { \n     @Bean   // 应用配置 \n     public   ApplicationConfig   applicationConfig ( )   { \n         ApplicationConfig  applicationConfig  =   new   ApplicationConfig ( ) ; \n        applicationConfig . setName ( "dubbo-annotation-consumer" ) ; \n         Map < String ,   String >  stringStringMap  =   new   HashMap < String ,   String > ( ) ; \n        stringStringMap . put ( "qos.enable" , "true" ) ; \n        stringStringMap . put ( "qos.accept.foreign.ip" , "false" ) ; \n        stringStringMap . put ( "qos.port" , "33333" ) ; \n        applicationConfig . setParameters ( stringStringMap ) ; \n         return  applicationConfig ; \n     } \n\n     @Bean   // 服务消费者配置 \n     public   ConsumerConfig   consumerConfig ( )   { \n         ConsumerConfig  consumerConfig  =   new   ConsumerConfig ( ) ; \n        consumerConfig . setTimeout ( 3000 ) ; \n         return  consumerConfig ; \n     } \n\n     @Bean   // 配置注册中心 \n     public   RegistryConfig   registryConfig ( )   { \n         RegistryConfig  registryConfig  =   new   RegistryConfig ( ) ; \n        registryConfig . setProtocol ( "zookeeper" ) ; \n        registryConfig . setAddress ( "localhost" ) ; \n        registryConfig . setPort ( 2181 ) ; \n         return  registryConfig ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 第三步：发起远程调用 \n 在main方法中通过启动一个Spring Context，从其中查找到组装好的Dubbo的服务消费者，并发起一次 远程调用 。 \n package   com . sihai . dubbo . consumer ; \n\n import   com . sihai . dubbo . consumer . Annotation . ConsumerAnnotationService ; \n import   com . sihai . dubbo . consumer . configuration . ConsumerConfiguration ; \n import   com . sihai . dubbo . provider . service . ProviderService ; \n import   org . springframework . context . annotation . AnnotationConfigApplicationContext ; \n import   org . springframework . context . support . ClassPathXmlApplicationContext ; \n\n import   java . io . IOException ; \n\n /**\n * 注解方式启动\n *\n */ \n public   class   AppAnnotation \n { \n     public   static   void   main (   String [ ]  args  )   throws   IOException   { \n\n         AnnotationConfigApplicationContext  context  =   new   AnnotationConfigApplicationContext ( ConsumerConfiguration . class ) ;  \n        context . start ( ) ;   // 启动 \n         ConsumerAnnotationService  consumerAnnotationService  =  context . getBean ( ConsumerAnnotationService . class ) ;  \n         String  hello  =  consumerAnnotationService . doSayHello ( "annotation" ) ;   // 调用方法 \n         System . out . println ( "result: "   +  hello ) ;   // 输出结果 \n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 结果如下所示： \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 121238.png) \n 3.1.2 特性 \n  启动时检查 \n Dubbo 缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止 Spring初始化完成，以便上线时，能及早发现问题，默认check="true"。 \n 但是，有的时候，我们并不是都需要启动时就检查的，比如测试的时候，我们是需要更快速的启动，所以，这种场景的时候，我们是需要关闭这个功能的。 \n 下面，我们看看如何使用这个功能。 \n 在服务端注册的时候（客户端注册时同样适用）； \n < dubbo: registry   protocol = " zookeeper "   address = " localhost:2181,localhost:2182,localhost:2183 "   check = " false " /> \n \n 1 在客户端引用服务端服务的时候； \n < dubbo: reference   check = " false "   id = " providerService "                       interface = " com.sihai.dubbo.provider.service.ProviderService " /> \n \n 1 就是这么简单，就是这么强！ \n 集群容错 \n dubbo 也是支持集群容错的，同时也有很多可选的方案，其中，默认的方案是 failover，也就是重试机制。 \n 首先，我们先把所有的容错机制都整理一遍，然后再看看使用。 \n \n \n \n 集群模式 \n 说明 \n 使用方法 \n \n \n \n \n Failover Cluster \n 失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过retries="2" 来设置重试次数(不含第一次)。 \n cluster="xxx"xxx：集群模式名称 ，例如cluster="failover" \n \n \n Failfast Cluster \n 快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。 \n \n \n \n Failsafe Cluster \n 失败安全，出现异常时，直接忽略。 \n \n \n \n Failback Cluster \n 失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 \n \n \n \n Forking Cluster \n 并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过forks="2" 来设置最大并行数。 \n \n \n \n Broadcast Cluster \n 广播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息。 \n \n \n \n \n 使用实例 \n在发布服务或者引用服务的时候设置 \n \x3c!--服务发布的配置，需要暴露的服务接口--\x3e \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n             interface = " com.sihai.dubbo.provider.service.ProviderService " \n             ref = " providerService " /> \n\n \n 1 2 3 4 5 \n < dubbo: reference   cluster = " failover "   retries = " 2 "   check = " false "   id = " providerService " \n                      interface = " com.sihai.dubbo.provider.service.ProviderService " /> \n\n \n 1 2 3 #   负载均衡 \n 负载均衡想必是一个再熟悉不过的概念了，所以，dubbo 支持也是再正常不过了，这里也总结一下dubbo支持的负载均衡的一些方案及使用方法。 \n \n \n \n 负载均衡模式 \n 说明 \n 使用方法 \n \n \n \n \n Random LoadBalance \n 随机 按权重设置随机概率 \n <dubbo:service loadbalance="xxx"/>xxx：负载均衡方法 \n \n \n RoundRobin LoadBalance \n 轮询 按公约后的权重设置轮询比率。 \n \n \n \n LeastActive LoadBalance \n 最少活跃调用数 相同活跃数的随机，活跃数指调用前后计数差。 \n \n \n \n ConsistentHash LoadBalance \n 一致性 Hash相同参数的请求总是发到同一提供者。 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 \n  直连提供者 \n 在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，所以，这种情况下，我们只需要直接连接服务端的地即可，其实，这种方法在前面的讲解已经使用到了，第一种讲解的方式就是这种方式，因为这种方式简单。 \n 使用方法如下所示： \n < dubbo: reference   id = " providerService "                       interface = " com.sihai.dubbo.provider.service.ProviderService "                       url = " dubbo://192.168.234.1:20880/com.sihai.dubbo.provider.service.ProviderService " /> \n \n 1 说明：可以看到，只要在消费端在·dubbo:reference 节点使用url给出服务端的方法即可。 \n 只订阅 \n 只订阅就是只能够订阅服务端的服务，而不能够注册。 \n 引用官方的使用场景如下： \n 为方便开发测试，经常会在线下共用一个所有服务可用的注册中心，这时，如果一个正在开发中的服务提供者注册，可能会影响消费者不能正常运行。 \n可以让服务提供者开发方，只订阅服务(开发的服务可能依赖其它服务)，而不注册正在开发的服务，通过直连测试正在开发的服务。 \n < dubbo: registry   register = " false "   protocol = " zookeeper "   address = " localhost:2181,localhost:2182,localhost:2183 "   check = " false " /> \n \n 1 1、使用只订阅方式 \n当在服务提供端使用 register="false" 的时候，我们使用下面的方式获取服务端的服务； \n < dubbo: reference   cluster = " failover "   retries = " 2 "   check = " false "   id = " providerService "                       interface = " com.sihai.dubbo.provider.service.ProviderService " /> \n \n 1 启动信息 \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 121910.png) \n 发现，这时候并不是向注册中心 zookeeper 注册，而只是做了发布服务和启动netty。 \n 2、不使用只订阅方式 \n < dubbo: registry   protocol = " zookeeper "   address = " localhost:2181,localhost:2182,localhost:2183 "   check = " false " /> \n \n 1 启动信息 \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 121932.png) \n 可以发现，这里就向注册中心 zookeeper 注册了。 \n 只注册 \n 只注册正好跟前面的只订阅相反，这个时候可以向注册中心注册，但是，消费端却不能够读到服务。 \n 应用场景 \n 如果有两个镜像环境，两个注册中心，有一个服务只在其中一个注册中心有部署，另一个注册中心还没来得及部署，而两个注册中心的其它应用都需要依赖此服务。这个时候，可以让服务提供者方只注册服务到另一注册中心，而不从另一注册中心订阅服务。 \n 使用说明 \n < dubbo: registry   subscribe = " false "   address = " localhost:2181 " > </ dubbo: registry > \n \n 1 在服务端的 dubbo:registry 节点下使用 subscribe="false" 来声明这个服务是只注册的服务。 \n 这个时候消费端调用的时候是不能调用的。 \n ![在这里插入图片描述](RPC.assets/Tue, 30 May 2023 121956.png) \n  多协议机制 \n 在前面我们使用的协议都是 dubbo 协议，但是 dubbo除了支持这种协议外还支持其他的协议，比如，rmi、hessian等，另外，而且还可以用多种协议同时暴露一种服务。 \n 使用方法 \n 1、一种接口使用一种协议 \n 先声明多种协议 \n   \x3c!--当前服务发布所依赖的协议；webserovice、Thrift、Hessain、http--\x3e \n     < dubbo: protocol   name = " dubbo "   port = " 20880 " /> \n     < dubbo: protocol   name = " rmi "   port = " 1099 "   /> \n\n \n 1 2 3 4 然后在发布接口的时候使用具体协议 \n \x3c!--服务发布的配置，需要暴露的服务接口--\x3e \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n             interface = " com.sihai.dubbo.provider.service.ProviderService " \n             ref = " providerService " /> \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n                    interface = " com.sihai.dubbo.provider.service.ProviderService " \n                    ref = " providerService "   protocol = " rmi " /> \n\n \n 1 2 3 4 5 6 7 8 在输出日志中，就可以找到rmi发布的接口。 \n rmi://192.168.234.1:1099/com.sihai.dubbo.provider.service.ProviderService?anyhost=true&application=provider&bean.name=com.sihai.dubbo.provider.service.ProviderService&cluster=failover&dubbo=2.0.2&generic=false&interface=com.sihai.dubbo.provider.service.ProviderService&methods=SayHello&owner=sihai&pid=796&retries=2&side=provider&timestamp=1564281053185, dubbo version: 2.6.6, current host: 192.168.234.1\n \n 1 2、一种接口使用多种协议 \n声明协议和上面的方式一样，在发布接口的时候有一点不一样。 \n < dubbo: service   cluster = " failover "   retries = " 2 "                     interface = " com.sihai.dubbo.provider.service.ProviderService "                     ref = " providerService "   protocol = " rmi,dubbo " /> \n \n 1 说明：protocol属性，可以用,隔开，使用多种协议。 \n 多注册中心 \n Dubbo支持同一服务向多注册中心同时注册，或者不同服务分别注册到不同的注册中心上去，甚至可以同时引用注册在不同注册中心上的同名服务。 \n 服务端多注册中心发布服务 \n 一个服务可以在不同的注册中心注册，当一个注册中心出现问题时，可以用其他的注册中心。 \n 注册 \n \x3c!--多注册中心--\x3e \n     < dubbo: registry   protocol = " zookeeper "   id = " reg1 "   timeout = " 10000 "   address = " localhost:2181 " /> \n     < dubbo: registry   protocol = " zookeeper "   id = " reg2 "   timeout = " 10000 "   address = " localhost:2182 " /> \n     < dubbo: registry   protocol = " zookeeper "   id = " reg3 "   timeout = " 10000 "   address = " localhost:2183 " /> \n\n \n 1 2 3 4 5 发布服务 \n \x3c!--服务发布的配置，需要暴露的服务接口--\x3e \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n             interface = " com.sihai.dubbo.provider.service.ProviderService " \n             ref = " providerService "   registry = " reg1 " /> \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n                    interface = " com.sihai.dubbo.provider.service.ProviderService " \n                    ref = " providerService "   protocol = " rmi "   registry = " reg2 " /> \n\n \n 1 2 3 4 5 6 7 8 说明：使用registry="reg2"指定该接口使用的注册中心，同时也可以使用多个，用，隔开，例如，registry="reg1,,reg2"。 \n 消费端多注册中心引用服务 \n 首先，先向不同注册中心注册; \n \x3c!--多注册中心--\x3e \n     < dubbo: registry   protocol = " zookeeper "   id = " reg1 "   timeout = " 10000 "   address = " localhost:2181 " /> \n     < dubbo: registry   protocol = " zookeeper "   id = " reg2 "   timeout = " 10000 "   address = " localhost:2182 " /> \n     < dubbo: registry   protocol = " zookeeper "   id = " reg3 "   timeout = " 10000 "   address = " localhost:2183 " /> \n\n \n 1 2 3 4 5 其次，不同的消费端服务引用使用不同的注册中心； \n \x3c!--不同的服务使用不同的注册中心--\x3e \n     < dubbo: reference   cluster = " failover "   retries = " 2 "   check = " false "   id = " providerService " \n                      interface = " com.sihai.dubbo.provider.service.ProviderService "   registry = " reg1 " /> \n     < dubbo: reference   cluster = " failover "   retries = " 2 "   check = " false "   id = " providerService2 " \n                      interface = " com.sihai.dubbo.provider.service.ProviderService "   registry = " reg2 " /> \n\n \n 1 2 3 4 5 6 说明：上面分别使用注册中心1和注册中心2。 \n 多版本 \n 不同的服务是有版本不同的，版本可以更新并且升级，同时，不同的版本之间是不可以调用的。 \n \x3c!--服务发布的配置，需要暴露的服务接口--\x3e \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n             interface = " com.sihai.dubbo.provider.service.ProviderService " \n             ref = " providerService "   registry = " reg1 "   version = " 1.0.0 " /> \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n                    interface = " com.sihai.dubbo.provider.service.ProviderService " \n                    ref = " providerService "   protocol = " rmi "   registry = " reg2 "   version = " 1.0.0 " /> \n\n \n 1 2 3 4 5 6 7 8 加入了版本控制。 \n 日志管理 \n dubbo也可以将日志信息记录或者保存到文件中的。 \n 1、使用accesslog输出到log4j \n < dubbo: protocol   accesslog = " true "   name = " dubbo "   port = " 20880 " /> \n < dubbo: protocol   accesslog = " true "   name = " rmi "   port = " 1099 "   /> \n\n \n 1 2 3 2、输出到文件 \n < dubbo: protocol   accesslog = " http://localhost/log.txt "   name = " dubbo "   port = " 20880 " /> \n < dubbo: protocol   accesslog = " http://localhost/log2.txt "   name = " rmi "   port = " 1099 "   /> \n \n 1 2 #  Dubbo架构 \n Dubbo架构图 ，如下所示： \n \n Dubbo整个架构包含：接口层、配置层、代理层、注册层、集群层、监控层、调用层、交换层、传输层、序列化层等10大层设计。 \n 1. 服务接口层 \n 服务接口层：根据服务提供方和服务消费方的业务设计对应的接口和实现。 \n 2. 配置层 \n 配置层：主要就是对外配置接口，可以通过Spring解析配置生成配置类。 \n 3.服务代理层 \n 服务代理层：主要就是生成服务的客户端代理。 \n 4.服务注册层 \n 服务注册层：主要就是封装服务地址的注册与发现。 \n 5.集群层 \n 集群层：就是Dubbo集群相关的封装，比如：多个提供者的路由、及服务负载均衡等。 \n 6.监控层 \n 监控层：主要就是RPC调用次数和调用时间监控。 \n 7.远程调用层 \n 远程调用层：主要就是封将RPC调用，比如：完成网络A服务器到B服务器的整个远程通信调用。 \n 8. 信息交换层 \n 信息交换层：主要就是封装请求响应模式，比如：同步转异步等。 \n 9.网络传输层 \n 网络传输层：主要就是解决网络传输的各种通信问题。 \n 10.数据序列化层 \n 数据序列化层：我们都知道数据在网络传输，不可能是对象进行传输，都是需要序列化后才能在网络两端传输，这就是会涉及到序列化。 \n Dubbo原理 \n Dubbo调用流程 ，大致分为如下11步： \n \n 1.首先服务提供者会启动服务，然后将服务注册到服务注册中心。 \n 2.服务消费者，会定时拉取服务提供者列表。 \n 3.生成一个动态代理对象，然后通过这个代理对象，去调用远程接口。 \n 4.生成代理对象之后，会走到Cluster层，这里会获取服务提供者列表的数据。 \n 5.然后，Cluster会根据指定的算法做负载均衡，选出要调用的服务提供者。 \n 6.选择好服务提供者之后，再选择指定的协议格式。 \n 7.Exchange会根据指定的协议格式进行请求数据封装，封装成request请求。 \n 9.服务提供者那边，同样会有网络通信框架，会监听指定的端口号，当接收到请求之后会将请求进行反序列化。 \n 10.反序列化之后，再根据Exchange根据指定协议格式将请求解析出来。 \n 11.最后再通过动态代理对象，调用服务提供者的对应接口。 \n dubbo整合redis \n (180条消息) springBoot+dubbo整合Redis - 脚手架系列（三）_六木老师的博客-CSDN博客 \n https://github.com/YClimb/redis-demo \n https://github.com/fhhly/springboot-dubbo.git \n 3.2 BRPC \n'},{frontmatter:{},regularPath:"/%E4%BA%91%E5%8E%9F%E7%94%9F/k8s.html",relativePath:"云原生/k8s.md",key:"v-2c8e9208",path:"/1970/01/01/k8s/",lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:""},{title:"geohash算法",frontmatter:{title:"geohash算法",date:"2019-09-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["算法"],tags:["索引","空间索引","附近的人"]},regularPath:"/%E5%85%B6%E4%BB%96/geohash%E7%AE%97%E6%B3%95.html",relativePath:"其他/geohash算法.md",key:"v-01787dc2",path:"/2019/09/08/geohash%E7%AE%97%E6%B3%95/",headers:[{level:3,title:"前言",slug:"前言"},{level:3,title:"Geohash算法介绍",slug:"geohash算法介绍"},{level:3,title:"第一步：将经纬度转换为二进制",slug:"第一步-将经纬度转换为二进制"},{level:3,title:"第二步：将经纬度的二进制编码合并",slug:"第二步-将经纬度的二进制编码合并"},{level:3,title:"第三步：将合并后的二进制数做Base32编码",slug:"第三步-将合并后的二进制数做base32编码"},{level:3,title:"网格与实际地理的精度",slug:"网格与实际地理的精度"},{level:3,title:"使用geohash算法推算附近位置的弊端",slug:"使用geohash算法推算附近位置的弊端"},{level:3,title:"邻近(8个)网格位置推算",slug:"邻近-8个-网格位置推算"},{level:3,title:"正向推导",slug:"正向推导"},{level:3,title:"反向推导",slug:"反向推导"},{level:3,title:"geohash进阶原理（Z阶曲线)",slug:"geohash进阶原理-z阶曲线"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:" 前言 \n 用户附近位置如何计算？ \n 常见的很多app功能都有附近位置的计算，那是如何实现的？是通过两点经纬度距离计算？想快速了解附近100米的超市？遍历每个超市拿经纬度计算距离显然不合适。 \n 经纬度与物理距离介绍 \n经纬度是经度与纬度的合称组成一个坐标系统，称为地理坐标系统，它是一种利用三度空间的球面来定义地球上的空间的球面坐标系统，能够标示地球上的任何一个位置。 \n \n 在一定误差范围内，通常情况下，经纬线和米的换算为：经度或者纬度0.00001度，约等于1米。以下表格列出更细致的换算关系： \n \n \n \n 在纬度相等的情况下 \n 在经度相等的情况下 \n \n \n \n \n 经度每隔0.00001度，距离相差约1米；每隔0.0001度，距离相差约10米；每隔0.001度，距离相差约100米；每隔0.01度，距离相差约1000米；每隔0.1度，距离相差约10000米。 \n 纬度每隔0.00001度，距离相差约1.1米；每隔0.0001度，距离相差约11米；每隔0.001度，距离相差约111米；每隔0.01度，距离相差约1113米；每隔0.1度，距离相差约11132米。 \n Geohash算法介绍 \n GeoHash是 空间索引 的一种方式，其基本原理是将地球理解为一个二维平面，通过把二维的空间经纬度数据编码为一个字符串，可以 把平面递归分解成更小的子块，每个子块在一定经纬度范围内拥有相同的编码。 \n 以GeoHash方式建立空间索引，可以提高对空间poi数据进行经纬度检索的效率。 \n 编码规则为：先将纬度范围(-90, 90)平分成两个区间(-90, 0)和(0, 90)，如果目标维度位于前一个区间，则编码为0，否则编码为1，然后根据目标纬度所落的区间再平均分成两个区间进行编码，以此类推，直到精度满足要求，经度也用同样的算法，对(-180, 180)依次细分，然后合并经度和纬度的编码， 奇数位放纬度，偶数位放经度 ，组成一串新的二进制编码，按照 Base32 进行编码。 \n 示例 \n 以当前所在办公区【两江国际】的位置坐标为例,  经纬度为（104.059684，30.559545） \n 第一步：将经纬度转换为二进制 \n \n \n \n 序号 \n 纬度范围 \n 划分区间0 \n 划分区间1 \n 30.559545所属区间 \n \n \n \n \n 1 \n (-90, 90) \n (-90, 0.0) \n (0.0, 90) \n 1 \n \n \n 2 \n (0.0, 90) \n (0.0, 45.0) \n (45.0, 90) \n 0 \n \n \n 3 \n (0.0, 45.0) \n (0.0, 22.5) \n (22.5, 45.0) \n 1 \n \n \n 4 \n (22.5, 45.0) \n (22.5, 33.75) \n (33.75, 45.0) \n 0 \n \n \n 5 \n (22.5, 33.75) \n (22.5, 28.125) \n (28.125, 33.75) \n 1 \n \n \n 6 \n (28.125, 33.75) \n (28.125, 30.9375) \n (30.9375, 33.75) \n 0 \n \n \n 7 \n (28.125, 30.9375) \n (28.125, 29.53125) \n (29.53125, 30.9375) \n 1 \n \n \n 8 \n (29.53125, 30.9375) \n (29.53125, 30.234375) \n (30.234375, 30.9375) \n 1 \n \n \n 9 \n (30.234375, 30.9375) \n (30.234375, 30.5859375) \n (30.5859375, 30.9375) \n 0 \n \n \n 10 \n (30.234375, 30.5859375) \n (30.234375, 30.41015625) \n (30.41015625, 30.5859375) \n 1 \n \n \n 11 \n (30.41015625, 30.5859375) \n (30.41015625, 30.498046875) \n (30.498046875, 30.5859375) \n 1 \n \n \n 12 \n (30.498046875, 30.5859375) \n (30.498046875, 30.541992188) \n (30.541992188, 30.5859375) \n 1 \n \n \n 13 \n (30.541992188, 30.5859375) \n (30.541992188, 30.563964844) \n (30.563964844, 30.5859375) \n 0 \n \n \n 14 \n (30.541992188, 30.563964844) \n (30.541992188, 30.552978516) \n (30.552978516, 30.563964844) \n 1 \n \n \n 15 \n (30.552978516, 30.563964844) \n (30.552978516, 30.55847168) \n (30.55847168, 30.563964844) \n 1 \n \n \n \n 最后得到维度的二进制编码为：101010110111011, 用同样的方式可以得到精度(104.059684)的二进制编码：110010011111111。 \n  第二步：将经纬度的二进制编码合并 \n 从偶数0开始，经度占偶数位，纬度占奇数位。 \n \n \n \n 序号 \n 0 \n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n 10 \n 11 \n 12 \n 13 \n 14 \n 15 \n 16 \n 17 \n 18 \n 19 \n 20 \n 21 \n 22 \n 23 \n 24 \n 25 \n 26 \n 27 \n 28 \n 29 \n \n \n \n \n 编码 \n 1 \n 1 \n 1 \n 0 \n 0 \n 1 \n 0 \n 0 \n 1 \n 1 \n 0 \n 0 \n 0 \n 1 \n 1 \n 1 \n 1 \n 0 \n 1 \n 1 \n 1 \n 1 \n 1 \n 1 \n 1 \n 0 \n 1 \n 1 \n 1 \n 1 \n 第三步：将合并后的二进制数做Base32编码 \n 按照每5位一组，分成6组，每组计算其对应的十进制数值，按照Base32进行编码。 \n Base32编码表的其中一种如下，是用0-9、b-z（去掉a, i, l, o）这32个字母进行编码. \n \n 11100   10011   00011   11011   11111   01111 \n 28 ( w )   19 ( m )   3 ( 3 )    27 ( v )   31 ( z )   15 ( g ) \n \n 1 2 最终得到的经纬度编码为：wm3vzg \n 如上文二进制编码的计算过程，如果 递归的次数越大，则生成的二进制编码越长 ，因此生成的geohash编码越长， 位置越精确 。目前 Geohash 使用的精度说明如下： \n \n GeoHash用一个字符串表示经度和纬度两个坐标, 比直接用经纬度的高效很多，而且使用者可以发布地址编码，既能表明自己位于某位置附近，又不至于暴露自己的精确坐标，有助于隐私保护。 \n 编码过程中，通过二分范围匹配的方式来决定某个经纬坐标是编码为1还是0，因此某些邻近坐标的编码是相同的，因此GeoHash表示的并不是一个点，而是一个矩形区域。 GeoHash编码的前缀可以表示更大的区域。例如wm3vzg，它的前缀wm3vz表示包含编码wm3vzg在内的更大范围。 这个特性可以用于附近地点搜索。 \n 网格与实际地理的精度 \n 如果把某个区域或整个地图上的地理位置都按照Geohash编码，则会得到一个网格，编码递归粒度越细，网格的矩形区域越小，geohash编码的长度越大，则Geohash编码越精确。 不同的编码长度，生成的网格与实际地理的精度如下(Geohash字符串编码长度对应网格大小）: \n \n \n \n 字符串长度 \n 网格宽度 \n 网格高度 \n \n \n \n \n 1 \n 5000Km \n 5000Km \n \n \n 2 \n 1250Km \n 625Km \n \n \n 3 \n 156Km \n 156Km \n \n \n 4 \n 39.1Km \n 19.5Km \n \n \n 5 \n 4.89Km \n 4.89Km \n \n \n 6 \n 1.22Km \n 0.61Km \n \n \n 7 \n 153m \n 153m \n \n \n 8 \n 38.2m \n 19.1m \n \n \n 9 \n 4.77m \n 4.77m \n \n \n 10 \n 1.19m \n 0.596m \n \n \n \n Geohash编码与网格 \n 当前选取的编码长度为6，因此一个网格实际的地理差异在1.2公里与0.6公里，示例中两江国际对应的网格大致效果如图： \n 使用geohash算法推算附近位置的弊端 \n  邻近(8个)网格位置推算 \n 结论 \n根据Geohash的编码规则将经纬度分解到二进制，结合地理常识，**中心网格在南北（上下）方向上体现为纬度的变化，往北则维度的二进制加1，往南则维度的二进制减1，在东西（左右）方向上体现为经度的变化，往东则经度的二进制加1，往西则减1，**可以计算出上下左右四个网格经纬度的二进制编码，再将加减得出的经纬度两两组合，计算出左上、左下、右上和右下四个网格的经纬度二进制编码，从而就可以根据Geohash的编码规则计算出周围八个网格的字符串。 \n 正向推导 \n 以Geohash编码长度为6为基础，网格的宽高与实际距离换算为：1.2Km*0.6Km. \n 参考上文提到的，在经度相同情况下，每隔0.001度，距离相差约111米。0.6Km换算为纬度为：0.005405405。 \n 当前两江国际粗粒度的wgs84坐标( 104.05503，30.562251),  纬度二进制编码：101010110111011，经度二进制编码：110010011111111, Geohash值为：wm3vzg \n 正北方向近邻的网格维度为增加一个网格的高度，即纬度增加0.005405405，为： 30.562251 +  0.005405405 = 30.567656405， 转换为二进制编码后为(可用工具快速转换)： 101010110111100 \n 正好是原纬度的二进制编码101010110111011 加1后的结果（101010110111011 + 000000000000001  =  101010110111100） \n 反向推导 \n 当前两江国际粗粒度的wgs84坐标( 104.05503，30.562251),  纬度二进制编码：101010110111011，经度二进制编码：110010011111111, Geohash值为：wm3vzg \n 基于当前坐标的网格，正北方向近邻的网格N，其纬度二进制加1后为：101010110111100，经度不变，其Geohash值为： wm3vzu \n 通过http://geohash.co/ 反向转换其经纬坐标为：(104.0570068359375,30.56671142578125） \n 通过https://www.box3.cn/tools/lbs.html 查询2个坐标的实际位置，误差在531m(符合精度范围） \n \n 邻近8个网格位置计算 \n \n \n \n ** *Geohash编码：* wm3vzs纬度二进制编码：101010110111100经度二进制编码：110010011111110公式：(Lat_bin + 1, Lon_bin - 1) \n ** *Geohash编码：* wm3vzu纬度二进制编码：101010110111100经度二进制编码：110100101101010公式：(Lat_bin + 1, Lon_bin) \n ** *Geohash编码：* wm6jbh纬度二进制编码：101010110111100经度二进制编码：110010100000000公式：(Lat_bin + 1, Lon_bin + 1) \n \n \n \n \n **Geohash编码：**wm3vze纬度二进制编码：101010110111011经度二进制编码：110010011111110公式：(Lat_bin, Lon_bin - 1) \n **Geohash编码：**wm3vzg纬度二进制编码：101010110111011经度二进制编码：110010011111111公式：(Lat_bin, Lon_bin) \n **Geohash编码：**wm6jb5纬度二进制编码：101010110111011经度二进制编码：110010100000000公式：(Lat_bin, Lon_bin + 1) \n \n \n **Geohash编码：**wm3vzd纬度二进制编码：101010110111010经度二进制编码：110010011111110公式：(Lat_bin - 1, Lon_bin - 1) \n **Geohash编码：**wm3vzf纬度二进制编码：101010110111010经度二进制编码：110010011111111公式：(Lat_bin - 1, Lon_bin) \n **Geohash编码：**wm6jb4纬度二进制编码：101010110111010经度二进制编码：110010100000000公式：(Lat_bin - 1, Lon_bin + 1) \n \n \n \n 附近3公里网格模型 \n geohash进阶原理（Z阶曲线) \n 上文讲了GeoHash的计算步骤，仅仅说明是什么而没有说明为什么？为什么分别给经度和维度编码？为什么需要将经纬度两串编码 交叉组合 成一串编码？本节试图回答这一问题。 \n 如下图所示，我们将二进制编码的结果填写到空间中，当将空间划分为四块时候，编码的顺序分别是左下角00，左上角01，右下脚10，右上角11，也就是类似于Z的曲线，当我们递归的将各个块分解成更小的子块时，编码的顺序是自相似的（分形），每一个子快也形成Z曲线，这种类型的曲线被称为 Peano 空间填充曲线。 \n 这种类型的空间填充曲线的优点是将二维空间转换成一维曲线（事实上是分形维），对大部分而言，编码相似的距离也相近， 但Peano空间填充曲线最大的缺点就是 突变性 ，有些编码相邻但距离却相差很远，比如 0111与1000 ，编码是相邻的，但距离相差很大。 \n \n 除 Peano 空间填充曲线外，还有很多空间填充曲线，如图所示，其中效果公认较好是  Hilbert  空间填充曲线，相较于 Peano 曲线而言，Hilbert 曲线 没有较大的突变 。为什么 GeoHash 不选择 Hilbert 空间填充曲线呢？可能是 Peano曲线思路以及计算上比较简单吧，事实上，Peano 曲线就是一种 四叉树线性编码方式。 \n"},{title:"Flink on K8s",frontmatter:{title:"Flink on K8s",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["云原生"],tags:["云原生","k8s","Flink","beam"],sticky:1},regularPath:"/%E4%BA%91%E5%8E%9F%E7%94%9F/fink-on-k8s.html",relativePath:"云原生/fink-on-k8s.md",key:"v-83d8af48",path:"/2023/06/10/fink-on-k8s/",headers:[{level:2,title:"前言",slug:"前言"},{level:3,title:"Flink部署模式简介",slug:"flink部署模式简介"},{level:2,title:"一、Flink Kubernetes Operator是什么",slug:"一、flink-kubernetes-operator是什么"},{level:2,title:"二、Flink Kubernetes Operator详解",slug:"二、flink-kubernetes-operator详解"},{level:3,title:"1、Flink Kubernetes Operator架构",slug:"_1、flink-kubernetes-operator架构"},{level:3,title:"2、Flink Kubernetes Operator的控制循环(资源更新)",slug:"_2、flink-kubernetes-operator的控制循环-资源更新"},{level:3,title:"3、Flink Webhook",slug:"_3、flink-webhook"},{level:3,title:"4、安装cert-manager",slug:"_4、安装cert-manager"},{level:3,title:"5.Flink Kubernetes Operator安装",slug:"_5-flink-kubernetes-operator安装"},{level:2,title:"三、轻松入门",slug:"三、轻松入门"},{level:3,title:"1、准备示例Flink作业",slug:"_1、准备示例flink作业"},{level:3,title:"2、运行示例Flink作业",slug:"_2、运行示例flink作业"},{level:3,title:"3、终止示例Flink作业",slug:"_3、终止示例flink作业"},{level:2,title:"四、部署模式",slug:"四、部署模式"},{level:3,title:"1.Apllication部署模式",slug:"_1-apllication部署模式"},{level:3,title:"2.Session部署模式",slug:"_2-session部署模式"},{level:3,title:"3.、Application模式和Session模式的选择",slug:"_3-、application模式和session模式的选择"},{level:2,title:"五、弹性扩缩容（HPA）",slug:"五、弹性扩缩容-hpa"},{level:3,title:"基于Flink Reactive模式结合原生k8s HPA实现副本",slug:"基于flink-reactive模式结合原生k8s-hpa实现副本"},{level:3,title:"基于flink 1.17版本实现",slug:"基于flink-1-17版本实现"},{level:2,title:"六、beam模型top on flink run k8s",slug:"六、beam模型top-on-flink-run-k8s"},{level:3,title:"轻松入门",slug:"轻松入门"},{level:3,title:"自建项目--体验流批一体编程",slug:"自建项目-体验流批一体编程"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' 前言 \n Apache Flink是当下主流了流式计算引擎，在企业的实时数仓、实时BI、数据湖、智能推荐和风险风控等场景中有广泛的应用。Apache Flink支持多种Resource Providers，也就是可以在多种资源平台上运行，本系列文章以当前热门的容器平台Kubernetes作为Flink的Resource Proivder，全面讲解如何在Kubernetes平台上以Flink Kubernetes Operator的方式运行Flink作业应用。 \n Flink部署模式简介 \n Apache Flink在1.14版本之前，支持4种类型的Resouce Providers，也称资源提供者，在本文中称之为部署模式，它们分别是Standalone、 Kubernetes、YARN和Mesos，但在1.14版本及后续版本中减少为3种，分别是Standalone、 Kubernetes和YARN，少了Mesos。有关Flink Resouce Providers的详细介绍，大家可以到Apache Flink官方网站查阅，官方网址为 flink resource providers 。 \n 目前在实际应用中，绝大部分公司主要使用Standalone和YARN模式，尤其是YARN模式居多，使用Kubernetes模式的比较少。 \n 1、Standalone模式 \n Standalone模式直接将Flink集群（这里的集群是指JobManager和TaskManager的组合）部署在虚拟机或物理机上，也就是JobManager和TaskManager是运行在宿主机上的。Standalone模式的主要特点是Flink集群的资源规模在启动时是确定的，也就是需要先定义好JobManager和TaskManager的CPU、内存以及每个TaskManager提供的Slot槽位等参数，在worker配置文件里定义好TaskManager的实例列表，后期如果需要扩容，则需要停止运行中的集群，重新调整资源和TaskManager的实例数量后再启动，这会有一个停机维护的过程。 \n 在 Standalone模式下，提交到Flink集群的 作业会竞争集群的资源，或者是共享集群的资源，当集群的可用Slot数量不足以满足新的作业运行要求时，新的作业会被挂起 。此外，性能差的作业，或运行异常的作业有可能会拖垮整个集群，导致同一个集群里的其他作业跟着挂掉。所以，Standalone模式通常用在开发和测试环境，生产环境上很少使用这种模式。 \n 2、YARN模式 \n Flink作为大数据流处理计算框架，虽然本身支持Standalone模式，无需其他框架也可以运行，但资 源调度并不是它的强项 ，所以，大多数场景下需要专业的框架做资源调度，比如说YARN或Kubernetes。在实际应用中，由于Hadoop的普遍使用，所以YARN是当下采用得最多的。整体来看，在YARN上部署Flink作业的过程是：客户端把Flink作业提交到ResourceManager，在资源满足需求的情况下，ResourceManager会在选定的NodeManager上创建Container容器，然后在这些容器上部署JobManager和TaskManger的实例，从而启动Flink集群。Flink会根据作业所需要的slot数量动态创建TaskkManger，如果作业运行完毕，相应的JobManager和TaskManger占用的YARN容器资源也会一同释放。 \n Flink在YARN模式下，分别支持Apllication、Session和Per-Job三种运行模式，其中Per-Job运行模式是YARN特有的，但从1.15版本开始被废弃。 \n 3、Kubernetes模式 \n 在Kubernetes模式下，Flink所需的计算资源由K8s容器平台提供，以容器Pod为单元进行资源的管理和调度，也就是说，Flink集群的JobManager和TaskManager是运行在Pod里，这与YARN模式下运行在Container里运行类似。 \n Kubernetes模式下， Flink又细分为Native Kubernetes和Flink Kubernetes Operator两种模式，在实际应用中，比较少使用Native Kubernetes，而是使用Flink Kubernetes Operator居多 。此外，Flink Kubernetes Operator也是Apache Flink官方提供和推荐的，它可以极大地简化将Flink应用部署到K8s上的配置。有关Kubernetes Operator的相关说明，大家可以到它的官网查看 Kubernetes Operator 。 \n 随着Kubernetes的普遍应用，越来越多的企业已经体会到基于容器所带来的优势和便利，这也是云原生现在很火的原因。 大数据主要解决的是数据存储和计算的问题，计算资源的隔离和弹性供给是计算任务得以稳定高效运行的关键，传统的存算一体的部署方式，将各种组件都安装在一台物理机或虚拟机上，组件之间的运行难以避免会出现CPU和内存资源的竞争。 而对于 容器化 和Kubernetes而言，它的 天然优势就是解决计算资源的供给问题 ，所以大数据与Kubernetes的结合，或者说大数据容器化（BigData On K8s），是未来大数据新的应用方式。 \n 一、Flink Kubernetes Operator是什么 \n 关于Flink Kubernetes Operator是什么，Flink官方已经给出了清晰定义，在此我引用它的定义，原文大家可以到 flink-kubernetes-operator 查阅。 \n Flink Kubernetes Operator扩展了Kubernetes API，能够管理和操作Flink部署，具有以下特点:\n1是部署和监控Flink Application和Session模式的FlinkDeployment（这里的FlinkDeployment是Flink集群在K8s上的资源类型）\n2是升级、挂起和删除FlinkDeployment\n3是提供完整的日志记录和运行指标监控集成\n4是能实现Flink 应用的灵活部署，与Kubernetes工具原生集成\n \n 1 2 3 4 5 \n 综合而言，Flink Kubernetes Operator作为一个Kubernetes的Control plane控制平面，它管理Flink应用程序的完整部署生命周期。尽管Flink也提供Native原生的方式在k8s上部署Flink应用程序，但使用自定义资源CRD和Operator模式才是官方主推的Flink On K8s部署方式。 \n 二、Flink Kubernetes Operator详解 \n 1、Flink Kubernetes Operator架构 \n Flink Kubernetes Operator的架构图如下所示 \n \n 就如前面所述, Flink Kubernetes Operator作为一个控制平面，管理Flink应用程序的完整部署生命周期。在实际的生产环境应用中，我们通常将Flink Kubernetes Operator部署在指定的K8s NameSpace中（这个NameSpace的名字通常是flink），然后在一个或多个托管名称空间中管理Flink应用的部署。 \n Flink Kubernetes Operator会创建和监控2种自定义资源, 它们分别是FlinkDeployment和FlinkSessionJob, 这2个自定义资源是一个集群范围的资源, 在使用之前需要在API Server上完成注册声明, 这个在安装Flink Kubernetes Operator时会自动完成。 \n Flink Kubernetes Operator运行态结构见红底部分, 它运行的时候会启动2个Container，这2个Container是运行在同一个Pod里，一个是flink-operator，另一个是flink-webhook，这个Pod由ReplicaSet和Deployment定义它的规格，例如JobManager和TaskManager的副本数量和资源配额等。此外, 还有Service和ConfigMap, 其中Service是用于提供flink-webhook接口服务的, ConfigMap是用于存储Flink默认的配置信息和operator自身的配置信息的. \n 用户要提交Flink作业到K8s上运行, 他要做的就是开发好Flink作业程序, 编写好FlinkDeployment Yaml文件，然后用kubectl提交到K8s，之后Flink Kubernetes Operator会根据Yaml的定义把这个Flink集群创建出来, 例如Yaml定义JobManager的数量为2，则会创建2个JobManager，此外，构成这个集群相应的Pod、Deployment、Service、ConfigMap和Ingress资源也都会自动创建出来。 \n 2、Flink Kubernetes Operator的控制循环(资源更新) \n Flink Kubernetes Operator会持续跟踪与FlinkDeployment和FlinkSessionJob自定义资源相关的集群事件。当Flink Kubernetes Operator接收到新的资源更新时，它将采取行动将Flink集群调整到所需的状态，这个过程称为reconcile，是一个持续进行的循环。 \n 3、Flink Webhook \n Webhook是一个HTTP回调，通过特定条件或事件触发HTTP POST请求发送到Webhook服务端，服务端根据请求数据进行相应的处理。 \n 在Kubernetes中，Webhook通常是用于实现动态准入控制的，它的功能主要是接受API Server的认证请求，然后调用不同的认证服务进行认证。 \n Flink Webhook就是用于实现准入控制，分为两种：一是验证性质的准入Webhook（Validating Admission Webhook），对应的是/validate接口，二是修改性质的准入 Webhook（Mutating Admission Webhook），对应的是/mutate接口。 \n 以Flink Webhook为例，在FlinkDeployment资源持久化到ETCD之前API Server需要调用/mutate接口对该资源规格描述进行修改，比如增加默认配置信息、init Container或者sidecar Container；此外，在将FlinkDeployment资源持久化到ETCD之前API Server也需要调用/validate接口校验yaml文件，如果yaml文件定义的信息不准确则拒绝创建资源并给出相应信息。 \n Flink Webhook默认使用TLS协议进行通信，也就是HTTPS，所以在使用Flink Kubernetes Operator时，需要先安装cert-manager组件，由它提供证书服务。 \n  4、安装cert-manager \n Flink Webhook默认使用TLS协议进行通信，也就是HTTPS，所以在使用Flink Kubernetes Operator时，需要先安装cert-manager组件，由它提供证书服务。 \n 通常使用cert-manager.yaml来安装cert-manager，cert-manager.yaml可以从cert-manager的官网获取，它的官网地址是 cert-manager 。需要注意的是，官网提供的cert-manager.yaml使用的镜像是国外的镜像仓库，在国内有可能会因为网络原因无法下载镜像，导致无法安装。为此，建议下载cert-manager.yaml后修改它的镜像地址，可以按如下方式将cert-manager.yaml里3个对应image:进行替换修改。 \n registry.cn-hangzhou.aliyuncs.com/cm_ns01/cert-manager-webhook:v1.10.0\nregistry.cn-hangzhou.aliyuncs.com/cm_ns01/cert-manager-cainjector:v1.10.0\nregistry.cn-hangzhou.aliyuncs.com/cm_ns01/cert-manager-controller:v1.10.0\n \n 1 2 3 下载并修改好cert-manager.yaml后，使用kubectl命令安装即可。 \n kubectl apply -f cert-manager.yaml\n \n 1 cert-manager安装好后会启动3个Pod和2个Service，使用命令 查看： \n  kubectl get all  -n  cert-manager\n \n 1 \n 5.Flink Kubernetes Operator安装 \n Helm和cert-manager安装好后，接下来就可以正式安装Flink Kubernetes Operator。 \n 5.1 helm在线安装 \n Flink Kubernetes Operator最简单直接的安装方式就是使用helm在线安装，命令如下： \n helm repo  add  flink-operator-repo https://downloads.apache.org/flink/flink-kubernetes-operator-1.5.0/\nhelm  install  flink-kubernetes-operator flink-operator-repo/flink-kubernetes-operator   --namespace  flink --create-namespace\n \n 1 2 此处，我们将Flink Kubernetes Operator安装到K8s的flink Namespace下，如果flink Namespace不存在，则创建之。 \n 可以使用如下命令检查安装情况： \n kubectl get all -n flink -owide\nhelm list -n flink\n \n 1 2 5.2 离线下载定制化安装 \n 如果需要定制Flink Kubernetes Operator，例如开启HA和修改它的启动参数配置，那么建议采用helm本地安装的方式。此方式需要先下载Flink Kubernetes Operator的Helm包文件，下载网址为 flink-kubernetes-operator-1.5.0-helm.tgz ， 然后解压进入目录，根据需要修改values.yaml文件， \n #副本从1改成2 \nreplicas:  2 \n #HA mode \nkubernetes.operator.leader-election.enabled:  true \nkubernetes.operator.leader-election.lease-name: flink-operator-lease\n \n 1 2 3 4 5 并使用如下命令安装： \n helm  install   -f   myvalues.yaml flink-kubernetes-operator  .   --namespace  flink --create-namespace\n #helm uninstall flink-kubernetes-operator -n flink \n \n 1 2 安装可能会因为网络原因下载不下来，按照官方给的提示，可以替换image仓库 \n #The Helm chart by default points to the ghcr.io/apache/flink-kubernetes-operator image repository. If you have connectivity issues or prefer to use Dockerhub instead you can use  \n --set   image.repository = apache/flink-kubernetes-operator \n \n 1 2 下图所示的是以HA方式安装的Flink Kubernetes Operator，其中副本数是2，所以启动了2个flink kubernetes operator pod，它们是主备关系，此外，也有1个Flink Webhook的Service。 \n #副本数修改为2 \nreplicas:  2 \n #使用HA mode \nkubernetes.operator.leader-election.enabled:  true \nkubernetes.operator.leader-election.lease-name: flink-operator-lease\n \n 1 2 3 4 5 \n 三、轻松入门 \n  1、准备示例Flink作业 \n Flink Kubernetes Operator的源码里提供了不少示例程序，这是体验Flink Kubernetes Operator的很好方式，可以到  https://github.com/apache/flink-kubernetes-operator/tags  获取Flink Kubernetes Operator源码。 \n 本文使用源码中的basic.yaml来演示Flink Kubernetes Operator的使用，该文件在源码解压后的examples目录下，以下是basic.yaml文件的内容（本文件对其做了部分改动，见注释描述） \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment   # Flink集群在K8s的资源类型 \n metadata : \n   name :  basic - example   # 作业的名字 \n   namespace :  flink         # 指定在flink命名空间下运行 \n spec : \n   image :  apache/flink : 1.14.6 - scala_2.12       # Flink的镜像，改为使用1.14.6  \n   flinkVersion :  v1_14     # Flink的版本，与镜像版本保持一致 \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "2" \n   serviceAccount :  flink - service - account\n   jobManager : \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   taskManager : \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   job : \n     jarURI :  local : ///opt/flink/examples/streaming/StateMachineExample.jar   # Flink作业的启动类所在的Jar包路径 \n     parallelism :   2 \n     upgradeMode :  stateless\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #   2、运行示例Flink作业 \n basic.yaml文件准备好后，使用如下命令提交Flink作业到K8s集群运行。 \n kubectl apply  -f  basic.yaml\n \n 1 可以使用如下任意一个命令检查作业的运行情况 \n kubectl get all  -n  flink\n #kubectl get FlinkDeployment -n flink \n \n 1 2 可以看到，basic.yaml文件提交到K8s后，K8s在flink命名空间下新启动了2个Pod，一个是JobManager的Pod，另一个是TaskManager的Pod。此外也可以看到创建了一个名字为basic-example，资源类型为FlinkDeployment的实例。 \n \n 编写查看web ui ingress-flink文件（前提是搭建k8s时，ingress-nginx-controller已经安装好） \n apiVersion :  networking.k8s.io/v1\n kind :  Ingress\n metadata : \n   name :  flink - ingress\n   namespace :  flink\n spec : \n   rules : \n     -   host :  ingress.flink.com\n       http : \n         paths : \n           -   path :   "/" \n             pathType :  Prefix\n             backend : \n               service : \n                 name :  basic - example - rest\n                 port : \n                   number :   8081 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply  -f  ingress-flink.yaml\n \n 1 查看ingress \n kubectl get ingress  -n  flink\n \n 1 查看web ui \n 3、终止示例Flink作业 \n 停止Flink作业运行很简单，只需要执行以下任意一条命令即可。 \n kubectl delete  -f  basic.yaml\n #kubectl delete FlinkDeployment basic-example -n flink \n \n 1 2 #  四、部署模式 \n Kubernetes支持Apllication和Session这两种部署模式。 \n 1.Apllication部署模式 \n 1.1、Application模式简介 \n 在Application部署模式下，Kubernetes会 为每个提交的Flink作业单独创建一个Flink集群 ，这个Flink集群由JobManager Pod和TaskManager Pod组成，其中拟启动的TaskManager Pod的数量由Flink作业所需的slot数量和每个TaskManager所能提供的可用slot数量决定，例如一个Flink作业需要10个slot，每个TaskManager提供4个slot，那么Kubernetes就会启动3个TaskManager Pod。在Flink作业运行完成的时候，或者终止Flink作业运行时候，Kubernetes会终止集群并释放全部的Pod。 \n Application部署模式的主要特点是它在不同Flink作业之间提供了资源隔离和负载平衡保证，使得作业间彼此独立，互不影响，此外，它还可以为不同的作业配置不同的资源，以此实现作业的精细化管理。 \n Application部署模式 在Kubernetes上 有2种Flink作业的提交方式 ，方式一是将Flink作业的 Jar包打进Flink镜像 里，在Flink作业的FlinkDeployment yaml文件里使用该镜像，当Flink作业运行（或称Flink集群创建）的时候，内部就会包含该作业的Jar包。这种方式的显著特点就是 每个Flink作业都要创建自己专属的镜像 ，如果Flink作业的数量较多，那么也会导致 镜像过多，从而占用大量镜像仓库空间 。方式二是将作业 Jar包放到外部存储上 ，例如NFS，然后通过Kubernetes的PVC+PV方式挂载到Pod里，好处就是 只需维护少数几个Base Flink镜像 即可，极大节省镜像仓库空间。 \n 1.2、示例程序 \n 为了进行接下来的演示，需要开发一个功能为WordCount的Flink程序，该程序从socket读取字符流，然后分词并统计单词出现次数，最后将统计结果打印到控制台。程序的代码如下所示。 \n maven pom.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 " \n          xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     < groupId > com.gordon </ groupId > \n     < artifactId > flink-on-k8s </ artifactId > \n     < version > 1.0-SNAPSHOT </ version > \n     < properties > \n         < maven.compiler.source > 8 </ maven.compiler.source > \n         < maven.compiler.target > 8 </ maven.compiler.target > \n         < flink.version > 1.14.6 </ flink.version > \n         < java.version > 1.8 </ java.version > \n         < scala.binary.version > 2.12 </ scala.binary.version > \n         < slf4j.version > 1.7.30 </ slf4j.version > \n     </ properties > \n\n     < dependencies > \n         \x3c!-- 引入Flink相关依赖--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-java </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-java_${scala.binary.version} </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-clients_${scala.binary.version} </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-scala_${scala.binary.version} </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         \x3c!--kafka--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-connector-kafka_${scala.binary.version} </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.commons </ groupId > \n             < artifactId > commons-lang3 </ artifactId > \n             < version > 3.9 </ version > \n         </ dependency > \n\n         \x3c!-- 引入日志管理相关依赖--\x3e \n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-api </ artifactId > \n             < version > ${slf4j.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-log4j12 </ artifactId > \n             < version > ${slf4j.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.logging.log4j </ groupId > \n             < artifactId > log4j-to-slf4j </ artifactId > \n             < version > 2.14.0 </ version > \n         </ dependency > \n\n     </ dependencies > \n\n\n     < build > \n         < plugins > \n             \x3c!--docker部署插件--\x3e \n             < plugin > \n                 < groupId > com.spotify </ groupId > \n                 < artifactId > docker-maven-plugin </ artifactId > \n                 < version > 1.0.0 </ version > \n                 < configuration > \n                     \x3c!--<dockerDirectory>src/main/docker</dockerDirectory>--\x3e \n                     < dockerDirectory > ${project.basedir} </ dockerDirectory > \n                     < resources > \n                         < resource > \n                             < targetPath > / </ targetPath > \n                             < directory > ${project.build.directory} </ directory > \n                             < includes > ${project.build.finalName}.jar </ includes > \n                         </ resource > \n                     </ resources > \n                 </ configuration > \n             </ plugin > \n             \x3c!--如果打包完成后,把构建结果复制到其他位置，可加如下插件--\x3e \n             < plugin > \n                 < artifactId > maven-antrun-plugin </ artifactId > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < configuration > \n                             < tasks > \n                                 < copy   todir = " ${project.basedir} "   file = " target/${project.artifactId}-${project.version}.${project.packaging} " > </ copy > \n                             </ tasks > \n                         </ configuration > \n                         < goals > \n                             < goal > run </ goal > \n                         </ goals > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n             \x3c!-- 打包配置--\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-shade-plugin </ artifactId > \n                 < version > 2.3 </ version > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < goals > \n                             < goal > shade </ goal > \n                         </ goals > \n                         < configuration > \n                             < filters > \n                                 < filter > \n                                     < artifact > *:* </ artifact > \n                                     < excludes > \n                                         \x3c!--\n                                        zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --\x3e \n                                         < exclude > META-INF/*.SF </ exclude > \n                                         < exclude > META-INF/*.DSA </ exclude > \n                                         < exclude > META-INF/*.RSA </ exclude > \n                                     </ excludes > \n                                 </ filter > \n                             </ filters > \n                             < transformers > \n                                 < transformer   implementation = " org.apache.maven.plugins.shade.resource.ManifestResourceTransformer " > \n                                     \x3c!-- 设置jar包的入口类(可选) --\x3e \n                                     < mainClass > </ mainClass > \n                                 </ transformer > \n                             </ transformers > \n                         </ configuration > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n             \x3c!--<plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <version>3.0.0</version>\n                <configuration>\n                    <descriptorRefs>\n                        <descriptorRef>jar-with-dependencies</descriptorRef>\n                    </descriptorRefs>\n                </configuration>\n                <executions>\n                    <execution>\n                        <id>make-assembly</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>--\x3e \n\n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.8.0 </ version > \n                 < configuration > \n                     < source > ${java.version} </ source > \n                     < target > ${java.version} </ target > \n                     < encoding > UTF-8 </ encoding > \n                 </ configuration > \n             </ plugin > \n\n         </ plugins > \n     </ build > \n\n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 程序StreamWordCount \n package   com . gordon . basic ; \n\n import   org . apache . flink . api . common . typeinfo . Types ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n import   org . apache . log4j . Logger ; \n import   java . util . Arrays ; \n\n\n public   class   StreamWordCount   { \n     private   static   Logger  logger  =   Logger . getLogger ( StreamWordCount . class ) ; \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         // 1. 创建流式执行环境 \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         // 2. 读取文本流  在k8s01行运行 nc -lk 7777 \n         DataStreamSource < String >  lineDSS  =  env . socketTextStream ( "192.168.8.103" ,   7777 ) ; \n         // 3. 转换数据格式 \n         SingleOutputStreamOperator < Tuple2 < String ,   Long > >  wordAndOne  =  lineDSS\n                 . flatMap ( ( String  line ,   Collector < String >  words )   ->   { \n                     Arrays . stream ( line . split ( " " ) ) . forEach ( words :: collect ) ; \n                 } ) \n                 . returns ( Types . STRING ) \n                 . map ( word  ->   Tuple2 . of ( word ,   1L ) ) \n                 . returns ( Types . TUPLE ( Types . STRING ,   Types . LONG ) ) ; \n         // 4. 分组 \n         KeyedStream < Tuple2 < String ,   Long > ,   String >  wordAndOneKS  =  wordAndOne\n                 . keyBy ( t  ->  t . f0 ) ; \n         // 5. 求和 \n         SingleOutputStreamOperator < Tuple2 < String ,   Long > >  result  =  wordAndOneKS\n                 . sum ( 1 ) ; \n         // 6. 打印 \n        result . print ( ) ; \n         // 7. 执行 \n        env . execute ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 Flink程序开发完成后，需要打成Jar包并上传到Kubernetes集群中。 \n  1.3、运行示例程序 \n 方式1 Jar包打进Flink镜像 \n 第一步，需要编写Dockerfile，并构建Flink作业镜像 ，Dockerfile的内容如下所示。 \n #私库没有，选官方 \n #FROM apache/flink:1.14.6-scala_2.12 \n FROM  personalharbor.com/bigdata/flink:1.14.6-scala_2.12 \n WORKDIR  /opt/flink \n #将jar包复制到镜像里，jar包原路径视实际放置位置设定 \n COPY  flink-on-k8s-1.0-SNAPSHOT.jar /opt/flink/flink-on-k8s-1.0-SNAPSHOT.jar \n ENTRYPOINT  [ "/docker-entrypoint.sh" ] \n EXPOSE  6123 8081 \n CMD  [ "help" ] \n \n 1 2 3 4 5 6 7 8 9 一般本地是不通生产网络的，所以镜像构建可以在本地打成jar包，放置在服务器上Dockerfile同一目录下。 \n 使用如下命令构建，此命令将会构建出一个名字为flink-wc，版本为1.13.6的新镜像。 \n docker build -t flink-wc-add-jar:1.14.6 .\n \n 1 flink-wc镜像构建好后，需要分发到各个Kubernetes Node节点，更推荐的做法是把它发布到自有的镜像仓库，以便后续提交Flink作业yaml文件时能下载该镜像。发布到镜像仓库的命令如下。 \n docker tag image_id registry_address/flink-wc-add-jar:1.14.6\ndocker push registry_address/flink-wc-add-jar:1.14.6\n \n 1 2 下面扩展介绍，学习时，使用本地ide工具连接远程docker进行build。 \n 修改pom文件将build增加plugin为如下 \n \x3c!--docker部署插件--\x3e \n             < plugin > \n                 < groupId > com.spotify </ groupId > \n                 < artifactId > docker-maven-plugin </ artifactId > \n                 < version > 1.0.0 </ version > \n                 < configuration > \n                     \x3c!--<dockerDirectory>src/main/docker</dockerDirectory>--\x3e \n                     < dockerDirectory > ${project.basedir} </ dockerDirectory > \n                     < resources > \n                         < resource > \n                             < targetPath > / </ targetPath > \n                             < directory > ${project.build.directory} </ directory > \n                             < includes > ${project.build.finalName}.jar </ includes > \n                         </ resource > \n                     </ resources > \n                 </ configuration > \n             </ plugin > \n             \x3c!--如果打包完成后,把构建结果复制到其他位置，可加如下插件--\x3e \n             < plugin > \n                 < artifactId > maven-antrun-plugin </ artifactId > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < configuration > \n                             < tasks > \n                                 < copy   todir = " ${project.basedir} "   file = " target/${project.artifactId}-${project.version}.${project.packaging} " > </ copy > \n                             </ tasks > \n                         </ configuration > \n                         < goals > \n                             < goal > run </ goal > \n                         </ goals > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 通过maven install生成jar包 \n \n 编写dockerfile配置并执行生成镜像，这里只是build，所以不勾选run。 \n #私库没有，选官方 \n #FROM apache/flink:1.14.6-scala_2.12 \n FROM  personalharbor.com/bigdata/flink:1.14.6-scala_2.12 \n WORKDIR  /opt/flink \n #将jar包复制到镜像里，jar包原路径视实际放置位置设定 \n COPY  *.jar /opt/flink/flink-on-k8s-1.0-SNAPSHOT.jar \n ENTRYPOINT  [ "/docker-entrypoint.sh" ] \n EXPOSE  6123 8081 \n CMD  [ "help" ] \n \n 1 2 3 4 5 6 7 8 9 \n \n 推送到私库 \n docker  push personalharbor.com/bigdata/flink-wc-add-jar:1.14.6\n \n 1 \n 第二步，编写Flink作业的yaml文件 ，yaml文件的内容如下所示。 \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   namespace :  flink\n   name :  application - deployment\n spec : \n   image :  personalharbor.com/bigdata/flink - wc - add - jar : 1.14.6   # 使用自行构建的镜像 \n   flinkVersion :  v1_14\n   imagePullPolicy :  IfNotPresent    # 镜像拉取策略，本地没有则从仓库拉取 \n   ingress :     # ingress配置，用于访问flink web页面 \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "2" \n   serviceAccount :  flink\n   jobManager : \n     replicas :   1 \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   taskManager : \n     replicas :   1 \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   job : \n     jarURI :  local : ///opt/flink/flink - on - k8s - 1.0 - SNAPSHOT.jar\n     entryClass :  com.gordon.basic.StreamWordCount\n     args : \n     parallelism :   1 \n     upgradeMode :  stateless\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 在yaml文件里有三处地方需要特别注意，首先是开头部分定义了资源的类型为FlinkDeployment，这是Flink Kubernetes Operator为Flink集群所定义的自定义资源类型（CRD）；其次，images引用了我们自行构建的Flink作业镜像；最后，job.jarURI指向了我们Flink作业Jar包在镜像内的路径，job.entryClass则为Flink作业的启动类名称。 \n 第三步，运行Flink作业 。由于示例Flink程序需要从nc读取字符流数据，因此在运行Flink作业前，需要先运行nc程序，命令是nc -lk 7777，否则Flink作业会运行不起来。 \n 使用以下命令提交Flink作业的yaml文件到Kubernetes运行，并查看Pod的创建情况。 \n kubectl apply  -f  flink-wc.yaml\nkubectl get all  -n  flink\n \n 1 2 \n 在nc发送一些测试字符串，使用kubectl logs taskmanager-pod-name -n flink查看Flink作业的输出结果。 \n \n 这里ingress集成到作业的yaml，webui显示正常：http://flink.k8s.io/flink/application-deployment/ \n  方式2 Jar包通过PV挂载到镜像 \n 第一步，需要先创建Kubernetes PVC ，申请分配PV存储，把Flink作业Jar包上传放置到该PV对应的实际储存的路径下。本文采用动态PVC申请PV的方式，存储设施是NFS，因此需要提前在Kubernetes上安装配置好NFS的组件服务，并创建好StorageClass资源 \n flink-jar-pvc.yaml \n Flink 作业jar 持久化存储pvc \n apiVersion :  v1\n kind :  PersistentVolumeClaim\n metadata : \n   name :  flink - jar - pvc   # jar pvc名称 \n   namespace :  flink\n spec : \n   storageClassName :  nfs - client    #sc名称,使用kubectl get sc查看 \n   accessModes : \n     -  ReadOnlyMany    #采用ReadOnlyMany的访问模式 \n   resources : \n     requests : \n       storage :  1Gi     #存储容量，根据实际需要更改 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 \n 查看NFS上flink-jar-pvc的路径 \n #通过kubectl get pvc -n flink获取路径Volumes \n #通过kubectl describe pv pvc-944394b1-2fe2-4bfe-80a8-5eff0ffd19ca  -n flink 获取存储路径 \n #将jar包上传到nfs路径 \n scp  flink-on-k8s-1.0-SNAPSHOT.jar  192.168 .8.210:/opt/nfsdata/flink-flink-jar-pvc-pvc-944394b1-2fe2-4bfe-80a8-5eff0ffd19ca\n \n 1 2 3 4 \n 第二步，编写Flink作业的yaml文件 （application-deployment-with-pv.yaml），yaml文件的内容如下所示。 \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   namespace :  flink\n   name :  application - deployment\n spec : \n   image :  personalharbor.com/bigdata/flink : 1.14.6 - scala_2.12   # 使用基础镜像包 \n   flinkVersion :  v1_14\n   imagePullPolicy :  IfNotPresent    # 镜像拉取策略，本地没有则从仓库拉取 \n   ingress :     # ingress配置，用于访问flink web页面 \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "2" \n   serviceAccount :  flink\n   jobManager : \n     replicas :   1 \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   taskManager : \n     replicas :   1 \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   podTemplate : \n     spec : \n       containers : \n         -   name :  flink - main - container\n           volumeMounts : \n             -   name :  flink - jar   # 挂载nfs上的jar \n              #创建一个flink目录下的空白文件夹，来放置 \n              #如果/opt/flink,会把该目录下的文件全部置空，来进行挂载，清空了安装包，程序启动不了 \n               mountPath :  /opt/flink/jar\n       volumes : \n         -   name :  flink - jar\n           persistentVolumeClaim : \n             claimName :  flink - jar - pvc\n   job : \n     jarURI :  local : ///opt/flink/jar/flink - on - k8s - 1.0 - SNAPSHOT.jar\n     entryClass :  com.gordon.basic.StreamWordCount\n     args : \n     parallelism :   1 \n     upgradeMode :  stateless\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 方式二的Flink作业yaml文件与方式一的yaml文件大体类似，但有三处地方需要特别注意，首先是images改为引用Flink的基础镜像，而不是我们自行构建的Flink作业镜像；其次新增了podTemplate的内容，目的是将我们创建的PV挂载到Flink容器里，挂载的路径是/opt/flink/jar( 这里要特别注意 )；最后，job.jarURI指向了我们Flink作业Jar包的挂载路径，job.entryClass则为Flink作业的启动类名称。 \n 运行,查看作业执行情况 \n kubectl apply  -f  flink-wc-with-pv.yaml\nkubectl get all  -n  flink\n \n 1 2 \n 如果报cpu资源不足，这里提供一种方式，清理污点 \n 0 /3 nodes are available:  1  node ( s )  had taint  { node.kubernetes.io/disk-pressure:  } , that the pod didn\'t tolerate,  2  Insufficient cpu.\n \n 1 查看污点 \n kubectl get  node   -o  yaml  |   grep  taint  -A   5 \n \n 1 #去除污点，允许master节点部署pod \n kubectl taint nodes  --all  node-role.kubernetes.io/master-\n \n 1 如果是硬盘不足，需要清理无用的镜像。 \n 1.4、两种方式的选择 \n 关于Application部署模式2种提交方式的选择, 大家可以参考如下策略。 \n Application模式的2种作业提交方式的最大区别在于是否需要将作业Jar包打入Flink镜像 。 \n 对于方式一，把Jar包打进镜像，这样会使得每个Flink作业都要打一个镜像，容易导致镜像数量过多，既不便于管理，同时也会占用大量镜像仓库空间，通常1个Flink镜像的大小约为600M，当镜像数量很多的时候，需要占有的空间就很大。所以在实际应用中，不太推荐使用这种方式。 \n 对于方式二，Jar包通过PV挂载，这样的好处就是Flink作业Jar包不用打到镜像，既可以省去镜像构建的工作，同时只需维护少数几个Flink基础镜像，这样可以极大地节省镜像仓库空间。综合考量，在实际应用中，更推荐使用方式二。 \n 2.Session部署模式 \n 2.1、Session模式简介 \n 在Session模式下，需要 先在Kubernetes上启动一个Flink集群 ，这个集群初始情况下只有JobManager（如果Flink集群开启了HA，JobManager会有多个实例），没有TaskManager，当客户端向该集群提交作业时，Kubernetes再为每个作业动态创建TaskManager，TaskManager Pod的数量由Flink作业所需的slot数量和每个TaskManager所能提供的可用slot数量决定。所有作业 共享该集群的JobManager ，作业终止时，如果TaskManager的slot没有被其他的Flink作业占用，那么该TaskManager的Pod会被释放，但JobManager会继续运行，等待后续Flink作业的提交。 \n Session部署模式的主要特点是所有Flink作业共享集群的JobManager ，因此Flink作业的 资源隔离比较差 ，作业间会存在相互影响，极端情况下， 个别异常作业会拖垮整个Flink集群 ，导致集群上的作业一起失败。 \n Session部署模式在Kubernetes上也有2种Flink作业的提交方式。 \n 方式一是通过Flink Web UI和Restful接口上传Flink作业Jar包方式 ，在Flink集群启动后，也就是JobManager运行起来后，可以通过人工访问Flink Web UI页面上传Jar包和提交作业，这种方式操作灵活，便于调试，也可以通过编写程序调用Flink Restful接口上传Jar包和提交作业，这样可以使外部系统例如调度系统灵活控制Flink作业的更新和启停。 \n 方式二是通过HTTP下载Flink作业Jar包方式 ，该方式有利于Flink作业Jar包的统一存放管理，例如可以统一将Flink作业Jar包发布到HTTP文件服务器，例如tomcat或nginx，使用这种方式, Fllink作业需要编写对应的FlinkSessionJob yaml文件, 然后通过Kubernetes的kubectl命令提交，当然，也可以通过编写程序调用Kubernetes API来实现FlinkSessionJob的提交、更新和停止。 \n需要补充说明的是，与FlinkDeployment一样，FlinkSessionJob是Flink Kubernetes Operator为Flink作业创建的K8s自定义资源（CRD），但它只定义了Flink作业相关的信息。在Session部署模式下，FlinkDeployment负责定义Flink集群相关的信息，例如Flink Web UI的Ingress、Flink集群的全局参数配置、JobManager和TaskManager的资源配额等，而FlinkSessionJob则负责定义Flink作业的JarURI、启动类、启动参数等相关的信息。 \n  2.2、示例程序 \n 复用StreamWordCount \n  2.3、运行示例程序 \n  方式1 通过Restful接口Flink Web UI上传Jar包 \n 第一步，编写启动Flink集群的yaml文件，启动Flink集群 ，yaml文件的内容如下所示。 \nFlink Session集群 \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   namespace :  flink\n   name :  session - deployment - only\n spec : \n   image :  personalharbor.com/bigdata/flink : 1.14.6 - scala_2.12\n   flinkVersion :  v1_14\n   imagePullPolicy :  IfNotPresent    # 镜像拉取策略，本地没有则从仓库拉取 \n   ingress :     # ingress配置，用于访问flink web页面 \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "2" \n   serviceAccount :  flink\n   jobManager : \n     replicas :   1 \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   taskManager : \n     replicas :   1 \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 在yaml文件里有三处地方需要特别注意，首先是开头部分定义了资源的类型为FlinkDeployment，这与Application部署模式的资源类型一样；其次，images使用的是Flink的基础镜像，而不是我们在Application部署模式方式一中自行构建的Flink作业镜像；再者，定义了Ingress，以便可以通过网页浏览器或Restful接口访问JobManager，Ingress的模板主要由三部分组成：域名、Flink集群所在的命名空间和Flink集群的名称 \n最后，与Application部署模式的yaml相比，Session部署模式的yaml少了job部分的定义。 \n使用如下命令提交Flink集群yaml文件到Kubernetes运行，并查看Pod的创建情况。 \n kubectl apply  -f  session-deployment-only.yaml\nkubectl get all  -n  flink\n \n 1 2 \n 第二步，访问Filnk Web UI，提交jar包，并运行 。在前面配置了使用Ingress的方式访问Flink Web UI，且Ingress的域名是flink.k8s.io，为此，在Windows的本地测试中，需要先在C:\\Windows\\System32\\drivers\\etc\\hosts中添加域名和IP的映射关系，此处的IP是Ingress Pod所在的Kubernetes Node节点的IP，对于Ubuntu或Mac，则在/etc/hosts里添加。 \n使用如下命令查看Ingress的具体详细。 \n kubectl get ingress  -n  flink \nkubectl get pod  -n  ingress-nginx  -owide \n \n 1 2 \n 三台机都有controller，都可以反代理，/etc/hosts里选一个做域名映射 \n web ui 地址是根据yaml文件定义拼接 ，注意最后有“/” \n flink.k8s.io/ { { namespace } } / { { name } } ( / | $ ) ( .* ) \n == > http://flink.k8s.io/flink/session-deployment-only/\n \n 1 2 \n \n \n 停止作业可在web ui上面点击cancell job，没有这个选项，说明属性web.cancel.enable被设置为false。 \n #查看cm确认 \nkubectl get cm  -n  flink \n #修改session-deployment-only.yaml，设置属性 \nweb.cancel.enable:  "true" \n #更新配置 \nkubectl apply  -f  session-deployment-only.yaml\n \n 1 2 3 4 5 6 \n 方式2 是通过HTTP下载Flink作业Jar包方式 \n 如果只是想体验下，不一定要自己搭一个HTTP服务器，jar包可以从maven仓库下载： \n https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.14.6/flink-examples-streaming_2.12-1.14.6-TopSpeedWindowing.jar \n 第一步，需要先搭建一个HTTP服务器 ，本文使用Tomcat。可以直接到Tomcat官网下载安装包，也可以使用如下命令获取。 \nwget https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.7/bin/apache-tomcat-10.1.7.tar.gz \n 简单的搭建，解压后改两个配置： \n 1.确认8080端口无占用，占用修改合适，使用以下命令查看端口是否被占用 \n netstat  -anp | grep   8080 \n \n 1 端口若是被占用，修改server.xml \n \n 2.由于Tomcat默认不允许下载文件，为此，Tomcat下载解压后需要修改它的conf/web.xml，将listings改为true，如下图所示。 \n \n 接着，将Flink作业Jar包上传到Tomcat的webapps目录下，本文将Flink作业Jar包上传到webapps/jars/flink目录下。此处需要注意的是，由于运行Flink作业的时候需要通过HTTP下载Jar包，为此需要确保TaskManager Pod能够访问Tomcat，为了便于测试，建议把Tomcat部署到Kubernetes的Node节点上。 \n 一切就绪后就可以启动Tomcat，同时使用网页浏览器访问Tomcat，确保Flink作业Jar可以下载。 \n \n 第二步，编写启动Flink集群的yaml文件，启动Flink集群 ，这与Restful接口模式一致，复用 \n 第三步，编写Flink作业下载jar包的yaml文件 ，如下所示。 \nFlink作业 \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkSessionJob\n metadata : \n   namespace :  flink\n   name :  session - job - only\n spec : \n   deploymentName :  session - deployment - only   # 需要与创建的集群名称一致 \n   job : \n     #jarURI: https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.14.6/flink-examples-streaming_2.12-1.14.6-TopSpeedWindowing.jar # 使用http方式下载jar包 \n     jarURI :  http : //192.168.8.104 : 9090/jars/flink/flink - on - k8s - 1.0 - SNAPSHOT.jar  # 使用tomcat http方式下载jar包 \n     entryClass :  com.gordon.basic.StreamWordCount\n     args : \n     parallelism :   1    # 并行度 \n     upgradeMode :  stateless\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 从maven仓库下载的jar包 \n \n 从搭建的tomcat的服务器上下载 \n 2.4、两种方式的选择 \n Session部署模式的2种作业提交方式的最大区别在于Flink作业Jar包的获取方式不同，它们特点在本文开头部分已作介绍，此处不再赘述，这里想表达的是，Session部署模式的2种作业提交方式优缺点并不明显，各有适用的场景，大家按照自己的实际情况选择即可。 \n 3.、Application模式和Session模式的选择 \n 关于Application模式和Session模式的选择，大家可以参考如下策略。 \nApplication模式和Session模式两者最大区别在于集群的生命周期和资源管理隔离程度的不同，因此，对于核心、优先级高和需要高保障 \n 的这类作业推荐使用Application模式。而那些对保障性要求相对不高，或者出于运维管理便利的考量，例如需要通过外部系统通过调用Flink Restful接口管理作业的提交和启停，那么可以考虑使用Session模式。如果对于作业的归属模棱两可，那么建议直接使用Application模式，也就是说，把Application模式作为Flink On K8s运行模式的默认选项。 \n 五、弹性扩缩容（HPA） \n 基于Flink Reactive模式结合原生k8s HPA实现副本 \n 1.代码编写 \n KafkaStringGeneratorJob数据生成代码 \n import   org . apache . commons . lang3 . RandomStringUtils ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . api . java . utils . ParameterTool ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . connector . kafka . sink . KafkaRecordSerializationSchema ; \n import   org . apache . flink . connector . kafka . sink . KafkaSink ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichParallelSourceFunction ; \n\n import   java . util . Random ; \n\n public   class   KafkaStringGeneratorJob   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         final   ParameterTool  params  =   ParameterTool . fromArgs ( args ) ; \n         String  kafkaTopic  =  params . get ( "kafka-topic" ,   "flink-kafka" ) ; \n         String  brokers  =  params . get ( "brokers" ,   "192.168.8.102:9092,192.168.8.103:9092,192.168.8.104:9092" ) ; \n\n         final   StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . addSource ( new   RichParallelSourceFunction < String > ( )   { \n             private   Random  random  ; \n             private   boolean  flag  =   true ; \n\n             @Override \n             public   void   open ( Configuration  parameters )   throws   Exception   { \n                 super . open ( parameters ) ; \n                random  =   new   Random ( ) ; \n             } \n\n             @Override \n             public   void   run ( SourceContext < String >  sourceContext )   throws   Exception   { \n                 while   ( flag ) { \n                     String  value  =   RandomStringUtils . randomAlphanumeric ( random . nextInt ( 5 ) ) ; \n                    sourceContext . collect ( value ) ; \n                     //Thread.sleep(50); \n                 } \n             } \n             @Override \n             public   void   cancel ( )   { \n                flag  =   false ; \n             } \n         } ) \n                 . sinkTo ( \n                         KafkaSink . < String > builder ( ) \n                                 . setBootstrapServers ( brokers ) \n                                 . setRecordSerializer ( \n                                         KafkaRecordSerializationSchema . builder ( ) \n                                                 . setValueSerializationSchema ( new   SimpleStringSchema ( ) ) \n                                                 . setTopic ( kafkaTopic ) \n                                                 . build ( ) ) \n                                 . build ( ) ) \n                 . setParallelism ( 6 ) ; \n\n        env . execute ( "HpaWordCount Kafka data generator job" ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 HpaWordCountForCpu 测试auto scale \n import   org . apache . flink . api . common . eventtime . WatermarkStrategy ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . api . java . utils . ParameterTool ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . connector . kafka . source . KafkaSource ; \n import   org . apache . flink . connector . kafka . source . enumerator . initializer . OffsetsInitializer ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . windowing . ProcessAllWindowFunction ; \n import   org . apache . flink . streaming . api . windowing . assigners . TumblingProcessingTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n import   org . apache . flink . streaming . api . windowing . windows . TimeWindow ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . HashMap ; \n\n public   class   HpaWordCountForCpu   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         final   ParameterTool  params  =   ParameterTool . fromArgs ( args ) ; \n         String  kafkaTopic  =  params . get ( "kafka-topic" ,   "hpa-test" ) ; \n         System . out . println ( "kafkaTopic = "   +  kafkaTopic ) ; \n         String  brokers  =  params . get ( "brokers" ,   "192.168.8.102:9092,192.168.8.103:9092,192.168.8.104:9092" ) ; \n         int  kafkaParallelism  =  params . getInt ( "kafka-parallelism" ,   1 ) ; \n         System . out . println ( "kafkaParallelism = "   +  kafkaParallelism ) ; \n         // create the environment to create streams and configure execution \n         final   StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . enableCheckpointing ( 5   *   60   *   1000L ) ; \n\n         KafkaSource < String >  kafkaSource  =   KafkaSource . < String > builder ( ) \n                 . setBootstrapServers ( brokers ) \n                 . setGroupId ( "stateMachineExample" ) \n                 . setTopics ( kafkaTopic ) \n                 . setValueOnlyDeserializer ( new   SimpleStringSchema ( ) ) \n                 . setStartingOffsets ( OffsetsInitializer . latest ( ) ) \n                 . build ( ) ; \n         DataStream < String >  source  =  env . fromSource ( \n                kafkaSource ,   WatermarkStrategy . noWatermarks ( ) ,   "HpaWordCount" ) . setParallelism ( kafkaParallelism ) . rebalance ( ) ; \n         //source.print(); \n         SingleOutputStreamOperator < String >  reduce  =  source\n                 . map ( new   RichMapFunction < String ,   Tuple2 < Integer ,   Integer > > ( )   { \n                     @Override \n                     public   Tuple2 < Integer ,   Integer >   map ( String  s )   throws   Exception   { \n                         return   Tuple2 . of ( s . length ( ) ,   1 ) ; \n                     } \n                 } ) \n                 . keyBy ( t  ->  t . f0 ) \n                 . window ( TumblingProcessingTimeWindows . of ( Time . minutes ( 2 ) ) ) \n                 . reduce ( ( x ,  y )   ->   ( Tuple2 . of ( x . f0 ,  x . f1  +  y . f1 ) ) ) \n                 . map ( x  ->   String . format ( "key is %S,value is %s" ,  x . f0 ,  x . f1 ) ) ; \n\n        reduce . print ( ) ; \n\n         // trigger program execution \n        env . execute ( "HpaWordCount" ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 #  2.编写部署的yaml文件 \n 踩坑提示：要提前建立执行账户角色及绑定，以及目录授权和域名映射。报错及解决可在本小章节后面找到。 \n 创建用户flink-service-account \n apiVersion :  v1\n kind :  ServiceAccount\n metadata : \n   name :  flink - service - account\n --- \n apiVersion :  rbac.authorization.k8s.io/v1\n kind :  Role\n metadata : \n   name :  flink - service\n rules : \n   -   apiGroups :   [ "" ] \n     resources :   [ "pods" ,   "services" , "configmaps" ] \n     verbs :   [ "create" ,   "get" ,   "list" ,   "watch" ,   "delete" ] \n   -   apiGroups :   [ "" ] \n     resources :   [ "pods/log" ] \n     verbs :   [ "get" ] \n   -   apiGroups :   [ "batch" ] \n     resources :   [ "jobs" ] \n     verbs :   [ "create" ,   "get" ,   "list" ,   "watch" ,   "delete" ] \n   -   apiGroups :   [ "extensions" ] \n     resources :   [ "ingresses" ] \n     verbs :   [ "create" ,   "get" ,   "list" ,   "watch" ,   "delete" ] \n --- \n apiVersion :  rbac.authorization.k8s.io/v1\n kind :  RoleBinding\n metadata : \n   name :  flink - service - role - binding\n roleRef : \n   apiGroup :  rbac.authorization.k8s.io\n   kind :  Role\n   name :  flink - service\n subjects : \n   -   kind :  ServiceAccount\n     name :  flink - service - account\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 使用helm批量部署（简单的使用） \n 1.创建自定义chart \n helm create flink-hpa\n \n 1 2.目录下的values可以不用，入门，暂时不配置动态修改变量，所以该文件不用 \n mv  values.yaml values.yaml.bak\n \n 1 3.清空template下的模板，把自定义的模板放进去 \n flink-hpa\n├── charts\n├── Chart.yaml\n├── templates\n│   ├── flink-configmap.yaml\n│   ├── jobmanager-application.yaml\n│   ├── jobmanager-rest-service.yaml\n│   ├── jobmanager-service.yaml\n│   ├── taskmanager-autoscaler.yaml\n│   ├── taskmanager-job-deployment.yaml\n│   └── taskmanager-query-state-service.yaml\n└── values.yaml.bak\n \n 1 2 3 4 5 6 7 8 9 10 11 12 jobmanager-application.yaml \n apiVersion :  batch / v1\nkind :   Job \nmetadata : \n  name :  flink - jobmanager\nspec : \n  parallelism :   1  #  Set  the value  to   greater  than  1   to   start  standby  JobManagers \n  template : \n    metadata : \n      #annotations : \n prometheus . io / port :   \'9249\' \n prometheus . io / scrape :   \'true\' \n      labels : \n        app :  flink\n        component :  jobmanager\n    spec : \n      restartPolicy :   OnFailure \n提前在容器内部映射域名\n      hostAliases : \n         -  ip :   192.168 .8 .102 \n          hostnames : \n             -   "node3" \n         -  ip :   192.168 .8 .103 \n          hostnames : \n             -   "node2" \n         -  ip :   192.168 .8 .104 \n          hostnames : \n             -   "node1" \n      containers : \n         -  name :  jobmanager\n          image :  personalharbor . com / bigdata / flink : 1.14 .6 - scala_2 . 12 \n          imagePullPolicy :   IfNotPresent \n          env : \n          args :   [ "standalone-job" ,   "--job-classname" ,   "com.gordon.hpa.HpaWordCountForCpu" , "--kafka-topic" , "hpa-test" , "--kafkaParallelism" , "1"   ] \n          ports : \n             -  containerPort :   6123 \n              name :  rpc\n             -  containerPort :   6124 \n              name :  blob - server\n             -  containerPort :   8081 \n              name :  webui\n          livenessProbe : \n            tcpSocket : \n              port :   6123 \n            initialDelaySeconds :   30 \n            periodSeconds :   60 \n          volumeMounts : \n             -  name :  flink - config - volume\n              mountPath :   / opt / flink / conf\n             -  name :  flink - data - volume\n              mountPath :   / flink - data\n             -  name :  flink - jar\n              mountPath :   / opt / flink / usrlib\n          securityContext : \n            runAsUser :   9999   # refers  to   user  _flink_ from official flink image ,  change  if  necessary\n      serviceAccountName :  flink - service - account #  Service  account which has the permissions  to   create ,  edit ,  delete  ConfigMaps \n      volumes : \n         -  name :  flink - config - volume\n          configMap : \n            name :  flink - config\n            items : \n               -  key :  flink - conf . yaml\n                path :  flink - conf . yaml\n               -  key :  log4j - console . properties\n                path :  log4j - console . properties\n         -  name :  flink - data - volume\n Flink  is designed  to   run  as a specific user  with   restricted  privileges . \n The  owner of the host path should be set  to   "flink"   with   the  user  ID   ( UID )  of  9999. \n          hostPath : \n            #本地创建的目录需修改用户，不然会报错 The  base directory of the  JobResultStore  isn\'t  accessible .  No  dirty  JobResults  can be restored\n            path :   / tmp / flink   # chown  9999 : 9999   - R   / tmp / flink\n            type :   Directory \n         -  name :  flink - jar\n          persistentVolumeClaim : \n            claimName :  flink - jar - pvc\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 taskmanager-job-deployment.yaml \n apiVersion :  apps/v1\n kind :  Deployment\n metadata : \n   name :  flink - taskmanager\n spec : \n   replicas :   1   # here, we configure the scale \n   selector : \n     matchLabels : \n       app :  flink\n       component :  taskmanager\n   template : \n     metadata : \n       #annotations: \n prometheus.io/port: \'9249\' \n prometheus.io/scrape: \'true\' \n       labels : \n         app :  flink\n         component :  taskmanager\n     spec : \n提前在容器内部映射域名 \n       hostAliases : \n         -   ip :  192.168.8.102\n           hostnames : \n             -   "node3" \n         -   ip :  192.168.8.103\n           hostnames : \n             -   "node2" \n         -   ip :  192.168.8.104\n           hostnames : \n             -   "node1" \n       containers : \n       -   name :  taskmanager\n         image :  personalharbor.com/bigdata/flink : 1.14.6 - scala_2.12\n         imagePullPolicy :  IfNotPresent\n         resources : \n           requests : \n             cpu :  500m\n           limits : \n             cpu :  1000m\n         env : \n         args :   [ "taskmanager" ] \n         ports : \n         -   containerPort :   6122 \n           name :  rpc\n         -   containerPort :   6125 \n           name :  query - state\n         livenessProbe : \n           tcpSocket : \n             port :   6122 \n           initialDelaySeconds :   30 \n           periodSeconds :   60 \n         volumeMounts : \n         -   name :  flink - config - volume\n           mountPath :  /opt/flink/conf/\n         -   name :  flink - data - volume\n           mountPath :  /flink - data\n         -   name :  flink - jar\n           mountPath :  /opt/flink/usrlib\n         securityContext : \n           runAsUser :   9999    # refers to user _flink_ from official flink image, change if necessary \n       serviceAccountName :  flink - service - account  # Service account which has the permissions to create, edit, delete ConfigMaps \n       volumes : \n       -   name :  flink - config - volume\n         configMap : \n           name :  flink - config\n           items : \n           -   key :  flink - conf.yaml\n             path :  flink - conf.yaml\n           -   key :  log4j - console.properties\n             path :  log4j - console.properties\n       -   name :  flink - data - volume\nFlink is designed to run as a specific user with restricted privileges. \nThe owner of the host path should be set to "flink" with the user ID (UID) of 9999. \n         hostPath : \n             #本地创建的目录需修改用户，不然会报错The base directory of the JobResultStore isn\'t accessible. No dirty JobResults can be restored \n           path :  /tmp/flink    # chown 9999:9999 -R /tmp/flink \n           type :  Directory\n       -   name :  flink - jar\n         persistentVolumeClaim : \n           claimName :  flink - jar - pvc\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 flink-configmap.yaml 这不是原先的conf文件 \n apiVersion :  v1\n kind :  ConfigMap\n metadata : \n   name :  flink - config\n   labels : \n     app :  flink\n data : \n   flink-conf.yaml :   | +\n     jobmanager.rpc.address :  flink - jobmanager\n     taskmanager.numberOfTaskSlots :   2 \n     blob.server.port :   6124 \n     jobmanager.rpc.port :   6123 \n     taskmanager.rpc.port :   6122 \n     jobmanager.memory.process.size :  4096m\n     taskmanager.memory.process.size :  4096m\n     taskmanager.memory.managed.fraction :   0.7 \n     state.backend.rocksdb.metrics.block-cache-capacity :   true \n     state.backend.rocksdb.metrics.block-cache-pinned-usage :   true \n     state.backend.rocksdb.metrics.block-cache-usage :   true \n     restart-strategy :  fixeddelay\n     restart-strategy.fixed-delay.attempts :   100000 \n     scheduler-mode :  reactive\n     heartbeat.timeout :   10000 \n     heartbeat.interval :   5000 \n     rest.flamegraph.enabled :   true \n     #生产选用非易失性存储 \n     state.savepoints.dir :  file : ///flink - data/savepoints\n     state.checkpoints.dir :  file : ///flink - data/checkpoints\n     kubernetes.cluster-id :  cluster1337\n     high-availability :  org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory\n     high-availability.storageDir :  file : ///flink - data/ha\n     execution.checkpointing.interval :   120000 \n     web.cancel.enable :   true \n     metrics.reporter.prom.class :  org.apache.flink.metrics.prometheus.PrometheusReporter\n     metrics.reporter.prom.port :   9249 \n     metrics.latency.interval :   1000 \n     metrics.latency.granularity :  operator\n     kubernetes.namespace :  flink\n     kubernetes.service-account :  flink - service - account\n   log4j-console.properties :   | +\nThis affects logging for both user code and Flink \n    rootLogger.level = INFO\n    rootLogger.appenderRef.console.ref = ConsoleAppender\n    rootLogger.appenderRef.rolling.ref = RollingFileAppender\nUncomment this if you want to _only_ change Flink\'s logging \n     #logger.flink.name = org.apache.flink \n     #logger.flink.level = INFO \nThe following lines keep the log level of common libraries/connectors on \nlog level INFO. The root logger does not override this. You have to manually \nchange the log levels here. \n    logger.akka.name = akka\n    logger.akka.level = INFO\n    logger.kafka.name= org.apache.kafka\n    logger.kafka.level = INFO\n    logger.hadoop.name = org.apache.hadoop\n    logger.hadoop.level = INFO\n    logger.zookeeper.name = org.apache.zookeeper\n    logger.zookeeper.level = INFO\nLog all infos to the console \n    appender.console.name = ConsoleAppender\n    appender.console.type = CONSOLE\n    appender.console.layout.type = PatternLayout\n    appender.console.layout.pattern = %d { yyyy - MM - dd HH : mm : ss , SSS }  % - 5p % - 60c %x  -  %m%n\nLog all infos in the given rolling file \n    appender.rolling.name = RollingFileAppender\n    appender.rolling.type = RollingFile\n    appender.rolling.append = false\n    appender.rolling.fileName = $ { sys : log.file } \n    appender.rolling.filePattern = $ { sys : log.file } .%i\n    appender.rolling.layout.type = PatternLayout\n    appender.rolling.layout.pattern = %d { yyyy - MM - dd HH : mm : ss , SSS }  % - 5p % - 60c %x  -  %m%n\n    appender.rolling.policies.type = Policies\n    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy\n    appender.rolling.policies.size.size=100MB\n    appender.rolling.strategy.type = DefaultRolloverStrategy\n    appender.rolling.strategy.max = 10\nSuppress the irrelevant (wrong) warnings from the Netty channel handler \n    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline\n    logger.netty.level = OFF    \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 jobmanager-service.yaml \n apiVersion :  v1\n kind :  Service\n metadata : \n   name :  flink - jobmanager\n spec : \n   type :  ClusterIP\n   ports : \n   -   name :  rpc\n     port :   6123 \n   -   name :  blob - server\n     port :   6124 \n   -   name :  webui\n     port :   8081 \n   selector : \n     app :  flink\n     component :  jobmanager\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 jobmanager-rest-service.yaml \n apiVersion :  v1\n kind :  Service\n metadata : \n   name :  flink - jobmanager - rest\n spec : \n   type :  NodePort\n   selector : \n     app :  flink\n     component :  jobmanager\n   ports : \n   -   name :  rest\n     port :   8081 \n     targetPort :   8081 \n     nodePort :   30091 \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 taskmanager-query-state-service.yaml \n apiVersion :  v1\n kind :  Service\n metadata : \n   name :  flink - taskmanager - query - state\n spec : \n   type :  NodePort\n   ports : \n     -   name :  query - state\n       port :   6125 \n       targetPort :   6125 \n       nodePort :   30025 \n   selector : \n     app :  flink\n     component :  taskmanager\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 taskmanager-autoscaler.yaml \n apiVersion :  autoscaling/v2\n kind :  HorizontalPodAutoscaler\n metadata : \n   name :  flink - taskmanager\n spec : \n   scaleTargetRef : \n     apiVersion :  apps/v1\n     kind :  Deployment\n     name :  flink - taskmanager\n   minReplicas :   1 \n   maxReplicas :   6 \n   metrics : \n     -   type :  Resource\n       resource : \n         name :  cpu\n         target : \n           type :  Utilization\n           averageUtilization :   60 \n     #- type: Pods \n pods: \n   metric: \n     name: state_usage \n   target: \n     type: AverageValue \n     averageValue: 0.6 \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #  3.使用helm一键部署 \n （不使用helm，上面的yaml文件要一个个执行） \n helm  install  flink-hpa flink-hpa/  -n  flink \n \n 1 4.查看target 变化、tm 副本数 \n \n \n 调整kafka生产数据的速率，查看缩放效果，为了方便快速查看效果，behavior设置窗口为0，立马生效 \n 扩容效果： \n \n 缩容效果： \n \n 问题1：使用pvc动态挂载自定义jar包，任务加载不到。容器内部映射路径：/opt/flink/jar \n Caused by: java.lang.ClassNotFoundException: com.gordon.hpa.HpaWordCountForCpu\n \n 1 解决1：flink的jar包是放在lib目录里面，如果挂载该目录，就会把该目录下原先的lib置空，所以挂载一个新目录，使用容器内部映射路径：/opt/flink/jar，依然报错，自定义目录不被识别，使用官方例子中的路径/opt/flink/usrlib。 \n 原因： \n 1.基础镜像创建了usrlib \n \n 2.源码中设定了可识别usrlib目录 \n \n usrlib和lib区别： \n #由系统默认加载的jar包 \nOne way is putting them  in  the  ` lib/ `  directory,  which  will result  in  the user code jar being loaded by the system classloader.\n #第三方和自定义jar包，代码中使用时加载 \nThe other way is creating a  ` usrlib/ `  directory  in  the parent directory of  ` lib/ `  and putting the user code jar  in  the  ` usrlib/ `  directory.\n \n 1 2 3 4 问题2：没提前建立账户及角色绑定报错： \n Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: GET at: https://10.10.0.1/api/v1/namespaces/default/pods?labelSelector=app%3Dkaibo-test%2Ccomponent%3Dtaskmanager%2Ctype%3Dflink-native-kubernetes. Message: Forbidden!Configured service account doesn\'t have access. Service account may have been revoked. pods is forbidden: User "system:serviceaccount:default:default" cannot list resource "pods" in API group "" in the namespace "default".\n \n 1 解决2：部署前，提前建立好角色。角色建立详见flink-service-account.yaml \n kubectl apply  -f  flink-service-account.yaml  -n  flink\n #或者使用命令行创建 \nkubectl create serviceaccount flink-service-account\nkubectl create clusterrolebinding flink-role-binding-flink  --clusterrole = edit  --serviceaccount = flink:flink-service-account\n \n 1 2 3 4 问题3：映射的目录所属用户的指定： \n #本地创建的目录需修改用户，不然会报错\nThe base directory of the JobResultStore isn\'t accessible. No dirty JobResults can be restored\n#或者\nMkdirs failed to create file\n \n 1 2 3 4 解决3： \n chown   9999 :9999  -R  /tmp/flink\n \n 1 #  基于flink 1.17版本实现 \n flink 1.17开始支持，1.15/1.16 需要添加 Job vertex parallelism overrides  (must have)才可以使用该功能。 \n 通过计算 \n pom \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 " \n          xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     < groupId > com.gordon </ groupId > \n     < artifactId > flink-on-k8s </ artifactId > \n     < version > 1.0-SNAPSHOT </ version > \n\n     < repositories > \n         < repository > \n             < id > central </ id > \n             < url > https://repo1.maven.org/maven2/ </ url > \n         </ repository > \n     </ repositories > \n\n     < properties > \n         < maven.compiler.source > 8 </ maven.compiler.source > \n         < maven.compiler.target > 8 </ maven.compiler.target > \n        \x3c!-- <flink.version>1.14.6</flink.version>--\x3e \n         < flink.version > 1.17.1 </ flink.version > \n         < java.version > 1.8 </ java.version > \n         < scala.binary.version > 2.12 </ scala.binary.version > \n         < slf4j.version > 1.7.30 </ slf4j.version > \n     </ properties > \n\n\n     < dependencies > \n         \x3c!-- 引入Flink相关依赖--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-java </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-java </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-clients </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-scala_${scala.binary.version} </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-connector-kafka </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n\n         \x3c!--1.14.6--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-clients_${scala.binary.version}</artifactId>\n            <version>${flink.version}</version>\n            <scope>provided</scope>\n        </dependency>--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-streaming-java_${scala.binary.version}</artifactId>\n            <version>${flink.version}</version>\n            <scope>provided</scope>\n        </dependency>--\x3e \n         \x3c!--kafka--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-connector-kafka_${scala.binary.version}</artifactId>\n            <version>${flink.version}</version>\n        </dependency>--\x3e \n\n         < dependency > \n             < groupId > org.apache.commons </ groupId > \n             < artifactId > commons-lang3 </ artifactId > \n             < version > 3.9 </ version > \n         </ dependency > \n\n         \x3c!-- 引入日志管理相关依赖--\x3e \n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-api </ artifactId > \n             < version > ${slf4j.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-log4j12 </ artifactId > \n             < version > ${slf4j.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.logging.log4j </ groupId > \n             < artifactId > log4j-to-slf4j </ artifactId > \n             < version > 2.14.0 </ version > \n         </ dependency > \n\n     </ dependencies > \n\n\n     < build > \n         < plugins > \n             \x3c!--docker部署插件--\x3e \n             \x3c!--<plugin>\n                <groupId>com.spotify</groupId>\n                <artifactId>docker-maven-plugin</artifactId>\n                <version>1.0.0</version>\n                <configuration>\n                    &lt;!&ndash;<dockerDirectory>src/main/docker</dockerDirectory>&ndash;&gt;\n                    <dockerDirectory>${project.basedir}</dockerDirectory>\n                    <resources>\n                        <resource>\n                            <targetPath>/</targetPath>\n                            <directory>${project.build.directory}</directory>\n                            <includes>${project.build.finalName}.jar</includes>\n                        </resource>\n                    </resources>\n                </configuration>\n            </plugin>\n            &lt;!&ndash;如果打包完成后,把构建结果复制到其他位置，可加如下插件&ndash;&gt;\n            <plugin>\n                <artifactId>maven-antrun-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <configuration>\n                            <tasks>\n                                <copy todir="${project.basedir}" file="target/${project.artifactId}-${project.version}.${project.packaging}"></copy>\n                            </tasks>\n                        </configuration>\n                        <goals>\n                            <goal>run</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>--\x3e \n             \x3c!-- 打包配置--\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-shade-plugin </ artifactId > \n                 < version > 2.3 </ version > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < goals > \n                             < goal > shade </ goal > \n                         </ goals > \n                         < configuration > \n                             < filters > \n                                 < filter > \n                                     < artifact > *:* </ artifact > \n                                     < excludes > \n                                         \x3c!--\n                                        zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --\x3e \n                                         < exclude > META-INF/*.SF </ exclude > \n                                         < exclude > META-INF/*.DSA </ exclude > \n                                         < exclude > META-INF/*.RSA </ exclude > \n                                     </ excludes > \n                                 </ filter > \n                             </ filters > \n                             < transformers > \n                                 < transformer   implementation = " org.apache.maven.plugins.shade.resource.ManifestResourceTransformer " > \n                                     \x3c!-- 设置jar包的入口类(可选) --\x3e \n                                     < mainClass > com.gordon.basic.StreamWordCount </ mainClass > \n                                 </ transformer > \n                             </ transformers > \n                         </ configuration > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n             \x3c!--<plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <version>3.0.0</version>\n                <configuration>\n                    <descriptorRefs>\n                        <descriptorRef>jar-with-dependencies</descriptorRef>\n                    </descriptorRefs>\n                </configuration>\n                <executions>\n                    <execution>\n                        <id>make-assembly</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>--\x3e \n\n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.8.0 </ version > \n                 < configuration > \n                     < source > ${java.version} </ source > \n                     < target > ${java.version} </ target > \n                     < encoding > UTF-8 </ encoding > \n                 </ configuration > \n             </ plugin > \n\n         </ plugins > \n     </ build > \n\n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 AutoscalingExample \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . sink . SinkFunction ; \n import   org . slf4j . Logger ; \n import   org . slf4j . LoggerFactory ; \n\n /** Autoscaling Example For Flink 1.17.1. */ \n public   class   AutoscalingExample   { \n     private   static   final   Logger   LOG   =   LoggerFactory . getLogger ( AutoscalingExample . class ) ; \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         long  numIterations  =   Long . parseLong ( args [ 0 ] ) ; \n         DataStream < Long >  stream  = \n                env . fromSequence ( Long . MIN_VALUE ,   Long . MAX_VALUE ) . filter ( i  ->   System . nanoTime ( )   >   1 ) ; \n        stream  = \n                stream . shuffle ( ) \n                         . map ( \n                                 new   RichMapFunction < Long ,   Long > ( )   { \n\n                                     @Override \n                                     public   Long   map ( Long  i )   throws   Exception   { \n                                         //LOG.info("LOG: current is {}",i); \n                                         //System.out.println(String.format("current is %s",i)); \n                                         long  end  =   0 ; \n                                         for   ( int  j  =   0 ;  j  <  numIterations ;  j ++ )   { \n                                            end  =   System . nanoTime ( ) ; \n                                         } \n                                         return  end ; \n                                     } \n                                 } ) ; \n        stream . addSink ( \n                 new   SinkFunction < Long > ( )   { \n                     @Override \n                     public   void   invoke ( Long  value ,   Context  context )   throws   Exception   { \n                         // Do nothing \n                     } \n                 } ) ; \n        env . execute ( "Autoscaling Example" ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   name :  autoscaling - example\n   namespace :  flink\n spec : \n   image :  personalharbor.com/bigdata/flink : 1.17.1 - scala_2.12\n   flinkVersion :  v1_17\n   imagePullPolicy :  IfNotPresent    # 镜像拉取策略，本地没有则从仓库拉取 \n   flinkConfiguration : \n     kubernetes.operator.job.autoscaler.enabled :   "true" \n     pipeline.max-parallelism :   "12" \n     kubernetes.operator.job.autoscaler.metrics.window :   "3m" \n     kubernetes.operator.job.autoscaler.stabilization.interval :   "1m" \n     kubernetes.operator.job.autoscaler.target.utilization :   "0.6" \n     kubernetes.operator.job.autoscaler.target.utilization.boundary :   "0.2" \n     kubernetes.operator.job.autoscaler.restart.time :   "2m" \n     kubernetes.operator.job.autoscaler.catch-up.duration :   "5m" \n     taskmanager.numberOfTaskSlots :   "4" \n     state.savepoints.dir :  file : ///flink - data/savepoints\n     state.checkpoints.dir :  file : ///flink - data/checkpoints\n     high-availability :  org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory\n     high-availability.storageDir :  file : ///flink - data/ha\n     execution.checkpointing.interval :   "1m" \n     web.cancel.enable :   "true" \n   serviceAccount :  flink\n   ingress :     # ingress配置，用于访问flink web页面 \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   jobManager : \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   taskManager : \n     resource : \n       memory :   "1024m" \n       cpu :   0.5 \n   podTemplate : \n     spec : \n提前在容器内部映射域名 \n       hostAliases : \n         -   ip :  192.168.8.102\n           hostnames : \n             -   "node3" \n         -   ip :  192.168.8.103\n           hostnames : \n             -   "node2" \n         -   ip :  192.168.8.104\n           hostnames : \n             -   "node1" \n       containers : \n         -   name :  flink - main - container\n           volumeMounts : \n             -   mountPath :  /flink - data\n               name :  flink - volume\n             -   mountPath :  /opt/flink/jar\n               name :  flink - jar\n           securityContext : \n             runAsUser :   9999    # refers to user _flink_ from official flink image, change if necessary \n       volumes : \n         -   name :  flink - jar\n           persistentVolumeClaim : \n             claimName :  flink - jar - pvc\n         -   name :  flink - volume\nFlink is designed to run as a specific user with restricted privileges. \nThe owner of the host path should be set to "flink" with the user ID (UID) of 9999. \n           hostPath : \n             #本地创建的目录需修改用户，不然会报错The base directory of the JobResultStore isn\'t accessible. No dirty JobResults can be restored \n             path :  /tmp/flink    # chown 9999:9999 -R /tmp/flink \n             type :  Directory\n   job : \n     jarURI :  local : ///opt/flink/jar/flink - on - k8s - 1.0 - SNAPSHOT.jar\n     entryClass :  com.gordon.hpa.AutoscalingExample\n     args :   [ "10" ] \n     parallelism :   1 \n     upgradeMode :  last - state\n     state :  running\n     savepointTriggerNonce :   0 \n   logConfiguration : \n     "log4j-console.properties" :   | \n      rootLogger.level = DEBUG\n      rootLogger.appenderRef.file.ref = LogFile\n      rootLogger.appenderRef.console.ref = LogConsole\n      appender.file.name = LogFile\n      appender.file.type = File\n      appender.file.append = false\n      appender.file.fileName = ${sys:log.file}\n      appender.file.layout.type = PatternLayout\n      appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n      appender.console.name = LogConsole\n      appender.console.type = CONSOLE\n      appender.console.layout.type = PatternLayout\n      appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n      logger.akka.name = akka\n      logger.akka.level = INFO\n      logger.kafka.name= org.apache.kafka\n      logger.kafka.level = INFO\n      logger.hadoop.name = org.apache.hadoop\n      logger.hadoop.level = INFO\n      logger.zookeeper.name = org.apache.zookeeper\n      logger.zookeeper.level = INFO\n      logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline\n      logger.netty.level = OFF \n\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 源码解读： \n JobVertexScaler \n 介绍 \n Component  responsible  for  computing vertex parallelism based on the scaling metrics . \n \n 1 目标并行度的计算 \n      public   int   computeScaleTargetParallelism ( \n             AbstractFlinkResource < ? ,   ? >  resource , \n             Configuration  conf , \n             JobVertexID  vertex , \n             Map < ScalingMetric ,   EvaluatedScalingMetric >  evaluatedMetrics , \n             SortedMap < Instant ,   ScalingSummary >  history )   { \n\t\t //获取当前的并行度 \n         var  currentParallelism  =   ( int )  evaluatedMetrics . get ( PARALLELISM ) . getCurrent ( ) ; \n         //根据收集的指标，获取到平均每秒处理的record条数 \n         double  averageTrueProcessingRate  =  evaluatedMetrics . get ( TRUE_PROCESSING_RATE ) . getAverage ( ) ; \n        \n\t\t //targetCapacity =Math.round(lagCatchupTargetRate + restartCatchupRate + inputTargetAtUtilization) \n         //目标的处理能力等于（当前能力不足+缩放重启）导致的数据滞后需要处理的额外能力+基于当前输入的处理能力 \n         double  targetCapacity  = \n                 AutoScalerUtils . getTargetProcessingCapacity ( \n                        evaluatedMetrics ,  conf ,  conf . get ( TARGET_UTILIZATION ) ,   true ) ; \n         //计算出来的缩放因子，限定在minScaleFactor<=scaleFactor<=maxScaleFactor范围内。 \n         double  scaleFactor  =  targetCapacity  /  averageTrueProcessingRate ; \n         double  minScaleFactor  =   1   -  conf . get ( MAX_SCALE_DOWN_FACTOR ) ; \n         double  maxScaleFactor  =   1   +  conf . get ( MAX_SCALE_UP_FACTOR ) ; \n         if   ( scaleFactor  <  minScaleFactor )   { \n             LOG . debug ( \n                     "Computed scale factor of {} for {} is capped by maximum scale down factor to {}" , \n                    scaleFactor , \n                    vertex , \n                    minScaleFactor ) ; \n            scaleFactor  =  minScaleFactor ; \n         }   else   if   ( scaleFactor  >  maxScaleFactor )   { \n             LOG . debug ( \n                     "Computed scale factor of {} for {} is capped by maximum scale up factor to {}" , \n                    scaleFactor , \n                    vertex , \n                    maxScaleFactor ) ; \n            scaleFactor  =  maxScaleFactor ; \n         } \n\n         // 重新计算出满足限定范围的TargetCapacity \n         double  cappedTargetCapacity  =  averageTrueProcessingRate  *  scaleFactor ; \n         LOG . debug ( "Capped target processing capacity for {} is {}" ,  vertex ,  cappedTargetCapacity ) ; \n        \n\t\t //简单的理解 \n         //int newParallelism =(int) Math.min(Math.ceil(scaleFactor * parallelism), Integer.MAX_VALUE) \n         //最终的newParallelism，还会做其他校验和调整。 \n        \n         int  newParallelism  = \n                 scale ( \n                        currentParallelism , \n                         ( int )  evaluatedMetrics . get ( MAX_PARALLELISM ) . getCurrent ( ) , \n                        scaleFactor , \n                        conf . getInteger ( VERTEX_MIN_PARALLELISM ) , \n                        conf . getInteger ( VERTEX_MAX_PARALLELISM ) ) ; \n  \n         if   ( newParallelism  ==  currentParallelism\n                 ||   blockScalingBasedOnPastActions ( \n                        resource , \n                        vertex , \n                        conf , \n                        evaluatedMetrics , \n                        history , \n                        currentParallelism , \n                        newParallelism ) )   { \n             return  currentParallelism ; \n         } \n\n         // We record our expectations for this scaling operation \n        evaluatedMetrics . put ( \n                 ScalingMetric . EXPECTED_PROCESSING_RATE , \n                 EvaluatedScalingMetric . of ( cappedTargetCapacity ) ) ; \n         return  newParallelism ; \n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 以下为动态扩容过程示例： \n 任务刚开始启动： \n \n 扩容日志： \n \n 动态扩容后： \n 六、beam模型top on flink run k8s \n 基于flink k8s operator api即flink需要1.13以上，小于该版本的使用原生k8s api。 \n 轻松入门 \n 使用官方例子WordCount \n 新建项目flink-beam-example，修改pom，新建Dockfile文件，构建镜像 \n \x3c!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n"License"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n--\x3e \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     \x3c!--<parent>\n        <groupId>org.apache.flink</groupId>\n        <artifactId>flink-kubernetes-operator-parent</artifactId>\n        <version>1.5.0</version>\n        <relativePath>../..</relativePath>\n    </parent>--\x3e \n     < groupId > org.apache.flink </ groupId > \n     < artifactId > flink-beam-example </ artifactId > \n     < version > 1.5.0 </ version > \n     < name > Flink Beam Example </ name > \n\n     \x3c!-- Given that this is an example skip maven deployment --\x3e \n     < properties > \n         < maven.deploy.skip > true </ maven.deploy.skip > \n         < beam.version > 2.47.0 </ beam.version > \n\n         < slf4j.version > 1.7.36 </ slf4j.version > \n         < log4j.version > 2.17.1 </ log4j.version > \n     </ properties > \n\n     < repositories > \n         < repository > \n             < id > confluent </ id > \n             < url > https://packages.confluent.io/maven/ </ url > \n         </ repository > \n     </ repositories > \n\n     < dependencies > \n         \x3c!-- Apache Beam dependencies --\x3e \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-core </ artifactId > \n             < version > ${beam.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-examples-java </ artifactId > \n             < version > ${beam.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > org.springframework </ groupId > \n                     < artifactId > spring-jcl </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-flink-1.14 </ artifactId > \n             < version > ${beam.version} </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-google-cloud-platform </ artifactId > \n             < version > ${beam.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > junit </ groupId > \n                     < artifactId > junit </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         \x3c!-- Add logging framework, to produce console output when running in the IDE. --\x3e \n         \x3c!-- These dependencies are excluded from the application JAR by default. --\x3e \n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-api </ artifactId > \n             < version > ${slf4j.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.logging.log4j </ groupId > \n             < artifactId > log4j-slf4j-impl </ artifactId > \n             < version > ${log4j.version} </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.logging.log4j </ groupId > \n             < artifactId > log4j-api </ artifactId > \n             < version > ${log4j.version} </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.logging.log4j </ groupId > \n             < artifactId > log4j-core </ artifactId > \n             < version > ${log4j.version} </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n     </ dependencies > \n\n     < build > \n         < plugins > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-shade-plugin </ artifactId > \n                 < version > 3.1.1 </ version > \n                 < executions > \n                     \x3c!-- Run shade goal on package phase --\x3e \n                     < execution > \n                         < phase > package </ phase > \n                         < goals > \n                             < goal > shade </ goal > \n                         </ goals > \n                         < configuration > \n                             < artifactSet > \n                                 < excludes > \n                                     < exclude > org.apache.flink:flink-shaded-force-shading </ exclude > \n                                     < exclude > com.google.code.findbugs:jsr305 </ exclude > \n                                     < exclude > org.slf4j:* </ exclude > \n                                     < exclude > org.apache.logging.log4j:* </ exclude > \n                                 </ excludes > \n                             </ artifactSet > \n                             < filters > \n                                 < filter > \n                                     \x3c!-- Do not copy the signatures in the META-INF folder.\n                                    Otherwise, this might cause SecurityExceptions when using the JAR. --\x3e \n                                     < artifact > *:* </ artifact > \n                                     < excludes > \n                                         < exclude > META-INF/versions/9/module-info.class </ exclude > \n                                         < exclude > META-INF/*.SF </ exclude > \n                                         < exclude > META-INF/*.DSA </ exclude > \n                                         < exclude > META-INF/*.RSA </ exclude > \n                                     </ excludes > \n                                 </ filter > \n                             </ filters > \n                             < transformers > \n                                 < transformer   implementation = " org.apache.maven.plugins.shade.resource.ServicesResourceTransformer " /> \n                             </ transformers > \n                         </ configuration > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n         </ plugins > \n     </ build > \n </ project > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 ################################################################################ \n Licensed to the Apache Software Foundation (ASF) under one \n or more contributor license agreements.  See the NOTICE file \n distributed with this work for additional information \n regarding copyright ownership.  The ASF licenses this file \n to you under the Apache License, Version 2.0 (the \n "License"); you may not use this file except in compliance \n with the License.  You may obtain a copy of the License at \n\n     http://www.apache.org/licenses/LICENSE-2.0 \n\n Unless required by applicable law or agreed to in writing, software \n distributed under the License is distributed on an "AS IS" BASIS, \n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n See the License for the specific language governing permissions and \nlimitations under the License. \n ################################################################################ \n #FROM flink:1.14.3 #没私库拉官方镜像 \n FROM  personalharbor.com/bigdata/flink:1.14.6-scala_2.12 \n\n RUN  mkdir /opt/flink/usrlib \n ADD  target/flink-beam-example-*.jar /opt/flink/usrlib/beam-runner.jar \n\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 创建部署beam-example.yaml文件 \n ################################################################################ \n Licensed to the Apache Software Foundation (ASF) under one \n or more contributor license agreements.  See the NOTICE file \n distributed with this work for additional information \n regarding copyright ownership.  The ASF licenses this file \n to you under the Apache License, Version 2.0 (the \n "License"); you may not use this file except in compliance \n with the License.  You may obtain a copy of the License at \n\n     http://www.apache.org/licenses/LICENSE-2.0 \n\n Unless required by applicable law or agreed to in writing, software \n distributed under the License is distributed on an "AS IS" BASIS, \n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n See the License for the specific language governing permissions and \nlimitations under the License. \n ################################################################################ \n\n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   name :  beam - example\n spec : \n   image :  personalharbor.com/bigdata/flink - beam - example : latest\n   flinkVersion :  v1_14\n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "1" \n   serviceAccount :  flink\n   jobManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   taskManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   job : \n     entryClass :  org.apache.beam.examples.WordCount\n     jarURI :  local : ///opt/flink/usrlib/beam - runner.jar\n     args :   [   "--runner=FlinkRunner" ,   "--output=file://opt/output.txt"   ] \n     parallelism :   1 \n     upgradeMode :  stateless\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 运行 \n kubectl apply  - f beam - example.yaml  - n flink \n \n 1 #  自建项目--体验流批一体编程 \n pom \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 " \n          xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     < groupId > com.gordon </ groupId > \n     < artifactId > apache-beam </ artifactId > \n     < version > 1.0-SNAPSHOT </ version > \n\n     < repositories > \n         < repository > \n             < id > central </ id > \n             < url > https://repo1.maven.org/maven2/ </ url > \n         </ repository > \n     </ repositories > \n\n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n         < apache.beam.version > 2.47.0 </ apache.beam.version > \n         < commons.io.version > 2.8.0 </ commons.io.version > \n         < flink.version > 1.14.6 </ flink.version > \n         < spark.version > 3.1.2 </ spark.version > \n         < jackson.version > 2.14.1 </ jackson.version > \n     </ properties > \n     \x3c!--第一版本管理--\x3e \n     < dependencyManagement > \n         < dependencies > \n             < dependency > \n                 < groupId > commons-io </ groupId > \n                 < artifactId > commons-io </ artifactId > \n                 < version > ${commons.io.version} </ version > \n             </ dependency > \n             < dependency > \n                 < groupId > com.fasterxml.jackson.core </ groupId > \n                 < artifactId > jackson-databind </ artifactId > \n                 < version > ${jackson.version} </ version > \n             </ dependency > \n             < dependency > \n                 < groupId > com.fasterxml.jackson.core </ groupId > \n                 < artifactId > jackson-core </ artifactId > \n                 < version > ${jackson.version} </ version > \n             </ dependency > \n\n         </ dependencies > \n     </ dependencyManagement > \n\n     < dependencies > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-core </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-direct-java </ artifactId > \n             < version > ${apache.beam.version} </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-flink-1.14 </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-spark-3 </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-jdbc </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > mysql </ groupId > \n             < artifactId > mysql-connector-java </ artifactId > \n             < version > 5.1.48 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > com.mchange </ groupId > \n             < artifactId > c3p0 </ artifactId > \n             < version > 0.9.5.4 </ version > \n         </ dependency > \n\n         \x3c!-- https://mvnrepository.com/artifact/org.apache.beam/beam-sdks-java-io-hcatalog --\x3e \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-hcatalog </ artifactId > \n             < version > ${apache.beam.version} </ version > \n\n         </ dependency > \n\n         \x3c!-- https://mvnrepository.com/artifact/org.apache.hive.hcatalog/hive-hcatalog-core --\x3e \n         < dependency > \n             < groupId > org.apache.hive.hcatalog </ groupId > \n             < artifactId > hive-hcatalog-core </ artifactId > \n             < version > 2.1.0 </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > org.apache.calcite </ groupId > \n                     < artifactId > calcite-avatica </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-hadoop-format </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n\n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-kafka </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.kafka </ groupId > \n             < artifactId > kafka-clients </ artifactId > \n             < version > 2.4.1 </ version > \n         </ dependency > \n\n\n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-examples-java </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         \x3c!-- https://mvnrepository.com/artifact/commons-cli/commons-cli --\x3e \n         < dependency > \n             < groupId > commons-cli </ groupId > \n             < artifactId > commons-cli </ artifactId > \n             < version > 1.4 </ version > \n         </ dependency > \n\n         \x3c!-- https://mvnrepository.com/artifact/commons-io/commons-io --\x3e \n         < dependency > \n             < groupId > commons-io </ groupId > \n             < artifactId > commons-io </ artifactId > \n         </ dependency > \n\n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-clients_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n         \x3c!--1.12--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-runtime_2.12</artifactId>\n            <version>${flink.version}</version>\n            <scope>provided</scope>\n        </dependency>--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-runtime </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-java_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n\n\n\n\n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-core_2.12 </ artifactId > \n             < version > ${spark.version} </ version > \n             < scope > provided </ scope > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.module </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-streaming_2.12 </ artifactId > \n             < version > ${spark.version} </ version > \n             < scope > provided </ scope > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > com.fasterxml.jackson.core </ groupId > \n             < artifactId > jackson-core </ artifactId > \n         </ dependency > \n\n         < dependency > \n             < groupId > com.fasterxml.jackson.core </ groupId > \n             < artifactId > jackson-databind </ artifactId > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-extensions-sql </ artifactId > \n             < version > ${apache.beam.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > jackson-databind </ artifactId > \n                 </ exclusion > \n                 < exclusion > \n                     < artifactId > jackson-annotations </ artifactId > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n\n     </ dependencies > \n\n     < build > \n         < plugins > \n\n             \x3c!-- 编译插件 --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.5.1 </ version > \n                 < configuration > \n                     < source > 1.8 </ source > \n                     < target > 1.8 </ target > \n                     \x3c!--<encoding>${project.build.sourceEncoding}</encoding>--\x3e \n                 </ configuration > \n             </ plugin > \n\n             \x3c!-- 打包插件(会包含所有依赖) --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-shade-plugin </ artifactId > \n                 < version > 2.3 </ version > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < goals > \n                             < goal > shade </ goal > \n                         </ goals > \n                         < configuration > \n                             < filters > \n                                 < filter > \n                                     < artifact > *:* </ artifact > \n                                     < excludes > \n                                         \x3c!--\n                                        zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --\x3e \n                                         < exclude > META-INF/*.SF </ exclude > \n                                         < exclude > META-INF/*.DSA </ exclude > \n                                         < exclude > META-INF/*.RSA </ exclude > \n                                     </ excludes > \n                                 </ filter > \n                             </ filters > \n                             < transformers > \n                                 < transformer   implementation = " org.apache.maven.plugins.shade.resource.ManifestResourceTransformer " > \n                                     \x3c!-- 设置jar包的入口类(可选) --\x3e \n                                     < mainClass > com.gordon.docker.HelloDocker </ mainClass > \n                                 </ transformer > \n                             </ transformers > \n                         </ configuration > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n\n             \x3c!--docker部署插件--\x3e \n             \x3c!--<plugin>\n                <groupId>com.spotify</groupId>\n                <artifactId>docker-maven-plugin</artifactId>\n                <version>1.0.0</version>\n                <configuration>\n                    <dockerDirectory>src/main/docker</dockerDirectory>\n                    <resources>\n                        <resource>\n                            <targetPath>/</targetPath>\n                            <directory>${project.build.directory}</directory>\n                            <include>${project.build.finalName}.jar</include>\n                        </resource>\n                    </resources>\n                </configuration>\n            </plugin>\n            &lt;!&ndash;打包完成后,把构建结果复制到其他位置&ndash;&gt;\n            <plugin>\n                <artifactId>maven-antrun-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <configuration>\n                            <tasks>\n                                <copy todir="src/main/docker" file="target/${project.artifactId}-${project.version}.${project.packaging}"></copy>\n                            </tasks>\n                        </configuration>\n                        <goals>\n                            <goal>run</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>--\x3e \n\n         </ plugins > \n     </ build > \n\n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 AboutText \n import   com . gordon . uniform . common . WriteOneFilePerWindow ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . TextIO ; \n import   org . apache . beam . sdk . io . kafka . KafkaIO ; \n import   org . apache . beam . sdk . options . PipelineOptions ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . Create ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . Values ; \n import   org . apache . beam . sdk . transforms . windowing . FixedWindows ; \n import   org . apache . beam . sdk . transforms . windowing . Window ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . apache . beam . vendor . guava . v26_0_jre . com . google . common . collect . ImmutableMap ; \n import   org . apache . kafka . common . serialization . LongDeserializer ; \n import   org . apache . kafka . common . serialization . StringDeserializer ; \n import   org . joda . time . Duration ; \n import   org . slf4j . Logger ; \n import   org . slf4j . LoggerFactory ; \n\n import   java . util . Arrays ; \n import   java . util . List ; \n\n public   class   AboutText   { \n     private   static   final   Logger   LOG   =   LoggerFactory . getLogger ( AboutText . class ) ; \n     // Create a Java Collection, in this case a List of Strings. \n     static   final   List < String >   LINES   =   Arrays . asList ( \n             "To be, or not to be: that is the question: " , \n             "Whether \'tis nobler in the mind to suffer " , \n             "The slings and arrows of outrageous fortune, " , \n             "Or to take arms against a sea of troubles, " ) ; \n\n     public   static   void   main ( String [ ]  args )   { \n\n\n         //todo 1.创建pipline \n         //MyOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(MyOptions.class); \n         PipelineOptions  options  =   PipelineOptionsFactory . create ( ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         //todo 2.读取数据源 \n         //todo  一般测试使用，由List生成一个PCollection \n         PCollection < String >  from_list  =  pipeline . apply ( "from List" ,   Create . of ( LINES ) ) ; \n        from_list . apply ( "Print elements" , \n                 MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                     LOG . warn ( "line is {}" , x ) ; \n                     System . out . println ( x ) ; \n                     return  x ; \n                 } ) ) ; \n\n\n         //todo  读取text file \n\n         PCollection < String >  from_text  =  pipeline . apply ( TextIO . read ( ) . from ( "input/input.txt" ) ) ; \n        from_text\n                 . apply ( "Print elements" , \n                         MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                             System . out . println ( x ) ; \n                             LOG . warn ( "input is {}" , x ) ; \n                             return  x ; \n                         } ) ) ; \n\n         //如何控制生成的文件数？ \n         //from_text.apply("write_text", TextIO.write().to(options.getOutput())); \n\n         //以streaming的方式监控文件下有无新文件，有则读取 \n\n         /*PCollection<String> from_text = pipeline.apply(TextIO.read()\n                .from(options.getInput())\n                .watchForNewFiles(\n                        // Check for new files every ten seconds\n                        Duration.standardSeconds(10),\n                        // Stop watching the filepattern if no new files appear within an minute\n                        afterTimeSinceNewOutput(Duration.standardMinutes(1)))\n        );\n\n        from_text.apply("Print elements",\n                MapElements.into(TypeDescriptors.strings()).via(x -> {\n                    System.out.println(x);\n                    return x;\n                }));*/ \n\n\n         //Must use windowed writes when applying WriteFiles to an unbounded PCollection \n         //流式数据写text file按时间分目录 \n        pipeline . apply ( KafkaIO . < Long ,   String > read ( ) . withBootstrapServers ( "node1:9092,node2:9092,node3:9092" ) \n                 . withTopic ( "hpa-test" ) \n                 . withKeyDeserializer ( LongDeserializer . class ) \n                 . withValueDeserializer ( StringDeserializer . class ) \n                 . withConsumerConfigUpdates ( ImmutableMap . of ( \n                         "group.id" ,   "beam-kafka" , \n                         "auto.offset.reset" ,   "latest" , \n                         "enable.auto.commit" ,   "true" \n                 ) ) \n                 . withoutMetadata ( ) ) \n                 . apply ( Values . < String > create ( ) ) \n                 . apply ( Window . < String > into ( FixedWindows . of ( Duration . standardMinutes ( 1 ) ) ) ) \n                 . apply ( new   WriteOneFilePerWindow ( "streamsink/part" , 2 ) ) ; \n                 //.apply(TextIO.write().to("streamsink/part").withShardNameTemplate(ShardNameTemplate.DIRECTORY_CONTAINER).withWindowedWrites().withNumShards(2)); \n\n         //如果读取大量文件，建议使用TextIO.Read.withHintMatchesManyFiles()提升性能 \n         // todo from_mysql \n\n         // todo from_hive \n\n         // todo kafka \n\n         // todo hadoop \n\n\n         //todo 4.运行 \n        pipeline . run ( ) . waitUntilFinish ( ) ; \n\n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 按时间窗口落地文件 \n import   static   org . apache . beam . vendor . guava . v26_0_jre . com . google . common . base . MoreObjects . firstNonNull ; \n\n import   javax . annotation . Nullable ; \n import   org . apache . beam . sdk . io . FileBasedSink ; \n import   org . apache . beam . sdk . io . FileBasedSink . FilenamePolicy ; \n import   org . apache . beam . sdk . io . FileBasedSink . OutputFileHints ; \n import   org . apache . beam . sdk . io . TextIO ; \n import   org . apache . beam . sdk . io . fs . ResolveOptions . StandardResolveOptions ; \n import   org . apache . beam . sdk . io . fs . ResourceId ; \n import   org . apache . beam . sdk . transforms . DoFn ; \n import   org . apache . beam . sdk . transforms . PTransform ; \n import   org . apache . beam . sdk . transforms . windowing . BoundedWindow ; \n import   org . apache . beam . sdk . transforms . windowing . IntervalWindow ; \n import   org . apache . beam . sdk . transforms . windowing . PaneInfo ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . PDone ; \n\n\n import   java . time . Instant ; \n import   java . time . ZoneId ; \n import   java . time . format . DateTimeFormatter ; \n\n /**\n * A {@link DoFn} that writes elements to files with names deterministically derived from the lower\n * and upper bounds of their key (an {@link IntervalWindow}).\n *\n * <p>This is test utility code, not for end-users, so examples can be focused on their primary\n * lessons.\n */ \n public   class   WriteOneFilePerWindow   extends   PTransform < PCollection < String > ,   PDone >   { \n   private   static   final   DateTimeFormatter   FORMATTER_WINDOW   =   java . time . format . DateTimeFormatter . ofPattern ( "HH-mm" ) . withZone ( ZoneId . of ( "Asia/Shanghai" ) ) ; \n   private   static   final   DateTimeFormatter   FORMATTER_HOUR   =   java . time . format . DateTimeFormatter . ofPattern ( "yyyy-MM-dd/HH" ) . withZone ( ZoneId . of ( "Asia/Shanghai" ) ) ; \n   private   String  filenamePrefix ; \n   @Nullable   private   Integer  numShards ; \n\n   public   WriteOneFilePerWindow ( String  filenamePrefix ,   Integer  numShards )   { \n     this . filenamePrefix  =  filenamePrefix ; \n     this . numShards  =  numShards ; \n   } \n\n   @Override \n   public   PDone   expand ( PCollection < String >  input )   { \n     ResourceId  resource  =   FileBasedSink . convertToFileResourceIfPossible ( filenamePrefix ) ; \n     TextIO . Write  write  = \n         TextIO . write ( ) \n             . to ( new   PerWindowFiles ( resource ) ) \n             . withTempDirectory ( resource . getCurrentDirectory ( ) ) \n             . withWindowedWrites ( ) ; \n     if   ( numShards  !=   null )   { \n      write  =  write . withNumShards ( numShards ) ; \n     } \n     return  input . apply ( write ) ; \n   } \n\n   /**\n   * A {@link FilenamePolicy} produces a base file name for a write based on metadata about the data\n   * being written. This always includes the shard number and the total number of shards. For\n   * windowed writes, it also includes the window and pane index (a sequence number assigned to each\n   * trigger firing).\n   */ \n   public   static   class   PerWindowFiles   extends   FilenamePolicy   { \n\n     private   final   ResourceId  baseFilename ; \n\n     public   PerWindowFiles ( ResourceId  baseFilename )   { \n       this . baseFilename  =  baseFilename ; \n     } \n\n     public   String   filenamePrefixForWindow ( IntervalWindow  window )   { \n       String  prefix  = \n          baseFilename . isDirectory ( )   ?   ""   :   firstNonNull ( baseFilename . getFilename ( ) ,   "" ) ; \n       return   String . format ( \n           "%s/%s-%s" ,    FORMATTER_HOUR . format ( Instant . ofEpochMilli ( window . start ( ) . getMillis ( ) ) ) , prefix ,   FORMATTER_WINDOW . format ( Instant . ofEpochMilli ( window . end ( ) . getMillis ( ) ) ) ) ; \n     } \n\n     @Override \n     public   ResourceId   windowedFilename ( \n         int  shardNumber , \n         int  numShards , \n         BoundedWindow  window , \n         PaneInfo  paneInfo , \n         OutputFileHints  outputFileHints )   { \n       IntervalWindow  intervalWindow  =   ( IntervalWindow )  window ; \n       String  filename  = \n           String . format ( \n               "%s-%s-of-%s%s" , //prefix：设计后是filenamePrefixForWindow,第几个分片数，共几个分片数,后缀 \n               filenamePrefixForWindow ( intervalWindow ) , \n              shardNumber , \n              numShards , \n              outputFileHints . getSuggestedFilenameSuffix ( ) ) ; \n       return  baseFilename\n           . getCurrentDirectory ( ) \n           . resolve ( filename ,   StandardResolveOptions . RESOLVE_FILE ) ; \n     } \n\n     @Override \n     public   ResourceId   unwindowedFilename ( \n         int  shardNumber ,   int  numShards ,   OutputFileHints  outputFileHints )   { \n       throw   new   UnsupportedOperationException ( "Unsupported." ) ; \n     } \n   } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 部署yaml \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   namespace :  flink\n   name :  beam - example\n spec : \n   image :  personalharbor.com/bigdata/flink : 1.14.6 - scala_2.12\n   flinkVersion :  v1_14\n   imagePullPolicy :  IfNotPresent    # 镜像拉取策略，本地没有则从仓库拉取 \n   ingress :     # ingress配置，用于访问flink web页面 \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "1" \n   serviceAccount :  flink\n   jobManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   taskManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   podTemplate : \n     spec : \n       containers : \n         -   name :  flink - main - container\n           volumeMounts : \n             -   name :  flink - data - volume\n               mountPath :  /flink - data\n             -   name :  flink - jar   # 挂载nfs上的jar \n               #创建一个flink目录下的空白文件夹，来放置 \n               #如果/opt/flink,会把该目录下的文件全部置空，来进行挂载，清空了安装包，程序启动不了 \n               mountPath :  /opt/flink/usrlib\n       volumes : \n         -   name :  flink - data - volume\nFlink is designed to run as a specific user with restricted privileges. \nThe owner of the host path should be set to "flink" with the user ID (UID) of 9999. \n           hostPath : \n             #本地创建的目录需修改用户，不然会报错The base directory of the JobResultStore isn\'t accessible. No dirty JobResults can be restored \n             path :  /tmp/flink    # chown 9999:9999 -R /tmp/flink \n             type :  Directory\n         -   name :  flink - jar\n           persistentVolumeClaim : \n             claimName :  flink - jar - pvc\n   job : \n     entryClass :  com.gordon.source_sink.AboutText\n     jarURI :  local : ///opt/flink/usrlib/apache - beam - 1.0 - SNAPSHOT.jar\n     args :   [   "--runner=org.apache.beam.runners.flink.FlinkRunner " , "--input=/flink-data/input/input.txt" ,   "--output=/flink-data/output/output"   ] \n     parallelism :   1 \n     upgradeMode :  stateless\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 About Kafka \n \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . runners . flink . FlinkRunner ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . kafka . KafkaIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . Values ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . apache . beam . vendor . guava . v26_0_jre . com . google . common . collect . ImmutableMap ; \n import   org . apache . flink . api . java . utils . ParameterTool ; \n import   org . apache . kafka . common . serialization . StringDeserializer ; \n import   org . apache . kafka . common . serialization . StringSerializer ; \n import   org . slf4j . Logger ; \n import   org . slf4j . LoggerFactory ; \n\n public   class   AboutKafka   { \n     private   static   final   Logger   LOG   =   LoggerFactory . getLogger ( AboutKafka . class ) ; \n     public   static   void   main ( String [ ]  args )   { \n\n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n        options . setRunner ( FlinkRunner . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         System . out . println ( "options.getRunner() = "   +  options . getRunner ( ) ) ; \n         final   ParameterTool  params  =   ParameterTool . fromArgs ( args ) ; \n         String  kafkaTopic  =  params . get ( "kafka-topic" ,   "hpa-test" ) ; \n         //todo KafkaIO \n         PCollection < String >  from_kafka  =  pipeline\n                 . apply ( KafkaIO . < String ,   String > read ( ) \n                         . withBootstrapServers ( "node1:9092,node2:9092,node3:9092" ) \n                         . withTopic ( kafkaTopic )    // use withTopics(List<String>) to read from multiple topics. \n                         . withKeyDeserializer ( StringDeserializer . class ) \n                         . withValueDeserializer ( StringDeserializer . class ) \n                         // Rest of the settings are optional : \n\n                         // you can further customize KafkaConsumer used to read the records by adding more \n                         // settings for ConsumerConfig. e.g : \n                         . withConsumerConfigUpdates ( ImmutableMap . of ( \n                                 "group.id" ,   "beam-kafka" , \n                                 "auto.offset.reset" ,   "latest" , \n                                 "enable.auto.commit" ,   "true" \n                         ) ) \n                         . withoutMetadata ( ) \n\n                 ) . apply ( Values . create ( ) ) \n                 . apply ( "from_kafka" ,   MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                     LOG . info ( "currnet value is: {}" , x ) ; \n                     System . out . println ( x ) ; \n                     return  x ; \n                 } ) ) ; \n\n\n         /*from_kafka.apply(KafkaIO.<Void, String>write()\n                        .withBootstrapServers("node1:9092,node2:9092,node3:9092")\n                        .withTopic("test01")  // use withTopics(List<String>) to read from multiple topics.\n                        .withValueSerializer(StringSerializer.class) // just need serializer for value\n                        .values());*/ \n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 beam-from-kafka.yaml \n \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   namespace :  flink\n   name :  beam - from - kafka\n spec : \n   image :  personalharbor.com/bigdata/flink : 1.14.6 - scala_2.12\n   flinkVersion :  v1_14\n   imagePullPolicy :  IfNotPresent    # 镜像拉取策略，本地没有则从仓库拉取 \n   ingress :     # ingress配置，用于访问flink web页面 \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "2" \n     web.cancel.enable :   "true" \n   serviceAccount :  flink\n   jobManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   taskManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   podTemplate : \n     spec : \n提前在容器内部映射域名 \n       hostAliases : \n         -   ip :  192.168.8.102\n           hostnames : \n             -   "node3" \n         -   ip :  192.168.8.103\n           hostnames : \n             -   "node2" \n         -   ip :  192.168.8.104\n           hostnames : \n             -   "node1" \n       containers : \n         -   name :  flink - main - container\n           volumeMounts : \n             -   name :  flink - data - volume\n               mountPath :  /flink - data\n             -   name :  flink - jar   # 挂载nfs上的jar \n               #创建一个flink目录下的空白文件夹，来放置 \n               #如果/opt/flink,会把该目录下的文件全部置空，来进行挂载，清空了安装包，程序启动不了 \n               mountPath :  /opt/flink/usrlib\n       volumes : \n         -   name :  flink - data - volume\nFlink is designed to run as a specific user with restricted privileges. \nThe owner of the host path should be set to "flink" with the user ID (UID) of 9999. \n           hostPath : \n             #本地创建的目录需修改用户，不然会报错The base directory of the JobResultStore isn\'t accessible. No dirty JobResults can be restored \n             path :  /tmp/flink    # chown 9999:9999 -R /tmp/flink \n             type :  Directory\n         -   name :  flink - jar\n           persistentVolumeClaim : \n             claimName :  flink - jar - pvc\n   job : \n     entryClass :  com.gordon.source_sink.AboutKafka\n     jarURI :  local : ///opt/flink/usrlib/apache - beam - 1.0 - SNAPSHOT.jar\n     args :   [   "--runner=org.apache.beam.runners.flink.FlinkRunner " ] \n     parallelism :   1 \n     upgradeMode :  stateless\n   logConfiguration : \n     "log4j-console.properties" :   | \n      rootLogger.level = INFO\n      rootLogger.appenderRef.file.ref = LogFile\n      rootLogger.appenderRef.console.ref = LogConsole\n      appender.file.name = LogFile\n      appender.file.type = File\n      appender.file.append = false\n      appender.file.fileName = ${sys:log.file}\n      appender.file.layout.type = PatternLayout\n      appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n      appender.console.name = LogConsole\n      appender.console.type = CONSOLE\n      appender.console.layout.type = PatternLayout\n      appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n      logger.akka.name = akka\n      logger.akka.level = WARN\n      logger.kafka.name= org.apache.kafka\n      logger.kafka.level = WARN\n      logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline\n      logger.netty.level = WARN \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 \n \n 实现简单的流批一体 \n \n import   com . gordon . create_pipeline . MyOptions ; \n import   com . gordon . uniform . common . MyHDFSSynchronization ; \n import   com . gordon . uniform . config . HDFSSinkConfig ; \n import   com . gordon . uniform . config . HdfsSourceConfig ; \n import   com . gordon . uniform . config . KafkaConfig ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . hadoop . format . HDFSSynchronization ; \n import   org . apache . beam . sdk . io . hadoop . format . HadoopFormatIO ; \n import   org . apache . beam . sdk . io . kafka . KafkaIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . * ; \n import   org . apache . beam . sdk . transforms . windowing . FixedWindows ; \n import   org . apache . beam . sdk . transforms . windowing . Window ; \n import   org . apache . beam . sdk . values . * ; \n import   org . apache . beam . vendor . guava . v26_0_jre . com . google . common . collect . ImmutableMap ; \n import   org . apache . flink . configuration . IllegalConfigurationException ; \n import   org . apache . hadoop . conf . Configuration ; \n import   org . apache . hadoop . io . LongWritable ; \n import   org . apache . hadoop . io . Text ; \n import   org . apache . hadoop . mapreduce . InputFormat ; \n import   org . apache . hadoop . mapreduce . OutputFormat ; \n import   org . apache . hadoop . mapreduce . lib . input . TextInputFormat ; \n import   org . apache . hadoop . mapreduce . lib . output . TextOutputFormat ; \n import   org . apache . kafka . common . serialization . StringDeserializer ; \n import   org . joda . time . Duration ; \n\n /**\n * 模拟从kafka或hdfs 获取数据源写入hdfs\n */ \n public   class   Computing   { \n     public   static   < V >   void   main ( String [ ]  args )   { \n\n         if ( System . getProperty ( "os.name" ) . toLowerCase ( ) . contains ( "windows" ) ) { \n             String  command_line  =   "--runner=org.apache.beam.runners.flink.FlinkRunner --kafkaConfig_isOpen=true --hdfsSourceConfig_isOpen=false --hdfsSinkConfig_isOpen=true --output=hdfs://node1:8020/data/output/fromKafka" ; \n            args  =  command_line . split ( "\\\\s+" ) ; \n         } \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . as ( MyOptions . class ) ; \n        options . setParallelism ( 1 ) ; \n        options . setCheckpointingInterval ( 2 * 60 * 1000L ) ; \n         System . out . println ( "options.getRunner() = "   +  options . getRunner ( ) ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         PCollection < KV < Text ,   Text > >  res  =   null ; \n\n         //模拟配置了kafka数据源 \n         boolean  kafkaConfig_isOpen  =  options . getKafkaConfig_isOpen ( ) ; \n         System . out . println ( "kafkaConfig_isOpen = "   +  kafkaConfig_isOpen ) ; \n         KafkaConfig  kafkaConfig  =   new   KafkaConfig ( ) ; \n        kafkaConfig . setOpen ( kafkaConfig_isOpen ) ; \n         if   ( kafkaConfig . isOpen ( ) )   { \n             PCollection < String >  from_kafka  =  pipeline\n                     . apply ( KafkaIO . < String ,   String > read ( ) \n                             . withBootstrapServers ( "node1:9092,node2:9092,node3:9092" ) \n                             . withTopic ( "hpa-test" )    // use withTopics(List<String>) to read from multiple topics. \n                             . withKeyDeserializer ( StringDeserializer . class ) \n                             . withValueDeserializer ( StringDeserializer . class ) \n                             // Rest of the settings are optional : \n                             // you can further customize KafkaConsumer used to read the records by adding more \n                             // settings for ConsumerConfig. e.g : \n                             . withConsumerConfigUpdates ( ImmutableMap . of ( \n                                     "group.id" ,   "beam-kafka" , \n                                     "auto.offset.reset" ,   "latest" , \n                                     "enable.auto.commit" ,   "true" \n                             ) ) \n                             . withoutMetadata ( ) \n                     ) . apply ( Values . create ( ) ) ; \n            res  =  from_kafka\n                     . apply ( "from_hadoop" ,   MapElements . into ( TypeDescriptors . kvs ( new   TypeDescriptor < Text > ( )   { \n                     } ,   new   TypeDescriptor < Text > ( )   { \n                     } ) ) . via ( x  ->   { \n                         System . out . println ( "kafka line is  = "   +  x ) ; \n                         return   KV . of ( new   Text ( String . valueOf ( System . currentTimeMillis ( ) ) ) ,   new   Text ( x ) ) ; \n                     } ) ) ; \n         } \n\n\n         Configuration  conf  =   new   Configuration ( ) ; \n        conf . set ( "fs.defaultFS" ,   "hdfs://node1:8020" ) ; \n        conf . set ( "fs.hdfs.impl" ,   "org.apache.hadoop.hdfs.DistributedFileSystem" ) ; \n\n         //模拟配置了hdfs 数据源 \n         boolean  hdfsSourceConfig_isOpen  =  options . getHdfsSourceConfig_isOpen ( ) ; \n         System . out . println ( "hdfsSourceConfig_isOpen = "   +  hdfsSourceConfig_isOpen ) ; \n         HdfsSourceConfig  hdfsSourceConfig  =   new   HdfsSourceConfig ( ) ; \n        hdfsSourceConfig . setOpen ( hdfsSourceConfig_isOpen ) ; \n\n         if   ( ! kafkaConfig . isOpen ( )   &&  hdfsSourceConfig . isOpen ( ) )   { \n             // Set Hadoop InputFormat, key and value class in configuration \n             //mapreduce 中map input 的key是偏移量，value才是一行的值 \n\n            conf . setClass ( "key.class" ,   LongWritable . class ,   Object . class ) ; \n            conf . setClass ( "value.class" ,   Text . class ,   Object . class ) ; \n            conf . setClass ( "mapreduce.job.inputformat.class" ,   TextInputFormat . class ,   InputFormat . class ) ; \n             //hdfs://node1:8020/data/input \n            conf . set ( "mapreduce.input.fileinputformat.inputdir" ,  options . getInput ( ) ) ; \n\n             PCollection < KV < LongWritable ,   Text > >  from_hdfs  =  pipeline . apply ( "from_hdfs" ,   HadoopFormatIO . < LongWritable ,   Text > read ( ) . withConfiguration ( conf ) ) ; \n            res  =  from_hdfs\n                     . apply ( "from_hadoop" ,   MapElements . into ( TypeDescriptors . kvs ( new   TypeDescriptor < Text > ( )   { \n                     } ,   new   TypeDescriptor < Text > ( )   { \n                     } ) ) . via ( x  ->   { \n                         System . out . println ( "line = "   +  x . getValue ( ) ) ; \n                         return   KV . of ( new   Text ( String . valueOf ( System . currentTimeMillis ( ) ) ) ,  x . getValue ( ) ) ; \n                     } ) ) ; \n         } \n\n\n         //模拟配置了输出hdfs \n         boolean  hdfsSinkConfig_isOpen  =  options . getHdfsSinkConfig_isOpen ( ) ; \n         System . out . println ( "hdfsSinkConfig_isOpen = "   +  hdfsSinkConfig_isOpen ) ; \n         HDFSSinkConfig  hdfsSinkConfig  =   new   HDFSSinkConfig ( ) ; \n        hdfsSinkConfig . setOpen ( hdfsSinkConfig_isOpen ) ; \n         if   ( hdfsSinkConfig . isOpen ( )   &&  res  !=   null )   { \n            conf . set ( HadoopFormatIO . JOB_ID ,   "AboutHadoop" ) ; \n            conf . setClass ( HadoopFormatIO . OUTPUT_FORMAT_CLASS_ATTR ,   TextOutputFormat . class ,   OutputFormat . class ) ; \n             //conf.setClass(HadoopFormatIO.OUTPUT_FORMAT_CLASS_ATTR, MyTextOutputFormat.class, OutputFormat.class); \n            conf . setClass ( HadoopFormatIO . OUTPUT_KEY_CLASS ,   Text . class ,   Object . class ) ; \n            conf . setClass ( HadoopFormatIO . OUTPUT_VALUE_CLASS ,   Text . class ,   Object . class ) ; \n            conf . setInt ( HadoopFormatIO . NUM_REDUCES ,   2 ) ; \n             //hdfs://node1:8020/data/output \n            conf . set ( HadoopFormatIO . OUTPUT_DIR ,  options . getOutput ( ) + "/current/" ) ; \n             if   ( res . isBounded ( ) . equals ( PCollection . IsBounded . UNBOUNDED ) \n                     ||   ! res . getWindowingStrategy ( ) . equals ( WindowingStrategy . globalDefault ( ) ) )   { \n\n                 ConfigTransform < Text ,   Text >  configTransform  =   new   ConfigTransform < > ( conf ) ; \n                res . apply ( Window . into ( FixedWindows . of ( Duration . standardMinutes ( 2 ) ) ) ) \n                         . setTypeDescriptor ( TypeDescriptors . kvs ( \n                                 new   TypeDescriptor < Text > ( )   { \n                                 } ,   new   TypeDescriptor < Text > ( )   { \n                                 } ) ) \n                         . apply ( \n                                 "writeStream" , \n                                 HadoopFormatIO . < Text ,   Text > write ( ) \n                                         . withConfigurationTransform ( configTransform ) \n                                         . withExternalSynchronization ( new   MyHDFSSynchronization ( conf . get ( HadoopFormatIO . OUTPUT_DIR ) ) ) ) ; \n                 //.withExternalSynchronization(new HDFSSynchronization(conf.get(HadoopFormatIO.OUTPUT_DIR)))); \n\n             }   else   { \n                res . apply ( \n                         "writeBatch" , \n                         HadoopFormatIO . < Text ,   Text > write ( ) \n                                 . withConfiguration ( conf ) \n                                 . withPartitioning ( ) \n                                 . withExternalSynchronization ( new   HDFSSynchronization ( conf . get ( HadoopFormatIO . OUTPUT_DIR ) ) ) ) ; \n             } \n         } \n\n         if   ( res  ==   null )   { \n             throw   new   IllegalConfigurationException ( "数据源配置错误" ) ; \n         }   else   { \n            pipeline . run ( ) . waitUntilFinish ( ) ; \n         } \n\n     } \n\n     /** Simple transform for providing Hadoop {@link Configuration} into {@link HadoopFormatIO}. */ \n     private   static   class   ConfigTransform < KeyT ,   ValueT > \n             extends   PTransform < \n             PCollection < ?   extends  KV < KeyT ,   ValueT > > ,   PCollectionView < Configuration > >   { \n\n         private   final   transient   Configuration  hConf ; \n\n         private   ConfigTransform ( Configuration  hConf )   { \n             this . hConf  =  hConf ; \n         } \n\n         @Override \n         public   PCollectionView < Configuration >   expand ( PCollection < ?   extends  KV < KeyT ,   ValueT > >  input )   { \n             return  input\n                     . getPipeline ( ) \n                     . apply ( Create . < Configuration > of ( hConf ) ) \n                     . apply ( View . < Configuration > asSingleton ( ) . withDefaultValue ( hConf ) ) ; \n         } \n     } \n\n\n     private   static   class   ConvertToHadoopFormatFn < InputT ,   OutputT >   extends   DoFn < InputT ,   OutputT >   { \n\n         private   static   final   long  serialVersionUID  =   - 6841922575497475096L ; \n         private   final   SerializableFunction < InputT ,   OutputT >  transformFn ; \n\n         ConvertToHadoopFormatFn ( SerializableFunction < InputT ,   OutputT >  transformFn )   { \n             this . transformFn  =  transformFn ; \n         } \n\n         @DoFn.ProcessElement \n         public   void   processElement ( @DoFn.Element   InputT  element ,   OutputReceiver < OutputT >  outReceiver )   { \n            outReceiver . output ( transformFn . apply ( element ) ) ; \n         } \n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 \n import   org . apache . beam . runners . flink . FlinkPipelineOptions ; \n import   org . apache . beam . runners . spark . SparkPipelineOptions ; \n import   org . apache . beam . sdk . options . Default ; \n import   org . apache . beam . sdk . options . Description ; \n import   org . apache . beam . sdk . options . PipelineOptions ; \n\n public   interface   MyOptions   extends   SparkPipelineOptions , FlinkPipelineOptions , PipelineOptions   { \n     //You can also specify a description, which appears when a user passes as a command-line argument, and a default value.--help \n     //You set the description and default value using annotations \n     @Description ( "Input for the pipeline" ) \n     @Default.String ( "hdfs://node1:8020/data/input" ) \n     String   getInput ( ) ; \n     void   setInput ( String  input ) ; \n\n     @Description ( "Output for the pipeline" ) \n     @Default.String ( "hdfs://node1:8020/data/output" ) \n     String   getOutput ( ) ; \n     void   setOutput ( String  output ) ; \n\n     @Description ( "kafkaConf for the pipeline" ) \n     @Default.Boolean ( false ) \n     Boolean   getKafkaConfig_isOpen ( ) ; \n     void   setKafkaConfig_isOpen ( Boolean  isOpen ) ; \n\n     @Description ( "HdfsSource for the pipeline" ) \n     @Default.Boolean ( false ) \n     Boolean   getHdfsSourceConfig_isOpen ( ) ; \n     void   setHdfsSourceConfig_isOpen ( Boolean  isOpen ) ; \n\n\n     @Description ( "HdfsSink for the pipeline" ) \n     @Default.Boolean ( false ) \n     Boolean   getHdfsSinkConfig_isOpen ( ) ; \n     void   setHdfsSinkConfig_isOpen ( Boolean  isOpen ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 落地文件保存（按时间窗口）:（未实现按大小） \n 方式一：从生成的临时文件同步到新文件夹 \n import   org . apache . beam . sdk . io . hadoop . format . ExternalSynchronization ; \n import   org . apache . beam . sdk . io . hadoop . format . HadoopFormatIO ; \n import   org . apache . beam . vendor . guava . v26_0_jre . com . google . common . base . Preconditions ; \n import   org . apache . hadoop . conf . Configuration ; \n import   org . apache . hadoop . fs . * ; \n import   org . apache . hadoop . hdfs . protocol . AlreadyBeingCreatedException ; \n import   org . apache . hadoop . ipc . RemoteException ; \n import   org . apache . hadoop . mapred . FileAlreadyExistsException ; \n import   org . apache . hadoop . mapreduce . JobID ; \n import   org . apache . hadoop . mapreduce . TaskAttemptID ; \n import   org . apache . hadoop . mapreduce . TaskID ; \n import   org . slf4j . Logger ; \n import   org . slf4j . LoggerFactory ; \n\n import   java . io . IOException ; \n import   java . io . Serializable ; \n import   java . time . Instant ; \n import   java . time . ZoneId ; \n import   java . time . format . DateTimeFormatter ; \n import   java . util . Random ; \n\n /**\n * Implementation of {@link ExternalSynchronization} which registers locks in the HDFS.\n *\n * <p>Requires {@code locksDir} to be specified. This directory MUST be different that directory\n * which is possibly stored under {@code "mapreduce.output.fileoutputformat.outputdir"} key.\n * Otherwise setup of job will fail because the directory will exist before job setup.\n */ \n public   class   MyHDFSSynchronization   implements   ExternalSynchronization   { \n\n     private   static   final   Logger   LOG   =   LoggerFactory . getLogger ( MyHDFSSynchronization . class ) ; \n     //生产环境需要改成合适的生成时间 \n     private   static   final   DateTimeFormatter  dateTimeFormatter  =   DateTimeFormatter . ofPattern ( "yyyy-MM-dd--HH-mm" ) . withZone ( ZoneId . of ( "Asia/Shanghai" ) ) ; \n     private   static   final   String   LOCKS_DIR_PATTERN   =   "%s/" ; \n     private   static   final   String   LOCKS_DIR_TASK_PATTERN   =   LOCKS_DIR_PATTERN   +   "%s" ; \n     private   static   final   String   LOCKS_DIR_TASK_ATTEMPT_PATTERN   =   LOCKS_DIR_TASK_PATTERN   +   "_%s" ; \n     private   static   final   String   LOCKS_DIR_JOB_FILENAME   =   LOCKS_DIR_PATTERN   +   "_job" ; \n\n     private   static   final   transient   Random   RANDOM_GEN   =   new   Random ( ) ; \n\n     private   final   String  locksDir ; \n     private   final   ThrowingFunction < Configuration ,   FileSystem ,   IOException >  fileSystemFactory ; \n\n     /**\n     * Creates instance of {@link MyHDFSSynchronization}.\n     *\n     * @param locksDir directory where locks will be stored. This directory MUST be different that\n     *                 directory which is possibly stored under {@code\n     *                 "mapreduce.output.fileoutputformat.outputdir"} key. Otherwise setup of job will fail\n     *                 because the directory will exist before job setup.\n     */ \n     public   MyHDFSSynchronization ( String  locksDir )   { \n         this ( locksDir ,   FileSystem :: newInstance ) ; \n     } \n\n     /**\n     * Creates instance of {@link MyHDFSSynchronization}. Exists only for easier testing.\n     *\n     * @param locksDir          directory where locks will be stored. This directory MUST be different that\n     *                          directory which is possibly stored under {@code\n     *                          "mapreduce.output.fileoutputformat.outputdir"} key. Otherwise setup of job will fail\n     *                          because the directory will exist before job setup.\n     * @param fileSystemFactory supplier of the file system\n     */ \n     MyHDFSSynchronization ( \n             String  locksDir ,   ThrowingFunction < Configuration ,   FileSystem ,   IOException >  fileSystemFactory )   { \n         this . locksDir  =  locksDir ; \n         this . fileSystemFactory  =  fileSystemFactory ; \n     } \n\n     @Override \n     public   boolean   tryAcquireJobLock ( Configuration  conf )   { \n         Path  path  =   new   Path ( locksDir ,   String . format ( LOCKS_DIR_JOB_FILENAME ,   getJobJtIdentifier ( conf ) ) ) ; \n         return   tryCreateFile ( conf ,  path ) ; \n     } \n\n     @Override \n     public   void   releaseJobIdLock ( Configuration  conf )   { \n         Path  path  =   new   Path ( locksDir ,   String . format ( LOCKS_DIR_PATTERN ,   getJobJtIdentifier ( conf ) ) ) ; \n\n         try   ( FileSystem  fileSystem  =  fileSystemFactory . apply ( conf ) )   { \n\n             Path  src  =   new   Path ( conf . get ( HadoopFormatIO . OUTPUT_DIR )   +   "/_temporary/0/" ) ; \n             System . out . println ( "src = "   +  src . toString ( ) ) ; \n             RemoteIterator < LocatedFileStatus >  sourceFiles  =  fileSystem . listFiles ( src ,   true ) ; \n             if   ( sourceFiles  !=   null )   { \n                 String  mid_dir  =  dateTimeFormatter . format ( Instant . ofEpochMilli ( System . currentTimeMillis ( ) ) ) ; \n                 String  storePath  =   new   Path ( conf . get ( HadoopFormatIO . OUTPUT_DIR ) ) . getParent ( ) . toString ( ) ; \n                 String  newPathStr  =  storePath  +   "/"   +  mid_dir  +   "/%s" ; \n                 Path  success  =   new   Path ( String . format ( newPathStr ,   "_SUCCESS" ) ) ; \n                 boolean  is_exist_dir  =  fileSystem . exists ( new   Path ( String . format ( newPathStr ,   "" ) ) ) ; \n                 if   ( is_exist_dir )   { \n                     //考虑扩展滚动策略的时候会使用到 \n                    newPathStr  =  storePath  +   "/"   +  mid_dir  +   "/%s-"   +   System . currentTimeMillis ( ) ; \n                    fileSystem . delete ( success ,   false ) ; \n                 } \n                 while   ( sourceFiles . hasNext ( ) )   { \n                     LocatedFileStatus  next  =  sourceFiles . next ( ) ; \n                     FileUtil . copy ( fileSystem ,  next ,  fileSystem , \n                             new   Path ( String . format ( newPathStr ,  next . getPath ( ) . getName ( ) ) ) , \n                             false ,   false ,  conf ) ; \n                 } \n                fileSystem . createNewFile ( success ) ; \n             } \n\n             if   ( fileSystem . delete ( path ,   true ) )   { \n                 LOG . info ( "Delete of lock directory {} was successful" ,  path ) ; \n             }   else   { \n                 LOG . warn ( "Delete of lock directory {} was unsuccessful" ,  path ) ; \n             } \n         }   catch   ( IOException  e )   { \n             String  formattedExceptionMessage  = \n                     String . format ( "Delete of lock directory %s was unsuccessful" ,  path ) ; \n             throw   new   IllegalStateException ( formattedExceptionMessage ,  e ) ; \n         } \n     } \n\n     @Override \n     public   TaskID   acquireTaskIdLock ( Configuration  conf )   { \n         JobID  jobId  =   HadoopFormats . getJobId ( conf ) ; \n         boolean  lockAcquired  =   false ; \n         int  taskIdCandidate  =   0 ; \n\n         while   ( ! lockAcquired )   { \n            taskIdCandidate  =   RANDOM_GEN . nextInt ( Integer . MAX_VALUE ) ; \n             Path  path  = \n                     new   Path ( \n                            locksDir , \n                             String . format ( LOCKS_DIR_TASK_PATTERN ,   getJobJtIdentifier ( conf ) ,  taskIdCandidate ) ) ; \n            lockAcquired  =   tryCreateFile ( conf ,  path ) ; \n         } \n\n         return   HadoopFormats . createTaskID ( jobId ,  taskIdCandidate ) ; \n     } \n\n     @Override \n     public   TaskAttemptID   acquireTaskAttemptIdLock ( Configuration  conf ,   int  taskId )   { \n         String  jobJtIdentifier  =   getJobJtIdentifier ( conf ) ; \n         JobID  jobId  =   HadoopFormats . getJobId ( conf ) ; \n         int  taskAttemptCandidate  =   0 ; \n         boolean  taskAttemptAcquired  =   false ; \n\n         while   ( ! taskAttemptAcquired )   { \n            taskAttemptCandidate ++ ; \n             Path  path  = \n                     new   Path ( \n                            locksDir , \n                             String . format ( \n                                     LOCKS_DIR_TASK_ATTEMPT_PATTERN ,  jobJtIdentifier ,  taskId ,  taskAttemptCandidate ) ) ; \n            taskAttemptAcquired  =   tryCreateFile ( conf ,  path ) ; \n         } \n\n         return   HadoopFormats . createTaskAttemptID ( jobId ,  taskId ,  taskAttemptCandidate ) ; \n     } \n\n     private   boolean   tryCreateFile ( Configuration  conf ,   Path  path )   { \n         try   ( FileSystem  fileSystem  =  fileSystemFactory . apply ( conf ) )   { \n             try   { \n                 return  fileSystem . createNewFile ( path ) ; \n             }   catch   ( FileAlreadyExistsException   |   org . apache . hadoop . fs . FileAlreadyExistsException  e )   { \n                 return   false ; \n             }   catch   ( RemoteException  e )   { \n                 // remote hdfs exception \n                 if   ( e . getClassName ( ) . equals ( AlreadyBeingCreatedException . class . getName ( ) ) )   { \n                     return   false ; \n                 } \n                 throw  e ; \n             } \n         }   catch   ( IOException  e )   { \n             throw   new   IllegalStateException ( String . format ( "Creation of file on path %s failed" ,  path ) ,  e ) ; \n         } \n     } \n\n     private   String   getJobJtIdentifier ( Configuration  conf )   { \n         JobID  job  = \n                 Preconditions . checkNotNull ( \n                         HadoopFormats . getJobId ( conf ) , \n                         "Configuration must contain jobID under key %s." , \n                         HadoopFormatIO . JOB_ID ) ; \n         return  job . getJtIdentifier ( ) ; \n     } \n\n     /**\n     * Function which can throw exception.\n     *\n     * @param <T1> parameter type\n     * @param <T2> result type\n     * @param <X>  exception type\n     */ \n     @FunctionalInterface \n     interface   ThrowingFunction < T1 ,  T2 ,   X   extends   Exception >   extends   Serializable   { \n         T2   apply ( T1  value )   throws   X ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 方式二：自定义输出类MyTextOutputFormat \n import   org . apache . hadoop . classification . InterfaceAudience ; \n import   org . apache . hadoop . classification . InterfaceStability ; \n import   org . apache . hadoop . conf . Configuration ; \n import   org . apache . hadoop . fs . FSDataOutputStream ; \n import   org . apache . hadoop . fs . FileSystem ; \n import   org . apache . hadoop . fs . Path ; \n import   org . apache . hadoop . io . NullWritable ; \n import   org . apache . hadoop . io . Text ; \n import   org . apache . hadoop . io . compress . CompressionCodec ; \n import   org . apache . hadoop . io . compress . GzipCodec ; \n import   org . apache . hadoop . mapreduce . OutputFormat ; \n import   org . apache . hadoop . mapreduce . RecordWriter ; \n import   org . apache . hadoop . mapreduce . TaskAttemptContext ; \n import   org . apache . hadoop . mapreduce . lib . output . FileOutputCommitter ; \n import   org . apache . hadoop . mapreduce . lib . output . FileOutputFormat ; \n import   org . apache . hadoop . util . ReflectionUtils ; \n\n import   java . io . DataOutputStream ; \n import   java . io . IOException ; \n import   java . io . UnsupportedEncodingException ; \n import   java . time . Instant ; \n import   java . time . ZoneId ; \n import   java . time . format . DateTimeFormatter ; \n\n /**\n * An {@link OutputFormat} that writes plain text files.\n */ \n @InterfaceAudience.Public \n @InterfaceStability.Stable \n public   class   MyTextOutputFormat < K ,   V >   extends   FileOutputFormat < K ,   V >   { \n     public   static   String   SEPERATOR   =   "mapreduce.output.textoutputformat.separator" ; \n     private   static   final   DateTimeFormatter   FORMATTER_HOUR   =   java . time . format . DateTimeFormatter . ofPattern ( "yyyy-MM-dd/HH-mm" ) . withZone ( ZoneId . of ( "Asia/Shanghai" ) ) ; \n\n     public   MyTextOutputFormat ( )   { \n         super ( ) ; \n     } \n\n     protected   static   class   LineRecordWriter < K ,   V > \n             extends   RecordWriter < K ,   V >   { \n         private   static   final   String  utf8  =   "UTF-8" ; \n         private   static   final   byte [ ]  newline ; \n\n         static   { \n             try   { \n                newline  =   "\\n" . getBytes ( utf8 ) ; \n             }   catch   ( UnsupportedEncodingException  uee )   { \n                 throw   new   IllegalArgumentException ( "can\'t find "   +  utf8  +   " encoding" ) ; \n             } \n         } \n\n         protected   DataOutputStream  out ; \n         private   final   byte [ ]  keyValueSeparator ; \n\n\n         public   LineRecordWriter ( DataOutputStream  out ,   String  keyValueSeparator )   { \n             this . out  =  out ; \n             try   { \n                 this . keyValueSeparator  =  keyValueSeparator . getBytes ( utf8 ) ; \n             }   catch   ( UnsupportedEncodingException  uee )   { \n                 throw   new   IllegalArgumentException ( "can\'t find "   +  utf8  +   " encoding" ) ; \n             } \n         } \n\n         public   LineRecordWriter ( DataOutputStream  out )   { \n             this ( out ,   "\\t" ) ; \n         } \n\n         /**\n         * Write the object to the byte stream, handling Text as a special\n         * case.\n         *\n         * @param o the object to print\n         * @throws IOException if the write throws, we pass it on\n         */ \n         private   void   writeObject ( Object  o )   throws   IOException   { \n             if   ( o  instanceof   Text )   { \n                 Text   to   =   ( Text )  o ; \n                out . write ( to . getBytes ( ) ,   0 ,   to . getLength ( ) ) ; \n             }   else   { \n                out . write ( o . toString ( ) . getBytes ( utf8 ) ) ; \n             } \n         } \n\n         public   synchronized   void   write ( K  key ,   V  value ) \n                 throws   IOException   { \n\n             boolean  nullKey  =  key  ==   null   ||  key  instanceof   NullWritable ; \n             boolean  nullValue  =  value  ==   null   ||  value  instanceof   NullWritable ; \n             if   ( nullKey  &&  nullValue )   { \n                 return ; \n             } \n             if   ( ! nullKey )   { \n                 writeObject ( key ) ; \n             } \n             if   ( ! ( nullKey  ||  nullValue ) )   { \n                out . write ( keyValueSeparator ) ; \n             } \n             if   ( ! nullValue )   { \n                 writeObject ( value ) ; \n             } \n            out . write ( newline ) ; \n         } \n\n         public   synchronized   void   close ( TaskAttemptContext  context )   throws   IOException   { \n            out . close ( ) ; \n         } \n     } \n\n     public   RecordWriter < K ,   V > \n     getRecordWriter ( TaskAttemptContext  job\n     )   throws   IOException ,   InterruptedException   { \n         Configuration  conf  =  job . getConfiguration ( ) ; \n         boolean  isCompressed  =   getCompressOutput ( job ) ; \n         String  keyValueSeparator  =  conf . get ( SEPERATOR ,   "\\t" ) ; \n         CompressionCodec  codec  =   null ; \n         String  extension  =   "" ; \n         if   ( isCompressed )   { \n             Class < ?   extends   CompressionCodec >  codecClass  = \n                     getOutputCompressorClass ( job ,   GzipCodec . class ) ; \n            codec  =   ( CompressionCodec )   ReflectionUtils . newInstance ( codecClass ,  conf ) ; \n            extension  =  codec . getDefaultExtension ( ) ; \n         } \n\n         Path  file  =   getDefaultWorkFile ( job ,  extension ) ; \n         System . out . println ( "file = "   +  file ) ; \n         FileSystem  fs  =  file . getFileSystem ( conf ) ; \n         if   ( ! isCompressed )   { \n             FSDataOutputStream  fileOut  =  fs . create ( file ,   false ) ; \n             return   new   LineRecordWriter < K ,   V > ( fileOut ,  keyValueSeparator ) ; \n         }   else   { \n             FSDataOutputStream  fileOut  =  fs . create ( file ,   false ) ; \n             return   new   LineRecordWriter < K ,   V > ( new   DataOutputStream \n                     ( codec . createOutputStream ( fileOut ) ) , \n                    keyValueSeparator ) ; \n         } \n     } \n\n\n     @Override \n     public   Path   getDefaultWorkFile ( TaskAttemptContext  context ,   String  extension )   throws   IOException   { \n         FileOutputCommitter  committer  = \n                 ( FileOutputCommitter )   getOutputCommitter ( context ) ; \n         return   new   Path ( committer . getWorkPath ( ) ,   getUniqueFile ( context , \n                 FORMATTER_HOUR . format ( Instant . ofEpochMilli ( System . currentTimeMillis ( ) ) )   +   "/" +   getOutputName ( context ) ,  extension ) ) ; \n     } \n\n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 部署的yaml \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   namespace :  flink\n   name :  kafka - 2 - hdfs\n spec : \n   image :  personalharbor.com/bigdata/flink : 1.14.6 - scala_2.12\n   flinkVersion :  v1_14\n   imagePullPolicy :  Never    # 镜像拉取策略，本地没有则从仓库拉取 \n   ingress :     # ingress配置，用于访问flink web页面 \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "2" \n     web.cancel.enable :   "true" \n   serviceAccount :  flink\n   jobManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   taskManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   podTemplate : \n     spec : \n提前在容器内部映射域名 \n       hostAliases : \n         -   ip :  192.168.8.102\n           hostnames : \n             -   "node3" \n         -   ip :  192.168.8.103\n           hostnames : \n             -   "node2" \n         -   ip :  192.168.8.104\n           hostnames : \n             -   "node1" \n       containers : \n         -   name :  flink - main - container\n           volumeMounts : \n             -   name :  flink - data - volume\n               mountPath :  /flink - data\n             -   name :  flink - jar   # 挂载nfs上的jar \n               #创建一个flink目录下的空白文件夹，来放置 \n               #如果/opt/flink,会把该目录下的文件全部置空，来进行挂载，清空了安装包，程序启动不了 \n               mountPath :  /opt/flink/usrlib\n       volumes : \n         -   name :  flink - data - volume\nFlink is designed to run as a specific user with restricted privileges. \nThe owner of the host path should be set to "flink" with the user ID (UID) of 9999. \n           hostPath : \n             #本地创建的目录需修改用户，不然会报错The base directory of the JobResultStore isn\'t accessible. No dirty JobResults can be restored \n             path :  /tmp/flink    # chown 9999:9999 -R /tmp/flink \n             type :  Directory\n         -   name :  flink - jar\n           persistentVolumeClaim : \n             claimName :  flink - jar - pvc\n   job : \n     entryClass :  com.gordon.uniform.Computing\n     jarURI :  local : ///opt/flink/usrlib/apache - beam - 1.0 - SNAPSHOT.jar\n     #在线源写hdfs \n     args :   [   "--runner=org.apache.beam.runners.flink.FlinkRunner  --kafkaConfig_isOpen=true --hdfsSinkConfig_isOpen=true --output=hdfs://node1:8020/data/output/fromKafka" ] \n     #离线源写hdfs \n     #args: [ "--runner=org.apache.beam.runners.flink.FlinkRunner --hdfsSourceConfig_isOpen=true --hdfsSinkConfig_isOpen=true --output=hdfs://node1:8020/data/output/fromHdfs"] \n     parallelism :   1 \n     upgradeMode :  stateless\n   logConfiguration : \n     "log4j-console.properties" :   | \n      rootLogger.level = INFO\n      rootLogger.appenderRef.file.ref = LogFile\n      rootLogger.appenderRef.console.ref = LogConsole\n      appender.file.name = LogFile\n      appender.file.type = File\n      appender.file.append = false\n      appender.file.fileName = ${sys:log.file}\n      appender.file.layout.type = PatternLayout\n      appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n      appender.console.name = LogConsole\n      appender.console.type = CONSOLE\n      appender.console.layout.type = PatternLayout\n      appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n      logger.akka.name = akka\n      logger.akka.level = WARN\n      logger.kafka.name= org.apache.kafka\n      logger.kafka.level = WARN\n      logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline\n      logger.netty.level = WARN \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 \n \n kafka数据源 \n \n hdfs数据源 \n \n'},{title:"Helm",frontmatter:{title:"Helm",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["其他"],tags:["k8s部署"]},regularPath:"/%E5%85%B6%E4%BB%96/helm.html",relativePath:"其他/helm.md",key:"v-c38fec2a",path:"/2023/06/10/helm/",headers:[{level:2,title:"介绍",slug:"介绍"},{level:2,title:"安装Helm",slug:"安装helm"},{level:2,title:"轻松入门",slug:"轻松入门"},{level:2,title:"Helm 命令的详细指南",slug:"helm-命令的详细指南"},{level:3,title:"三大概念",slug:"三大概念"},{level:3,title:"'helm search'：查找 Charts",slug:"helm-search-查找-charts"},{level:3,title:"'helm install'：",slug:"helm-install"},{level:3,title:"'helm upgrade' 和 'helm rollback'：升级 release 和失败时恢复",slug:"helm-upgrade-和-helm-rollback-升级-release-和失败时恢复"},{level:3,title:"'helm uninstall'：卸载 release",slug:"helm-uninstall-卸载-release"},{level:3,title:"'helm repo'：使用仓库",slug:"helm-repo-使用仓库"},{level:2,title:"创建自己的 charts",slug:"创建自己的-charts"},{level:3,title:"简单入门",slug:"简单入门"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' 介绍 \n Helm 是 Kubernetes 的包管理器，类似yum。 \n Helm架构 \n 安装Helm \n下载包,使用离线包 \n wget  https://get.helm.sh/helm-v3.7.1-linux-amd64.tar.gz  -O  /tmp/helm-v3.7.1-linux-amd64.tar.gz\n解压压缩包 \n tar   -xf  /tmp/helm-v3.7.1-linux-amd64.tar.gz  -C  /root/\n制作软连接 \n ln   -s  /root/linux-amd64/helm /usr/local/bin/helm\n \n 1 2 3 4 5 6 #  轻松入门 \n Helm | 快速入门指南 \n #添加仓库源 \nhelm repo  add  bitnami https://charts.bitnami.com/bitnami\n #展示仓库的包信息charts列表 \nhelm search repo bitnami\n #了解相应的chart信息 \nhelm show chart bitnami/mysql\n #安装 \nhelm  install  bitnami/mysql --generate-name\n #查看已安装的chart \nhelm  ls \n #卸载 \nhelm uninstall mysql-1612624192\n ##该命令会从Kubernetes卸载 mysql-1612624192， 它将删除和该版本相关的所有相关资源（service、deployment、 pod等等）甚至版本历史。如果您在执行 helm uninstall 的时候提供 --keep-history 选项， Helm将会保存版本历史。 您可以通过命令查看该版本的信息 \n #查看安装chart的状态 \nhelm status mysql-1612624192\n #查看版本，使用 helm rollback 回滚版本。 \n #更新仓库 \nhelm repo update\n #查看命令帮助 \nhelm get  -h \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #  Helm 命令的详细指南 \n 三大概念 \n Chart  代表着 Helm 包。它包含在 Kubernetes 集群内部运行应用程序，工具或服务所需的所有资源定义。你可以把它看作是 Apt dpkg，或 Yum RPM 在Kubernetes 中的等价物。 \n Repository（仓库）  是用来存放和共享 charts 的地方。 \n Release  是运行在 Kubernetes 集群中的 chart 的实例。一个 chart 通常可以在同一个集群中安装多次。每一次安装都会创建一个新的  release 。以 MySQL chart为例，如果你想在你的集群中运行两个数据库，你可以安装该chart两次。每一个数据库都会拥有它自己的  release  和  release name 。 \n \'helm search\'：查找 Charts \n Helm 自带一个强大的搜索命令，可以用来从两种来源中进行搜索： \n \n helm search hub  从  Artifact Hub  中查找并列出 helm charts。 Artifact Hub中存放了大量不同的仓库。 \n helm search repo  从你添加（使用  helm repo add ）到本地 helm 客户端中的仓库中进行查找。该命令基于本地数据进行搜索，无需连接互联网。 \n \n 你可以通过运行  helm search hub  命令找到公开可用的charts： \n $ helm search hub wordpress\nURL                                                 CHART VERSION APP VERSION DESCRIPTION\nhttps://hub.helm.sh/charts/bitnami/wordpress        7.6.7         5.2.4       Web publishing platform for building blogs and ...\nhttps://hub.helm.sh/charts/presslabs/wordpress-...  v0.6.3        v0.6.3      Presslabs WordPress Operator Helm Chart\nhttps://hub.helm.sh/charts/presslabs/wordpress-...  v0.7.1        v0.7.1      A Helm chart for deploying a WordPress site on ...\n \n 1 2 3 4 5 上述命令从 Artifact Hub 中搜索所有的  wordpress  charts。 \n 如果不进行过滤， helm search hub  命令会展示所有可用的 charts。 \n 使用  helm search repo  命令，你可以从你所添加的仓库中查找chart的名字。 \n $ helm repo add brigade https://brigadecore.github.io/charts\n"brigade" has been added to your repositories\n$ helm search repo brigade\nNAME                          CHART VERSION APP VERSION DESCRIPTION\nbrigade/brigade               1.3.2         v1.2.1      Brigade provides event-driven scripting of Kube...\nbrigade/brigade-github-app    0.4.1         v0.2.1      The Brigade GitHub App, an advanced gateway for...\nbrigade/brigade-github-oauth  0.2.0         v0.20.0     The legacy OAuth GitHub Gateway for Brigade\nbrigade/brigade-k8s-gateway   0.1.0                     A Helm chart for Kubernetes\nbrigade/brigade-project       1.0.0         v1.0.0      Create a Brigade project\nbrigade/kashti                0.4.0         v0.4.0      A Helm chart for Kubernetes\n \n 1 2 3 4 5 6 7 8 9 10 Helm 搜索使用模糊字符串匹配算法，所以你可以只输入名字的一部分： \n $ helm search repo kash\nNAME            CHART VERSION APP VERSION DESCRIPTION\nbrigade/kashti  0.4.0         v0.4.0      A Helm chart for Kubernetes\n \n 1 2 3 搜索是用来发现可用包的一个好办法。一旦你找到你想安装的 helm 包，你便可以通过使用  helm install  命令来安装它。 \n \'helm install\'： \n 安装一个 helm 包 \n 使用  helm install  命令来安装一个新的 helm 包。最简单的使用方法只需要传入两个参数：你命名的release名字和你想安装的chart的名称。 \n $ helm install happy-panda bitnami/wordpress\nNAME: happy-panda\nLAST DEPLOYED: Tue Jan 26 10:27:17 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n** Please be patient while the chart is being deployed **\n\nYour WordPress site can be accessed through the following DNS name from within your cluster:\n\n    happy-panda-wordpress.default.svc.cluster.local (port 80)\n\nTo access your WordPress site from outside the cluster follow the steps below:\n\n1. Get the WordPress URL by running these commands:\n\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        Watch the status with: \'kubectl get svc --namespace default -w happy-panda-wordpress\'\n\n   export SERVICE_IP=$(kubectl get svc --namespace default happy-panda-wordpress --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")\n   echo "WordPress URL: http://$SERVICE_IP/"\n   echo "WordPress Admin URL: http://$SERVICE_IP/admin"\n\n2. Open a browser and access WordPress using the obtained URL.\n\n3. Login with the following credentials below to see your blog:\n\n  echo Username: user\n  echo Password: $(kubectl get secret --namespace default happy-panda-wordpress -o jsonpath="{.data.wordpress-password}" | base64 --decode)\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 现在 wordpress  chart 已经安装。注意安装chart时创建了一个新的  release  对象。上述发布被命名为  happy-panda 。 （如果想让Helm生成一个名称，删除发布名称并使用 --generate-name 。） \n 在安装过程中， helm  客户端会打印一些有用的信息，其中包括：哪些资源已经被创建，release当前的状态，以及你是否还需要执行额外的配置步骤。 \n Helm按照以下顺序安装资源： \n \n Namespace \n NetworkPolicy \n ResourceQuota \n LimitRange \n PodSecurityPolicy \n PodDisruptionBudget \n ServiceAccount \n Secret \n SecretList \n ConfigMap \n StorageClass \n PersistentVolume \n PersistentVolumeClaim \n CustomResourceDefinition \n ClusterRole \n ClusterRoleList \n ClusterRoleBinding \n ClusterRoleBindingList \n Role \n RoleList \n RoleBinding \n RoleBindingList \n Service \n DaemonSet \n Pod \n ReplicationController \n ReplicaSet \n Deployment \n HorizontalPodAutoscaler \n StatefulSet \n Job \n CronJob \n Ingress \n APIService \n \n Helm 客户端不会等到所有资源都运行才退出。许多 charts 需要大小超过 600M 的 Docker 镜像，可能需要很长时间才能安装到集群中。 \n 你可以使用  helm status  来追踪 release 的状态，或是重新读取配置信息： \n $ helm status happy-panda\nNAME: happy-panda\nLAST DEPLOYED: Tue Jan 26 10:27:17 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n** Please be patient while the chart is being deployed **\n\nYour WordPress site can be accessed through the following DNS name from within your cluster:\n\n    happy-panda-wordpress.default.svc.cluster.local (port 80)\n\nTo access your WordPress site from outside the cluster follow the steps below:\n\n1. Get the WordPress URL by running these commands:\n\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        Watch the status with: \'kubectl get svc --namespace default -w happy-panda-wordpress\'\n\n   export SERVICE_IP=$(kubectl get svc --namespace default happy-panda-wordpress --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")\n   echo "WordPress URL: http://$SERVICE_IP/"\n   echo "WordPress Admin URL: http://$SERVICE_IP/admin"\n\n2. Open a browser and access WordPress using the obtained URL.\n\n3. Login with the following credentials below to see your blog:\n\n  echo Username: user\n  echo Password: $(kubectl get secret --namespace default happy-panda-wordpress -o jsonpath="{.data.wordpress-password}" | base64 --decode)\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 上述信息展示了 release 的当前状态。 \n 安装前自定义 chart \n 上述安装方式只会使用 chart 的默认配置选项。很多时候，我们需要自定义 chart 来指定我们想要的配置。 \n 使用  helm show values  可以查看 chart 中的可配置选项： \n $ helm show values bitnami/wordpress\n## Global Docker image parameters\n## Please, note that this will override the image parameters, including dependencies, configured to use the global value\n## Current available global Docker image parameters: imageRegistry and imagePullSecrets\n##\nglobal:\n  imageRegistry: myRegistryName\n  imagePullSecrets:\n    - myRegistryKeySecretName\n  storageClass: myStorageClass\n\n## Bitnami WordPress image version\n## ref: https://hub.docker.com/r/bitnami/wordpress/tags/\n##\nimage:\n  registry: docker.io\n  repository: bitnami/wordpress\n  tag: 5.6.0-debian-10-r35\n  [..]\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 然后，你可以使用 YAML 格式的文件覆盖上述任意配置项，并在安装过程中使用该文件。 \n $ echo \'{mariadb.auth.database: user0db, mariadb.auth.username: user0}\' > values.yaml\n$ helm install -f values.yaml bitnami/wordpress --generate-name\n \n 1 2 上述命令将为 MariaDB 创建一个名称为  user0  的默认用户，并且授予该用户访问新建的  user0db  数据库的权限。chart 中的其他默认配置保持不变。 \n 安装过程中有两种方式传递配置数据： \n \n --values  (或  -f )：使用 YAML 文件覆盖配置。可以指定多次，优先使用最右边的文件。 \n --set ：通过命令行的方式对指定项进行覆盖。 \n \n 如果同时使用两种方式，则  --set  中的值会被合并到  --values  中，但是  --set  中的值优先级更高。在 --set  中覆盖的内容会被被保存在 ConfigMap 中。可以通过  helm get values  来查看指定 release 中  --set  设置的值。也可以通过运行  helm upgrade  并指定  --reset-values  字段来清除  --set  中设置的值。 \n --set  的格式和限制 \n --set  选项使用0或多个 name/value 对。最简单的用法类似于： --set name=value ，等价于如下 YAML 格式： \n name :  value\n \n 1 多个值使用逗号分割，因此  --set a=b,c=d  的 YAML 表示是： \n a :  b\n c :  d\n \n 1 2 支持更复杂的表达式。例如， --set outer.inner=value  被转换成了： \n outer : \n   inner :  value\n \n 1 2 列表使用花括号（ {} ）来表示。例如， --set name={a, b, c}  被转换成了： \n name : \n   -  a\n   -  b\n   -  c\n \n 1 2 3 4 某些name/key可以设置为 null 或者空数组，例如  --set name=[],a=null  由 \n name : \n   -  a\n   -  b\n   -  c\n a :  b\n \n 1 2 3 4 5 变成了 \n name :   [ ] \n a :   null \n \n 1 2 从 2.5.0 版本开始，可以使用数组下标的语法来访问列表中的元素。例如  --set servers[0].port=80  就变成了： \n servers : \n   -   port :   80 \n \n 1 2 多个值也可以通过这种方式来设置。 --set servers[0].port=80,servers[0].host=example  变成了： \n servers : \n   -   port :   80 \n     host :  example\n \n 1 2 3 如果需要在  --set  中使用特殊字符，你可以使用反斜线来进行转义； --set name=value1\\,value2  就变成了： \n name :   "value1,value2" \n \n 1 类似的，你也可以转义点\b序列（英文句号）。这可能会在 chart 使用  toYaml  函数来解析 annotations，labels，和 node selectors 时派上用场。 --set nodeSelector."kubernetes\\.io/role"=master  语法就变成了： \n nodeSelector : \n   kubernetes.io/role :  master\n \n 1 2 深层嵌套的数据结构可能会很难用  --set  表达。我们希望 Chart 的设计者们在设计  values.yaml  文件的格式时，考虑到  --set  的使用。（更多内容请查看  Values 文件 ） \n 更多安装方法 \n helm install  命令可以从多个来源进行安装： \n \n chart 的仓库（如上所述） \n 本地 chart 压缩包（ helm install foo foo-0.1.1.tgz ） \n 解压后的 chart 目录（ helm install foo path/to/foo ） \n 完整的 URL（ helm install foo https://example.com/charts/foo-1.2.3.tgz ） \n \'helm upgrade\' 和 \'helm rollback\'：升级 release 和失败时恢复 \n 当你想升级到 chart 的新版本，或是修改 release 的配置，你可以使用  helm upgrade  命令。 \n 一次升级操作会使用已有的 release 并根据你提供的信息对其进行升级。由于 Kubernetes 的 chart 可能会很大而且很复杂，Helm 会尝试执行最小侵入式升级。即它只会更新自上次发布以来发生了更改的内容。 \n $ helm upgrade -f panda.yaml happy-panda bitnami/wordpress\n \n 1 在上面的例子中， happy-panda  这个 release 使用相同的 chart 进行升级，但是使用了一个新的 YAML 文件： \n mariadb.auth.username :  user1\n \n 1 我们可以使用  helm get values  命令来看看配置值是否真的生效了： \n $ helm get values happy-panda\nmariadb:\n  auth:\n    username: user1\n \n 1 2 3 4 helm get  是一个查看集群中 release 的有用工具。正如我们上面所看到的， panda.yaml  中的新值已经被部署到集群中了。 \n 现在，假如在一次发布过程中，发生了不符合预期的事情，也很容易通过  helm rollback [RELEASE] [REVISION]  命令回滚到之前的发布版本。 \n $ helm rollback happy-panda 1\n \n 1 上面这条命令将我们的  happy-panda  回滚到了它最初的版本。release 版本其实是一个增量修订（revision）。 每当发生了一次安装、升级或回滚操作，revision 的值就会加1。第一次 revision 的值永远是1。我们可以使用  helm history [RELEASE]  命令来查看一个特定 release 的修订版本号。 \n 安装、升级、回滚时的有用选项 \n 你还可以指定一些其他有用的选项来自定义 Helm 在安装、升级、回滚期间的行为。请注意这并不是 cli 参数的完整列表。 要查看所有参数的说明，请执行  helm --help  命令。 \n \n --timeout ：一个  Go duration  类型的值， 用来表示等待 Kubernetes 命令完成的超时时间，默认值为  5m0s 。 \n --wait ：表示必须要等到所有的 Pods 都处于 ready 状态，PVC 都被绑定，Deployments 都至少拥有最小 ready 状态 Pods 个数（ Desired 减去  maxUnavailable ），并且 Services 都具有 IP 地址（如果是 LoadBalancer ， 则为 Ingress），才会标记该 release 为成功。最长等待时间由  --timeout  值指定。如果达到超时时间，release 将被标记为  FAILED 。注意：当 Deployment 的  replicas  被设置为1，但其滚动升级策略中的  maxUnavailable  没有被设置为0时， --wait  将返回就绪，因为已经满足了最小 ready Pod 数。 \n --no-hooks ：不运行当前命令的钩子。 \n --recreate-pods （仅适用于  upgrade  和  rollback ）：这个参数会导致重建所有的 Pod（deployment中的Pod 除外）。（在 Helm 3 中已被废弃） \n \'helm uninstall\'：卸载 release \n 使用  helm uninstall  命令从集群中卸载一个 release： \n $ helm uninstall happy-panda\n \n 1 该命令将从集群中移除指定 release。你可以通过  helm list  命令看到当前部署的所有 release： \n $ helm list\nNAME            VERSION UPDATED                         STATUS          CHART\ninky-cat        1       Wed Sep 28 12:59:46 2016        DEPLOYED        alpine-0.1.0\n \n 1 2 3 从上面的输出中，我们可以看到， happy-panda  这个 release 已经被卸载。 \n 在上一个 Helm 版本中，当一个 release 被删除，会保留一条删除记录。而在 Helm 3 中，删除也会移除 release 的记录。 如果你想保留删除记录，使用  helm uninstall --keep-history 。使用  helm list --uninstalled  只会展示使用了  --keep-history  删除的 release。 \n helm list --all  会展示 Helm 保留的所有 release 记录，包括失败或删除的条目（指定了  --keep-history ）： \n $  helm list --all\nNAME            VERSION UPDATED                         STATUS          CHART\nhappy-panda     2       Wed Sep 28 12:47:54 2016        UNINSTALLED     wordpress-10.4.5.6.0\ninky-cat        1       Wed Sep 28 12:59:46 2016        DEPLOYED        alpine-0.1.0\nkindred-angelf  2       Tue Sep 27 16:16:10 2016        UNINSTALLED     alpine-0.1.0\n \n 1 2 3 4 5 注意，因为现在默认会删除 release，所以你不再能够回滚一个已经被卸载的资源了。 \n \'helm repo\'：使用仓库 \n Helm 3 不再附带一个默认的 chart 仓库。 helm repo  提供了一组命令用于添加、列出和移除仓库。 \n 使用  helm repo list  来查看配置的仓库： \n $ helm repo list\nNAME            URL\nstable          https://charts.helm.sh/stable\nmumoshu         https://mumoshu.github.io/charts\n \n 1 2 3 4 使用  helm repo add  来添加新的仓库： \n $ helm repo add dev https://example.com/dev-charts\n \n 1 因为 chart 仓库经常在变化，在任何时候你都可以通过执行  helm repo update  命令来确保你的 Helm 客户端是最新的。 \n 使用  helm repo remove  命令来移除仓库。 \n 创建自己的 charts \n 简单入门 \n chart 开发指南  介绍了如何开发你自己的chart。 但是你也可以通过使用  helm create  命令来快速开始： \n $ helm create deis-workflow\nCreating deis-workflow\n \n 1 2 现在， ./deis-workflow  目录下已经有一个 chart 了。你可以编辑它并创建你自己的模版。 \n \n Chart.yaml：用于描述这个 Chart 的基本信息，包括名字、描述信息以及版本等。 \n values.yaml ：用于存储 templates 目录中模板文件中用到变量的值。 \n Templates： 目录里面存放所有 yaml 模板文件。 \n charts：目录里存放这个 chart 依赖的所有子 chart。 \n NOTES.txt ：用于介绍 Chart 帮助信息， helm install 部署后展示给用户。例如： 如何使用这个 Chart、列出缺省的设置等。 \n helpers.tpl：放置模板助手的地方，可以在整个 chart 中重复使用 \n \n 在编辑 chart 时，可以通过  helm lint  验证格式是否正确。 \n 2 在templates文件夹下创建两个yaml文件 \n deployment.yaml \n \n service.yaml \n \n 3 安装mychart \n \n 4 应用升级 \nhelm upgrade [chart名称] 目录 \n \n 5 chart模板使用 \n通过传递参数，动态渲染模板，yaml内容动态传入参数生成，实现yaml高效复用。chart中有values.vaml文件，定义yaml全局变量 \nyaml文件大体有如下几个地方不同 \n image \ntag \nlabel \nport \nreplicas \n1 在values.yaml文件定义变量 \n \n 2 在templates的yaml文件引用变量 \n \n 通过Values前缀引用值，通过Release动态生成名字 \n 如果需要打包，也可以通过打包安装 \n 当准备将 chart 打包分发时，你可以运行  helm package  命令： \n $ helm package deis-workflow\ndeis-workflow-0.1.0.tgz\n \n 1 2 然后这个 chart 就可以很轻松的通过  helm install  命令安装： \n $ helm install deis-workflow ./deis-workflow-0.1.0.tgz\n...\n \n 1 2 打包好的 chart 可以上传到 chart 仓库中 \n'},{title:"sqoop",frontmatter:{title:"sqoop",date:"2022-05-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["中间件","数据同步"],tags:["离线同步数据","近实时"]},regularPath:"/%E4%B8%AD%E9%97%B4%E4%BB%B6/Sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.html",relativePath:"中间件/Sqoop基本使用.md",key:"v-7e69d236",path:"/2022/05/10/sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/",headers:[{level:2,title:"一、Sqoop 基本命令",slug:"一、sqoop-基本命令"},{level:3,title:"1. 查看所有命令",slug:"_1-查看所有命令"},{level:3,title:"2. 查看某条命令的具体使用方法",slug:"_2-查看某条命令的具体使用方法"},{level:2,title:"二、Sqoop 与 MySQL",slug:"二、sqoop-与-mysql"},{level:3,title:"1. 查询MySQL所有数据库",slug:"_1-查询mysql所有数据库"},{level:3,title:"2. 查询指定数据库中所有数据表",slug:"_2-查询指定数据库中所有数据表"},{level:2,title:"三、Sqoop 与 HDFS",slug:"三、sqoop-与-hdfs"},{level:3,title:"3.1 MySQL数据导入到HDFS",slug:"_3-1-mysql数据导入到hdfs"},{level:3,title:"3.2 HDFS数据导出到MySQL",slug:"_3-2-hdfs数据导出到mysql"},{level:2,title:"四、Sqoop 与 Hive",slug:"四、sqoop-与-hive"},{level:3,title:"4.1 MySQL数据导入到Hive",slug:"_4-1-mysql数据导入到hive"},{level:3,title:"4.2 Hive 导出数据到MySQL",slug:"_4-2-hive-导出数据到mysql"},{level:2,title:"五、Sqoop 与 HBase",slug:"五、sqoop-与-hbase"},{level:3,title:"5.1 MySQL导入数据到HBase",slug:"_5-1-mysql导入数据到hbase"},{level:2,title:"六、全库导出",slug:"六、全库导出"},{level:2,title:"七、Sqoop 数据过滤",slug:"七、sqoop-数据过滤"},{level:3,title:"7.1 query参数",slug:"_7-1-query参数"},{level:3,title:"7.2 增量导入",slug:"_7-2-增量导入"},{level:2,title:"八、类型支持",slug:"八、类型支持"},{level:2,title:"参考资料",slug:"参考资料"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:" 一、Sqoop 基本命令 \n 1. 查看所有命令 \nsqoop help \n \n 1 #  2. 查看某条命令的具体使用方法 \nsqoop help 命令名 \n \n 1 #  二、Sqoop 与 MySQL \n 1. 查询MySQL所有数据库 \n 通常用于 Sqoop 与 MySQL 连通测试： \n sqoop list-databases  \\ \n --connect  jdbc:mysql://hadoop001:3306/  \\ \n --username  root  \\ \n --password  root\n \n 1 2 3 4 #  2. 查询指定数据库中所有数据表 \n sqoop list-tables  \\ \n --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n --username  root  \\ \n --password  root\n \n 1 2 3 4 #  三、Sqoop 与 HDFS \n 3.1 MySQL数据导入到HDFS \n 1. 导入命令 \n 示例：导出 MySQL 数据库中的  help_keyword  表到 HDFS 的  /sqoop  目录下，如果导入目录存在则先删除再导入，使用 3 个  map tasks  并行导入。 \n \n 注：help_keyword 是 MySQL 内置的一张字典表，之后的示例均使用这张表。 \n \n sqoop  import   \\ \n --connect  jdbc:mysql://hadoop001:3306/mysql  \\      \n --username  root  \\ \n --password  root  \\ \n --table  help_keyword  \\             # 待导入的表 \n--delete-target-dir  \\              # 目标目录存在则先删除 \n--target-dir /sqoop  \\              # 导入的目标目录 \n--fields-terminated-by  '\\t'    \\     # 指定导出数据的分隔符 \n -m   3                               # 指定并行执行的 map tasks 数量 \n \n 1 2 3 4 5 6 7 8 9 日志输出如下，可以看到输入数据被平均  split  为三份，分别由三个  map task  进行处理。数据默认以表的主键列作为拆分依据，如果你的表没有主键，有以下两种方案： \n \n 添加  -- autoreset-to-one-mapper  参数，代表只启动一个  map task ，即不并行执行； \n 若仍希望并行执行，则可以使用  --split-by <column-name>  指明拆分数据的参考列。 \n 2. 导入验证 \n查看导入后的目录 \nhadoop fs  -ls    -R  /sqoop\n查看导入内容 \nhadoop fs  -text   /sqoop/part-m-00000\n \n 1 2 3 4 查看 HDFS 导入目录,可以看到表中数据被分为 3 部分进行存储，这是由指定的并行度决定的。 \n 3.2 HDFS数据导出到MySQL \n sqoop  export    \\ \n     --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n     --username  root  \\ \n     --password  root  \\ \n     --table  help_keyword_from_hdfs  \\          # 导出数据存储在 MySQL 的 help_keyword_from_hdf 的表中 \n    --export-dir /sqoop   \\ \n    --input-fields-terminated-by  '\\t' \\ \n     --m   3  \n \n 1 2 3 4 5 6 7 8 表必须预先创建，建表语句如下： \n CREATE   TABLE  help_keyword_from_hdfs  LIKE  help_keyword  ; \n \n 1 #  四、Sqoop 与 Hive \n 4.1 MySQL数据导入到Hive \n Sqoop 导入数据到 Hive 是通过先将数据导入到 HDFS 上的临时目录，然后再将数据从 HDFS 上  Load  到 Hive 中，最后将临时目录删除。可以使用  target-dir  来指定临时目录。 \n 1. 导入命令 \n sqoop  import   \\ \n   --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n   --username  root  \\ \n   --password  root  \\ \n   --table  help_keyword  \\          # 待导入的表      \n  --delete-target-dir  \\           # 如果临时目录存在删除 \n  --target-dir /sqoop_hive   \\     # 临时目录位置 \n  --hive-database sqoop_test  \\    # 导入到 Hive 的 sqoop_test 数据库，数据库需要预先创建。不指定则默认为 default 库 \n  --hive-import  \\                 # 导入到 Hive \n  --hive-overwrite  \\              # 如果 Hive 表中有数据则覆盖，这会清除表中原有的数据，然后再写入 \n   -m   3                            # 并行度 \n \n 1 2 3 4 5 6 7 8 9 10 11 导入到 Hive 中的  sqoop_test  数据库需要预先创建，不指定则默认使用 Hive 中的  default  库。 \n查看 hive 中的所有数据库 \n hive >   SHOW DATABASES ; \n创建 sqoop_test 数据库 \n hive >   CREATE DATABASE sqoop_test ; \n \n 1 2 3 4 #  2. 导入验证 \n查看 sqoop_test 数据库的所有表 \n hive >   SHOW  TABLES  IN  sqoop_test ; \n查看表中数据 \n hive >  SELECT * FROM sqoop_test.help_keyword ; \n \n 1 2 3 4 #  3. 可能出现的问题 \n 如果执行报错  java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf ，则需将 Hive 安装目录下  lib  下的  hive-exec-**.jar  放到 sqoop 的  lib  。 \n [ root@hadoop001 lib ] # ll hive-exec-* \n-rw-r--r--.  1   1106   4001   19632031   11  月  13   21 :45 hive-exec-1.1.0-cdh5.15.2.jar\n [ root@hadoop001 lib ] # cp hive-exec-1.1.0-cdh5.15.2.jar  ${SQOOP_HOME}/lib \n \n 1 2 3 \n 4.2 Hive 导出数据到MySQL \n 由于 Hive 的数据是存储在 HDFS 上的，所以 Hive 导入数据到 MySQL，实际上就是 HDFS 导入数据到 MySQL。 \n 1. 查看Hive表在HDFS的存储位置 \n进入对应的数据库 \nhive >  use sqoop_test ; \n查看表信息 \nhive >  desc formatted help_keyword ; \n \n 1 2 3 4 Location  属性为其存储位置： \n 这里可以查看一下这个目录，文件结构如下： \n 3.2 执行导出命令 \n sqoop  export    \\ \n     --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n     --username  root  \\ \n     --password  root  \\ \n     --table  help_keyword_from_hive  \\ \n    --export-dir /user/hive/warehouse/sqoop_test.db/help_keyword   \\ \n    -input-fields-terminated-by  '\\001'   \\               # 需要注意的是 hive 中默认的分隔符为 \\001 \n     --m   3  \n \n 1 2 3 4 5 6 7 8 MySQL 中的表需要预先创建： \n CREATE   TABLE  help_keyword_from_hive  LIKE  help_keyword  ; \n \n 1 #  五、Sqoop 与 HBase \n \n 本小节只讲解从 RDBMS 导入数据到 HBase，因为暂时没有命令能够从 HBase 直接导出数据到 RDBMS。 \n 5.1 MySQL导入数据到HBase \n 1. 导入数据 \n 将  help_keyword  表中数据导入到 HBase 上的  help_keyword_hbase  表中，使用原表的主键  help_keyword_id  作为  RowKey ，原表的所有列都会在  keywordInfo  列族下，目前只支持全部导入到一个列族下，不支持分别指定列族。 \n sqoop  import   \\ \n     --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n     --username  root  \\ \n     --password  root  \\ \n     --table  help_keyword  \\                # 待导入的表 \n    --hbase-table help_keyword_hbase  \\    # hbase 表名称，表需要预先创建 \n    --column-family keywordInfo  \\         # 所有列导入到 keywordInfo 列族下  \n    --hbase-row-key help_keyword_id      # 使用原表的 help_keyword_id 作为 RowKey \n \n 1 2 3 4 5 6 7 8 导入的 HBase 表需要预先创建： \n查看所有表 \nhbase >  list\n创建表 \nhbase >  create  'help_keyword_hbase' ,  'keywordInfo' \n查看表信息 \nhbase >  desc  'help_keyword_hbase' \n \n 1 2 3 4 5 6 #  2. 导入验证 \n 使用  scan  查看表数据： \n 六、全库导出 \n Sqoop 支持通过  import-all-tables  命令进行全库导出到 HDFS/Hive，但需要注意有以下两个限制： \n \n 所有表必须有主键；或者使用  --autoreset-to-one-mapper ，代表只启动一个  map task ; \n 你不能使用非默认的分割列，也不能通过 WHERE 子句添加任何限制。 \n \n \n 第二点解释得比较拗口，这里列出官方原本的说明： \n \n You must not intend to use non-default splitting column, nor impose any conditions via a  WHERE  clause. \n \n \n 全库导出到 HDFS： \n sqoop import-all-tables  \\ \n     --connect  jdbc:mysql://hadoop001:3306/数据库名  \\ \n     --username  root  \\ \n     --password  root  \\ \n    --warehouse-dir  /sqoop_all  \\       # 每个表会单独导出到一个目录，需要用此参数指明所有目录的父目录 \n    --fields-terminated-by  '\\t'    \\ \n     -m   3 \n \n 1 2 3 4 5 6 7 全库导出到 Hive： \n sqoop import-all-tables  -Dorg.apache.sqoop.splitter.allow_text_splitter = true  \\ \n   --connect  jdbc:mysql://hadoop001:3306/数据库名  \\ \n   --username  root  \\ \n   --password  root  \\ \n  --hive-database sqoop_test  \\           # 导出到 Hive 对应的库    \n  --hive-import  \\ \n  --hive-overwrite  \\ \n   -m   3 \n \n 1 2 3 4 5 6 7 8 #  七、Sqoop 数据过滤 \n 7.1 query参数 \n Sqoop 支持使用  query  参数定义查询 SQL，从而可以导出任何想要的结果集。sqoop暂不支持export +query使用示例如下： \n sqoop  import   \\ \n   --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n   --username  root  \\ \n   --password  root  \\ \n   --query   'select * from help_keyword where  $CONDITIONS and  help_keyword_id < 50'   \\   \n  --delete-target-dir  \\             \n  --target-dir /sqoop_hive   \\  \n  --hive-database sqoop_test  \\             # 指定导入目标数据库 不指定则默认使用 Hive 中的 default 库 \n  --hive-table filter_help_keyword  \\       # 指定导入目标表 \n  --split-by help_keyword_id  \\             # 指定用于 split 的列       \n  --hive-import  \\                          # 导入到 Hive \n  --hive-overwrite  \\                      、\n   -m   3                                   \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 在使用  query  进行数据过滤时，需要注意以下三点： \n \n 必须用  --hive-table  指明目标表； \n 如果并行度  -m  不为 1 或者没有指定  --autoreset-to-one-mapper ，则需要用  --split-by  指明参考列； \n SQL 的  where  字句必须包含  $CONDITIONS ，这是固定写法，作用是动态替换。 \n 7.2 增量导入 \n sqoop  import   \\ \n     --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n     --username  root  \\ \n     --password  root  \\ \n     --table  help_keyword  \\ \n    --target-dir /sqoop_hive   \\ \n    --hive-database sqoop_test  \\          \n     --incremental   append   \\               # 指明模式 \n    --check-column  help_keyword_id  \\      # 指明用于增量导入的参考列 \n    --last-value  300    \\                    # 指定参考列上次导入的最大值 \n    --hive-import  \\    \n     -m   3   \n \n 1 2 3 4 5 6 7 8 9 10 11 12 incremental  参数有以下两个可选的选项： \n \n append ：要求参考列的值必须是递增的，所有大于  last-value  的值都会被导入； \n lastmodified ：要求参考列的值必须是  timestamp  类型，且插入数据时候要在参考列插入当前时间戳，更新数据时也要更新参考列的时间戳，所有时间晚于  last-value  的数据都会被导入。 \n \n 通过上面的解释我们可以看出来，其实 Sqoop 的增量导入并没有太多神器的地方，就是依靠维护的参考列来判断哪些是增量数据。当然我们也可以使用上面介绍的  query  参数来进行手动的增量导入，这样反而更加灵活。 \n 八、类型支持 \n Sqoop 默认支持数据库的大多数字段类型，但是某些特殊类型是不支持的。遇到不支持的类型，程序会抛出异常  Hive does not support the SQL type for column xxx  异常，此时可以通过下面两个参数进行强制类型转换： \n \n --map-column-java<mapping>    ：重写 SQL 到 Java 类型的映射； \n --map-column-hive <mapping>  ： 重写 Hive 到 Java 类型的映射。 \n \n 示例如下，将原先  id  字段强制转为 String 类型， value  字段强制转为 Integer 类型： \n $ sqoop import ... --map-column-java id=String,value=Integer\n \n 1 导入和导出的目标表不能有数据。 \n 参考资料 \n Sqoop User Guide (v1.4.7) \n"},{title:"linux常用命令",frontmatter:{title:"linux常用命令",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["其他"],tags:["linux命令"]},regularPath:"/%E5%85%B6%E4%BB%96/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93.html",relativePath:"其他/linux命令总结.md",key:"v-cdeeea2e",path:"/2023/06/10/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/",headers:[{level:2,title:"1. 基本命令",slug:"_1-基本命令"},{level:2,title:"2. 关机",slug:"_2-关机"},{level:2,title:"3. 文件和目录",slug:"_3-文件和目录"},{level:2,title:"4. 文件搜索",slug:"_4-文件搜索"},{level:2,title:"5. 挂载一个文件系统",slug:"_5-挂载一个文件系统"},{level:2,title:"6. 磁盘空间",slug:"_6-磁盘空间"},{level:2,title:"7. 用户和群组",slug:"_7-用户和群组"},{level:2,title:"8. 文件的权限 使用 “+” 设置权限，使用 “-” 用于取消",slug:"_8-文件的权限-使用-设置权限-使用-用于取消"},{level:2,title:"9. 文件的特殊属性 ，使用 “+” 设置权限，使用 “-” 用于取消",slug:"_9-文件的特殊属性-使用-设置权限-使用-用于取消"},{level:2,title:"10. 打包和压缩文件",slug:"_10-打包和压缩文件"},{level:2,title:"11. RPM 包",slug:"_11-rpm-包"},{level:2,title:"12. YUM 软件包升级器",slug:"_12-yum-软件包升级器"},{level:2,title:"13. deb 包",slug:"_13-deb-包"},{level:2,title:"14. 查看文件内容",slug:"_14-查看文件内容"},{level:2,title:"15. 文本处理",slug:"_15-文本处理"},{level:2,title:"16. 字符设置和文件格式转换",slug:"_16-字符设置和文件格式转换"},{level:2,title:"17. 文件系统分析",slug:"_17-文件系统分析"},{level:2,title:"18. 初始化一个文件系统",slug:"_18-初始化一个文件系统"},{level:2,title:"19. SWAP 文件系统",slug:"_19-swap-文件系统"},{level:2,title:"20. 备份",slug:"_20-备份"},{level:2,title:"21. 光盘",slug:"_21-光盘"},{level:2,title:"22. 网络（以太网和 WIFI 无线）",slug:"_22-网络-以太网和-wifi-无线"},{level:2,title:"23. 列出目录内容",slug:"_23-列出目录内容"},{level:2,title:"24. 查看文件的类型",slug:"_24-查看文件的类型"},{level:2,title:"25. 复制文件目录",slug:"_25-复制文件目录"},{level:2,title:"26. 系统常用命令",slug:"_26-系统常用命令"},{level:3,title:"26.1、显示命令",slug:"_26-1、显示命令"},{level:3,title:"26.2、输出查看命令",slug:"_26-2、输出查看命令"},{level:3,title:"26.3、查看硬件信息",slug:"_26-3、查看硬件信息"},{level:3,title:"26.4、关机、重启",slug:"_26-4、关机、重启"},{level:3,title:"26.5、归档、压缩",slug:"_26-5、归档、压缩"},{level:3,title:"26.6、查找",slug:"_26-6、查找"},{level:3,title:"26.7、ctrl+c :终止当前的命令",slug:"_26-7、ctrl-c-终止当前的命令"},{level:3,title:"26.8、who 或 w 命令",slug:"_26-8、who-或-w-命令"},{level:3,title:"26.9、dmesg 命令",slug:"_26-9、dmesg-命令"},{level:3,title:"26.10、df 命令",slug:"_26-10、df-命令"},{level:3,title:"26.11、du 命令",slug:"_26-11、du-命令"},{level:3,title:"26.12、free 命令",slug:"_26-12、free-命令"},{level:2,title:"27. VIM",slug:"_27-vim"},{level:2,title:"28. 软件包管理命令(RPM)",slug:"_28-软件包管理命令-rpm"},{level:3,title:"28.1、软件包的安装",slug:"_28-1、软件包的安装"},{level:3,title:"28.2、软件包的删除",slug:"_28-2、软件包的删除"},{level:3,title:"28.3、软件包升级",slug:"_28-3、软件包升级"},{level:3,title:"28.4、软件包更新",slug:"_28-4、软件包更新"},{level:3,title:"28.5、软件包查询",slug:"_28-5、软件包查询"},{level:3,title:"29.服务器之间传输数据",slug:"_29-服务器之间传输数据"},{level:2,title:"3、发送json格式请求：",slug:"_3、发送json格式请求"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:" 1. 基本命令 \n uname -m 显示机器的处理器架构\nuname -r 显示正在使用的内核版本\ndmidecode -q 显示硬件系统部件\n(SMBIOS / DMI) hdparm -i /dev/hda 罗列一个磁盘的架构特性\nhdparm -tT /dev/sda 在磁盘上执行测试性读取操作系统信息\narch 显示机器的处理器架构\nuname -m 显示机器的处理器架构\nuname -r 显示正在使用的内核版本\ndmidecode -q 显示硬件系统部件 - (SMBIOS / DMI)\nhdparm -i /dev/hda 罗列一个磁盘的架构特性\nhdparm -tT /dev/sda 在磁盘上执行测试性读取操作\ncat /proc/cpuinfo 显示CPU info的信息\ncat /proc/interrupts 显示中断\ncat /proc/meminfo 校验内存使用\ncat /proc/swaps 显示哪些swap被使用\ncat /proc/version 显示内核的版本\ncat /proc/net/dev 显示网络适配器及统计\ncat /proc/mounts 显示已加载的文件系统\nlspci -tv 罗列 PCI 设备\nlsusb -tv 显示 USB 设备\ndate 显示系统日期\ncal 2007 显示2007年的日历表\ndate 041217002007.00 设置日期和时间 - 月日时分年.秒\nclock -w 将时间修改保存到 BIOS\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #  2. 关机 \n shutdown -h now 关闭系统(1)\ninit 0 关闭系统(2)\ntelinit 0 关闭系统(3)\nshutdown -h hours:minutes & 按预定时间关闭系统\nshutdown -c 取消按预定时间关闭系统\nshutdown -r now 重启(1)\nreboot 重启(2)\nlogout 注销\n \n 1 2 3 4 5 6 7 8 #  3. 文件和目录 \n cd /home 进入 '/ home' 目录'\ncd .. 返回上一级目录\ncd ../.. 返回上两级目录\ncd 进入个人的主目录\ncd ~user1 进入个人的主目录\ncd - 返回上次所在的目录\npwd 显示工作路径\nls 查看目录中的文件\nls -F 查看目录中的文件\nls -l 显示文件和目录的详细资料\nls -a 显示隐藏文件\nls *[0-9]* 显示包含数字的文件名和目录名\ntree 显示文件和目录由根目录开始的树形结构(1)\nlstree 显示文件和目录由根目录开始的树形结构(2)\nmkdir dir1 创建一个叫做 'dir1' 的目录'\nmkdir dir1 dir2 同时创建两个目录\nmkdir -p /tmp/dir1/dir2 创建一个目录树\nrm -f file1 删除一个叫做 'file1' 的文件'\nrmdir dir1 删除一个叫做 'dir1' 的目录'\nrm -rf dir1 删除一个叫做 'dir1' 的目录并同时删除其内容\nrm -rf dir1 dir2 同时删除两个目录及它们的内容\nmv dir1 new_dir 重命名/移动 一个目录\ncp file1 file2 复制一个文件\ncp dir/* . 复制一个目录下的所有文件到当前工作目录\ncp -a /tmp/dir1 . 复制一个目录到当前工作目录\ncp -a dir1 dir2 复制一个目录\nln -s file1 lnk1 创建一个指向文件或目录的软链接\nln file1 lnk1 创建一个指向文件或目录的物理链接\ntouch -t 0712250000 file1 修改一个文件或目录的时间戳 - (YYMMDDhhmm)\nfile file1 outputs the mime type of the file as text\niconv -l 列出已知的编码\niconv -f fromEncoding -t toEncoding inputFile > outputFile creates a new from the given input file by assuming it is encoded in fromEncoding and converting it to toEncoding.\nfind . -maxdepth 1 -name *.jpg -print -exec convert \"{}\" -resize 80x60 \"thumbs/{}\" \\; batch resize files in the current directory and send them to a thumbnails directory (requires convert from Imagemagick)\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #  4. 文件搜索 \n find / -name file1 从 '/' 开始进入根文件系统搜索文件和目录\nfind / -user user1 搜索属于用户 'user1' 的文件和目录\nfind /home/user1 -name \\*.bin 在目录 '/ home/user1' 中搜索带有'.bin' 结尾的文件\nfind /usr/bin -type f -atime +100 搜索在过去100天内未被使用过的执行文件\nfind /usr/bin -type f -mtime -10 搜索在10天内被创建或者修改过的文件\nfind / -name \\*.rpm -exec chmod 755 '{}' \\; 搜索以 '.rpm' 结尾的文件并定义其权限\nfind / -xdev -name \\*.rpm 搜索以 '.rpm' 结尾的文件，忽略光驱、捷盘等可移动设备\nlocate \\*.ps 寻找以 '.ps' 结尾的文件 - 先运行 'updatedb' 命令\nwhereis halt 显示一个二进制文件、源码或man的位置\nwhich halt 显示一个二进制文件或可执行文件的完整路径\n \n 1 2 3 4 5 6 7 8 9 10 #  5. 挂载一个文件系统 \n mount /dev/hda2 /mnt/hda2 挂载一个叫做hda2的盘 - 确定目录 '/ mnt/hda2' 已经存在\numount /dev/hda2 卸载一个叫做hda2的盘 - 先从挂载点 '/ mnt/hda2' 退出\nfuser -km /mnt/hda2 当设备繁忙时强制卸载\numount -n /mnt/hda2 运行卸载操作而不写入 /etc/mtab 文件- 当文件为只读或当磁盘写满时非常有用\nmount /dev/fd0 /mnt/floppy 挂载一个软盘\nmount /dev/cdrom /mnt/cdrom 挂载一个cdrom或dvdrom\nmount /dev/hdc /mnt/cdrecorder 挂载一个cdrw或dvdrom\nmount /dev/hdb /mnt/cdrecorder 挂载一个cdrw或dvdrom\nmount -o loop file.iso /mnt/cdrom 挂载一个文件或ISO镜像文件\nmount -t vfat /dev/hda5 /mnt/hda5 挂载一个Windows FAT32文件系统\nmount /dev/sda1 /mnt/usbdisk 挂载一个usb 捷盘或闪存设备\nmount -t smbfs -o username=user,password=pass //WinClient/share /mnt/share 挂载一个windows网络共享\n \n 1 2 3 4 5 6 7 8 9 10 11 12 #虚拟机挂载宿主机共享目录 \n #查看当前虚拟机的共享目录 \nvmware-hgfsclient\n #执行如下命令可完成挂载 \nvmhgfs-fuse .host:/ /mnt/hgfs/\n #用以下方法可设置开机自动挂载： \n #打开文件 \n vim  /etc/rc.local\n #在最后添加一行： \nvmhgfs-fuse .host:/ /mnt/hgfs\n \n 1 2 3 4 5 6 7 8 9 10 #  6. 磁盘空间 \n df -h 显示已经挂载的分区列表\nls -lSr |more 以尺寸大小排列文件和目录\ndu -sh dir1 估算目录 'dir1' 已经使用的磁盘空间'\ndu -sk * | sort -rn 以容量大小为依据依次显示文件和目录的大小\nrpm -q -a --qf '%10{SIZE}t%{NAME}n' | sort -k1,1n 以大小为依据依次显示已安装的rpm包所使用的空间 (fedora, redhat类系统)\ndpkg-query -W -f='${Installed-Size;10}t${Package}n' | sort -k1,1n 以大小为依据显示已安装的deb包所使用的空间 (ubuntu, debian类系统)\n \n 1 2 3 4 5 6 #  7. 用户和群组 \n groupadd group_name 创建一个新用户组\ngroupdel group_name 删除一个用户组\ngroupmod -n new_group_name old_group_name 重命名一个用户组\nuseradd -c \"Name Surname \" -g admin -d /home/user1 -s /bin/bash user1 创建一个属于 \"admin\" 用户组的用户\nuseradd user1 创建一个新用户\nuserdel -r user1 删除一个用户 ( '-r' 排除主目录)\nusermod -c \"User FTP\" -g system -d /ftp/user1 -s /bin/nologin user1 修改用户属性\npasswd 修改口令\npasswd user1 修改一个用户的口令 (只允许root执行)\nchage -E 2005-12-31 user1 设置用户口令的失效期限\npwck 检查 '/etc/passwd' 的文件格式和语法修正以及存在的用户\ngrpck 检查 '/etc/passwd' 的文件格式和语法修正以及存在的群组\nnewgrp group_name 登陆进一个新的群组以改变新创建文件的预设群组\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 #  8. 文件的权限 使用 “+” 设置权限，使用 “-” 用于取消 \n ls -lh 显示权限\nls /tmp | pr -T5 -W$COLUMNS 将终端划分成5栏显示\nchmod ugo+rwx directory1 设置目录的所有人(u)、群组(g)以及其他人(o)以读（r ）、写(w)和执行(x)的权限\nchmod go-rwx directory1 删除群组(g)与其他人(o)对目录的读写执行权限\nchown user1 file1 改变一个文件的所有人属性\nchown -R user1 directory1 改变一个目录的所有人属性并同时改变改目录下所有文件的属性\nchgrp group1 file1 改变文件的群组\nchown user1:group1 file1 改变一个文件的所有人和群组属性\nfind / -perm -u+s 罗列一个系统中所有使用了SUID控制的文件\nchmod u+s /bin/file1 设置一个二进制文件的 SUID 位 - 运行该文件的用户也被赋予和所有者同样的权限\nchmod u-s /bin/file1 禁用一个二进制文件的 SUID位\nchmod g+s /home/public 设置一个目录的SGID 位 - 类似SUID ，不过这是针对目录的\nchmod g-s /home/public 禁用一个目录的 SGID 位\nchmod o+t /home/public 设置一个文件的 STIKY 位 - 只允许合法所有人删除文件\nchmod o-t /home/public 禁用一个目录的 STIKY 位\nchmod +x 文件路径 为所有者、所属组和其他用户添加执行的权限\nchmod -x 文件路径 为所有者、所属组和其他用户删除执行的权限\nchmod u+x 文件路径 为所有者添加执行的权限\nchmod g+x 文件路径 为所属组添加执行的权限\nchmod o+x 文件路径 为其他用户添加执行的权限\nchmod ug+x 文件路径 为所有者、所属组添加执行的权限\nchmod =wx 文件路径 为所有者、所属组和其他用户添加写、执行的权限，取消读权限\nchmod ug=wx 文件路径 为所有者、所属组添加写、执行的权限，取消读权限\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #  9. 文件的特殊属性 ，使用 “+” 设置权限，使用 “-” 用于取消 \n chattr +a file1 只允许以追加方式读写文件\nchattr +c file1 允许这个文件能被内核自动压缩/解压\nchattr +d file1 在进行文件系统备份时，dump程序将忽略这个文件\nchattr +i file1 设置成不可变的文件，不能被删除、修改、重命名或者链接\nchattr +s file1 允许一个文件被安全地删除\nchattr +S file1 一旦应用程序对这个文件执行了写操作，使系统立刻把修改的结果写到磁盘\nchattr +u file1 若文件被删除，系统会允许你在以后恢复这个被删除的文件\nlsattr 显示特殊的属性\n \n 1 2 3 4 5 6 7 8 #  10. 打包和压缩文件 \n bunzip2 file1.bz2 解压一个叫做 'file1.bz2'的文件\nbzip2 file1 压缩一个叫做 'file1' 的文件\ngunzip file1.gz 解压一个叫做 'file1.gz'的文件\ngzip file1 压缩一个叫做 'file1'的文件\ngzip -9 file1 最大程度压缩\nrar a file1.rar test_file 创建一个叫做 'file1.rar' 的包\nrar a file1.rar file1 file2 dir1 同时压缩 'file1', 'file2' 以及目录 'dir1'\nrar x file1.rar 解压rar包\nunrar x file1.rar 解压rar包\ntar -cvf archive.tar file1 创建一个非压缩的 tarball\ntar -cvf archive.tar file1 file2 dir1 创建一个包含了 'file1', 'file2' 以及 'dir1'的档案文件\ntar -tf archive.tar 显示一个包中的内容\ntar -xvf archive.tar 释放一个包\ntar -xvf archive.tar -C /tmp 将压缩包释放到 /tmp目录下\ntar -cvfj archive.tar.bz2 dir1 创建一个bzip2格式的压缩包\ntar -xvfj archive.tar.bz2 解压一个bzip2格式的压缩包\ntar -cvfz archive.tar.gz dir1 创建一个gzip格式的压缩包\ntar -xvfz archive.tar.gz 解压一个gzip格式的压缩包\nzip file1.zip file1 创建一个zip格式的压缩包\nzip -r file1.zip file1 file2 dir1 将几个文件和目录同时压缩成一个zip格式的压缩包\nunzip file1.zip 解压一个zip格式压缩包\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #  11. RPM 包 \n rpm -ivh package.rpm 安装一个rpm包\nrpm -ivh --nodeeps package.rpm 安装一个rpm包而忽略依赖关系警告\nrpm -U package.rpm 更新一个rpm包但不改变其配置文件\nrpm -F package.rpm 更新一个确定已经安装的rpm包\nrpm -e package_name.rpm 删除一个rpm包\nrpm -qa 显示系统中所有已经安装的rpm包\nrpm -qa | grep httpd 显示所有名称中包含 \"httpd\" 字样的rpm包\nrpm -qi package_name 获取一个已安装包的特殊信息\nrpm -qg \"System Environment/Daemons\" 显示一个组件的rpm包\nrpm -ql package_name 显示一个已经安装的rpm包提供的文件列表\nrpm -qc package_name 显示一个已经安装的rpm包提供的配置文件列表\nrpm -q package_name --whatrequires 显示与一个rpm包存在依赖关系的列表\nrpm -q package_name --whatprovides 显示一个rpm包所占的体积\nrpm -q package_name --scripts 显示在安装/删除期间所执行的脚本l\nrpm -q package_name --changelog 显示一个rpm包的修改历史\nrpm -qf /etc/httpd/conf/httpd.conf 确认所给的文件由哪个rpm包所提供\nrpm -qp package.rpm -l 显示由一个尚未安装的rpm包提供的文件列表\nrpm --import /media/cdrom/RPM-GPG-KEY 导入公钥数字证书\nrpm --checksig package.rpm 确认一个rpm包的完整性\nrpm -qa gpg-pubkey 确认已安装的所有rpm包的完整性\nrpm -V package_name 检查文件尺寸、 许可、类型、所有者、群组、MD5检查以及最后修改时间\nrpm -Va 检查系统中所有已安装的rpm包- 小心使用\nrpm -Vp package.rpm 确认一个rpm包还未安装\nrpm2cpio package.rpm | cpio --extract --make-directories *bin* 从一个rpm包运行可执行文件\nrpm -ivh /usr/src/redhat/RPMS/`arch`/package.rpm 从一个rpm源码安装一个构建好的包\nrpmbuild --rebuild package_name.src.rpm 从一个rpm源码构建一个 rpm 包\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #  12. YUM 软件包升级器 \n yum install package_name 下载并安装一个rpm包\nyum localinstall package_name.rpm 将安装一个rpm包，使用你自己的软件仓库为你解决所有依赖关系\nyum update package_name.rpm 更新当前系统中所有安装的rpm包\nyum update package_name 更新一个rpm包\nyum remove package_name 删除一个rpm包\nyum list 列出当前系统中安装的所有包\nyum search package_name 在rpm仓库中搜寻软件包\nyum clean packages 清理rpm缓存删除下载的包\nyum clean headers 删除所有头文件\nyum clean all 删除所有缓存的包和头文件\n \n 1 2 3 4 5 6 7 8 9 10 #  13. deb 包 \n dpkg -i package.deb 安装/更新一个 deb 包\ndpkg -r package_name 从系统删除一个 deb 包\ndpkg -l 显示系统中所有已经安装的 deb 包\ndpkg -l | grep httpd 显示所有名称中包含 \"httpd\" 字样的deb包\ndpkg -s package_name 获得已经安装在系统中一个特殊包的信息\ndpkg -L package_name 显示系统中已经安装的一个deb包所提供的文件列表\ndpkg --contents package.deb 显示尚未安装的一个包所提供的文件列表\ndpkg -S /bin/ping 确认所给的文件由哪个deb包提供\nAPT 软件工具 (Debian, Ubuntu 以及类似系统)\napt-get install package_name 安装/更新一个 deb 包\napt-cdrom install package_name 从光盘安装/更新一个 deb 包\napt-get update 升级列表中的软件包\napt-get upgrade 升级所有已安装的软件\napt-get remove package_name 从系统删除一个deb包\napt-get check 确认依赖的软件仓库正确\napt-get clean 从下载的软件包中清理缓存\napt-cache search searched-package 返回包含所要搜索字符串的软件包名称\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #  14. 查看文件内容 \n cat file1 从第一个字节开始正向查看文件的内容\ntac file1 从最后一行开始反向查看一个文件的内容\nmore file1 查看一个长文件的内容\nless file1 类似于 'more' 命令，但是它允许在文件中和正向操作一样的反向操作\nhead -2 file1 查看一个文件的前两行\ntail -2 file1 查看一个文件的最后两行\ntail -f /var/log/messages 实时查看被添加到一个文件中的内容\n \n 1 2 3 4 5 6 7 #  15. 文本处理 \n cat file1 file2 ... | command <> file1_in.txt_or_file1_out.txt general syntax for text manipulation using PIPE, STDIN and STDOUT\ncat file1 | command( sed, grep, awk, grep, etc...) > result.txt 合并一个文件的详细说明文本，并将简介写入一个新文件中\ncat file1 | command( sed, grep, awk, grep, etc...) >> result.txt 合并一个文件的详细说明文本，并将简介写入一个已有的文件中\ngrep Aug /var/log/messages 在文件 '/var/log/messages'中查找关键词\"Aug\"\ngrep ^Aug /var/log/messages 在文件 '/var/log/messages'中查找以\"Aug\"开始的词汇\ngrep [0-9] /var/log/messages 选择 '/var/log/messages' 文件中所有包含数字的行\ngrep Aug -R /var/log/* 在目录 '/var/log' 及随后的目录中搜索字符串\"Aug\"\nsed 's/stringa1/stringa2/g' example.txt 将example.txt文件中的 \"string1\" 替换成 \"string2\"\nsed '/^$/d' example.txt 从example.txt文件中删除所有空白行\nsed '/ *#/d; /^$/d' example.txt 从example.txt文件中删除所有注释和空白行\necho 'esempio' | tr '[:lower:]' '[:upper:]' 合并上下单元格内容\nsed -e '1d' result.txt 从文件example.txt 中排除第一行\nsed -n '/stringa1/p' 查看只包含词汇 \"string1\"的行\nsed -e 's/ *$//' example.txt 删除每一行最后的空白字符\nsed -e 's/stringa1//g' example.txt 从文档中只删除词汇 \"string1\" 并保留剩余全部\nsed -n '1,5p;5q' example.txt 查看从第一行到第5行内容\nsed -n '5p;5q' example.txt 查看第5行\nsed -e 's/00*/0/g' example.txt 用单个零替换多个零\ncat -n file1 标示文件的行数\ncat example.txt | awk 'NR%2==1' 删除example.txt文件中的所有偶数行\necho a b c | awk '{print $1}' 查看一行第一栏\necho a b c | awk '{print $1,$3}' 查看一行的第一和第三栏\npaste file1 file2 合并两个文件或两栏的内容\npaste -d '+' file1 file2 合并两个文件或两栏的内容，中间用\"+\"区分\nsort file1 file2 排序两个文件的内容\nsort file1 file2 | uniq 取出两个文件的并集(重复的行只保留一份)\nsort file1 file2 | uniq -u 删除交集，留下其他的行\nsort file1 file2 | uniq -d 取出两个文件的交集(只留下同时存在于两个文件中的文件)\ncomm -1 file1 file2 比较两个文件的内容只删除 'file1' 所包含的内容\ncomm -2 file1 file2 比较两个文件的内容只删除 'file2' 所包含的内容\ncomm -3 file1 file2 比较两个文件的内容只删除两个文件共有的部分\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #  16. 字符设置和文件格式转换 \n dos2unix filedos.txt fileunix.txt 将一个文本文件的格式从MSDOS转换成UNIX\nunix2dos fileunix.txt filedos.txt 将一个文本文件的格式从UNIX转换成MSDOS\nrecode ..HTML < page.txt > page.html 将一个文本文件转换成html\nrecode -l | more 显示所有允许的转换格式\n \n 1 2 3 4 #  17. 文件系统分析 \n badblocks -v /dev/hda1 检查磁盘hda1上的坏磁块\nfsck /dev/hda1 修复/检查hda1磁盘上linux文件系统的完整性\nfsck.ext2 /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性\ne2fsck /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性\ne2fsck -j /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性\nfsck.ext3 /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性\nfsck.vfat /dev/hda1 修复/检查hda1磁盘上fat文件系统的完整性\nfsck.msdos /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性\ndosfsck /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性\n \n 1 2 3 4 5 6 7 8 9 #  18. 初始化一个文件系统 \n mkfs /dev/hda1 在hda1分区创建一个文件系统\nmke2fs /dev/hda1 在hda1分区创建一个linux ext2的文件系统\nmke2fs -j /dev/hda1 在hda1分区创建一个linux ext3(日志型)的文件系统\nmkfs -t vfat 32 -F /dev/hda1 创建一个 FAT32 文件系统\nfdformat -n /dev/fd0 格式化一个软盘\nmkswap /dev/hda3 创建一个swap文件系统\n \n 1 2 3 4 5 6 #  19. SWAP 文件系统 \n mkswap /dev/hda3 创建一个swap文件系统\nswapon /dev/hda3 启用一个新的swap文件系统\nswapon /dev/hda2 /dev/hdb3 启用两个swap分区\n \n 1 2 3 #  20. 备份 \n dump -0aj -f /tmp/home0.bak /home 制作一个 '/home' 目录的完整备份\ndump -1aj -f /tmp/home0.bak /home 制作一个 '/home' 目录的交互式备份\nrestore -if /tmp/home0.bak 还原一个交互式备份\nrsync -rogpav --delete /home /tmp 同步两边的目录\nrsync -rogpav -e ssh --delete /home ip_address:/tmp 通过SSH通道rsync\nrsync -az -e ssh --delete ip_addr:/home/public /home/local 通过ssh和压缩将一个远程目录同步到本地目录\nrsync -az -e ssh --delete /home/local ip_addr:/home/public 通过ssh和压缩将本地目录同步到远程目录\ndd bs=1M if=/dev/hda | gzip | ssh user@ip_addr 'dd of=hda.gz' 通过ssh在远程主机上执行一次备份本地磁盘的操作\ndd if=/dev/sda of=/tmp/file1 备份磁盘内容到一个文件\ntar -Puf backup.tar /home/user 执行一次对 '/home/user' 目录的交互式备份操作\n( cd /tmp/local/ && tar c . ) | ssh -C user@ip_addr 'cd /home/share/ && tar x -p' 通过ssh在远程目录中复制一个目录内容\n( tar c /home ) | ssh -C user@ip_addr 'cd /home/backup-home && tar x -p' 通过ssh在远程目录中复制一个本地目录\ntar cf - . | (cd /tmp/backup ; tar xf - ) 本地将一个目录复制到另一个地方，保留原有权限及链接\nfind /home/user1 -name '*.txt' | xargs cp -av --target-directory=/home/backup/ --parents 从一个目录查找并复制所有以 '.txt' 结尾的文件到另一个目录\nfind /var/log -name '*.log' | tar cv --files-from=- | bzip2 > log.tar.bz2 查找所有以 '.log' 结尾的文件并做成一个bzip包\ndd if=/dev/hda of=/dev/fd0 bs=512 count=1 做一个将 MBR (Master Boot Record)内容复制到软盘的动作\ndd if=/dev/fd0 of=/dev/hda bs=512 count=1 从已经保存到软盘的备份中恢复MBR内容\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #  21. 光盘 \n cdrecord -v gracetime=2 dev=/dev/cdrom -eject blank=fast -force 清空一个可复写的光盘内容\nmkisofs /dev/cdrom > cd.iso 在磁盘上创建一个光盘的iso镜像文件\nmkisofs /dev/cdrom | gzip > cd_iso.gz 在磁盘上创建一个压缩了的光盘iso镜像文件\nmkisofs -J -allow-leading-dots -R -V \"Label CD\" -iso-level 4 -o ./cd.iso data_cd 创建一个目录的iso镜像文件\ncdrecord -v dev=/dev/cdrom cd.iso 刻录一个ISO镜像文件\ngzip -dc cd_iso.gz | cdrecord dev=/dev/cdrom - 刻录一个压缩了的ISO镜像文件\nmount -o loop cd.iso /mnt/iso 挂载一个ISO镜像文件\ncd-paranoia -B 从一个CD光盘转录音轨到 wav 文件中\ncd-paranoia -- \"-3\" 从一个CD光盘转录音轨到 wav 文件中（参数-3）\ncdrecord --scanbus 扫描总线以识别scsi通道\ndd if=/dev/hdc | md5sum 校验一个设备的md5sum编码，例如一张 CD\n \n 1 2 3 4 5 6 7 8 9 10 11 #  22. 网络（以太网和 WIFI 无线） \n ifconfig eth0 显示一个以太网卡的配置\nifup eth0 启用一个 'eth0' 网络设备\nifdown eth0 禁用一个 'eth0' 网络设备\nifconfig eth0 192.168.1.1 netmask 255.255.255.0 控制IP地址\nifconfig eth0 promisc 设置 'eth0' 成混杂模式以嗅探数据包 (sniffing)\ndhclient eth0 以dhcp模式启用 'eth0'\nroute -n show routing table\nroute add -net 0/0 gw IP_Gateway configura default gateway\nroute add -net 192.168.0.0 netmask 255.255.0.0 gw 192.168.1.1 configure static route to reach network '192.168.0.0/16'\nroute del 0/0 gw IP_gateway remove static route\necho \"1\" > /proc/sys/net/ipv4/ip_forward activate ip routing\nhostname show hostname of system\nhost www.example.com lookup hostname to resolve name to ip address and viceversa(1)\nnslookup www.example.com lookup hostname to resolve name to ip address and viceversa(2)\nip link show show link status of all interfaces\nmii-tool eth0 show link status of 'eth0'\nethtool eth0 show statistics of network card 'eth0'\nnetstat -tup show all active network connections and their PID\nnetstat -tupl show all network services listening on the system and their PID\ntcpdump tcp port 80 show all HTTP traffic\niwlist scan show wireless networks\niwconfig eth1 show configuration of a wireless network card\nhostname show hostname\nhost www.example.com lookup hostname to resolve name to ip address and viceversa\nnslookup www.example.com lookup hostname to resolve name to ip address and viceversa\nwhois www.example.com lookup on Whois database\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #  23. 列出目录内容 \n ls -a：显示所有文件（包括隐藏文件）；\nls -l：显示详细信息；\nls -R：递归显示子目录结构；\nls -ld：显示目录和链接信息；\nctrl+r：历史记录中所搜命令（输入命令中的任意一个字符）；\nLinux中以.开头的文件是隐藏文件；\npwd:显示当前目录\n \n 1 2 3 4 5 6 7 #  24. 查看文件的类型 \n file:查看文件的类型\n \n 1 #  25. 复制文件目录 \n 1、 cp ：复制文件和目录 cp 源文件（文件夹）目标文件（文件夹） \n \n 常用参数：-r:递归复制整个目录树；-v：显示详细信息； \n 复制文件夹时要在 cp 命令后面加一个-r 参数： \n 如：cp -r 源文件夹 目标文件夹 \n \n 2、 touch+文件名 ：当文件不存在的时候，创建相应的文件；当文件存在的时候，修改文件的创建时间。 \n \n 功能：生成一个空文件或修改文件的存取/修改的时间记录值。 \n touch * ：将当前下的文件时间修改为系统的当前时间 \n touch –d 20040210 test：将 test 文件的日期改为 20040210 \n touch abc 　：若 abc 文件存在，则修改为系统的当前时间；若不存在，则生成一个为当前时间的空文件 \n \n 3、 mv 文件 目标目录 ：移动或重命名文件或目录（如果指定文件名，则可以重命名文件）。可以将文件及目录移到另一目录下，或更改文件及目录的名称。 \n \n 格式为：mv [参数]<源文件或目录> <目标文件或目录> \n mva.txt ../：将 a.txt 文件移动上层目录 \n mv a.txt b.txt：将 a.txt 改名为 b.txt \n mvdir2 ../：将 dir2 目录上移一层 \n \n 4、 rm ：删除文件； \n \n 常用参数：-i：交互式 -r：递归的删除包括目录中的所有内容 \n \n 5、 mkdir +文件夹名称 ：创建文件夹； \n 6、 rm -r +文件夹名称 ：删除文件夹（空文件夹和非空文件夹都可删除） \n \n rmdir 文件夹名称：删除文件夹（只能删除空文件夹） \n \n 7、 mkdir -p dir1/dir2 ：在当前目录下创建 dir1 目录，并在 dir1 目录下创建 dir2 目录， 也就是连续创建两个目录（dir1/和 dir1/dir2） \n 8、 rmdir –p dir1/dir2 ：删除 dir1 下的 dir2 目录，若 dir1 目录为空也删除它 \n 9、 rm * ：删除当前目录下的所有文件 \n 10、 -f 参数 ：强迫删除文件 rm –f *.txt：强迫删除所有以后缀名为 txt 文件 \n 11、 -i 参数 ：删除文件时询问 \n \n rm 　–i * ：删除当前目录下的所有文件会有如下提示： \n rm:backup:is a directory 　　　 遇到目录会略过 \n rm: remove ‘myfiles.txt’ ? Y \n 删除文件时会询问,可按 Y 或 N 键表示允许或拒绝删除文件 \n \n 12、 -r 参数 ：递归删除（连子目录一同删除，这是一个相当常用的参数） \n \n rm -r test ：删除 test 目录（含 test 目录下所有文件和子目录） \n rm -r *：删除所有文件（含当前目录所有文件、所有子目录和子目录下的文件） 一般在删除目录时 r 和 f 一起用，避免麻烦 \n rm -rf test ：强行删除、不加询问 \n \n 13、 grep ：功能：在文件中搜索匹配的字符并进行输出 \n \n 格式：grep[参数] <要找的字串> <要寻找字 串的源文件> \n greplinux test.txt：搜索 test.txt 文件中字符串 linux 并输出 \n \n 14、 ln 命令 \n \n 功能：在文件和目录之间建立链接 \n 格式：ln [参数] <源文件或目录> <目标文件或目录> \n 链接分“软链接”和“硬链接” \n 1.软链接: \n ln–s /usr/share/do doc ：创建一个链接文件 doc,并指向目录/usr/share/do \n 2.硬链接: \n ln /usr/share/test hard：创建一个硬链接文件 hard，这时对于 test 文件对应 的存储区域来说，又多了一个文件指向它 \n 26. 系统常用命令 \n 26.1、显示命令 \n date:查看或设置当前系统的时间：格式化显示时间：+%Y--%m--%d； \n date -s:设置当前系统的时间 \n hwclock(clock)：显示硬件时钟时间(需要管理员权限)； \n cal：查看日历 \n 格式 cal [参数] 月年 \n cal：显示当月的日历 cal4 2004 ：显示 2004 年 4 月的日历 \n cal- y 2003：显示 2003 年的日历 \n uptime：查看系统运行时间 \n 26.2、输出查看命令 \n echo：显示输入的内容 追加文件 echo \"liuyazhuang\" >> liuyazhuang.txt \n cat：显示文件内容,也可以将数个文件合并成一个文件。 \n 格式：格式：cat[参数]<文件名> \n cat test.txt：显示 test.txt 文件内容 \n cat test.txt | more ：逐页显示 test.txt 文件中的内容 \n cat test.txt >> test1.txt ：将 test.txt 的内容附加到 test1.txt 文件之后 \n cat test.txt test2.txt >readme.txt 　: 将 test.txt 和 test2.txt 文件合并成 readme.txt 文件 \n head:显示文件的头几行（默认 10 行） -n:指定显示的行数格式：head -n 文件名 \n tail：显示文件的末尾几行（默认 10 行）-n：指定显示的行数 -f：追踪显示文件更新 （一般用于查看日志，命令不会退出，而是持续显示新加入的内容） \n 格式：格式：tail[参数]<文件名> \n tail-10 /etc/passwd ：显示/etc/passwd/文件的倒数 10 行内容 \n tail+10 /etc/passwd ：显示/etc/passwd/文件从第 10 行开始到末尾的内容 \n more：用于翻页显示文件内容（只能向下翻页） \n more 命令是一般用于要显示的内容会超过一个画面长度的情况。为了避免画 面显示时瞬间就闪过去，用户可以使用 more 命令，让画面在显示满一页时暂停，此时可按空格键继续显示下一个画面，或按 Q 键停止显示。 \n ls -al |more：以长格形式显示 etc 目录下的文件列表，显示满一个画面便暂停，可 按空格键继续显示下一画面，或按 Q 键跳离 \n less：翻页显示文件内容（带上下翻页）按下上键分页，按 q 退出、‘ \n less 命令的用法与 more 命令类似，也可以用来浏览超过一页的文件。所不同 的是 less 命令除了可以按空格键向下显示文件外，还可以利用上下键来卷动文件。当要结束浏览时，只要在 less 命令的提示符“：”下按 Q 键即可。 \n ls -al | less：以长格形式列出/etc 目录中所有的内容。用户可按上下键浏览或按 Q 键跳离 \n 26.3、查看硬件信息 \n Ispci：查看 PCI 设备 -v：查看详细信息 \n Isusb：查看 USB 设备 -v：查看详细信息 \n Ismod：查看加载的模块(驱动) \n 26.4、关机、重启 \n shutdown 关闭、重启计算机 \n shutdown[关机、重启]时间 -h 关闭计算机 -r：重启计算机 \n 如：立即关机：shutdown -h now \n 10 分钟后关机：shutdown -h +10 \n 23:30 分关机：shutdown -h 23:30 \n 立即重启：shutdown -r now \n poweroff：立即关闭计算机 \n reboot：立即重启计算机 \n 26.5、归档、压缩 \n zip:压缩文件 zip liuyazhuang.zip myfile 格式为：“zip 压缩后的 zip 文件文件名” \n unzip：解压文件 unzip liuyazhuang.zip \n gzip：压缩文件 gzip 文件名 \n tar：归档文件 \n tar -cvf out.tar liuyazhuang 打包一个归档（将文件\"liuyazhuang\"打包成一个归档） \n tar -xvf liuyazhuang.tar 释放一个归档（释放 liuyazhuang.tar 归档） \n tar -cvzf backup.tar.gz/etc \n -z 参数将归档后的归档文件进行 gzip 压缩以减少大小。 \n -c：创建一个新 tar 文件 \n -v：显示运行过程的信息 \n -f：指定文件名 \n -z：调用 gzip 压缩命令进行压缩 \n -t：查看压缩文件的内容 \n -x：解开 tar 文件 \n tar -cvf test.tar *：将所有文件打包成 test.tar,扩展名.tar 需自行加上 \n tar -zcvf test.tar.gz *：将所有文件打包成 test.tar,再用 gzip 命令压缩 \n tar -tf test.tar ：查看 test.tar 文件中包括了哪些文件 \n tar -xvf test.tar 将 test.tar 解开 \n tar -zxvf foo.tar.gz 解压缩 \n gzip 各 gunzip 命令 \n gziptest.txt ：压缩文件时，不需要任何参数 \n gizp–l test.txt.gz：显示压缩率 \n 26.6、查找 \n locate：快速查找文件、文件夹：locate keyword \n 此命令需要预先建立数据库，数据库默认每天更新一次，可用 updatedb 命令手工建立、更新数据库。欢迎关注我们，公号终码一生。 \n find 查找位置查找参数 \n 如： \n find . -name liuyazhuang 查找当前目录下名称中含有\"liuyazhuang\"的文件 \n find / -name *.conf 查找根目录下（整个硬盘）下后缀为.conf 的文件 \n find / -perm 777 查找所有权限是 777 的文件 \n find / -type d 返回根目录下所有的目录 \n find . -name \"a*\"-exec ls -l {} ; \n find 功能：用来寻找文件或目录。 \n 格式：find [<路径>][匹配条件] \n find / -name httpd.conf 搜索系统根目录下名为 httpd.conf 的文件 \n 26.7、ctrl+c :终止当前的命令 \n 26.8、who 或 w 命令 \n 功能：查看当前系统中有哪些用户登录 \n 格式：who/w[参数] \n 26.9、dmesg 命令 \n 功能：显示系统诊断信息、操作系统版本号、物理内存的大小以及其它信息 \n 26.10、df 命令 \n 功能：用于查看文件系统的各个分区的占用情况 \n 26.11、du 命令 \n 功能：查看某个目录中各级子目录所使用的硬盘空间数 \n 格式：du [参数] <目录名> \n 26.12、free 命令 \n 功能：用于查看系统内存，虚拟内存（交换空间）的大小占用情况 \n 27. VIM \n VIM 是一款功能强大的命令行文本编辑器，在 Linux 中通过 vim 命令可以启动 vim 编辑器。 \n 一般使用 vim + 目标文件路径 的形式使用 vim \n 如果目标文件存在，则 vim 打开目标文件，如果目标文件不存在，则 vim 新建并打开该文件 \n :q：退出 vim 编辑器 \n VIM 模式 \n vim 拥有三种模式： \n （1）命令模式（常规模式） \n vim 启动后，默认进入命令模式，任何模式都可以通过 esc 键回到命令模式（可以多按几次），命令模式下可以键入不同的命令完成选择、复制、粘贴、撤销等操作。 \n 命名模式常用命令如下： \n i : 在光标前插入文本； \n o:在当前行的下面插入新行； \n dd:删除整行； \n yy：将当前行的内容放入缓冲区（复制当前行） \n n+yy :将 n 行的内容放入缓冲区（复制 n 行） \n p:将缓冲区中的文本放入光标后（粘贴） \n u：撤销上一个操作 \n r:替换当前字符 \n / 查找关键字 \n （2）插入模式 \n 在命令模式下按 \" i \"键，即可进入插入模式，在插入模式可以输入编辑文本内容，使用 esc 键可以返回命令模式。 \n （3）ex 模式 \n 在命令模式中按\" : \"键可以进入 ex 模式，光标会移动到底部，在这里可以保存修改或退出 vim. \n ext 模式常用命令如下： \n :w ：保存当前的修改 \n :q ：退出 \n :q! ：强制退出，保存修改 \n :x :保存并退出，相当于:wq \n :set number 显示行号 \n :! 系统命令 执行一个系统命令并显示结果 \n :sh ：切换到命令行，使用 ctrl+d 切换回 vim \n 28. 软件包管理命令(RPM) \n 28.1、软件包的安装 \n 使用 RPM 命令的安装模式可以将软件包内所有的组件放到系统中的正确路径，安装软件包的命令是:rpm –ivh wu-ftpd-2.6.2-8.i386.rpm \n i：作用 rpm 的安装模式 v: 校验文件信息 h: 以＃号显示安装进度 \n 28.2、软件包的删除 \n 删除模式会将指定软件包的内容全部删除，但并不包括已更改过的配置文件，删除 RPM 软件包的命令如下：rpm –e wu-ftpd \n 注意：这里必须使用软件名“wu-ftpd”或”wu-ftpd-2.6.2-8 而不是使用当初安装时的软件包名.wu-ftpd-2.6.2-8.i386.rpm \n 28.3、软件包升级 \n 升级模式会安装用户所指定的更新版本，并删除已安装在系统中的相同软件包，升级软件包命令如下：rpm –Uvh wu-ftpd-2.6.2-8.i386.rpm –Uvh：升级参数 \n 28.4、软件包更新 \n 更新模式下，rpm 命令会检查在命令行中所指定的软件包是否比系统中原有的软件 包更新。如果情况属实，rpm 命令会自动更新指定的软件包；反之，若系统中并没有指定软件包的较旧版本，rpm 命令并不会安装此软件包。而在升级模式下，不管系统中是否有较旧的版本，rpm 命令都会安装指定的软件包。 \n rpm –Fvhwu-ftpd-2.6.2-8.i386.rpm -Fvh：更新参数 \n 28.5、软件包查询 \n 若要获取 RPM 软件包的相关信息，可以使用查询模式。使用-q 参数可查询一个已 安装的软件包的内容 \n rpm –q wu-ftpd \n 查询软件包所安装的位置：rpm –ql package-name \n rpm –ql xv (l 参数：显示文件列表) \n 29.服务器之间传输数据 \n 1、发送GET请求 \n curl  URL\n curl  URL?a = 1 & b = nihao\n \n 1 2 #  2、发送POST请求 \n curl   -X  POST  -d   'a=1&b=nihao'  URL\n \n 1 #  3、发送json格式请求： \n curl   -H   \"Content-Type: application/json\"   -X  POST  -d   '{\"abc\":123,\"bcd\":\"nihao\"}'  URL\n curl   -H   \"Content-Type: application/json\"   -X  POST  -d  @test.json URL\n   \n \n 1 2 3 其中， -H 代表header头， -X 是指定什么类型请求(POST/GET/HEAD/DELETE/PUT/PATCH)， -d 代表传输什么数据。这几个是最常用的。 \n 查看所有curl命令： man curl或者curl -h \n"},{title:"gitlab通过CICD流水线部署",frontmatter:{title:"gitlab通过CICD流水线部署",date:"2022-10-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["CICD"],tags:["CICD","持续集成部署","pipeline"]},regularPath:"/%E5%85%B6%E4%BB%96/gitlab%E9%80%9A%E8%BF%87CICD%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2.html",relativePath:"其他/gitlab通过CICD流水线部署.md",key:"v-10e8b782",path:"/2022/10/08/gitlab%E9%80%9A%E8%BF%87cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2/",headers:[{level:2,title:"环境准备",slug:"环境准备"},{level:3,title:"1.JDK：",slug:"_1-jdk"},{level:3,title:"2.maven：",slug:"_2-maven"},{level:3,title:"3.Git",slug:"_3-git"},{level:3,title:"4.Docker (本次安装没有使用到，仅做了解使用)",slug:"_4-docker-本次安装没有使用到-仅做了解使用"},{level:2,title:"GitLab安装",slug:"gitlab安装"},{level:3,title:"1.安装：",slug:"_1-安装"},{level:3,title:"2.安装gitlab-runner",slug:"_2-安装gitlab-runner"},{level:3,title:"3.gitlab-runner 注册：",slug:"_3-gitlab-runner-注册"},{level:3,title:"4.编辑流水线配置：",slug:"_4-编辑流水线配置"},{level:3,title:"5.安装过程中遇到的问题",slug:"_5-安装过程中遇到的问题"},{level:2,title:"gitlab CICD 流程规范",slug:"gitlab-cicd-流程规范"},{level:2,title:"附录常用命令及配置",slug:"附录常用命令及配置"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:" 环境准备 \n 1.JDK： \n \n 将包上传到对应的目录下 \n 使用tar -zxvf jdk-8u161-linux-x64.tar.gz 进行解压包 \n 修改配置文件：vi /etc/profile \n \n export   JAVA_HOME = /home/soft/java8/jdk\n export   PATH = $JAVA_HOME /bin: $PATH \n export   CLASSPATH = .: $JAVA_HOME /lib/dt.jar: $JAVA_HOME /lib/tools.jar\n \n 1 2 3 \n 通过source /etc/profile 进行重新编译 \n 通过java -version查看是否已经安装成功 \n 2.maven： \n \n 包解压跟jdk一样 \n 找到localRepository,并进行修改本地仓库地址（conf/setting.xml文件下） \n 添加mirror: \n \n < mirror > \n       < id > alimaven </ id > \n       < name > aliyun maven </ name > \n       < url > http://maven.aliyun.com/nexus/content/groups/public/ </ url > \n       < mirrorOf > central </ mirrorOf > \n </ mirror > \n\n \n 1 2 3 4 5 6 7 \n 修改配置文件 \n \n export   MAVEN_HOME = /home/soft/maven/maven\n export   PATH = $PATH : $MAVEN_HOME /bin  \n \n 1 2 \n 通过source /etc/proflie进行重新编译 \n 通过mvn -version查看是否已经安装成功 \n 3.Git \n \n 解压跟jdk一样 \n 安装编译所需要的依赖 \n \n yum  install  curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker\n\n \n 1 2 \n 编译git源码 make prefix=/home/soft/git/git all (/home/soft/git为文件所在的路径) \n 安装git的路径 make prefix=/home/socf/git/git install \n 配置环境变量：vi /etc/profile \n \n export   PATH = $PATH :/home/soft/git/git\n\n \n 1 2 #  4.Docker (本次安装没有使用到，仅做了解使用) \n 安装： yum install -y docker \n查看状态：systemctl status docker \ndocker启动：systemctl start docker/systemctl start docker.service \nservice docker start \n重启docker服务：systemctl restart docker \nsudo service docker restart \n关闭docker：systemctl stop docker \nservice docker stop \n查看所有镜像：docker images \n生成镜像：docker build -t 镜像名称： \n删除镜像:docker rmi 镜像名称 \n启动镜像并实现端口映射：docker run -p 192.168.211.128:8081:8081 镜像名称 \n生成容器并且启动镜像：docker run -d -p --name 容器名称 镜像名称 \n GitLab安装 \n 前置条件：关闭防火墙，查看防火墙的状态： \n systemctl status firewalld.service\nsystemctl stop firewalld 关闭防火墙\n \n 1 2 #  1.安装： \n \n 配置yum源： vi /etc/yum.repos.d/gitlab-ce.repo \n \n [ gitlab-ce ] \n name = Gitlab CE Repository\n baseurl = https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el $releasever /\n gpgcheck = 0 \n enabled = 1 \n\n \n 1 2 3 4 5 6 \n 更新本地源：yum makecache \n 安装gitlab社区版：yum install gitlab-ce \n 安装完成后的默认目录： \n \n gitlab组件日志路径：/var/log/gitlab\ngitlab配置路径：/etc/gitlab/  路径下有gitlab.rb配置文件\n应用代码和组件依赖程序：/opt/gitlab\n各个组件存储路径： /var/opt/gitlab/\n仓库默认存储路径   /var/opt/gitlab/git-data/repositories\n版本文件备份路径：/var/opt/gitlab/backups/\nnginx安装路径：/var/opt/gitlab/nginx/\nredis安装路径：/var/opt/gitlab/redis\n\n \n 1 2 3 4 5 6 7 8 9 \n 修改默认ip和端口： vi /etc/gitlab/gitlab.rb \n \n \n \n 配置邮箱 \n 需要安装postfix邮箱 \n 安装:yum install -y postfix \n 设置开机自启：systemctl enable postfix \n 启动：systemctl start psotfix \n \n 还是要修改上方的gitlab.rb （本次测试没有使用到，新建用户的时候，如果没有添加邮件是不会添加成功的，执行失败还是会发送邮件的）\n\n \n 1 2 #修改以下内容 \ngitlab_rails [ 'gitlab_email_enabled' ]   =   true \ngitlab_rails [ 'gitlab_email_from' ]   =   '发信邮箱' \ngitlab_rails [ 'gitlab_email_display_name' ]   =   'xxx' \n \ngitlab_rails [ 'smtp_enable' ]   =   true \ngitlab_rails [ 'smtp_address' ]   =   \"smtp.163.com\" \ngitlab_rails [ 'smtp_port' ]   =   465 \ngitlab_rails [ 'smtp_user_name' ]   =   \"发信邮箱\" \ngitlab_rails [ 'smtp_password' ]   =   \"smtp客户端授权码\" \ngitlab_rails [ 'smtp_domain' ]   =   \"163.com\" \ngitlab_rails [ 'smtp_authentication' ]   =   \"login\" \ngitlab_rails [ 'smtp_enable_starttls_auto' ]   =   true \ngitlab_rails [ 'smtp_tls' ]   =   true \ngitlab_rails [ 'smtp_openssl_verify_mode' ]   =   'none' \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n 修改完成后，重新加载配置文件：gitlab-ctl reconfigure \n 如果修改了邮箱配置，测试邮箱是否开启：gitlab-rails console \n 性能优化： \n \n \n 同样，也是修改gitlab.rb文件 \n \n puma [ 'worker_processes' ]   =   2                           #官方建议值为CPU核数+1（服务器只部署gitLab的情况下），可提高服务器响应速度，此参数最小值为2，设为1服务器可能卡死 \npuma [ 'work_timeout' ]   =   60                              #设置超时时间 \npuma [ 'worker_memory_limit_min' ]   =   \"200 * 1 << 20\"      #减少最小内存 \npuma [ 'worker_memory_limit_max' ]   =   \"300 * 1 << 20\"      #减少最大内存 \npostgresql [ 'shared_buffers' ]   =   \"128MB\"                    #减少数据库缓存 \npostgresql [ 'max_worker_processes' ]   =   6                    #减少数据库并发数 \nsidekiq [ 'concurrency' ]   =   15                               #减少sidekiq并发数 \n\n由于GitLab核心功能是代码托管，所以有些额外的组件比较浪费资源，所以可以考虑关闭。\nprometheus [ 'enable' ]   =   false \nprometheus [ 'monitor_kubernetes' ]   =   false \nalertmanager [ 'enable' ]   =   false   \nnode_exporter [ 'enable' ]   =   false  \nredis_exporter [ 'enable' ]   =   false  \npostgres_exporter [ 'enable' ]   =   false \ngitlab_exporter [ 'probe_sidekiq' ]   =   false \nprometheus_monitoring [ 'enable' ]   =   false \ngrafana [ 'enable' ]   =   false \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \n 重新启动：sudo gitlab-ctl restart \n 其他命令： \n \n gitlab-ctl start  #启动全部服务 \ngitlab-ctl restart #重启全部服务 \ngitlab-ctl stop  #停止全部服务 \ngitlab-ctl restart nginx  #重启单个服务，如重启nginx \ngitlab-ctl status  #查看服务状态 \ngitlab-ctl reconfigure  #使配置文件生效 \ngitlab-ctl show-config  #验证配置文件 \ngitlab-ctl uninstall  #删除gitlab（保留数据） \ngitlab-ctl cleanse  #删除所有数据，从新开始 \ngitlab-ctl  tail   < service name > 查看服务的日志\ngitlab-ctl  tail  nginx   #如查看gitlab下nginx日志 \ngitlab-rails console   #进入控制台 \ngitlab-ctl  help                    #查看gitlab帮助信息 \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 安装完成后，默认的用户是root,密码需要查看： cat /etc/gitlab/initial_root_password \n 2.安装gitlab-runner \n \n 到清华园中找到gitlab-runner:https://mirrors.tuna.tsinghua.edu.cn/gitlab-runner/yum/el7/ \n 上传到服务器中 \n 通过rpm -ivh gitlab-runner-12.9.1-1.x86_64.rpm --nodeps --force 去安装 \n 启动服务：systemctl start gitlab-runner \n 查看服务状态：systemctl status gitlab-runner \n 验证是否安装成功：gitlab-runner -h \n 3.gitlab-runner 注册： \n \n 注册共享型的runner:gitlab-runner register (下方是注册的时候，需要填写的内容)\n \n \n gitlab地址：Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/) \n \n \n \n runner注册令牌：Please enter the gitlab-ci token for this runner \n \n \n \n gitlab-runner的注释：Please enter the gitlab-ci description for this runner \n \n \n gitlab-runner的标签：Please enter the gitlab-ci tags for this runner (comma separated) \n \n \n 执行器使用的方式：Please enter the executor: custom, docker, parallels, docker+machine, kubernetes, docker-ssh, shell, ssh, virtualbox, docker-ssh+machine \n \n \n \n \n \n \n 查看已注册的runner列表：gitlab-runner list \n 取消所有注册列表：gitlab-runner unregister --all-runners \n 4.编辑 流水线 配置： \n 首先：需要通过git生成私钥，然后上传到项目中： \n \n 生成ssh秘钥：ssh-keygen -t rsa \n查看生成的秘钥： cat ~/.ssh/id_rsa \n将私钥的值放到项目内的CICD变量中，命名为：SSH_PRIVATE_KEY \n \n \n \n 修改如下文件，就可以操作了： \n \n \n \n 或者直接修改.gitlab-ci.yml文件 \n定义一些变量, 下面各阶段会使用 \n variables : \n   server_ip :  192.168.44.129\n   jar_name :  demo - 0.0.1 - SNAPSHOT.jar\n   java_path :  /home/java/java1.8/bin\n   upload_path :  /home/gitlab - project/\n   ssh_password :   111111 \n   repo_path :  /home/soft/maven/repo\n   jar_path :  com/example/demo/0.0.1 - SNAPSHOT\n定义执行的各个阶段及顺序 \n stages : \n   -  build\n   -  upload\n   -  deploy\n使用 maven 镜像打包项目 \n maven-build : \n   stage :  build\n   script : \n     -  mvn clean install  - Dmaven.test.skip=true\n     -  echo \"打包结束\"\n将打包完成的jar包移动到指定的位置 \n upload-jar : \n   stage :  upload\n   script : \n     -  echo \"拷贝jar包到/home/gitlab - project中\"\n     -  cd $repo_path/$jar_path\n     -  cp  - r $jar_name $upload_path\n启动 SpringBoot jar包 \n deploy-test : \n   stage :  deploy\n   script : \n     -  echo \"启动$jar_name\"\n     -  cd $upload_path\n     -  nohup java  - jar $jar_name  > log.out &\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 \n stages:用于定义作何可以使用的阶段，并且是全局定义的。同一阶段的作业并行运行，不同阶段按照顺序进行 \n script：运行的脚本 \n 5.安装过程中遇到的问题 \n 1.build正常,deploy失败,报错如下 \n fatal:  git  fetch-pack: expected shallow list\nfatal: The remote end hung up unexpectedly\nERROR: Job failed:  exit  status  1 \n \n 1 2 3 解决方案: \n TS：gitlab-ci运行作业报错fatal: git fetch-pack: expected shallow list,fatal: - 知乎 (zhihu.com) \n 解决方法，修改gitlab的cicd配置，如下图所示： \n \n 这里设置为 git clone 后，保存配置. \n cicd主要工作流程： \n \n 关于管道的设计： \n 1.从job角度：你的工作流程是什么？可以分成几个阶段？每个阶段如何工作的？ \n 2.从stage角度：你的工作流程是分成了几个阶段？每个阶段如何工作的？整体流程是怎样的？ \n 管道类型体系架构： \n gitlab CICD 流程规范 \n (183条消息) 基于gitlab的CICD流程规范_李先森&Mr.Li的博客-CSDN博客 \n 附录常用命令及配置 \n gitlab命令 \n gitlab-ctl start  #启动全部服务 \ngitlab-ctl restart #重启全部服务 \ngitlab-ctl stop  #停止全部服务 \ngitlab-ctl restart nginx  #重启单个服务，如重启nginx \ngitlab-ctl status  #查看服务状态 \ngitlab-ctl reconfigure  #使配置文件生效 \ngitlab-ctl show-config  #验证配置文件 \ngitlab-ctl uninstall  #删除gitlab（保留数据） \ngitlab-ctl cleanse  #删除所有数据，从新开始 \ngitlab-ctl  tail   < service name > 查看服务的日志\ngitlab-ctl  tail  nginx   #如查看gitlab下nginx日志 \ngitlab-rails console   #进入控制台 \ngitlab-ctl  help                    #查看gitlab帮助信息 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 gitlab-runnner \n - 启动服务：systemctl start gitlab-runner\n- 查看服务状态：systemctl status gitlab-runner\n- 验证是否安装成功：gitlab-runner  -h \n \n 1 2 3 .gitlab-ci.yml配置 \n 官方模板：https://gitlab.com/gitlab-org/gitlab-foss/-/blob/master/lib/gitlab/ci/templates \n定义一些变量, 下面各阶段会使用 \n variables : \n   server_ip :  192.168.44.129\n   jar_name :  demo - 0.0.1 - SNAPSHOT.jar\n   java_path :  /home/java/java1.8/bin\n   upload_path :  /home/gitlab - project/\n   ssh_password :   111111 \n   repo_path :  /home/soft/maven/repo\n   jar_path :  com/example/demo/0.0.1 - SNAPSHOT\n定义执行的各个阶段及顺序 \n stages : \n   -  build\n   -  upload\n   -  deploy\n使用 maven 镜像打包项目 \n maven-build : \n   stage :  build\n   script : \n     -  mvn clean install  - Dmaven.test.skip=true\n     -  echo \"打包结束\"\n将打包完成的jar包移动到指定的位置 \n upload-jar : \n   stage :  upload\n   script : \n     -  echo \"拷贝jar包到/home/gitlab - project中\"\n     -  cd $repo_path/$jar_path\n     -  cp  - r $jar_name $upload_path\n启动 SpringBoot jar包 \n deploy-test : \n   stage :  deploy\n   script : \n     -  echo \"启动$jar_name\"\n     -  cd $upload_path\n     -  nohup java  - jar $jar_name  > log.out &\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # 执行job的阶段 按顺序串行执行 \n stages : \n   -  build\n   -  cleanup\n   -  deploy\n自定义阶段build的job流程 \n build :   # 自定义名字 \n   stage :  build  # 指定这阶段操作的名称 \n   only :   # 指定那些分支会进入该处理流程 \n     -  master  # 正式环境 \n     -  pre  # 预发环境 \n     -  testN  # 测试环境 test1 test2 ... testN \n     -  devN  # 联调环境 dev1 dev2 ... devN \n   variables : \n     VERSION :  1.0.10  # 除了后面会说到的私密变量 还可以在这里定义变量 \n   before_script : \n一些特殊情况需要SSH key的场景，该部分见下文 \n- ... \n定义变量 如NODE环境变量 \n     -  NODE_ENV=`if  [ [  $ { CI_COMMIT_REF_NAME : 0 : 3 }  = \"dev\"  | |  $ { CI_COMMIT_REF_NAME : 0 : 4 }  = \"test\"  ] ] ; then echo \"development\"; else echo \"production\"; fi`;\n   script : \n为node modules做缓存， 有缓存用缓存，没有则你npm install并添加缓存 \n     -  PACKAGE_HASH=$(md5sum package.json  |  awk ' { print $1 } ');\n     -  mkdir  - p ~/builds/cache/node_modules  # 使用docker模式时需要配置volume 保证缓存起作用 在后面runner部分会提到。 \n     -  NPM_CACHE=~/builds/cache/node_modules/$ { PACKAGE_HASH } .tar\n     -  if  [   - f $NPM_CACHE  ] ;\n      then\n        echo \"Use Cache\";\n        tar xf $NPM_CACHE;\n      else\n        npm install;\n        tar cf  -  ./node_modules  >  $NPM_CACHE;\n      fi\nnpm build \n     -  echo \"NODE_ENV=$NODE_ENV node build/build.js\"\n     -  NODE_ENV=$NODE_ENV node build/build.js\nupload to CDN \n- ... \ndocker build \n     -  echo `docker build  - t \"$CI_PIPELINE_ID\" .  |  awk  - F \"Successfully built \" ' { print $2 } '`\ndocker push \n     -  if  [  $NODE_ENV = \"development\"  ] ;  # 如果需要部署的server的runner在同一个机器则可不必push到仓库 \n      then\n        docker login dev.hub.xxx.com  - u$USERNAME  - p$PASSWORD;\n        docker tag $CI_PIPELINE_ID dev.hub.xxx.com/namespace/webapp : $CI_COMMIT_REF_NAME;\n        docker push dev.hub.xxx.com/namespace/webapp : $CI_COMMIT_REF_NAME;\n        docker rmi dev.hub.xxx.com/namespace/webapp : $CI_COMMIT_REF_NAME;\n        echo \" --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- - - \";\n        echo \"dev.hub.xxx.com/namespace/webapp : $CI_COMMIT_REF_NAME\";\n      else\n参考上面then中脚本 \n        \" --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- - - \";\n        echo \"hub.prd.xxx.com/namespace/webapp : $DATE.$CI_BUILD_ID\";\n      fi\n开发和测试机部署 \n clean_testN : \n   stage :  cleanup\n   only : \n     -  testN\n   tags : \n     -  dc2fe - deploy - testN\n   script : \n     -  docker stop webapp\n     -  docker rm webapp\n   allow_failure :   true \n\n deploy_testN : \n   stage :  deploy\n   only : \n     -  testN\n   tags : \n     -  dc2fe - deploy - testN\n   script : \n- ssh root@IP \"docker stop webapp; docker rm webapp; docker run --name webapp -d -p 8000:8000 dev.hub.xxx.com/namespace/webapp-testN:latest\" \n或者 \n     -  docker pull dev.hub.xxx.com/namespace/webapp : testN\n     -  docker run  - - name webapp  - d  - p 8000 : 8000 dev.hub.xxx.com/namespace/webapp : testN\nclean_dev: # dev或其他环境部署配置与上面test环境配置类似 \ndeploy_dev: \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 "},{title:"一致性hash算法",frontmatter:{title:"一致性hash算法",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["算法"],tags:["redis","一致性hash","水平扩容"]},regularPath:"/%E5%85%B6%E4%BB%96/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95.html",relativePath:"其他/一致性hash算法.md",key:"v-851571e2",path:"/2023/06/10/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/",headers:[{level:2,title:"背景",slug:"背景"},{level:3,title:"如何分配请求？",slug:"如何分配请求"},{level:3,title:"使用哈希算法有什么问题？",slug:"使用哈希算法有什么问题"},{level:2,title:"一致性哈希算法的简介",slug:"一致性哈希算法的简介"},{level:3,title:"什么是一致性哈希算法",slug:"什么是一致性哈希算法"},{level:3,title:"使用场景",slug:"使用场景"},{level:2,title:"一致性哈希算法的原理",slug:"一致性哈希算法的原理"},{level:3,title:"一致性哈希算法是如何减少数据的迁移成本？",slug:"一致性哈希算法是如何减少数据的迁移成本"},{level:3,title:"如何通过虚拟节点提高均衡度？",slug:"如何通过虚拟节点提高均衡度"},{level:3,title:"虚拟节点映射到哈希环上是如何分布的？",slug:"虚拟节点映射到哈希环上是如何分布的"},{level:2,title:"一致性哈希算法的简单实现",slug:"一致性哈希算法的简单实现"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' 背景 \n 如何分配请求？ \n 大多数网站背后肯定不是只有一台服务器提供服务，因为单机的并发量和数据量都是有限的，所以都会用多台服务器构成集群来对外提供服务。 \n 但是问题来了，现在有那么多个节点（后面统称服务器为节点，因为少一个字），要如何分配客户端的请求呢？ \n \n 其实这个问题就是「负载均衡问题」。解决负载均衡问题的算法很多，不同的负载均衡算法，对应的就是不同的分配策略，适应的业务场景也不同。 \n 最简单的方式，引入一个中间的负载均衡层，让它将外界的请求「轮流」的转发给内部的集群。比如集群有 3 个节点，外界请求有 3 个，那么每个节点都会处理 1 个请求，达到了分配请求的目的。 \n \n 考虑到每个节点的硬件配置有所区别，我们可以引入权重值，将硬件配置更好的节点的权重值设高，然后根据各个节点的权重值，按照一定比重分配在不同的节点上，让硬件配置更好的节点承担更多的请求，这种算法叫做加权轮询。 \n 加权轮询算法使用场景是建立在每个节点存储的数据都是相同的前提。所以，每次读数据的请求，访问任意一个节点都能得到结果。 \n 但是，加权轮询算法是无法应对「分布式系统」的，因为分布式系统中，每个节点存储的数据是不同的。 \n 当我们想提高系统的容量，就会将数据水平切分到不同的节点来存储，也就是将数据分布到了不同的节点。比如 一个分布式 KV（key-valu） 缓存系统，某个 key 应该到哪个或者哪些节点上获得，应该是确定的 ，不是说任意访问一个节点都可以得到缓存结果的。 \n 因此，我们要想一个能应对分布式系统的负载均衡算法。 \n 使用哈希算法有什么问题？ \n 有的同学可能很快就想到了： 哈希算法 。因为对同一个关键字进行哈希计算，每次计算都是相同的值，这样就可以将某个 key 确定到一个节点了，可以满足分布式系统的负载均衡需求。 \n 哈希算法最简单的做法就是进行取模运算，比如分布式系统中有 3 个节点，基于  hash(key) % 3  公式对数据进行了映射。 \n 如果客户端要获取指定 key 的数据，通过下面的公式可以定位节点： \n hash ( key )   %   3 \n \n 1 如果经过上面这个公式计算后得到的值是 0，就说明该 key 需要去第一个节点获取。 \n 但是有一个很致命的问题， 如果节点数量发生了变化，也就是在对系统做扩容或者缩容时，必须迁移改变了映射关系的数据 ，否则会出现查询不到数据的问题。 \n 举个例子，假设我们有一个由 A、B、C 三个节点组成分布式 KV 缓存系统，基于计算公式  hash(key) % 3  将数据进行了映射，每个节点存储了不同的数据： \n \n 现在有 3 个查询 key 的请求，分别查询 key-01，key-02，key-03 的数据，这三个 key 分别经过 hash() 函数计算后的值为 hash( key-01) = 6、hash( key-02) = 7、hash(key-03) = 8，然后再对这些值进行取模运算。 \n 通过这样的哈希算法，每个 key 都可以定位到对应的节点。 \n \n 当 3 个节点不能满足业务需求了，这时我们增加了一个节点，节点的数量从 3 变化为 4，意味取模哈希函数中基数的变化，这样会导致 大部分映射关系改变 ，如下图： \n \n 比如，之前的 hash(key-01) %  3  = 0，就变成了 hash(key-01) %  4  = 2，查询 key-01 数据时，寻址到了节点 C，而 key-01 的数据是存储在节点 A 上的，不是在节点 C，所以会查询不到数据。 \n 同样的道理，如果我们对分布式系统进行缩容，比如移除一个节点，也会因为取模哈希函数中基数的变化，可能出现查询不到数据的问题。 \n 要解决这个问题的办法，就需要我们进行 迁移数据 ，比如节点的数量从 3 变化为 4 时，要基于新的计算公式 hash(key) % 4 ，重新对数据和节点做映射。 \n 假设总数据条数为 M，哈希算法在面对节点数量变化时， 最坏情况下所有数据都需要迁移，所以它的数据迁移规模是 O(M) ，这样数据的迁移成本太高了。 \n 所以，传统的按服务器节点数量取模在集群扩容和收缩时存在一定的局限性。而一致性哈希算法正好解决了简单哈希算法在分布式集群中存在的动态伸缩的问题，降低节点上下线的过程中带来的数据迁移成本。 \n 一致性哈希算法的简介 \n 什么是一致性哈希算法 \n 一致性哈希(Consistent Hash)算法是1997年提出，也用了取模运算，但与哈希算法不同的是，哈希算法是对节点的数量进行取模运算，而 一致哈希算法是对 2^32 进行取模运算，是一个固定的值 。 \n 目的是解决分布式系统的数据分区问题：当分布式集群移除或者添加一个服务器时，必须尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系。 \n 使用场景 \n 一致性哈希算法是分布式系统中的重要算法，使用场景也非常广泛。主要是是负载均衡、缓存数据分区等场景。 \n 一致性哈希应该是实现负载均衡的首选算法，它的实现比较灵活，既可以在客户端实现，也可以在中间件上实现，比如日常使用较多的缓存中间件memcached 使用的路由算法用的就是一致性哈希算法。 \n 此外，其它的应用场景还有很多： \n \n RPC框架Dubbo用来选择服务提供者 \n 分布式关系数据库分库分表：数据与节点的映射关系 \n LVS负载均衡调度器 \n 一致性哈希算法的原理 \n 一致性哈希算法是如何减少数据的迁移成本？ \n 我们可以把一致哈希算法是对 2^32 进行取模运算的结果值组织成一个圆环，就像钟表一样，钟表的圆可以理解成由 60 个点组成的圆，而此处我们把这个圆想象成由 2^32 个点组成的圆，这个圆环被称为 哈希环 ，如下图： \n \n 一致性哈希要进行两步哈希： \n \n 第一步：对存储节点进行哈希计算，也就是对存储节点做哈希映射，比如根据节点的 IP 地址进行哈希； \n 第二步：当对数据进行存储或访问时，对数据进行哈希映射； \n \n 所以， 一致性哈希是指将「存储节点」和「数据」都映射到一个首尾相连的哈希环上 。 \n 问题来了，对「数据」进行哈希映射得到一个结果要怎么找到存储该数据的节点呢？ \n 答案是，映射的结果值往 顺时针的方向的找到第一个节点 ，就是存储该数据的节点。 \n 举个例子，有 3 个节点经过哈希计算，映射到了如下图的位置： \n \n 接着，对要查询的 key-01 进行哈希计算，确定此 key-01 映射在哈希环的位置，然后从这个位置往顺时针的方向找到第一个节点，就是存储该 key-01 数据的节点。 \n 比如，下图中的 key-01 映射的位置，往顺时针的方向找到第一个节点就是节点 A。 \n \n 所以，当需要对指定 key 的值进行读写的时候，要通过下面 2 步进行寻址： \n \n 首先，对 key 进行哈希计算，确定此 key 在环上的位置； \n 然后，从这个位置沿着顺时针方向走，遇到的第一节点就是存储 key 的节点。 \n \n 知道了一致哈希寻址的方式，我们来看看，如果增加一个节点或者减少一个节点会发生大量的数据迁移吗？ \n 假设节点数量从 3 增加到了 4，新的节点 D 经过哈希计算后映射到了下图中的位置： \n \n 你可以看到，key-01、key-03 都不受影响，只有 key-02 需要被迁移节点 D。 \n 假设节点数量从 3 减少到了 2，比如将节点 A 移除： \n \n 你可以看到，key-02 和 key-03 不会受到影响，只有 key-01 需要被迁移节点 B。 \n 因此， 在一致哈希算法中，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响 。 \n 上面这些图中 3 个节点映射在哈希环还是比较分散的，所以看起来请求都会「均衡」到每个节点。 \n 但是 一致性哈希算法并不保证节点能够在哈希环上分布均匀 ，这样就会带来一个问题，会有大量的请求集中在一个节点上。 \n 比如，下图中 3 个节点的映射位置都在哈希环的右半边： \n \n 这时候有一半以上的数据的寻址都会找节点 A，也就是访问请求主要集中的节点 A 上，这肯定不行的呀，说好的负载均衡呢，这种情况一点都不均衡。 \n 另外，在这种节点分布不均匀的情况下，进行容灾与扩容时，哈希环上的相邻节点容易受到过大影响，容易发生雪崩式的连锁反应。 \n 比如，上图中如果节点 A 被移除了，当节点 A 宕机后，根据一致性哈希算法的规则，其上数据应该全部迁移到相邻的节点 B 上，这样，节点 B 的数据量、访问量都会迅速增加很多倍，一旦新增的压力超过了节点 B 的处理能力上限，就会导致节点 B 崩溃，进而形成雪崩式的连锁反应。 \n 所以， 一致性哈希算法虽然减少了数据迁移量，但是存在节点分布不均匀的问题 。 \n 如何通过虚拟节点提高均衡度？ \n 要想解决节点能在哈希环上分配不均匀的问题，就是要有大量的节点，节点数越多，哈希环上的节点分布的就越均匀。 \n 但问题是，实际中我们没有那么多节点。所以这个时候我们就加入 虚拟节点 ，也就是对一个真实节点做多个副本。 \n 具体做法是， 不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。 \n 比如对每个节点分别设置 3 个虚拟节点： \n \n 对节点 A 加上编号来作为虚拟节点：A-01、A-02、A-03 \n 对节点 B 加上编号来作为虚拟节点：B-01、B-02、B-03 \n 对节点 C 加上编号来作为虚拟节点：C-01、C-02、C-03 \n \n 引入虚拟节点后，原本哈希环上只有 3 个节点的情况，就会变成有 9 个虚拟节点映射到哈希环上，哈希环上的节点数量多了 3 倍。 \n \n 你可以看到， 节点数量多了后，节点在哈希环上的分布就相对均匀了 。这时候，如果有访问请求寻址到「A-01」这个虚拟节点，接着再通过「A-01」虚拟节点找到真实节点 A，这样请求就能访问到真实节点 A 了。 \n 上面为了方便你理解，每个真实节点仅包含 3 个虚拟节点，这样能起到的均衡效果其实很有限。而在实际的工程中，虚拟节点的数量会大很多，比如 Nginx 的一致性哈希算法，每个权重为 1 的真实节点就含有160 个虚拟节点。 \n 另外，虚拟节点除了会提高节点的均衡度，还会提高系统的稳定性。 当节点变化时，会有不同的节点共同分担系统的变化，因此稳定性更高 。 \n 比如，当某个节点被移除时，对应该节点的多个虚拟节点均会移除，而这些虚拟节点按顺时针方向的下一个虚拟节点， 可能会对应不同的真实节点 ，即这些不同的真实节点共同分担了节点变化导致的压力。 \n 而且，有了虚拟节点后，还可以为硬件配置更好的节点增加权重，比如对权重更高的节点增加更多的虚拟机节点即可。 \n 因此， 带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景 。 \n 虚拟节点映射到哈希环上是如何分布的？ \n 是上面提到的间隔分布还是下面的依次连续？虚拟节点是如何映射到哈希环上，使hash(key)即使集中在某一扇区，也能均匀的分散请求。 \n 一致性哈希算法的简单实现 \n Node的实体类 \n import   com . gordon . cicd . uitl . HashUtils ; \n\n import   java . util . * ; \n\n public   class   Node   { \n     private   static   final   int   VIRTUAL_NODE_NO_PER_NODE   =   3 ; //设置虚拟化节点的分布 \n     private   final   String  ip ; \n     private   final   List < Integer >  virtualNodeHashes  =   new   ArrayList < > ( VIRTUAL_NODE_NO_PER_NODE ) ; \n     private   final   Map < Object ,   Object >  cacheMap  =   new   HashMap < > ( ) ; \n\n     public   Node ( String  ip )   { \n         Objects . requireNonNull ( ip ) ; \n         this . ip  =  ip ; \n         initVirtualNodes ( ) ; \n     } \n\n\n     private   void   initVirtualNodes ( )   { \n         String  virtualNodeKey ; \n         for   ( int  i  =   1 ;  i  <=   VIRTUAL_NODE_NO_PER_NODE ;  i ++ )   { \n            virtualNodeKey  =  ip  +   "#"   +  i ; \n            virtualNodeHashes . add ( HashUtils . hashcode ( virtualNodeKey ) ) ; \n         } \n     } \n\n     public   void   addCacheItem ( Object  key ,   Object  value )   { \n        cacheMap . put ( key ,  value ) ; \n     } \n\n\n     public   Object   getCacheItem ( Object  key )   { \n         return  cacheMap . get ( key ) ; \n     } \n\n\n     public   void   removeCacheItem ( Object  key )   { \n        cacheMap . remove ( key ) ; \n     } \n\n\n     public   List < Integer >   getVirtualNodeHashes ( )   { \n         return  virtualNodeHashes ; \n     } \n\n     public   String   getIp ( )   { \n         return  ip ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 hash函数 \n public   class   HashUtils   { \n\n     /**\n     * FNV1_32_HASH\n     *\n     * @param obj\n     *         object\n     * @return hashcode\n     */ \n     public   static   int   hashcode ( Object  obj )   { \n         final   int  p  =   16777619 ; \n         int  hash  =   ( int )   2166136261L ; \n         String  str  =  obj . toString ( ) ; \n         for   ( int  i  =   0 ;  i  <  str . length ( ) ;  i ++ ) \n            hash  =   ( hash  ^  str . charAt ( i ) )   *  p ; \n        hash  +=  hash  <<   13 ; \n        hash  ^=  hash  >>   7 ; \n        hash  +=  hash  <<   3 ; \n        hash  ^=  hash  >>   17 ; \n        hash  +=  hash  <<   5 ; \n\n         if   ( hash  <   0 ) \n            hash  =   Math . abs ( hash ) ; \n         //System.out.println("hash computer:" + hash); \n         return  hash ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 一致性hash的实现，该示例中未实现根据权重新增虚拟节点 \n import   com . gordon . cicd . entity . Node ; \n import   com . gordon . cicd . uitl . HashUtils ; \n\n import   java . util . * ; \n\n public   class   ConsistentHash   { \n     private   final   TreeMap < Integer ,   Node >  hashRing  =   new   TreeMap < > ( ) ; \n\n     public   List < Node >  nodeList  =   new   ArrayList < > ( ) ; \n\n     /**\n     * 增加节点\n     * 每增加一个节点，就会在闭环上增加给定虚拟节点\n     * 例如虚拟节点数是2，则每调用此方法一次，增加两个虚拟节点，这两个节点指向同一Node\n     * @param ip\n     */ \n     public   void   addNode ( String  ip )   { \n         Objects . requireNonNull ( ip ) ; \n         Node  node  =   new   Node ( ip ) ; \n        nodeList . add ( node ) ; \n         for   ( Integer  virtualNodeHash  :  node . getVirtualNodeHashes ( ) )   { \n            hashRing . put ( virtualNodeHash ,  node ) ; \n             //System.out.println("虚拟节点[" + node + "] hash:" + virtualNodeHash + "，被添加"); \n         } \n     } \n\n     /**\n     * 移除节点\n     * @param node\n     */ \n     public   void   removeNode ( Node  node ) { \n        nodeList . remove ( node ) ; \n     } \n\n     /**\n     * 获取缓存数据\n     * 先找到对应的虚拟节点，然后映射到物理节点\n     * @param key\n     * @return\n     */ \n     public   Object   get ( Object  key )   { \n         Node  node  =   findMatchNode ( key ) ; \n         System . out . println ( "获取到节点:"   +  node . getIp ( ) ) ; \n         return  node . getCacheItem ( key ) ; \n     } \n\n     /**\n     * 添加缓存\n     * 先找到hash环上的节点，然后在对应的节点上添加数据缓存\n     * @param key\n     * @param value\n     */ \n     public   void   put ( Object  key ,   Object  value )   { \n         Node  node  =   findMatchNode ( key ) ; \n\n        node . addCacheItem ( key ,  value ) ; \n     } \n\n     /**\n     * 删除缓存数据\n     */ \n     public   void   evict ( Object  key )   { \n         findMatchNode ( key ) . removeCacheItem ( key ) ; \n     } \n\n\n     /**\n     *  获得一个最近的顺时针节点\n     * @param key 为给定键取Hash，取得顺时针方向上最近的一个虚拟节点对应的实际节点\n     *      * @return 节点对象\n     * @return\n     */ \n     private   Node   findMatchNode ( Object  key )   { \n         Map . Entry < Integer ,   Node >  entry  =  hashRing . ceilingEntry ( HashUtils . hashcode ( key ) ) ; \n         if   ( entry  ==   null )   { \n            entry  =  hashRing . firstEntry ( ) ; \n         } \n         return  entry . getValue ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 测试 \n import   com . gordon . cicd . entity . Node ; \n\n import   java . util . ArrayList ; \n import   java . util . Comparator ; \n import   java . util . HashMap ; \n import   java . util . List ; \n\n public   class   ConsistentHashTest   { \n     public   static   final   int   NODE_SIZE   =   3 ; \n     public   static   final   int   STRING_COUNT   =   10 ; \n     private   static   ConsistentHash  consistentHash  =   new   ConsistentHash ( ) ; \n     private   static   List < String >  sList  =   new   ArrayList < > ( ) ; \n\n     public   static   void   main ( String [ ]  args )   { \n         // 增加节点 \n         for   ( int  i  =   0 ;  i  <   NODE_SIZE ;  i ++ )   { \n             String  ip  =   new   StringBuilder ( "10.2.1." ) . append ( i ) \n                     . toString ( ) ; \n            consistentHash . addNode ( ip ) ; \n         } \n\n         // 生成需要缓存的数据; \n         for   ( int  i  =   0 ;  i  <   10 ;  i ++ )   { \n            sList . add ( i + "" ) ; \n         } \n\n         // 将数据放入到缓存中。 \n         for   ( String  s  :  sList )   { \n            consistentHash . put ( s ,  s ) ; \n         } \n\n\n\n         // 输出节点及数据分布情况 \n         HashMap < String ,   List < Object > >  map  =   new   HashMap < > ( ) ; \n         for   ( int  i  =   0 ;  i  <  sList . size ( ) ;  i ++ )   { \n             String  key  =  sList . get ( i ) ; \n             Node  matchNode  =  consistentHash . findMatchNode ( key ) ; \n             List < Object >  list  =  map . getOrDefault ( matchNode . getIp ( ) ,   new   ArrayList < > ( ) ) ; \n            list . add ( i ) ; \n            map . put ( matchNode . getIp ( ) ,  list ) ; \n         } \n         for   ( String  ip  :  map . keySet ( ) )   { \n             System . out . println ( "ip:" + ip + "对应数据:" + map . get ( ip ) ) ; \n         } \n\n         ArrayList < Ip_Vm >  ip_vms  =   new   ArrayList < > ( ) ; \n\n         for   ( Node  node  :  consistentHash . nodeList ) { \n             System . out . println ( "ip:" + node . getIp ( ) + "对应虚拟节点:" + node . getVirtualNodeHashes ( ) ) ; \n             for   ( Integer  virtualNodeHash  :  node . getVirtualNodeHashes ( ) )   { \n                ip_vms . add ( new   Ip_Vm ( node . getIp ( ) , virtualNodeHash ) ) ; \n             } \n         } \n        ip_vms . sort ( ( Comparator . comparingInt ( Ip_Vm :: getVirtualNodeHashCode ) ) ) ; \n         System . out . println ( "节点在hash环上的分布：" ) ; \n         for   ( Ip_Vm  ip_vm  :  ip_vms )   { \n             System . out . print ( ip_vm . getIp ( ) + "\\t" ) ; \n         } \n         System . out . println ( ) ; \n\n         System . out . println ( "=====================新增节点后的情况===========================" ) ; \n\n         // 新增一个数据节点 \n        consistentHash . addNode ( "10.2.1.3" ) ; \n\n         // 输出节点及数据分布情况 \n\n         HashMap < String ,   List < Object > >  map_n  =   new   HashMap < > ( ) ; \n         for   ( int  i  =   0 ;  i  <  sList . size ( ) ;  i ++ )   { \n             String  key  =  sList . get ( i ) ; \n             Node  matchNode  =  consistentHash . findMatchNode ( key ) ; \n             List < Object >  list  =  map_n . getOrDefault ( matchNode . getIp ( ) ,   new   ArrayList < > ( ) ) ; \n            list . add ( i ) ; \n            map_n . put ( matchNode . getIp ( ) ,  list ) ; \n         } \n         for   ( String  ip  :  map_n . keySet ( ) )   { \n             System . out . println ( "ip:" + ip + "对应数据:" + map_n . get ( ip ) ) ; \n         } \n\n\n         ArrayList < Ip_Vm >  ip_vms_n  =   new   ArrayList < > ( ) ; \n         for   ( Node  node  :  consistentHash . nodeList ) { \n             System . out . println ( "ip:" + node . getIp ( ) + "对应虚拟节点:" + node . getVirtualNodeHashes ( ) ) ; \n             for   ( Integer  virtualNodeHash  :  node . getVirtualNodeHashes ( ) )   { \n                ip_vms_n . add ( new   Ip_Vm ( node . getIp ( ) , virtualNodeHash ) ) ; \n             } \n         } \n        ip_vms_n . sort ( ( Comparator . comparingInt ( Ip_Vm :: getVirtualNodeHashCode ) ) ) ; \n         System . out . println ( "新增节点在hash环上的分布：" ) ; \n         for   ( Ip_Vm  ip_vm  :  ip_vms_n )   { \n             System . out . print ( ip_vm . getIp ( ) + "\\t" ) ; \n         } \n         System . out . println ( ) ; \n\n\n     } \n\n     private   static   class   Ip_Vm { \n         String  ip ; \n         Integer  virtualNodeHashCode ; \n         List < Object >  keys ; \n\n         public   List < Object >   getKeys ( )   { \n             return  keys ; \n         } \n\n         public   void   addKey ( Object  key )   { \n            keys . add ( key ) ; \n         } \n\n\n         public   Ip_Vm ( String  ip ,   Integer  virtualNodeHashCode )   { \n             this . ip  =  ip ; \n             this . virtualNodeHashCode  =  virtualNodeHashCode ; \n            keys = new   ArrayList < > ( ) ; \n         } \n\n         public   String   getIp ( )   { \n             return  ip ; \n         } \n\n         public   void   setIp ( String  ip )   { \n             this . ip  =  ip ; \n         } \n\n         public   Integer   getVirtualNodeHashCode ( )   { \n             return  virtualNodeHashCode ; \n         } \n\n         public   void   setVirtualNodeHashCode ( Integer  virtualNodeHashCode )   { \n             this . virtualNodeHashCode  =  virtualNodeHashCode ; \n         } \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 \n 虚拟节点和数据更多情况的测试 \n public   class   ConsistentHashTest   { \n     public   static   final   int   NODE_SIZE   =   10 ; \n     public   static   final   int   STRING_COUNT   =   100   *   100 ; \n     private   static   ConsistentHash  consistentHash  =   new   ConsistentHash ( ) ; \n     private   static   List < String >  sList  =   new   ArrayList < > ( ) ; \n\n     public   static   void   main ( String [ ]  args )   { \n         // 增加节点 \n         for   ( int  i  =   0 ;  i  <   NODE_SIZE ;  i ++ )   { \n             String  ip  =   new   StringBuilder ( "10.2.1." ) . append ( i ) \n                     . toString ( ) ; \n            consistentHash . addNode ( ip ) ; \n         } \n\n         // 生成需要缓存的数据; \n         for   ( int  i  =   0 ;  i  <   STRING_COUNT ;  i ++ )   { \n            sList . add ( RandomStringUtils . randomAlphanumeric ( 10 ) ) ; \n         } \n\n         // 将数据放入到缓存中。 \n         for   ( String  s  :  sList )   { \n            consistentHash . put ( s ,  s ) ; \n         } \n\n         for ( int  i  =   0   ;  i  <   10   ;  i  ++ )   { \n             int  index  =   RandomUtils . nextInt ( 0 ,   STRING_COUNT ) ; \n             String  key  =  sList . get ( index ) ; \n             String  cache  =   ( String )  consistentHash . get ( key ) ; \n             System . out . println ( "Random:" + index + ",key:"   +  key  +   ",consistentHash get value:"   +  cache  + ",value is:"   +  key . equals ( cache ) ) ; \n         } \n\n         // 输出节点及数据分布情况 \n         for   ( Node  node  :  consistentHash . nodeList ) { \n             System . out . println ( node ) ; \n         } \n\n         // 新增一个数据节点 \n        consistentHash . addNode ( "10.2.1.110" ) ; \n         for ( int  i  =   0   ;  i  <   10   ;  i  ++ )   { \n             int  index  =   RandomUtils . nextInt ( 0 ,   STRING_COUNT ) ; \n             String  key  =  sList . get ( index ) ; \n             String  cache  =   ( String )  consistentHash . get ( key ) ; \n             System . out . println ( "Random:" + index + ",key:"   +  key  +   ",consistentHash get value:"   +  cache  + ",value is:"   +  key . equals ( cache ) ) ; \n         } \n\n         // 输出节点及数据分布情况 \n         for   ( Node  node  :  consistentHash . nodeList ) { \n             System . out . println ( node ) ; \n         } \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 '},{title:"布隆过滤器和布谷鸟过滤器",frontmatter:{title:"布隆过滤器和布谷鸟过滤器",date:"2023-02-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["算法"],tags:["黑名单过滤","缓存穿透优化"]},regularPath:"/%E5%85%B6%E4%BB%96/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8.html",relativePath:"其他/布隆过滤器和布谷鸟过滤器.md",key:"v-7350f07e",path:"/2023/02/08/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8/",headers:[{level:3,title:"前言",slug:"前言"},{level:3,title:"BitMap数据结构",slug:"bitmap数据结构"},{level:3,title:"布隆过滤器",slug:"布隆过滤器"},{level:3,title:"布谷鸟过滤器",slug:"布谷鸟过滤器"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' 前言 \n 布隆过滤器 (Bloom Filter)是由Burton Howard Bloom于1970年提出，它是一种space efficient的概率型数据结构，用于 判断一个元素是否在集合 中。在垃圾邮件过滤的黑白名单方法、爬虫(Crawler)的网址判重模块中等等经常被用到。哈希表也能用于判断元素是否在集合中，但是布隆过滤器只需要哈希表的1/8或1/4的空间复杂度就能完成同样的问题。布隆过滤器可以插入元素，但不可以删除已有元素。其中的元素越多，false positive rate(误报率)越大，但是false negative (漏报)是不可能的。 \n 传统HashMap数据结构的不足 \n 一般来说，将网页 URL 存入数据库进行查找，或者建立一个哈希表进行查找就 OK 了。 \n 当数据量小的时候，这么思考是对的，确实可以将值映射到 HashMap 的 Key，然后可以在 O(1) 的时间复杂度内 返回结果，效率奇高。但是 HashMap 的实现也有缺点，例如存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的，举个例子如果一个 1000 万 HashMap，Key=String（长度不超过 16 字符，且重复性极小），Value=Integer，会占据多少空间呢？1.2 个 G。 \n 实际上用 bitmap，1000 万个 int 型，只需要 40M（ 10 000 000 * 4/1024/1024 =40M）左右空间，占比 3%，1000 万个 Integer，需要 161M 左右空间，占比 13.3%。 \n 可见一旦你的值很多例如上亿的时候，那 HashMap 占据的内存大小就可想而知了。 \n 但如果整个网页黑名单系统包含 100 亿个网页 URL，在数据库查找是很费时的，并且如果每个 URL 空间为 64B，那么需要内存为 640GB，一般的服务器很难达到这个需求。 \n BitMap数据结构 \n 现代计算机用二进制（bit，位）作为信息的基础单位，1 个字节等于 8 位。许多开发语言都提供了操作位的功能，合理地使用位能够有效地提高内存使用率和开发效率。 \n Bit-map 的基本思想就是用一个 bit 位来标记某个元素对应的 value，而 key 即是该元素。由于采用了 bit 为单位来存储数据，因此在存储空间方面，可以大大节省。 \n 在 Java 中，int 占 4 字节，1 字节 = 8位（1 byte = 8 bit），如果我们用这个 32 个 bit 位的每一位的值来表示一个数的话是不是就可以表示 32 个数字，也就是说  32 个数字只需要一个 int 所占的空间大小就可以了，那就可以缩小空间 32 倍。 \n \n 1 Byte = 8 Bit，1 KB = 1024 Byte，1 MB = 1024 KB，1GB = 1024 MB \n \n 举个例子：假设网站有 1 亿用户，每天独立访问的用户有 5 千万，如果每天用集合类型和 BitMap 分别存储活跃用户： \n 集合类型：假如用户 id 是 int 型，4 字节，32 位，则集合类型占据的空间为 50 000 000 * 4/1024/1024 = 200M； \n BitMap：.如果按位存储，5 千万个数就是 5 千万位，占据的空间为 50 000 000/8/1024/1024 = 6M。 \n 那么如何用 BitMap 来表示一个数呢？ \n 上面说了用 bit 位来标记某个元素对应的 value，而 key 即是该元素，我们可以把 BitMap 想象成一个以位为单位的 数组 ，数组的每个单元只能存储 0 和 1（0 表示这个数不存在，1 表示存在），数组的下标在 BitMap 中叫做偏移量。 \n 比如我们需要表示 {1,3,5,7} 这四个数，如下： \n \n 那如果还存在一个数 65 呢？只需要开 int[N/32+1] 个 int 数组就可以存储完这些数据（其中 N 表示这群数据中的最大值），即： \n int[0] ：可以表示 0~31 \n int[1] ：可以表示 32~63 \n int[2] ：可以表示 64~95 \n \n 假设我们要判断任意整数是否在列表中，则  M/32  就得到下标， M%32 就知道它在此下标的哪个位置，如： \n 65/32 = 2 ， 65%32=1 ，即 65 在 int[2]  中的第 1 位。 \n Java BitArray使用long类型来实现，long占8字节，64位，所以需要构建long[Math.ceil(n/64)]大小的long数组。\n \n 1 #  布隆过滤器 \n 概念 \n 本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构， 特点是高效地插入和查询 ，可以用来告诉你 “ 某 样东西一定不存在或者可能存在 ”。 \n 网页交互初探： Bloom Filters (jasondavies.com) \n 特点 \n 相比于传统的 List、Set、Map 等数据结构，它更高效、占用空间更少，但是缺点是其返回的结果是概率性的，而不是确切的。 \n 实际上，布隆过滤器广泛应用于 网页黑名单系统 、 垃圾邮件过滤系统 、 爬虫网址判重系统 等，Google 著名的分布式数据库 Bigtable 使用了布隆过滤器来查找不存在的行或列，以减少磁盘查找的 IO 次数，Google Chrome 浏览器使用了布隆过滤器加速安全浏览服务。 \n 在很多 Key-Value 系统中也使用了布隆过滤器来加快查询过程，如 Hbase，Redis。一般而言，Value 保存在磁盘中，访问磁盘需要花费大量时间，然而使用布隆过滤器可以快速判断某个 Key 对应的 Value 是否存在，因此可以避免很多不必要的磁盘 IO 操作。通过一个 Hash 函数将一个元素映射成一个位阵列（Bit Array）中的一个点。这样一来，我们只要看看这个点是不是 1 就知道可以集合中有没有它了。这就是布隆过滤器的基本思想。 \n 当一个元素加入布隆过滤器中的时候，会进行如下操作： \n 使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。\n根据得到的哈希值，在位数组中把对应下标的值置为 1。\n \n 1 2 当我们需要判断一个元素是否存在于布隆过滤器的时候，会进行如下操作： \n 对给定元素再次进行相同的哈希计算；\n得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中(注意：由于hash冲突问题，所谓的存在只能是可能存在)，如果存在一个值不为 1，说明该元素不在布隆过滤器中。\n \n 1 2 布隆过滤器不支持删除 \n 传统的布隆过滤器并不支持删除操作 。因为hash碰撞的原因，你想要 删除的元素有可能存储着其他元素的信息 。但是名为 Counting Bloom filter 的变种可以用来测试元素计数个数是否绝对小于某个阈值，它支持元素删除。 \n 综上，我们可以得出： \n 1.布隆过滤器说某个元素存在，小概率会误判，不一定存在； \n 2.布隆过滤器说某个元素不在，那么这个元素一定不在； \n 3.布隆过滤器不支持删除。（不包含变种） \n  运用场景 \n 1、目前有 10 亿数量的自然数，乱序排列，需要对其排序。限制条件在 32 位机器上面完成，内存限制为 2G。如何完成？ \n 2、如何快速在亿级黑名单中快速定位 URL 地址是否在黑名单中？(每条 URL 平均 64 字节) \n 3、需要进行用户登陆行为分析，来确定用户的活跃情况？ \n 4、网络爬虫-如何判断 URL 是否被爬过？ \n 5、快速定位用户属性（黑名单、白名单等）？ \n 6、数据存储在磁盘中，如何避免大量的无效 IO？ \n 7、判断一个元素在亿级数据中是否存在？ \n 8、缓存穿透。 \n 解决缓存穿透\n\n缓存穿透是指查询一个根本不存在的数据，缓存层和存储层都不会命中，如果从存储层查不到数据则不写入缓存层。\n缓存穿透将导致不存在的数据每次请求都要到存储层去查询，失去了缓存保护后端存储的意义，可能会使后端存储负载加大，数据库宕机。\n\n因此我们可以用布隆过滤器来解决，在访问缓存层和存储层之前，将存在的 key 用布隆过滤器提前保存起来，做第一层拦截。\n\n例如：一个推荐系统有 4 亿个用户 id，每个小时算法工程师会根据每个用户之前历史行为计算出推荐数据放到存储层中，但是最新的用户由于没有历史行为，就会发生缓存穿透的行为，为此可以将所有推荐数据的用户做成布隆过滤器。\n\n如果布隆过滤器认为该用户 id 不存在，那么就不会访问存储层，在一定程度保护了存储层。\n\n注：布隆过滤器可能会误判，放过部分请求，当不影响整体，所以目前该方案是处理此类问题最佳方案\n \n 1 2 3 4 5 6 7 8 9 10 11 12 9.短视频的曝光页面瀑布流式展示 \n \n 为了兼顾短视频质量和时效性，短视频排序采用了重力算法： \n \n H为短视频的质量分，通过观看，点赞，评论，转发等数据加权求和计算，T为短视频发布时间戳，T0位基准时间，取发现页最早发布的短视频创建时间戳，单位均为秒。A为时间系数，根据发现页短视频的平均更新间隔，取36000(10小时)。该算法的效果是，发布时间接近，质量分高的短视频靠前，随着时间推移，短视频不断下沉，削弱头部曝光产生的马太效应。 \n 为了提高内容的新鲜感，我们希望用户在每次下拉刷新以及翻页的时候，都能看到新的短视频，同时在短视频列表头部加入新的短视频时，能得到优先展示，如下图所示： \n \n 左图为首屏显示的短视频，如在此时，短视频列表顺序发生了更新，C+和D+插在了看过的视频中间，我们希望在下次刷新的时候，把浏览过的其他视频去掉，相当于优先插入C+、D+。实现这个需求最简单的方法是保存用户最近观看过的全部短视频作为过滤器，每次返回列表的时候，从头部开始遍历，去掉用户看过的短视频。显然，过滤器的容量，决定了短视频列表的最大展示深度？？？？？（返回的列表一定，只是对列表进行过滤？？？）。根据产品需求，发现页需要展示最近一个月的短视频，大约4000个，平均每个短视频id的长度为50字节，这个过滤器如果用传统的redis set等手段实现，存储成本和过滤效率都比较低，针对这个问题，我们采用了一个简单而强大的数据结构---布隆过滤器。 \n Bloom Filter(布隆过滤器)是一种空间效率很高的随机数据结构，它利用位数组很简洁地表示一个集合，并能判断一个元素是否属于这个集合。Bloom Filter的这种高效是有一定代价的：在判断一个元素是否属于某个集合时，有可能会把不属于这个集合的元素误认为属于这个集合（false positive）。因此，Bloom Filter不适合那些“零错误”的应用场合。而在能容忍低错误率的应用场合下，Bloom Filter通过极少的错误换取了存储空间的极大节省。 \n 我们使用MurmurHash和bitset实现了一个可以序列化成整形数组的布隆过滤器，可以利用redis支持的简单key-value数据结构进行存取，在本地实现高效的过滤运算，一个能保存4000个短视频id的布隆过滤器，只占用不到8KB的空间，get&set的效率都比较高。 \n 因为布隆过滤器容量有限，且无法删除元素， 需要配合重建策略使用 。我们用redis维护了一个最近观看的100个短视频id，当布隆过滤器空间利用率超过百分之50的时候，清空并使用这100个id进行重建，避免了极端情况下的重复问题。 \n 注：常见的布隆过滤器： \n 1.RedisBloom\n布隆过滤器可以使用 Redis 中的位图(bitmap)操作实现，直到 Redis4.0 版本提供了插件功能，Redis 官方提供的布隆过滤器才正式登场，布隆过滤器作为一个插件加载到 Redis Server 中，官网推荐了一个 RedisBloom 作为 Redis 布隆过滤器的 Module。\n详细安装、指令操作参考：https://github.com/RedisBloom/RedisBloom\n文档地址：https://oss.redislabs.com/redisbloom/\n整合redis方案:\n①.将算法和bitmap数据放在client\n②.算法在client，bitmap数据在redis\n③.讲算法和bitmap数据都放在redis\n\n\n2.Guava 的 BloomFilter\nGuava 项目发布版本11.0时，新添加的功能之一是BloomFilter类。\nGuava 提供的布隆过滤器的实现还是很不错的（想要详细了解的可以看一下它的源码实现），但是它有一个重大的缺陷就是只能单机使用（另外，容量扩展也不容易），而现在互联网一般都是分布式的场景，还得是redis出马。\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 #  实现原理 \n 假设我们有个集合 A，A 中有 n 个元素。利用 k个哈希散列 函数，将A中的每个元素 映射 到一个长度为 a 位的数组 B中的不同位置上，这些位置上的二进制数均设置为 1。 \n 如果待检查的元素，经过这 k个哈希散列函数的映射后，发现其 k 个位置上的二进制数 全部为 1 ，这个元素很可能属于集合A，反之， 一定不属于集合A 。 \n 比如我们有 3 个 URL  {URL1,URL2,URL3} ，通过一个hash 函数把它们映射到一个长度为 16 的数组上，如下： \n \n 若当前哈希函数为  Hash1() ，通过哈希运算映射到数组中，假设 Hash1(URL1) = 3 ， Hash1(URL2) = 6 ， Hash1(URL3) = 6 ，如下： \n \n 因此，如果我们需要判断 URL1 是否在这个集合中，则通过 Hash(1) 计算出其下标，并得到其值若为 1 则说明存在。 \n 由于 Hash 存在哈希冲突，如上面 URL2,URL3 都定位到一个位置上，假设 Hash 函数是良好的，如果我们的数组长度为 m 个点，那么如果我们想将冲突率降低到例如  1% ， 这个散列表就只能容纳  m/100  个元素，显然空间利用率就变低了，也就是没法做到 空间有效 （space-efficient）。 \n 解决方法也简单，就是使用多个 Hash 算法，如果它们有一个说元素不在集合中，那肯定就不在，如下： \n Hash1(URL1) = 3,Hash2(URL1) = 5,Hash3(URL1) = 6\nHash1(URL2) = 5,Hash2(URL2) = 8,Hash3(URL2) = 14\nHash1(URL3) = 4,Hash2(URL3) = 7,Hash3(URL3) = 10\n \n 1 2 3 \n 以上就是布隆过滤器做法，使用了 k个哈希函数 ，每个字符串跟 k 个 bit 对应，从而降低了冲突的概率。 \n 误判现象 \n 上面的做法同样存在问题，因为随着增加的值越来越多，被置为 1 的 bit 位也会越来越多，这样某个值即使没有被存储过，但是万一哈希函数返回的三个 bit 位都被其他值置位了 1 ，那么程序还是会判断这个值存在。 \n 比如此时来一个不存在的  URL1000 ，经过哈希计算后，发现 bit 位为下： \n Hash1(URL1000) = 7,Hash2(URL1000) = 8,Hash3(URL1000) = 14\n \n 1 \n 但是上面这些 bit 位已经被 URL1,URL2,URL3 置为 1 了，此时程序就会判断  URL1000  值存在。 \n 这就是布隆过滤器的误判现象，所以， 布隆过滤器判断存在的不一定存在，但是，判断不存在的一定不存在。 \n 布隆过滤器可精确的代表一个集合，可精确判断某一元素是否在此集合中，精确程度由用户的具体设计决定，达到 100% 的正确是不可能的。 \n 但是布隆过滤器的优势在于， 利用很少的空间可以达到较高的精确率 。 \n Bloom Filter公式推导 \n Coding \n 1.spark中构建布隆过滤器，然后进行广播； \n 2.整合redis布隆过滤 \n 3.Hbase也有布隆过滤器 \n 布谷鸟过滤器 \n 背景 \n 除了删除这个问题之外，布隆过滤器还有一个问题：查询性能不高。 \n 因为真实场景中过滤器中的数组长度是非常长的，经过多个不同 Hash 函数后，得到的数组下标在内存中的跨度可能会非常的大。跨度大，就是不连续。不连续，就会导致 CPU 缓存行命中率低。 \n 变种的Counting Bloom Filter，虽然实现了删除操作，但是用多占用3 到 4 倍倍的存储空间的代价，才给 Bloom Filter 增加了删除操作，不够优雅。 \n 因此出现新的解决方案，那就是布谷鸟过滤器。 \n 特点 \n 论文《Cuckoo Filter: Practically Better Than Bloom》提出了删除并不需要在空间或性能上提出更高的开销。 \n 布谷鸟过滤器是一个实用的数据结构，提供了四大优势： \n \n \n 1.支持动态的新增和删除元素。 \n 2.提供了比传统布隆过滤器更高的查找性能，即使在接近满的情况下（比如空间利用率达到 95% 的时候）。 \n 3.比诸如商过滤器（quotient filter，另一种过滤器）之类的替代方案更容易实现。 \n 4.如果要求错误率小于3%，那么在许多实际应用中，它比布隆过滤器占用的空间更小。 \n 实现原理 \n 布谷鸟过滤器不同于布隆过滤器主要有两点改动： \n 1.hash算法： \n在布谷鸟过滤器中，数组中存储的是每个元素的"指纹信息"，也就是hash运算之后的几个bit位。查询数据的时候，就是看看对应的位置上有没有对应的“指纹”信息，删除数据的时候，也只是抹掉该位置上的“指纹”而已。 \n由于指纹是对元素进行 hash 计算得出的，那么必然会出现 hash 碰撞的问题，也就是“指纹”相同的情况，也就是会出现误判的情况，所以这点和布隆过滤器一样。 \n布谷鸟过滤器的hash算法是基于布谷鸟哈希算法做了改进，计算公式如下： \n fp = fingerprint(x)\nh1 = hash(x)\nh2 = h1 ^ hash(fp)  // 异或\n \n 1 2 3 在上列公式可以看出，h2的位置是根据h1的位置计算出来的，也就是说我们知道了其中一个位置，就可以直接获取到另外一个位置，不需要再做全量的hash运算。因为使用的异或运算，所以这两个位置具有对偶性。这也是提高查询效率的一个点。 \n只要保证 hash(fp) !=0，那么就可以确保 h2!=h1，也就可以确保不会出现自己踢自己的死循环问题了。 \n这里还有个注意点：就是hash运算的时候，并没有对值进行长度取模运算，那么他是如何保证计算出来hash坐标，一定是在数组长度范围内呢？这就说到布谷鸟过滤器的一个限制条件了，那就是强制数组的长度必须是 2 的指数倍。2 的指数倍的二进制一定是这样的：10000000...（n个0）。 \n这个限制带来的好处就是，进行异或运算时，可以保证计算出来的下标一定是落在数组中的 \n 2.存储结构： \n 布谷鸟过滤器的存储结构是每个坐标下的空位是多个，不同于布隆过滤器的一个空位。如下图所示： \n \n 布谷鸟过滤器会记录每个元素的两个hash位置，每个位置下都会有多个空位，空位内存储的就是元素的“指纹信息”。 \n \n 布谷鸟过滤器添加元素的流程是这样的： \n布谷鸟过滤器会先计算出元素对应的指纹信息，然后对元素进行hash运算，计算出元素的第一个存储坐标，该坐标下存在四个空位，如果四个空位中有空闲的，就将该元素的指纹信息存进去；如果没有空位，就会根据指纹和第一个hash坐标进行异或运算，计算出第二个坐标，如果第二个坐标下有空位，就将该元素的指纹信息存进去；如果还没有空位，那么该元素就会随机将一个空位中的指纹信息挤出，然后自己存进去，被挤出的指纹信息会计算出自己的第二个坐标，然后判断是否有空位，重复上述操作，直到达到一个阀值，布谷鸟过滤器返回false或进行扩容处理。 \n \n 数据Data(指纹11)想要存储到布谷鸟过滤器中，首先会计算出h1和h2两个存储坐标，结果发现两个坐标的空位都已经“满员”了，此时会随机挤掉一个元素的指纹信息，假设挤掉了h1坐标的指纹3，然后指纹3会找自己的第二个坐标，然后判断是否有空位，有空位就存到第二个坐标下，如下图： \n \n 扩容：如果数组过小，会发生循环挤兑的情况，就可以设置最大挤兑次数，如果超过该次数，进行扩容，重新计算每个指纹的位置。 \n 当 hash 函数固定为 2 个的时候，如果一个下标只能放一个元素的指纹信息，那么空间利用率是 50%。如果为 2，4，8 个元素的时候，空间利用率分别是 84%，95%，98%，可以发现空间利用率飙升。 \n 但是需要对重复数据进行限制：如果需要布谷鸟过滤器支持删除，它必须知道一个数据插入过多少次。不能让同一个数据插入 kb+1 次。其中 k 是 hash 函数的个数，b 是一个下标的位置能放几个元素。 \n 比如 2 个 hash 函数，一个二维数组，它的每个下标最多可以插入 4 个元素。那么对于同一个元素，最多支持插入 8 次。 \n 例如下面这种情况： \n \n why 已经插入了 8 次了，如果再次插入一个 why，则会出现循环踢出的问题，直到最大循环次数，然后返回一个 false。 \n 怎么避免这个问题呢？ \n 我们维护一个记录表，记录每个元素插入的次数就行了。 \n 虽然逻辑简单，但是架不住数据量大呀。你想想，这个表的存储空间又怎么算呢？ \n 优缺点 \n 优点 \n 1. 支持删除元素 。 \n \n 布隆过滤器不支持删除元素 \n \n 2. 更节省空间 。 \n \n 布谷鸟哈希表更加紧凑 \n布谷鸟过滤器在错误率小于3%的时候空间性能是优于布隆过滤器 \n布谷鸟过滤器比布隆过滤器空间节省40%多 \n \n 3. 查询效率很高 \n \n 布隆过滤器要采用多种哈希函数进行多次哈希 \n \n 缺点 \n 1.插入性能较差 \n \n 布谷鸟过滤器在计算哈希后可能当前位置上已经存储了指纹，这时就要将已存储的项踢到候选桶，随着桶越来越满，产生冲突的可能性越来越大，插入耗时越来越高 \n布隆过滤器插入时计算好哈希直接写入位即可 \n \n 2.插入重复元素存在上限 \n \n 布谷鸟过滤器对已存在的值会做踢出操作，因此重复元素的插入存在上限。 \n布隆过滤器在插入重复元素时并没有影响，只是对已存在的位再置一遍。 \n \n 3.空间大小 \n \n 布谷鸟过滤器必须是2的指数。 \n布隆过滤器不需要2的指数。 \n \n 4.删除问题 \n \n 有上述重复插入的限制，删除时也会出现相关的问题： \n1.删除仅在相同哈希值被插入一次时是完美的; \n2.如果元素没有插入便进行删除， 可能会出现误删除 ，这和假阳性率的原因相同; \n3.如果元素插入了多次，那么每次删除仅会删除一个值，你需要知道元素插入了多少次才能删除干净，或者循环运行删除直到删除失败为止. \n \n Redis布谷鸟过滤器：https://github.com/kristoff-it/redis-cuckoofilter \n Redis通过插件支持sql及布谷鸟过滤器：https://github.com/RedBeardLab/rediSQL \n 扩展布谷鸟 hash原理 \n 首先，先了解下布谷鸟hash。 \n \n 它的工作原理，总结起来是这样的： \n 它有两个 hash 表，记为 T1，T2。 \n 两个 hash 函数，记为 h1，h2。 \n 当一个不存在的元素插入的时候，会先根据 h1 计算出其在 T1 表的位置，如果该位置为空则可以放进去。 \n 如果该位置不为空，则根据 h2 计算出其在 T2 表的位置，如果该位置为空则可以放进去。 \n 如果该位置不为空，就把当前位置上的元素踢出去，然后把当前元素放进去就行了。 \n 也可以随机踢出两个位置中的一个，总之会有一个元素被踢出去。 \n 被踢出去的元素怎么办呢？没事啊，它也有自己的另外一个位置。 \n \n 上面的图说的是这样的一个事儿： \n 我想要插入元素 x，经过两个 hash 函数计算后，它的两个位置分别为 T1 表的 2 号位置和 T2 表的 1 号位置。 \n 两个位置都被占了，那就随机把 T1 表 2 号位置上的 y 踢出去吧。 \n 而 y 的另一个位置被 z 元素占领了。 \n 于是 y 毫不留情把 z 也踢了出去。 \n z 发现自己的备用位置还空着（虽然这个备用位置也是元素 v 的备用位置），赶紧就位。 \n 所以，当 x 插入之后，图就变成了这样： \n \n 这种类似于套娃的解决方式看是可行，但是总是有出现循环踢出导致放不进 x 的问题。 \n 当遇到这种情况时候，说明布谷鸟 hash 已经到了极限情况，应该进行扩容，或者 hash 函数的优化。 \n 当踢来踢去了 （MaxLoop）次还没插入完成后，它会告诉你，需要 rehash 并对数组扩容了。 \n 优化 \n 上面的布谷鸟哈希算法的平均空间利用率并不高，大概只有 50%。到了这个百分比，就会很快出现连续挤兑次数超出阈值。这样的哈希算法价值并不明显，所以需要对它进行改良。 \n 改良的方案之一是 增加 hash 函数 ，让 每个元素不止有两个巢，而是三个巢、四个巢 。这样可以大大降低碰撞的概率，将空间利用率提高到 95%左右。 \n 另一个改良方案是 在数组的每个位置上挂上多个座位 ，这样即使两个元素被 hash 在了同一个位置，也不必立即「鸠占鹊巢」，因为这里有多个座位，你可以随意坐一个。除非这多个座位都被占了，才需要进行挤兑。很明显这也会显著降低挤兑次数。这种方案的空间利用率只有 85%左右，但是查询效率会很高，同一个位置上的多个座位在内存空间上是连续的，可以有效利用 CPU 高速缓存。 \n 所以更加高效的方案是将上面的两个改良方案融合起来，比如 使用 4 个 hash 函数，每个位置上放 2 个座位 。这样既可以得到时间效率，又可以得到空间效率。这样的组合甚至可以 将空间利用率提到高 99% ，这是非常了不起的空间效率。 \n Coding \n 1.redis4.0支持module,将布谷鸟过滤器加载到module中. \n'},{title:"分布式一致性算法",frontmatter:{title:"分布式一致性算法",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["其他"],tags:["分布式","一致性"]},regularPath:"/%E5%85%B6%E4%BB%96/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95.html",relativePath:"其他/分布式一致性算法.md",key:"v-22a43ff6",path:"/2023/06/10/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/",headers:[{level:2,title:"前言",slug:"前言"},{level:3,title:"分布式快照为什么难？",slug:"分布式快照为什么难"},{level:2,title:"2. 分布式系统模型",slug:"_2-分布式系统模型"},{level:2,title:"3. 算法",slug:"_3-算法"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' 前言 \n 分布式算法保证分布式集群环境的一致性 \n  分布式快照为什么难？ \n \n \n 系统状态是不断变化的，各个Process时间也不是完全一致的； \n \n \n 不能让系统停下来做快照，快照对系统的负面影响应该尽量小； \n \n \n 想象给一大群奔跑的羊拍照，由于羊群太大，一张照片没法捕捉完整，需要好几张照片合成，但是羊群不会停下来，如何能够获得一张有意义的照片？比如，每一只羊正好都只出现了一次？如果羊倌在回家前数数羊是不是少了，这种快照就是有意义的。虽然不是照片上所有的羊的位置都是同一个物理时刻的。 \n Chandy-Lamport algorithm分布式快照 \n 2. 分布式系统模型 \n 2.1 分布式系统的抽象模型 \n \n 分布式系统由一系列Processes和通信Channel组成； \n 认为每个Channel是有向的(一个双向连接可以认为是两个Channel); \n 整个系统构成了一个有向图： Process是图的顶点，Channel是图的边； \n 进程可以自己执行运算，还可以Send/Recv消息； \n 进程可以记录状态(进程当前状态、已发送的消息，已接收的消息)。 \n \n 2.2 对Channel的假设 \n \n 不会乱序 ，后面会看到，Snapshot Marker与其他msg之间的顺序，是不能打乱的； \n 没有错误。 \n \n 2.3 Channel的State \n \n 一个Channel上已被接收的消息，是已发送消息的一个子序列(Subsequence)； \n Channel的状态，用“已发送的消息” 减去 “已被接收的消息”表示，即： 已发 - 已收。 \n \n \n The state of a channel is the sequence of messages sent along the channel, excluding the messages received along the channel \n \n \n Channel状态是由接收者来记录的。 \n \n 2.4 Process的State \n \n 2.5 分布式系统的Global State \n \n 2.6 一个简单例子 - 单令牌保护系统 \n \n \n 注意这个例子中，每个状态都维护了一个不变式： token一直在p, q, c或者c\'之一上。不会在某个状态出现两个 token，token也不会消失。在后续的快照算法中，会以这个简单的分布式系统为例，说明快照算法可能出现的问题。 \n 3. 算法 \n 3.1 算法是怎么来的？ \n 3.1.1 初始思路 \n \n 每个Process负责记录自己的状态； \n 每个Channel由其两端的Process分别记录(已发和已收)； \n 不能保证每个process严格在同一个时刻(instant)记录状态，但是最终形成的需要是个**"有意义"状态**； \n 快照需要被叠加(SuperImpose)到已有计算上，即尽量不干扰系统固有的计算。 \n \n 3.1.2 容易出现的问题分析 \n Example 3.1 的快照过程 \n 这里先假设每个进程可以原子地记录自己的状态，Channel的状态也可以被原子地记录(实际实现要复杂些)。考虑下图中的记录步骤：最终记录的状态是： p和c都认为自己有token 。 \n \n 出现两个token问题原因分析 \n \n \n \n 3.2 算法要点 \n 3.2.1 算法的规则 \n \n 3.2.2 算法的结束 \n \n 结束的含义：所有的 Process和Chan，都完成快照状态记录; \n 如果是个强连通图，那么从任意一个节点开始，都能在有限时间内结束； \n 如果不是强连通图，但是所有节点可达，那么从某些指定节点开始，也能在有限时间内结束。 \n \n \n \n \n \n 3.3 结合token的例子讨论下算法 \n 3.3.1  假设p要给q发送token，且p在q之前记录状态，随后发送了Marker给q \n \n \n 如果q收到的第一个Marker来自p，那么q的状态记录中包含了p进行记录之前的所有消息； \n \n \n 如果q收到的第一个Marker不是来自p，那么q等待收到p的Marker才算完成chan的状态记录。即生成的chan状态记录中包含了p在record之前的所有消息(在p record时，这些消息可能在chan中，也可能已经被q接收完)。 \n \n \n 对于上面的例子，考虑p是发送token给q，p记录快照的时间点可以是：(A) In-p; (B). In-chan; (C). In-q。无论如何，记录的快照整体应该包含且只包含一个 token。 \n \n 3.3.2 假设p给q发送token, 但是q先于p 记录状态 \n \n 如果q在收到token后才开始记录，即q记录时已经token传递完成，那么二者记录的都是token已经传递完成的一致状态; \n 如果q在记录时，token在路上(in-chan)，那么q记录的状态，不含token，随后发送marker给p，但是此时q并没有完成chan的的快照记录；随后，p 记录自己的状态，并发送marker给 q，q才完成chan的状态记录。最终记录的是token出于in-chan状态，是个正确的状态记录。 \n \n \n 与前面相比，为什么这里q有了in-chan? 因为 token位于q收到的两个Marker之间。而不是首个Maker之前。 \n 总结 \n \n 无论p还是q先开始记录，最终都能得到一致的状态。 \n 在q收到首个Marker之前收到的token，计入了q自身的状态(已接收)； \n 在 q收到首个Marker之后收到的token，计入了 chan 的状态。 \n \n 应用： \n Flink checkpoint底层分布式快照算法 \n 参考https://zhuanlan.zhihu.com/p/96045864 \n Paxos一致性算法 \n 应用： \n zk \n'},{title:"差分算法",frontmatter:{title:"差分算法",date:"2022-10-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["算法"],tags:["增量更新","bisdiff/bispatch"]},regularPath:"/%E5%85%B6%E4%BB%96/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95.html",relativePath:"其他/差分算法.md",key:"v-0ddfb2e2",path:"/2022/10/08/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95/",headers:[{level:2,title:"前言",slug:"前言"},{level:3,title:"为何需要差异更新？",slug:"为何需要差异更新"},{level:3,title:"差异化更新方案有哪些？",slug:"差异化更新方案有哪些"},{level:2,title:"bsdiff算法",slug:"bsdiff算法"},{level:3,title:"bsdiff初认识",slug:"bsdiff初认识"},{level:3,title:"BSDiff基本步骤",slug:"bsdiff基本步骤"},{level:3,title:"BSPatch基本步骤",slug:"bspatch基本步骤"},{level:3,title:"进一步优化方案：",slug:"进一步优化方案"},{level:3,title:"使用方法",slug:"使用方法"},{level:3,title:"扩展：差分数组",slug:"扩展-差分数组"},{level:2,title:"ZSTD差异化更新",slug:"zstd差异化更新"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:' 前言 \n  为何需要差异更新？ \n 差异更新即在软件更新时只更新差异化的部分，以达到用最小的下载量完成软件的更新需求 。该思想由来已久，从刚接触电脑时的操作系统、应用软件快速更新功能或填补漏洞，到迭代更加频繁的移动应用时代更多了节省下载流量费用的需求。尤其在移动游戏领域，随着手机性能的提升和玩家对游戏体验的追求，安装包亦是越来越大，并且会频繁的更新以不断给玩家带来更新的玩法和更为优化的体验。然而，这种频繁的更新也同样会带来负面的影响：更新包太大没流量；更新速度太慢错过了本该用来玩游戏的碎片时间；本地空间不足无法更新等问题，这些负面影响都会导致一定程度上的玩家流失。因此，差异化更新能力目前已成为各应用下载渠道的必备能力之一，更小的更新包才能提高更新的成功率。 \n 差异化更新方案有哪些？ \n 一种是基于源文件的差异化更新 ，该种方式成功率高， 算法简单，常用于平台相关的差异更新，但在移动端保存巨大的 源文件 、下载更新文件整合后再 编译 ， 比如bsdiff、Xdelta3。 \n 另一种可执行文件的二进制更新方式 ，比如Courgette \n Courgette来自于谷歌，主要解决的是可执行文件的差分，而更新资源不仅包含可执行文件，还包含了图片等各种资源文件，则需要基于所以，我们主要对比bsdiff和Xdelta3方案。 \n bsdiff和Xdelta3方案比较 \n 下面是我们对选取的几个文件做的bsdiff和Xdelta3差分性能对比： \n \n bsdiff的优势是压缩比高，生成的差分文件非常小，但Patch过程耗时。而Xdelta3的优势是Patch过程耗时极短，但内存消耗非常大。 \n bsdiff算法 \n 普通二进制文件对比 \n 熟悉Linux的同学提到二进制文件对比自然会想到一个命令：cmp。那可执行文件的二进制更新岂不是有了这个对比结果后， 然后拿更新结果修改旧文件的二进制串为新文件不就OK了？用个最简单的方法测试下，旧文件testDiffUpdate_Old与更新后文件testDiffUpdate_New内容仅差了第一个字符0。 \n xiaoyzhang$  cat  testDiffUpdate_Old\n\n 123456789 \n\nxiaoyzhang$  cat  testDiffUpdate_New\n\n0123456789\n \n 1 2 3 4 5 6 7 通过CMP做两文件的对比后输出文件为testDiffUpdate_Delta，内容下： \n xiaoyzhang$  cmp   -l  testDiffUpdate_New testDiffUpdate_Old  >  testDiffUpdate_Delta\n\ncmp: EOF on testDiffUpdate_Old\n\nxiaoyzhang$  cat  testDiffUpdate_Delta\n\n 1   60   61 \n\n 2   61   62 \n\n 3   62   63 \n\n 4   63   64 \n\n 5   64   65 \n\n 6   65   66 \n\n 7   66   67 \n\n 8   67   70 \n\n 9   70   71 \n\n 10   71   12 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 如文件内容可知，通过简单的二进制对比得出的差异文件用来更新显然是不靠谱的，差异文件的内容远远比新旧两文件还要大的多。 \n -rw-r--r--  1  xiaoyzhang  1085706827  **110**  12   23   12 :33 testDiffUpdate_Delta\n\n-rw-r--r--  1  xiaoyzhang  1085706827  **11**  12   23   12 :29 testDiffUpdate_New\n\n-rw-r--r--  1  xiaoyzhang  1085706827  **10**  12   23   12 :28 testDiffUpdate_Old\n \n 1 2 3 4 5 这个差异更新的问题看起来没有如此简单. \n BSDiff通过引入diff string的概念，大大减少了要记录的指针控制字的数目，从而使得patch包更小。 \n bsdiff初认识 \n 著名的计算机科学家--Colin Percival提出了BsDiff/BsPatch算法，用于差分及合成两个差异不超过50%的文件（如果差异差异超过50%，差分的效果较差）。 \n  BSDiff基本步骤 \n bsdiff主要可以分为三部分： \n 一、通过排序技术对old文件的内容进行排序，形成字典序。这里的排序使用的是后缀排序时间复杂度nlogn,空间复杂度O(n)，当然也可以使用hash技术进行排序。 \n 二、通过二分法查找最长的匹配len,有了这个len,就可以计算出diff string,和extra string. \n 三、将diff string+extra string压缩到更新文件中。 \n **步骤一.**后缀排序生成子串的排名索引 \n 是所有差量更新算法的瓶颈，时间复杂度为O(nlogn),空间复杂度为O(n)，n为old文件的长度。BSDiff采用 Faster suffix sorting方法获得一个字典序，使用了类似于快速排序的二分思想，使用了bucket，I，V三个辅助数组。最终得到一个数组I，记录了以前缀分组的各个字符串组的最后一个字符串在old中的开始位置 \n 后缀排序 \n 构造后缀数组目的是为了找最长公共子序列 \n 后缀排序之朴素（n * n * logn）： \n 通过朴素思路理解后缀数组含义 \n 字符串“ABCA“ \n 1.遍历取出所有后缀suffix[i] \n \n 2.后缀排序，得到sa[i] (排序后的后缀在原后缀的顺序),rank[i] (后缀经过排序后的位置)，与sa互逆。 \n \n 生成后缀串，是枚举每个开始位置，然后往后截取，这个动作就是O(N^2) \n那么对这N个后缀串排序，就是O(N * logN)，而字符串比较本身又不是O(1)，所以其实排序还不止O(N * logN)，应该是O(N * logN * N) \n这个时间复杂度是很高的 \n diff中的qusort,倍增（n * logn），SA-IS是线性的（n）,具体见扩展。 \n 步骤二、找公共子串，划分diff string和extra string \n 公共子序列，允许有50%不同。 \n 是BSDiff产生patch包的核心部分，详细描述如下： \n 这里以old：ababa，new：abacd14327为例进行举例说明。 \n 1.新旧文件，经过search后生成的len为5，对比发现这5个字符串并不完全相同（这里相差2，实际需要相差8才进入生成步骤）； \n 2.ababa以及abacd，生成diff string，如：00011（其中0表示相同，1表示不同）； \n 3.这时old已经没有字符了，new文件中的14327为extra； \n 4.按顺序将各个block压缩后，写入文件（注：bsdiff()方法中本身没有对stream进行压缩，只是在main中加入了bzip2压缩） \n I代表已经排好的字典序，scan代表new中要查询的字符，pos代表old中相匹配的字符，len代表匹配的长度，lastscan=scan-lenb,lastpos=pos-lenb。lastoffset=scan-pos。lastoffset为new和old的偏移量,如果在old中的内容A在new中可以找到，而且A+lastoffset=new中的A，则认为old和new中的A相同。oldscore代表相同内容的len,scsc代表new中开始和old中比较是否相同开始的位置，而old中开始的位置是scsc+lastoffset。lenf代表扩展前缀，lenb代表扩展后缀。 \n \n \n \n 一般动态规划算法——最长公共子串 \n 1.基本概念 \n   首先需要科普一下，最长公共子序列（longest common sequence）和最长公共子串（longest common substring）不是一回事儿。什么是子序列呢？即一个给定的序列的子序列，就是将给定序列中零个或多个元素去掉之后得到的结果。什么是子串呢？给定串中任意个连续的字符组成的子序列称为该串的子串。给一个图再解释一下：\n \n \n    如上图，给定的字符序列： {a,b,c,d,e,f,g,h}，它的子序列示例： {a,c,e,f} 即元素b,d,g,h被去掉后，保持原有的元素序列所得到的结果就是子序列。同理，{a,h},{c,d,e}等都是它的子序列。\n   它的字串示例：{c,d,e,f} 即连续元素c,d,e,f组成的串是给定序列的字串。同理，{a,b,c,d},{g,h}等都是它的字串。\n 这个问题说明白后，最长公共子序列（以下都简称LCS）就很好理解了。\n \n 根据以上条件可知状态转移方程为： \n \n \n 最长公共子串{a,d} \n 构建c[i][j]表需要O(mn) \n suffix_search采用二分查找 （O(nlogn)） \n **步骤三、**合并压缩成patch包 \n 将diff string 和extrastring 以及相应的控制字用zip压缩成一个patch包。 \n \n 可以看出在用zip压缩之前的patch包是没有节约任何字符的，但diff strings可以被高效的压缩，故BSDiff是一个很依赖于压缩与解压的算法！ \n  BSPatch基本步骤 \n 客户端合成patch的基本步骤如下： \n 1．接收patch包； \n 2．解压patch包； \n 3．还原new文件。 \n 三个步骤同时在O（m）时间内完成，但在时间常数上更依赖于解压patch包的部分，m为新文件的长度 \n 复杂度分析 \n 根据以上步骤，不难得出BSDiff与BSPatch的时间与空间复杂度如下： \n BSDiff \n 时间复杂度 O(nlogn) 空间复杂度 O(n) \n BSPatch \n 时间复杂度 O(n+m)  空间复杂度 O(n+m) \n 进一步优化方案： \n 原生bsdiff方案缺陷与改进 \n 1.排序算法替换成divsufsort算法 \n 原生bsdiff方案使得压缩比问题得到解决，但在车载导航自更新中还存在下面两个缺陷： \n \n 内存消耗大，整个过程会占用10~35MB左右的内存。 \n 耗时长，整个包更新时间在3分钟左右。 \n \n 在对bsdiff的优化探索中我们发现Chromium开源项目中存在一份基于bsdiff的优化版本。该版本将bsidff的默认sufsort算法替换成了divsufsort算法，在Patch时间上有了较大的提升。 \n \n \n 2.压缩方式上面做文章 \n Patch过程的主要性能瓶颈在于Bzip2的解压算法中，即使调整Bzip2参数也无法减少本身的计算量。 \n bsdiff差分算法的一个特性就是差分出的Patch数据包含了大量连续的01冗余数据，而Bzip2算法的优点就是对这类数据可以做到高度的压缩，这也是bsdiff压缩比高的原因。不过现在是目前的瓶颈。 \n 此外，我们会制作软件整体的压缩差分包(即生成tar.bz2或zip格式文件)，也就是说针对每个Bzip2压缩后的差分文件还要再经过一次压缩归档。这也意味着在客户段要进行两次的解压。 \n 替换压缩算法 \n 类似的冗余压缩算法有RLE(Run-length encoding)，这个算法也是Bzip2算法的第一步。简单来说RLE算法就是针对连续多个冗余字节去掉其冗余字节，仅保留冗余的长度信息。这个算法相对更简单。 \n 因此，我们将Bzip2压缩算法替换成了RLE算法，实际结果发现生成的Patch文件很大, 压缩比很低。但是可以通过再次压缩归档制作一次差分包，就可以达到和Bzip2几乎相同的压缩比效果。唯一的不足就是在客户端解压后会占用多一些磁盘空间, 而这个代价相对廉价多了。 \n 使用方法 \n 1.直接调用java的实现jbsdiff方式 \n < dependency > \n   < groupId > io.sigpipe </ groupId > \n   < artifactId > jbsdiff </ artifactId > \n   < version > 1.0 </ version > \n </ dependency > \n \n 1 2 3 4 5 ①命令行的方式： \n #将项目打成jar包 执行如下命令： \n java   -jar  jbsdiff.jar diff/patch  oldfile newfile patchfile\n #支持指定的压缩算法（但算法仅仅是来自Apache Commons Compress 库），bzip2（默认值）、gz、pack200 和 xz。 \n java   -Djbsdiff.compressor = gz  -jar  jbsdiff.jar  diff  a.bin b.bin patch.gz\n \n 1 2 3 4 ②当成一个library使用： \n //调用FileUI类 \n //生成差分包： \n FileUI . diff ( oldFile ,  newFile ,  patchFile ,  compression ) ; \n //diff(oldFile, newFile, patchFile, CompressorStreamFactory.BZIP2);//压缩默认bzip2 \n //还原： \n FileUI . patch ( oldFile ,  newFile ,  patchFile ) ; \n \n 1 2 3 4 5 6 命令行的图解使用过程 \n \n \n \n 导入library调用图解 \n \n import   io . sigpipe . jbsdiff . InvalidHeaderException ; \n import   io . sigpipe . jbsdiff . ui . FileUI ; \n import   org . apache . commons . compress . compressors . CompressorException ; \n\n import   java . io . File ; \n import   java . io . IOException ; \n\n public   class   BsDiffTest   { \n\n     public   static   void   main ( String [ ]  args )   throws   IOException ,   InvalidHeaderException ,   CompressorException   { \n         File  oldFile  =   new   File ( "E:\\\\BigData\\\\learning\\\\bsdiff\\\\1.zip" ) ; \n         File  newFile  =   new   File ( "E:\\\\BigData\\\\learning\\\\bsdiff\\\\2.zip" ) ; \n         File  patchFile_bzip2  =   new   File ( "E:\\\\BigData\\\\learning\\\\bsdiff\\\\delta_bzip2" ) ; \n         File  patchFile_gz  =   new   File ( "E:\\\\BigData\\\\learning\\\\bsdiff\\\\delta_gz" ) ; \n         File  outFile = new   File ( "E:\\\\BigData\\\\learning\\\\bsdiff\\\\out.zip" ) ; \n         String  compression_bzip2 = "bzip2" ; \n         String  compression_gz = "gz" ; \n         FileUI . diff ( oldFile ,  newFile ,  patchFile_bzip2 ,  compression_bzip2 ) ; \n         FileUI . diff ( oldFile ,  newFile ,  patchFile_gz ,  compression_gz ) ; \n         FileUI . patch ( oldFile ,  outFile ,  patchFile_gz ) ; \n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2.JNI调用C语言的实现方式（不建议花时间自己实现） \n 运行的环境可能是在window，mac或者linux等。所以，我们需要把bsdiff的源码编译成对应环境需要的 native 库文件，以便于给Java调用。 \n由于大多数项目基本都是在linux上运行的，这里就演示下如何把bsdiff源码编译成linux环境的so文件，然后在java代码中调用。 \n bsdiff 官网介绍的时候提到bsdiff用到了 bzip2 。\n\n所以我们需要下载bsdiff和bzip2 源码。\n\nbsdiff: http://www.daemonology.net/bsdiff/bsdiff-4.3.tar.gz(被禁用了，用最底下广大网友提供的资源）\nbzip2: https://www.sourceware.org/bzip2/downloads.html\n\n\n \n 1 2 3 4 5 6 7 8 #  扩展：差分数组 \n 前缀和 主要适用的场景是 原始数组不会被修改 的情况下，频繁 查询 某个区间的累加和。 \n 这里简单介绍一下前缀和，核心代码就是下面这段： \n class   PrefixSum   { \n   // 前缀和数组 \n   private   int [ ]  prefix ; \n   // 输入一个数组，构造前缀和  \n   public   PrefixSum ( int [ ]  nums )   { \n    prefix  =   new   int [ nums . length  +   1 ] ; \n     // 计算 nums 的累加和 \n     for   ( int  i  =   1 ;  i  <  prefix . length ;  i ++ )   { \n      prefix [ i ]   =  prefix [ i  -   1 ]   +  nums [ i  -   1 ] ; \n     } \n   } \n   // 查询闭区间 [i, j] 的累加和  \n   public   int   query ( int  i ,   int  j )   { \n     return  prefix [ j  +   1 ]   -  prefix [ i ] ; \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n prefix[i] 就代表着 nums[0..i-1] 所有元素的累加和，如果我们想求区间 nums[i..j] 的累加和，只要计算 prefix[j+1] - prefix[i] 即可，而不需要遍历整个区间求和。 \n 本文讲一个和前缀和思想非常类似的算法技巧「差分数组」， 差分数组 的主要适用场景是 频繁对原始数组 的 某个区间的元素进行增减 。 \n 比如说，我给你输入一个数组 nums ，然后又要求给区间 nums[2..6] 全部加 1，再给 nums[3..9] 全部减 3，再给 nums[0..4] 全部加 2，再给… \n 一通操作猛如虎，然后问你，最后 nums 数组的值是什么？ \n 常规的思路很容易，你让我给区间 nums[i..j] 加上 val ，那我就一个  for 循环 给它们都加上呗，还能咋样？这种思路的 时间复杂度是 O(N) ，由于这个场景下对 nums 的修改非常频繁，所以效率会很低下。 \n 这里就需要差分数组的技巧，类似前缀和技巧构造的 prefix 数组，我们先对 nums 数组构造一个 diff 差分数组， diff[i] 就是 nums[i] 和 nums[i-1] 之差 ： \n int [ ]  diff  =   new   int [ nums . length ] ; \n // 构造差分数组 \ndiff [ 0 ]   =  nums [ 0 ] ; \n for   ( int  i  =   1 ;  i  <  nums . length ;  i ++ )   { \n    diff [ i ]   =  nums [ i ]   -  nums [ i  -   1 ] ; \n } \n \n 1 2 3 4 5 6 \n 通过这个 diff 差分数组是可以反推出原始数组 nums 的，代码逻辑如下： \n int [ ]  res  =   new   int [ diff . length ] ; \n // 根据差分数组构造结果数组 \nres [ 0 ]   =  diff [ 0 ] ; \n for   ( int  i  =   1 ;  i  <  diff . length ;  i ++ )   { \n    res [ i ]   =  res [ i  -   1 ]   +  diff [ i ] ; \n } \n \n 1 2 3 4 5 6 这样构造差分数组 diff ，就可以快速进行区间增减的操作 ，如果你想对区间 nums[i..j] 的元素全部加 3，那么只需要让 diff[i] += 3 ，然后再让 diff[j+1] -= 3 即可： \n \n 原理很简单，回想 diff 数组反推 nums 数组的过程， diff[i] += 3 意味着给 nums[i..] 所有的元素都加了 3，然后 diff[j+1] -= 3 又意味着对于 nums[j+1..] 所有元素再减 3，那综合起来，是不是就是对 nums[i..j] 中的所有元素都加 3 了 ？ \n 只要花费  O(1) 的时间 修改 diff 数组，就相当于给 nums 的整个区间做了修改。多次修改 diff ，然后通过 diff 数组反推，即可得到 nums 修改后的结果。 \n 现在我们把差分数组抽象成一个类，包含 increment 方法和 result 方法： \n // 差分数组工具类 \n class   Difference   { \n     // 差分数组 \n     private   int [ ]  diff ; \n\n     /* 输入一个初始数组，区间操作将在这个数组上进行 */ \n     public   Difference ( int [ ]  nums )   { \n         assert  nums . length  >   0 ; \n        diff  =   new   int [ nums . length ] ; \n         // 根据初始数组构造差分数组 \n        diff [ 0 ]   =  nums [ 0 ] ; \n         for   ( int  i  =   1 ;  i  <  nums . length ;  i ++ )   { \n            diff [ i ]   =  nums [ i ]   -  nums [ i  -   1 ] ; \n         } \n     } \n\n     /* 给闭区间 [i,j] 增加 val（可以是负数）*/ \n     public   void   increment ( int  i ,   int  j ,   int  val )   { \n        diff [ i ]   +=  val ; \n         if   ( j  +   1   <  diff . length )   { \n            diff [ j  +   1 ]   -=  val ; \n         } \n     } \n\n     /* 返回结果数组 */ \n     public   int [ ]   result ( )   { \n         int [ ]  res  =   new   int [ diff . length ] ; \n         // 根据差分数组构造结果数组 \n        res [ 0 ]   =  diff [ 0 ] ; \n         for   ( int  i  =   1 ;  i  <  diff . length ;  i ++ )   { \n            res [ i ]   =  res [ i  -   1 ]   +  diff [ i ] ; \n         } \n         return  res ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 这里注意一下 increment 方法中的 if 语句： \n public   void   increment ( int  i ,   int  j ,   int  val )   { \n    diff [ i ]   +=  val ; \n     if   ( j  +   1   <  diff . length )   { \n        diff [ j  +   1 ]   -=  val ; \n     } \n } \n \n 1 2 3 4 5 6 当 j+1 >= diff.length 时，说明是对 nums[i] 及以后的整个数组都进行修改，那么就不需要再给 diff 数组减 val 了。 \n 应用1：航班预订 \n \n int [ ]   corpFlightBookings ( int [ ] [ ]  bookings ,   int  n )   { \n     // nums 初始化为全 0 \n     int [ ]  nums  =   new   int [ n ] ; \n     // 构造差分解法 \n     Difference  df  =   new   Difference ( nums ) ; \n\n     for   ( int [ ]  booking  :  bookings )   { \n         // 注意转成数组索引要减一哦 \n         int  i  =  booking [ 0 ]   -   1 ; \n         int  j  =  booking [ 1 ]   -   1 ; \n         int  val  =  booking [ 2 ] ; \n         // 对区间 nums[i..j] 增加 val \n        df . increment ( i ,  j ,  val ) ; \n     } \n     // 返回最终的结果数组 \n     return  df . result ( ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 动态规划--最长公共子序列（不连续） \n 给定序列s1={1,3,4,5,6,7,7,8},s2={3,5,7,4,8,6,7,8,2}，s1和s2的相同子序列，且该子序列的长度最长，即是LCS。 \ns1和s2的其中一个最长公共子序列是 {3,4,6,7,8} \n 求解步骤①递归构建二阶矩阵，计算LCS长度 \n 递归公式 \n \n \n 求解步骤②，找出最长子序列的其中一种组成。动态规划的可能有多个可行解，返回一种可能的子序列 \n \n 这就是倒推回去的路径，棕色方格为相等元素，即LCS = {3,4,6,7,8}，这是其中一个结果。 \n \n 另一种结果，即LCS ={3,5,7,7,8}。 \n 时间复杂度 \n ​    构建c[i][j]表需要O(mn)，输出1个LCS的序列需要O(m+n)。 \n 后缀数组之倍增法（n * logn*logn）： \n 首先用计数排序的方法对单个字符进行排序，得到按单字符进行排序后的rank[]，以后的排序就是以此数组代表字符进行排序。 \n ａａｂａａａａｂａ \n １１２１１１１２１（rank[]） \n 单个字符很有可能是有重复的，所以要比较第二个字符。但是第二个字符的大小已经比较过了(最后一个字符开始的串没有第二个字符，所以补0)。即 \n ａｂａａａａｂａ0 \n １２１１１１２１0 \n 这样就以第二个字符的大小为第二关键字，第一个字符的大小为第一关键字进行基数排序。得到以两个字符进行排序后的rank[]。 \n 同样，我们可以用后面已经算好的两个字符的大小算出按4个字符排序的顺序。然后是8个、16个……。直到字符串的长度。 \n 1 将字符串 s(aabaaaab)从每个下标开始长度为 1 的子串进行排名，直接将每个字符转换成 s[i]-\'a\'+1即可，如下图所示。 \n \n 2 求解长度为 2 的子串排名。 \n 将上一次 rank 值的第 i 个和 第 i+1 个结合，相对于得到长度为 2 的子串的每个位置排名，然后排序，即可得到长度为 2 的子串排名。 \n \n 3 求解长度为 2^2 的子串排名。 \n 将上一次 rank 值的第 i 个和第 i+2 个结合，相对于得到长度为 2^2 的子串的每个位置排名，排序后得到长度为 2^2 的子串排名。 \n \n 4 求解长度为 2^3 的子串排名。 \n 将上一次 rank 值的第 i 个和第 i+4 个结合，相对于得到长度为 2^3 的子串的每个位置排名，排序后得到长度为 2^3 的子串排名。 \n \n 第 4 步和第 3 步的结果一样， 实际上，若在 rank 没有相同值时，就已经得到了后缀排名，就不需要再继续运算了 。因为根据字符串比较规则，两个字符串的前几个字符已经分出大小，后面无须判断。 \n 将排名数组转换为后缀数组，排名第1的下标为3，排名第2的下标为4，排名第3的下标为5，排名第4的下标为0，排名第5的下标为6，排名为6的下标为1，排名第7的下标为7，排名第8的下标为2，因此SA[]={3,4,5,0,6,1,7,2}。 \n public   abstract   class   SuffixArray   { \n\n   // Length of the suffix array \n   protected   final   int   N ; \n\n   // T is the text \n   protected   int [ ]   T ; \n\n   // The sorted suffix array values. \n   protected   int [ ]  sa ; \n\n   // Longest Common Prefix array \n   protected   int [ ]  lcp ; \n\n   private   boolean  constructedSa  =   false ; \n   private   boolean  constructedLcpArray  =   false ; \n\n   public   SuffixArray ( int [ ]  text )   { \n     if   ( text  ==   null )   throw   new   IllegalArgumentException ( "Text cannot be null." ) ; \n     this . T   =  text ; \n     this . N   =  text . length ; \n   } \n\n   public   int   getTextLength ( )   { \n     return   T . length ; \n   } \n\n   // Returns the suffix array. \n   public   int [ ]   getSa ( )   { \n     buildSuffixArray ( ) ; \n     return  sa ; \n   } \n\n   // Returns the LCP array. \n   public   int [ ]   getLcpArray ( )   { \n     buildLcpArray ( ) ; \n     return  lcp ; \n   } \n\n   // Builds the suffix array by calling the construct() method. \n   protected   void   buildSuffixArray ( )   { \n     if   ( constructedSa )   return ; \n     construct ( ) ; \n    constructedSa  =   true ; \n   } \n\n   // Builds the LCP array by first creating the SA and then running the kasai algorithm. \n   protected   void   buildLcpArray ( )   { \n     if   ( constructedLcpArray )   return ; \n     buildSuffixArray ( ) ; \n     kasai ( ) ; \n    constructedLcpArray  =   true ; \n   } \n\n   protected   static   int [ ]   toIntArray ( String  s )   { \n     if   ( s  ==   null )   return   null ; \n     int [ ]  t  =   new   int [ s . length ( ) ] ; \n     for   ( int  i  =   0 ;  i  <  s . length ( ) ;  i ++ )  t [ i ]   =  s . charAt ( i ) ; \n     return  t ; \n   } \n\n   // The suffix array construction algorithm is left undefined \n   // as there are multiple ways to do this. \n   protected   abstract   void   construct ( ) ; \n\n   // Use Kasai algorithm to build LCP array \n   // http://www.mi.fu-berlin.de/wiki/pub/ABI/RnaSeqP4/suffix-array.pdf \n   private   void   kasai ( )   { \n    lcp  =   new   int [ N ] ; \n     int [ ]  inv  =   new   int [ N ] ; \n     for   ( int  i  =   0 ;  i  <   N ;  i ++ )  inv [ sa [ i ] ]   =  i ; \n     for   ( int  i  =   0 ,  len  =   0 ;  i  <   N ;  i ++ )   { \n       if   ( inv [ i ]   >   0 )   { \n         int  k  =  sa [ inv [ i ]   -   1 ] ; \n         while   ( ( i  +  len  <   N )   &&   ( k  +  len  <   N )   &&   T [ i  +  len ]   ==   T [ k  +  len ] )  len ++ ; \n        lcp [ inv [ i ] ]   =  len ; \n         if   ( len  >   0 )  len -- ; \n       } \n     } \n   } \n\n } \n\n public   class   SuffixArrayMed   extends   SuffixArray   { \n\n   // Wrapper class to help sort suffix ranks \n   static   class   SuffixRankTuple   implements   Comparable < SuffixRankTuple >   { \n\n     int  firstHalf ,  secondHalf ,  originalIndex ; \n\n     // Sort Suffix ranks first on the first half then the second half \n     @Override \n     public   int   compareTo ( SuffixRankTuple  other )   { \n       int  cmp  =   Integer . compare ( firstHalf ,  other . firstHalf ) ; \n       if   ( cmp  ==   0 )   return   Integer . compare ( secondHalf ,  other . secondHalf ) ; \n       return  cmp ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n       return  originalIndex  +   " -> ("   +  firstHalf  +   ", "   +  secondHalf  +   ")" ; \n     } \n   } \n\n   public   SuffixArrayMed ( String  text )   { \n     super ( toIntArray ( text ) ) ; \n   } \n\n   public   SuffixArrayMed ( int [ ]  text )   { \n     super ( text ) ; \n   } \n\n   // Construct a suffix array in O(nlog^2(n)) \n   @Override \n   protected   void   construct ( )   { \n    sa  =   new   int [ N ] ; \n\n     // Maintain suffix ranks in both a matrix with two rows containing the \n     // current and last rank information as well as some sortable rank objects \n     int [ ] [ ]  suffixRanks  =   new   int [ 2 ] [ N ] ; \n     SuffixRankTuple [ ]  ranks  =   new   SuffixRankTuple [ N ] ; \n\n     // Assign a numerical value to each character in the text \n     for   ( int  i  =   0 ;  i  <   N ;  i ++ )   { \n      suffixRanks [ 0 ] [ i ]   =   T [ i ] ; \n      ranks [ i ]   =   new   SuffixRankTuple ( ) ; \n     } \n\n     // O(log(n)) \n     for   ( int  pos  =   1 ;  pos  <   N ;  pos  *=   2 )   { \n\n       for   ( int  i  =   0 ;  i  <   N ;  i ++ )   { \n         SuffixRankTuple  suffixRank  =  ranks [ i ] ; \n        suffixRank . firstHalf  =  suffixRanks [ 0 ] [ i ] ; \n        suffixRank . secondHalf  =  i  +  pos  <   N   ?  suffixRanks [ 0 ] [ i  +  pos ]   :   - 1 ; \n        suffixRank . originalIndex  =  i ; \n       } \n\n       // O(nlog(n)) \n       java . util . Arrays . sort ( ranks ) ; \n\n       int  newRank  =   0 ; \n      suffixRanks [ 1 ] [ ranks [ 0 ] . originalIndex ]   =   0 ; \n\n       for   ( int  i  =   1 ;  i  <   N ;  i ++ )   { \n\n         SuffixRankTuple  lastSuffixRank  =  ranks [ i  -   1 ] ; \n         SuffixRankTuple  currSuffixRank  =  ranks [ i ] ; \n\n         // If the first half differs from the second half \n         if   ( currSuffixRank . firstHalf  !=  lastSuffixRank . firstHalf\n             ||  currSuffixRank . secondHalf  !=  lastSuffixRank . secondHalf )  newRank ++ ; \n\n        suffixRanks [ 1 ] [ currSuffixRank . originalIndex ]   =  newRank ; \n       } \n\n       // Place top row (current row) to be the last row \n      suffixRanks [ 0 ]   =  suffixRanks [ 1 ] ; \n\n       // Optimization to stop early \n       if   ( newRank  ==   N   -   1 )   break ; \n     } \n\n     // Fill suffix array \n     for   ( int  i  =   0 ;  i  <   N ;  i ++ )   { \n      sa [ i ]   =  ranks [ i ] . originalIndex ; \n      ranks [ i ]   =   null ; \n     } \n\n     // Cleanup \n    suffixRanks [ 0 ]   =  suffixRanks [ 1 ]   =   null ; \n    suffixRanks  =   null ; \n    ranks  =   null ; \n   } \n\n   public   static   void   main ( String [ ]  args )   { \n\n     // String[] strs = { "AAGAAGC", "AGAAGT", "CGAAGC" }; \n     // String[] strs = { "abca", "bcad", "daca" }; \n     // String[] strs = { "abca", "bcad", "daca" }; \n     // String[] strs = { "AABC", "BCDC", "BCDE", "CDED" }; \n     // String[] strs = { "abcdefg", "bcdefgh", "cdefghi" }; \n     // String[] strs = { "xxx", "yyy", "zzz" }; \n     // TreeSet <String> lcss = SuffixArrayMed.lcs(strs, 2); \n     // System.out.println(lcss); \n\n     // SuffixArrayMed sa = new SuffixArrayMed("abracadabra"); \n     // System.out.println(sa); \n     // System.out.println(java.util.Arrays.toString(sa.sa)); \n     // System.out.println(java.util.Arrays.toString(sa.lcp)); \n\n     SuffixArrayMed  sa  =   new   SuffixArrayMed ( "ABBABAABAA" ) ; \n     // SuffixArrayMed sa = new SuffixArrayMed("GAGAGAGAGAGAG"); \n     System . out . println ( sa ) ; \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 后缀排序之DC3（n）--基数排序，据多方测试dc3 \n 后缀数组构造之SA-IS（n） \n 步骤： \n ①增加特殊尾字符(取一个比数字和字母小的字符比如*，$，！)； \n ②确定后缀类型（尾字符规定为S类型，其他的倒序遍历i<（i+1）则为S,大于则为L，相同与（i+1）类型一样 )； \n \n ③找出LMS子串基数排序，如果子串相同，需要递归①到③，直到LMS子串能排序； \n ④遍历桶数组，添加元素前一个字符的L型后缀（尾插：遍历桶数组时从小到大的，所以尾插能保证顺序），非L型的不添加； \n \n ⑤去除LMS子串； \n \n ⑥倒序头插S型后缀（倒序遍历从大到小，头插保证小的在前）； \n \n ⑦取得后缀数组。 \n LSM子串需要进一步判断的情况： \n \n class   SAIS   { \n\t static   char  saisyo  =   \'!\' ; \n\n\t public   static   int [ ]   SAISENGINE ( String  s )   { \n\t\t int [ ]  str  =   new   int [ s . length ( ) ] ; \n\t\t for   ( int  i  =   0 ;  i  <  s . length ( ) ;  i ++ )   { \n\t\t\tstr [ i ]   =  s . charAt ( i )   -  saisyo  +   1 ; \n\t\t } \n\t\t int [ ]  result  =   SAIS ( str ,   127 ) ; \n\t\t return  result ; \n\t } \n\n\t public   static   int [ ]   SAIS ( int [ ]  s ,   int  spe )   { \n\t\t if   ( s . length  ==   1 )   { \n\t\t\t return   new   int [ ]   {   0   } ; \n\t\t }   else   { \n\t\t\t int  len  =  s . length  +   1 ;   //字符数，包括尾随字符 \n\t\t\t int [ ]  str  =   new   int [ len ] ; \n\t\t\t for   ( int  i  =   0 ;  i  <  s . length ;  i ++ )   { \n\t\t\t\tstr [ i ]   =  s [ i ] ;   //收到的字符串数组不包含尾随字符，因此需要创建一个新数组并分配它。 \n\t\t\t } \n\t\t\tstr [ str . length  -   1 ]   =   0 ; \n\t\t\t int [ ]   LS   =   new   int [ len ] ;   //包含 L 型 S 型 L=1 S=0 的数组 \n\t\t\t int [ ]   LMS   =   new   int [ len ] ;   //包含 LMS 是否的数组 LMS 的正整数 -1 如果不是 \n\t\t\t int [ ]   LMSsublen   =   new   int [ len ] ;   //包含 LMS 子字符串长度的数组 \n\t\t\t int [ ]   LMSlist ; // 仅包含 LMS 索引的数组（从原始字符串的长度中不知道 LMS 的数量，因此可变长度数组更容易） \n\t\t\t int   LMScount   =   0 ;   //LMS 的数量（如果您在可变长度数组中只有一个 LMS 的列表，则不必为索引管理而烦恼） \n\t\t\t int [ ]  used  =   new   int [ len ] ;   //是否放入后缀数组的数组 \n\t\t\t Arrays . fill ( used ,   0 ) ;   //初始化 如果您正在使用 1 并且您没有使用它，则输入 0 \n\t\t\t SA [ ]  sa  =   new   SA [ spe ] ;   //保存 SA 放入后缀数组的索引 \n\t\t\t int [ ]  dict  =   new   int [ spe ] ; // 包含出现哪些字符和出现次数的数组 \n\t\t\t for   ( int  i  =   0 ;  i  <  len ;  i ++ )   { \n\t\t\t\tdict [ str [ i ] ] ++ ; \n\t\t\t } \n\t\t\t for   ( int  i  =   0 ;  i  <  spe ;  i ++ )   { \n\t\t\t\tsa [ i ]   =   new   SA ( dict [ i ] ) ; // 初始化后缀数组 \n\t\t\t } \n\t\t\t Arrays . fill ( LMS ,   - 1 ) ; \n\t\t\t Arrays . fill ( LMSsublen ,   0 ) ; \n\t\t\t LS [ len  -   1 ]   =   0 ;   //L型或S型计算 \n\t\t\t for   ( int  i  =  len  -   2 ;  i  >=   0 ;  i -- )   {   //从后面看 \n\t\t\t\t if   ( str [ i ]   ==  str [ i  +   1 ] )   { \n\t\t\t\t\t LS [ i ]   =   LS [ i  +   1 ] ;   //如果两个相邻字符相同，则后面的字符取决于 \n\t\t\t\t }   else   if   ( str [ i ]   >  str [ i  +   1 ] )   { \n\t\t\t\t\t LS [ i ]   =   1 ;   // L \n\t\t\t\t }   else   if   ( str [ i ]   <  str [ i  +   1 ] )   { \n\t\t\t\t\t LS [ i ]   =   0 ;   // S \n\t\t\t\t } \n\t\t\t } \n\t\t\t for   ( int  i  =   1 ;  i  <  len ;  i ++ )   {   //探索您的学习管理系统 \n\t\t\t\t if   ( LS [ i ]   ==   0   &&   LS [ i  -   1 ]   ==   1 )   { \n\t\t\t\t\t LMS [ i ]   =   1 ;   // LMS \n\t\t\t\t\t LMScount ++ ;   //计算 LMS 的数量 \n\t\t\t\t }   else   { \n\t\t\t\t\t LMS [ i ]   =   - 1 ;   //当不是 LMS-1 时 \n\t\t\t\t } \n\t\t\t } \n\t\t\t LMS [ 0 ]   =   - 1 ;   //第一个字符不会成为 LMS \n\t\t\t LMSlist   =   new   int [ LMScount ] ; // 仅 LMS 阵列 \n\t\t\t int  tmpindex  =   0 ; \n\t\t\t for   ( int  i  =   0 ;  i  <  len ;  i ++ )   { \n\t\t\t\t if   ( LMS [ i ]   ==   1 )   { \n\t\t\t\t\t LMSlist [ tmpindex ]   =  i ;   //创建仅包含 LMS 索引的数组 \n\t\t\t\t\ttmpindex ++ ; \n\t\t\t\t } \n\t\t\t } \n\t\t\t //验证到 Coco 5/13 标志 1 号 \n\t\t\tsa  =   resetindex ( sa ) ; \n\t\t\t for   ( int  i  =   0 ;  i  <   LMSlist . length ;  i ++ )   {   //计算 LMS 子字符串的长度 \n\t\t\t\t if   ( LMSlist [ i ]   !=  len  -   1 )   { \n\t\t\t\t\t LMSsublen [ LMSlist [ i ] ]   =   LMSlist [ i  +   1 ]   -   LMSlist [ i ] ;   //LMS 和 LMS 之间的长度是 LMS 子字符串的长度 \n\t\t\t\t }   else   { \n\t\t\t\t\t LMSsublen [ LMSlist [ i ] ]   =   1 ; // 最后一个 LMS 必须仅包含最后一个字符，其长度为 1。 \n\t\t\t\t } \n\t\t\t } \n\t\t\tsa  =   resetindex ( sa ) ; //4 步结束 \n\t\t\t for   ( int  i  =   0 ;  i  <   LMSlist . length ;  i ++ )   {   //第 1 步：添加学习管理系统 \n\t\t\t\t int  index  =   LMSlist [ i ] ; \n\t\t\t\tsa [ str [ index ] ] . addusirokara ( index ) ;   //从后面看时，继续在末尾添加值 = 从正面看并从后到前放置 \n\t\t\t\tused [ index ]   =   1 ;   //我把它放在后缀数组中，所以我把它标记为已使用 \n\t\t\t }   //步骤 1 结束 \n\t\t\tsa  =   resetindex ( sa ) ;   //重新计算数组索引 \n\n\t\t\t for   ( int  i  =   0 ;  i  <  sa . length ;  i ++ )   {   //第 2 步：添加 L 形字母 \n\t\t\t\t for   ( int  j  =   0 ;  j  <  sa [ i ] . size ( ) ;  j ++ )   { \n\t\t\t\t\t int  value  =  sa [ i ] . get ( j )   -   1 ; \n\t\t\t\t\t if   ( value  >=   0   &&  used [ value ]   ==   0   &&   LS [ value ]   ==   1 )   { \n\t\t\t\t\t\tused [ value ]   =   1 ; \n\t\t\t\t\t\tsa [ str [ value ] ] . addmaekara ( value ) ;   //从前到后放置值 \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t } \n\t\t\tsa  =   resetindex ( sa ) ; //3 步结束 \n\t\t\t for   ( int  i  =   1 ;  i  <  sa . length ;  i ++ )   {   //3 个步骤 \n\t\t\t\t for   ( int  j  =   0 ;  j  <  sa [ i ] . size ( ) ;  j ++ )   { \n\t\t\t\t\t int  value  =  sa [ i ] . get ( j ) ; \n\t\t\t\t\t if   ( value  >=   0   &&   LMS [ value ]   ==   1 )   { \n\t\t\t\t\t\tsa [ i ] . set ( j ,   - 1 ) ;   //从后缀数组中删除 LMS \n\t\t\t\t\t\tused [ value ]   =   0 ;   //我从后缀数组中获取了LMS，因此它变得未使用 \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t }   //3 步结束 \n\t\t\tsa  =   resetindex ( sa ) ; //4 步结束 \n\t\t\t for   ( int  i  =  sa . length  -   1 ;  i  >=   0 ;  i -- )   { // 4 个步骤 \n\t\t\t\t for   ( int  j  =  sa [ i ] . size ( )   -   1 ;  j  >=   0 ;  j -- )   { \n\t\t\t\t\t if   ( sa [ i ] . get ( j )   !=   - 1 )   { \n\t\t\t\t\t\t int  index  =  sa [ i ] . get ( j )   -   1 ; \n\t\t\t\t\t\t if   ( index  >=   0   &&  used [ index ]   ==   0   &&   LS [ index ]   ==   0 )   { \n\t\t\t\t\t\t\tsa [ str [ index ] ] . addusirokara ( index ) ;   //从后到前放置值 \n\t\t\t\t\t\t\tused [ index ]   =   1 ; \n\t\t\t\t\t\t } \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t } \n\t\t\tsa  =   resetindex ( sa ) ; //4 步结束 \n\t\t\t int  counter  =   0 ; \n\t\t\t int [ ]  nowsa  =   new   int [ 1 ] ; // 用于放置您现在正在查看的 LMS 子字符串的数组 \n\t\t\t int [ ]  oldsa  =   new   int [ 1 ] ;   //包含上一个 LMS 子字符串的数组 \n\t\t\t Arrays . fill ( oldsa ,   - 1 ) ; \n\t\t\t int [ ]   LMScounter   =   new   int [ len ] ; \n\t\t\t for   ( int  i  =   0 ;  i  <  sa . length ;  i ++ )   {   //首先，不要搞错这里 \n\t\t\t\t for   ( int  j  =   0 ;  j  <  sa [ i ] . size ( ) ;  j ++ )   { \n\t\t\t\t\t if   ( LMS [ sa [ i ] . get ( j ) ]   !=   - 1 )   { // 如果是 LMS，则 LMS 子字符串从那里开始 \n\t\t\t\t\t\tcounter ++ ; \n\t\t\t\t\t\t LMScounter [ sa [ i ] . get ( j ) ]   =  counter ; \n\t\t\t\t\t\tnowsa  =   new   int [ LMSsublen [ sa [ i ] . get ( j ) ]   +   1 ] ; // LMS 索引开始告知 LMS 子字符串的长度 \n\t\t\t\t\t\t Arrays . fill ( nowsa ,   - 1 ) ; \n\t\t\t\t\t\ttmpindex  =   0 ;   //LMS 子字符串索引 \n\t\t\t\t\t\t for   ( int  k  =  sa [ i ] . get ( j ) ;  k  <  len ;  k ++ )   { \n\t\t\t\t\t\t\tnowsa [ tmpindex ]   =  str [ k ] ; \n\t\t\t\t\t\t\ttmpindex ++ ; \n\t\t\t\t\t\t\t if   ( k  !=  sa [ i ] . get ( j )   &&   LMS [ k ]   !=   - 1 )   { \n\t\t\t\t\t\t\t\t break ;   //当下一个 LMS 到来时，LMS 子字符串已完成 \n\t\t\t\t\t\t\t } \n\t\t\t\t\t\t } \n\t\t\t\t\t\t if   ( equals ( oldsa ,  nowsa ) /* equals(oldsa, nowsa) */ )   {   //如果您现在查看的 LMS 子字符串与上一个子字符串相同，请为同一子字符串分配相同的数字 \n\t\t\t\t\t\t\tcounter -- ; \n\t\t\t\t\t\t\t LMScounter [ sa [ i ] . get ( j ) ]   =  counter ; \n\t\t\t\t\t\t } \n\t\t\t\t\t\toldsa  =   clone ( nowsa ) ; \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t } \n\t\t\t int [ ]  new_str  =   new   int [ LMSlist . length ] ; \n\t\t\t int [ ]   LMSindex   =   new   int [ LMSlist . length  +   1 ] ; \n\n\t\t\t for   ( int  i  =   0 ;  i  <   LMSlist . length ;  i ++ )   {   //使用递归 SAIS 查找 LMS 子字符串的词典顺序 \n\t\t\t\tnew_str [ i ]   =   LMScounter [ LMSlist [ i ] ] ; \n\t\t\t\t LMSindex [ i ]   =   LMSlist [ i ] ; \n\t\t\t } \n\t\t\t int [ ]  newLMSindex  =   SAIS ( new_str ,  counter  +   1 ) ; \n\n\t\t\tsa  =   resetindex ( sa ) ; \n\t\t\t Arrays . fill ( used ,   0 ) ; // 返回所有未使用 \n\t\t\tsa  =   clear ( sa ) ; // 现在我们知道了新 LMS 子字符串的词典顺序，初始化后缀数组以将其重新放入 \n\n\t\t\t for   ( int  i  =  newLMSindex . length  -   1 ;  i  >=   0 ;  i -- )   {   // 新1段階目 \n\t\t\t\t int  value  =   LMSindex [ newLMSindex [ i ] ] ; \n\t\t\t\tsa [ str [ value ] ] . addusirokara ( value ) ; \n\t\t\t\tused [ value ]   =   1 ; \n\t\t\t } \n\t\t\tsa  =   resetindex ( sa ) ; \n\t\t\t //2 步 从现在开始，与上述 3 步和 4 步相同 \n\t\t\t for   ( int  i  =   0 ;  i  <  sa . length ;  i ++ )   { \n\t\t\t\t for   ( int  j  =   0 ;  j  <  sa [ i ] . size ( ) ;  j ++ )   { \n\t\t\t\t\t int  value  =  sa [ i ] . get ( j )   -   1 ; \n\t\t\t\t\t if   ( value  >=   0   &&  used [ value ]   ==   0   &&   LS [ value ]   ==   1 )   { \n\t\t\t\t\t\tused [ value ]   =   1 ; \n\t\t\t\t\t\tsa [ str [ value ] ] . addmaekara ( value ) ; \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t } \n\t\t\tsa  =   resetindex ( sa ) ; //2 个步骤结束 \n\t\t\t for   ( int  i  =   1 ;  i  <  sa . length ;  i ++ )   { // 2.5 步骤 \n\t\t\t\t for   ( int  j  =   0 ;  j  <  sa [ i ] . size ( ) ;  j ++ )   { \n\t\t\t\t\t int  value  =  sa [ i ] . get ( j ) ; \n\t\t\t\t\t if   ( value  >=   0   &&   LMS [ value ]   >=   1 )   { \n\t\t\t\t\t\tsa [ i ] . set ( j ,   - 1 ) ;   //返回 \n\t\t\t\t\t\tused [ value ]   =   0 ; \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t }   //2.5 步骤结束 \n\t\t\t //3 个步骤 \n\t\t\t for   ( int  i  =  sa . length  -   1 ;  i  >=   0 ;  i -- )   { \n\t\t\t\t for   ( int  j  =  sa [ i ] . size ( )   -   1 ;  j  >=   0 ;  j -- )   { \n\t\t\t\t\t if   ( sa [ i ] . get ( j )   !=   - 1 )   { \n\t\t\t\t\t\t int  index  =  sa [ i ] . get ( j )   -   1 ; \n\t\t\t\t\t\t if   ( index  >=   0   &&  used [ index ]   ==   0   &&   LS [ index ]   ==   0 )   { \n\t\t\t\t\t\t\tsa [ str [ index ] ] . addusirokara ( index ) ; \n\t\t\t\t\t\t\tused [ index ]   =   1 ; \n\t\t\t\t\t\t } \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t } \n\t\t\tsa  =   resetindex ( sa ) ; //3 个步骤结束 \n\t\t\t int [ ]  ret  =   convert ( sa ,  len  -   1 ) ;   //排除尾随字符索引 \n\t\t\t return  ret ; \n\t\t } \n\t } \n\n\t static   boolean   equals ( int [ ]  a1 ,   int [ ]  a2 )   { \n\t\t if   ( a1 . length  !=  a2 . length )   { \n\t\t\t return   false ; \n\t\t }   else   { \n\t\t\t for   ( int  i  =   0 ;  i  <  a1 . length ;  i ++ )   { \n\t\t\t\t if   ( a1 [ i ]   !=  a2 [ i ] )   { \n\t\t\t\t\t return   false ; \n\t\t\t\t } \n\t\t\t } \n\t\t\t return   true ; \n\t\t } \n\t } \n\n\t static   SA [ ]   resetindex ( SA [ ]  a )   { \n\t\t for   ( int  i  =   0 ;  i  <  a . length ;  i ++ )   { \n\t\t\ta [ i ] . resetindex ( ) ; \n\t\t } \n\t\t return  a ; \n\t } \n\n\t static   SA [ ]   clear ( SA [ ]  a )   { \n\t\t for   ( int  i  =   0 ;  i  <  a . length ;  i ++ )   { \n\t\t\ta [ i ] . reset ( ) ; \n\t\t } \n\t\t return  a ; \n\t } \n\n\t static   class   SA   { \n\t\t private   int [ ]  ary ; \n\t\t private   int  mae ,  ato ; \n\n\t\t SA ( int  volume )   { \n\t\t\tary  =   new   int [ volume ] ; \n\t\t\t Arrays . fill ( ary ,   - 1 ) ; \n\t\t\tmae  =   0 ; \n\t\t\tato  =  ary . length  -   1 ; \n\t\t } \n\n\t\t void   addmaekara ( int  value )   { \n\t\t\t if   ( mae  <  ary . length )   { \n\t\t\t\tary [ mae ]   =  value ; \n\t\t\t\tmae ++ ; \n\t\t\t } \n\t\t } \n\n\t\t void   addusirokara ( int  value )   { \n\t\t\t if   ( ato  >=   0 )   { \n\t\t\t\tary [ ato ]   =  value ; \n\t\t\t\tato -- ; \n\t\t\t } \n\t\t } \n\n\t\t void   resetindex ( )   { \n\t\t\tmae  =   0 ; \n\t\t\tato  =  ary . length  -   1 ; \n\t\t } \n\n\t\t void   set ( int  index ,   int  element )   { \n\t\t\tary [ index ]   =  element ; \n\t\t } \n\n\t\t void   reset ( )   { \n\t\t\t Arrays . fill ( ary ,   - 1 ) ; \n\t\t\t resetindex ( ) ; \n\t\t } \n\n\t\t int   get ( int  index )   { \n\t\t\t return  ary [ index ] ; \n\t\t } \n\n\t\t int   size ( )   { \n\t\t\t return  ary . length ; \n\t\t } \n\t } \n\n\t static   int [ ]   convert ( SA [ ]  s ,   int  len )   { \n\t\t int [ ]  ret  =   new   int [ len ] ; \n\t\t int  counter  =   0 ; \n\t\t for   ( int  i  =   1 ;  i  <  s . length ;  i ++ )   { \n\t\t\t for   ( int  j  =   0 ;  j  <  s [ i ] . size ( ) ;  j ++ )   { \n\t\t\t\tret [ counter ]   =  s [ i ] . get ( j ) ; \n\t\t\t\tcounter ++ ; \n\t\t\t } \n\t\t } \n\t\t return  ret ; \n\t } \n\n\t public   static   int [ ]   clone ( int [ ]  a )   { \n\t\t int [ ]  ret  =   new   int [ a . length ] ; \n\t\t for   ( int  i  =   0 ;  i  <  a . length ;  i ++ )   { \n\t\t\tret [ i ]   =  a [ i ] ; \n\t\t } \n\t\t return  ret ; \n\t } \n\n\t public   static   void   main ( String [ ]  args )   { \n\t\t String  s  =   "aabaaaab" ; \n\t\t int [ ]  res  =   SAISENGINE ( s ) ; \n\t\t for   ( int  i  =   0 ;  i  <  res . length ;  i ++ )   { \n\t\t\t System . out . println ( res [ i ] ) ; \n\t\t } \n\t } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 #  ZSTD差异化更新 \n 据测是 \n'},{title:"常见的序列化方式",frontmatter:{title:"常见的序列化方式",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["其他"],tags:["序列化"]},regularPath:"/%E5%85%B6%E4%BB%96/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F.html",relativePath:"其他/常见的序列化方式.md",key:"v-41ccc7e2",path:"/2023/06/10/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F/",headers:[{level:2,title:"前言",slug:"前言"},{level:3,title:"为什么需要序列化",slug:"为什么需要序列化"},{level:3,title:"什么是序列化和反序列化",slug:"什么是序列化和反序列化"},{level:2,title:"1.Java序列化",slug:"_1-java序列化"},{level:2,title:"2.Hessian 序列化",slug:"_2-hessian-序列化"},{level:2,title:"3.Json序列化",slug:"_3-json序列化"},{level:2,title:"4.ProtoBuf",slug:"_4-protobuf"},{level:2,title:"5.Kryo",slug:"_5-kryo"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' 前言 \n 为什么需要序列化 \n 序列化的本质是为了进行网络数据传输，而数据又只能够以二进制的形式在网络中进行传输，所以我们就需要把对象转为二进制的形式，也就是需要序列化这么一个过程，而因为二进制形式对我们使用者来说是不方便的，所以就需要有一个反序列化的过程，将数据重新还原。 \n 什么是序列化和反序列化 \n \n 序列化：是把对象的状态信息转化为可存储或传输的形式过程，也就是把对象转化为字节序列的过程称为对象的序列化。 \n 反序列化：是序列化的逆向过程，把字节数组反序列化为对象，把字节序列恢复为对象的过程称为对象的反序列化。 \n 1.Java序列化 \n 序列化方式常见的有5种，包含：Java序列化、Json、kryo、protobuf、Hessian等序列化方式， \n Java类通过实现Serializable接口来实现该类对象的序列化，如下代码示例： \n public   class   TestBean   implements   Serializable   { \n \n     private   Integer  id ; \n \n     private   String  name ; \n \n     private   Date  date ; \n  //省去getter和setter方法和toString \n } \n \n 1 2 3 4 5 6 7 8 9 这个接口非常特殊，没有任何方法，只起标识作用，Java序列化保留了对象类的元数据（如类、成员变量、继承类信息等），以及对象数据等，兼容性最好， 但不支持跨语言，而且性能一般。 \n 实现Serializable接口的类建议设置 serialVersionUID 字段值，如果不设置，那么每次运行时，编译器会根据类的内部实现，包括类名、接口名、方法和属性等来自动生成 serialVersionUID 。 \n Transient关键字 \n Transient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是null。 \n 如果类的源代码有修改，那么重新编译后serial VersionUID的取值可能会发生变化。 \n 因此实现Serializable接口的类一定要显式地定义 serialVersionUID 属性值。 \n import   java . io . Serializable ; \n \n public   class   Person   implements   Serializable { \n    private   static   final   long  serialVersionUID  =   1234567890L ; \n    private   int  id ; \n    private   String  name ; \n    public   Person ( int  id ,   String  name ) { \n        this . id  =  id ; \n        this . name  =  name ; \n    } \n \n    public   String   toString ( ) { \n        return   "Person: "   +  id  +   " "   +  name ; \n    } \n \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #  2.Hessian 序列化 \n Hessian 序列化是一种支持动态类型、跨语言、基于对象传输的网络协议。 \n Hessian 序列化特性： \n \n 自描述序列化类型，不依赖外部描述文件或者接口定义，用一个字节表示常用的基础类型，极大缩短二进制流； \n 语言无关，支持脚本语言； \n 协议简单，比Java原生序列化高效； \n 相比hessian1，hessian2中增加了压缩编码，其序列化二进制流大小是Java序列化的50%，序列化耗时是Java序列化的30%，反序列化耗时是Java序列化的20%。 \n \n 在使用Hessian序列化之前，需要在maven工程中，引入Hessian依赖： \n < dependency > \n   < groupId > com.caucho </ groupId > \n   < artifactId > hessian </ artifactId > \n   < version > 4.0.62 </ version > \n </ dependency > \n \n 1 2 3 4 5 无论jdk序列化，还是hessian序列化，实体类均需要实现Serializable接口。 \n 具体代码示例： \n \n public   class   Test   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         ByteArrayOutputStream  os  =   new   ByteArrayOutputStream ( ) ; \n         Hessian2Output  output  =   new   Hessian2Output ( os ) ; \n        output . writeObject ( Person . hehe ( 123L ,   "wangyong" ) ) ; \n        output . close ( ) ; \n \n         ByteArrayInputStream  in  =   new   ByteArrayInputStream ( os . toByteArray ( ) ) ; \n         Hessian2Input  input  =   new   Hessian2Input ( in ) ; \n         System . out . println ( input . readObject ( ) ) ; \n     } \n } \n\n class   Person   implements   Serializable   { \n     private   Long  id ; \n     private   String  name ; \n     private   Person ( long  id ,   String  name )   { \n         this . id  =  id ; \n         this . name  =  name ; \n         System . out . println ( "call dd" ) ; \n     } \n \n     public   static   Person   hehe ( Long  id ,   String  name )   { \n         Person  p  =   new   Person ( id ,  name ) ; \n         return  p ; \n     } \n \n     @Override \n     public   String   toString ( )   { \n         return   "id="   +  id  +   ", name="   +  name ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 更多参考官方：http://hessian.caucho.com/doc/hessian-serialization.html \n 3.Json序列化 \n 关于Json 可以说是web 开发不管是前端还是后端都是非常的熟悉，JSON是一种 轻量级 的数据交换格式。 \n JSON 序列化就是将数据对象转换为 JSON 字符串，在序列化过程中抛弃了类型信息，相比前两种方式， JSON 可读性比较好 ，方便调试。 \n Json语法规则： \n \n 数据在键值对中 \n 数据由逗号分隔 \n 花括号保存对象 \n 方括号保存数组 \n \n 具体示例如下： \n { \n "employees" :   [ \n {   "firstName" : "Mike"   ,   "lastName" : "Chen"   } , \n {   "firstName" : "Anna"   ,   "lastName" : "Smith"   } , \n {   "firstName" : "Peter"   ,   "lastName" : "Jones"   } \n ] \n } \n \n 1 2 3 4 5 6 7 #  4.ProtoBuf \n 谷歌推出的，是一种语言无关、平台无关、可扩展的序列化结构数据的方法，它可用于通信协议、数据存储等。 \n Protobuf应用场景 \n 序列化后体积小 ，一般用于对传输性能有较高要求的系统，Protobuf序列化方式适用于 跨语言 通信、对码流大小和 性能要求高 、且 pojo不经常变化 的场景。 \n Protobuf使用 \n 要想使用Protobuf就需要先定义proto文件，先熟悉protobuf消息定义的相关语法。 \n 定义消息类型，示例如下： \n syntax   =   "proto3" ; \n \n message   SendRequest   { \n   string  query  =   1 ; \n   int32  page_number  =   2 ; \n   repeated   int32  result_per_page  =   3 ; \n } \n \n 1 2 3 4 5 6 7 \n .proto文件的第一行指定了使用proto3语法，说明使用的是proto3版本，如果省略protocol buffer编译器就默认使用proto2语法； \n message表示消息类型，可以有多个； \n SendRequest定义中指定了三个字段(name/value键值对)，每个字段都会有名称和类型； \n repeated是字段规则。 \n 5.Kryo \n Kryo 是一个 Java 序列化框架，号称 Java 最快的序列化框架，Kryo 在序列化速度上很有优势，底层依赖于字节码生成机制。 \n Kryo 的特点： \n \n 序列化的 性能非常高 \n 序列化结果 体积较小 \n 提供了简单易用的API \n \n 但是由于只能限定在 JVM 语言上，所以  Kryo 不支持跨语言使用 。 \n Kryo序列化被很多开源项目使用，社区非常活跃： \n \n Apache Hive \n Apache Spark \n Twitter’s Chill \n Storm \n akka-kryo-serialization \n \n Kryo序列化具体示例如下： \n static   void   quickStart ( )   throws   FileNotFoundException   { \n     Kryo  kryo  =   new   Kryo ( ) ; \n     Output  output  =   new   Output ( new   FileOutputStream ( "file.bin" ) ) ; \n     SomeClass  someObject  =   new   SomeClass ( ) ; \n    someObject . setValue ( "this is someObject." ) ; \n    kryo . writeObject ( output ,  someObject ) ; \n    output . close ( ) ; \n \n     Input  input  =   new   Input ( new   FileInputStream ( "file.bin" ) ) ; \n     SomeClass  deSomeObject  =  kryo . readObject ( input ,   SomeClass . class ) ; \n    input . close ( ) ; \n    \n     Assert . assertEquals ( someObject . getValue ( ) ,  deSomeObject . getValue ( ) ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 '},{title:"本地缓存",frontmatter:{title:"本地缓存",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["其他"],tags:["缓存","一级缓存"]},regularPath:"/%E5%85%B6%E4%BB%96/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98.html",relativePath:"其他/本地缓存.md",key:"v-29f87cb0",path:"/2023/06/10/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98/",headers:[{level:2,title:"前言",slug:"前言"},{level:3,title:"JVM缓存",slug:"jvm缓存"},{level:2,title:"Guava Cache",slug:"guava-cache"},{level:3,title:"一、创建",slug:"一、创建"},{level:3,title:"二、回收（逐出）策略",slug:"二、回收-逐出-策略"},{level:3,title:"三、删除缓存和删除监听器",slug:"三、删除缓存和删除监听器"},{level:3,title:"四、刷新策略",slug:"四、刷新策略"},{level:2,title:"Caffeine Cache",slug:"caffeine-cache"},{level:3,title:"一、性能",slug:"一、性能"},{level:3,title:"二、 命中率",slug:"二、-命中率"},{level:3,title:"三、运用",slug:"三、运用"},{level:3,title:"四、Coding",slug:"四、coding"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' 前言 \n JVM缓存 \n JVM 缓存，是堆缓存。其实就是创建一些全局容器，比如List、Set、Map等。 \n这些容器用来做数据存储。 \n这样做的问题： \n不能 按照一定的规则淘汰数据 ，如 LRU，LFU，FIFO 等。 \n清除数据时的回调通知 \n并发处理能力差，针对 并发 可以使用CurrentHashMap，但缓存的其他功能需要自行实现缓存过期处理，缓存数据加载刷新等都需要手工实现 \n 在项目开发中，为提升系统性能，减少 IO 开销，本地缓存是必不可少的。最常见的本地缓存是 Guava 和 Caffeine。 \n Guava Cache \n guava cache是google开源的一款本地缓存工具库，它的设计灵感来源于ConcurrentHashMap，使用多个segments方式的细粒度锁，在保证 线程安全的同时，支持高并发 场景需求，同时支持多种类型的 缓存清理 策略，包括基于容量的清理、基于时间的清理、基于引用的清理等。 \n guava cache 位于 com.google.common.cache 包下，核心的类有两个，一个是 CacheBuilder ，是用来构建缓存的，另一个是 Cache ，也就是缓存容器，用来存放缓存数据的。 \n 一、创建 \n \x3c!--guava 工具包 本地缓存--\x3e \n         < dependency > \n             < groupId > com.google.guava </ groupId > \n             < artifactId > guava </ artifactId > \n             < version > 19.0 </ version > \n         </ dependency > \n \n 1 2 3 4 5 6 #  1、CacheLoader \n LoadingCache < Key ,   Graph >  graphs  =   CacheBuilder . newBuilder ( ) \n        . maximumSize ( 1000 ) \n        . build ( \n            new   CacheLoader < Key ,   Graph > ( )   { \n              public   Graph   load ( Key  key )   throws   AnyException   { \n                return   createExpensiveGraph ( key ) ; \n              } \n            } ) ; \n . . . \n try   { \n   return  graphs . get ( key ) ; \n }   catch   ( ExecutionException  e )   { \n   throw   new   OtherException ( e . getCause ( ) ) ; \n } \n \n . . . \n //或者使用下面方法，不抛出异常 \n return  graphs . getUnchecked ( key ) ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 在每次从cache中get(K)时，如果不存在会自动调用load方法原子的将值计算出来并加到缓存中。（调用load方法是同步的） \n 1）get(K)和getUnchecked(K)方法： \n 由于CacheLoader可能会引发异常，因此LoadingCache.get（K）会引发ExecutionException。还可以选择使用getUnchecked（K）方法获取值，不抛出异常。 \n 2）批量get和批量load： \n getAll(Iterable<? extendsK>)方法用来执行批量查询。默认情况下，对每个不在缓存中的键，getAll方法会单独调用CacheLoader.load来加载缓存项。如果批量的加载比多个单独加载更高效，可以重载CacheLoader.loadAll来利用这一点提示getAll(Iterable)的性能。看一个例子： \n public   LoadingCache < String ,   String >  caches  =   CacheBuilder \n\t . newBuilder ( ) . maximumSize ( 100 ) \n\t . expireAfterWrite ( 100 ,   TimeUnit . SECONDS )   // 根据写入时间过期 \n\t . build ( new   CacheLoader < String ,   String > ( )   { \n\t\t @Override \n\t\t public   String   load ( final   String  key )   { \n\t\t\t return   getSchema ( key ) ; \n\t\t } \n\t\t\t\t\n\t\t @Override \n\t\t public   Map < String , String >   loadAll ( final   Iterable < ?   extends   String >  keys )   throws   Exception   { \n             //com.google.common.collect.Lists \n\t\t\t ArrayList < String >  keysList  =   Lists . newArrayList ( keys ) ; \n\t\t\t return   getSchemas ( keysList ) ; \n\t\t } \n } ) ; \n \n private   static   Map < String , String >   getSchemas ( List < String >  keys )   { \n\t\t Map < String , String >  map  =   new   HashMap < > ( ) ; \n\t\t\n\t\t //... \n\t\t System . out . println ( "loadall..." ) ; \n\t\t return  map ; \n } \n \n List < String >  keys  =   new   ArrayList < > ( ) ; \nkeys . add ( "key2" ) ; \nkeys . add ( "key3" ) ; \n \n try   { \n\tcaches . getAll ( keys ) ; \n }   catch   ( ExecutionException  e1 )   { \n\te1 . printStackTrace ( ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 注意：expireAfterWrite时guava重新加载数据时使用的是load方法，不会调用loadAll。 \n  2、callable \n LoadingCache < String ,   String >  cache  =   CacheBuilder \n\t . newBuilder ( ) . maximumSize ( 100 ) \n\t . expireAfterWrite ( 100 ,   TimeUnit . SECONDS )   // 根据写入时间过期 \n\t . build ( new   CacheLoader < String ,   String > ( )   { \n\t\t @Override \n\t\t public   String   load ( String  key )   { \n\t\t\t return   getSchema ( key ) ; \n\t\t } \n } ) ; \n \n private   static   String   getSchema ( String  key )   { \n     System . out . println ( "load..." ) ; \n     return  key + "schema" ; \n } \n \n try   { \n\t String  value  =  cache . get ( "key4" ,   new   Callable < String > ( )   { \n\t\t\t @Override \n\t\t\t public   String   call ( )   throws   Exception   { \n\t\t\t\t System . out . println ( "i am callable..." ) ; \n\t\t\t\t return   "i am callable..." ; \n\t\t\t } \n\t   } ) ; \n\t System . out . println ( value ) ; \n }   catch   ( ExecutionException  e1 )   { \n\te1 . printStackTrace ( ) ; \n } \n //输出值：i am callable...  \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 或 \n Cache < String ,   String >  cache2  =   CacheBuilder . newBuilder ( ) \n\t\t . maximumSize ( 1000 ) \n\t\t . expireAfterWrite ( 100 ,   TimeUnit . SECONDS )   // 根据写入时间过期 \n\t\t . build ( ) ;   // look Ma, no CacheLoader \n \n try   { \n\t String  value  =  cache2 . get ( "key4" ,   new   Callable < String > ( )   { \n\t\t\t @Override \n\t\t\t public   String   call ( )   throws   Exception   { \n\t\t\t\t System . out . println ( "i am callable..." ) ; \n\t\t\t\t return   "i am callable..." ; \n\t\t\t } \n\t   } ) ; \n\t System . out . println ( value ) ; \n }   catch   ( ExecutionException  e1 )   { \n\te1 . printStackTrace ( ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 callable同样实现了原子的“ get-if-absent-compute”语义。上面两个例子说明：无论是LoadingCache还是Cache都可以使用callable的方式，需要说明的是： \n 1）Cache类型的缓存只能使用Callable的方式get（K，Callable）方法； \n 2）LoadingCache类型的缓存，可以使用get（K）或get（K，Callable）方法，并且如果使用的是get（K，Callable）方法，当K值不存在时，使用的是Callable计算值，不走load方法计算，然后将值放入缓存。 \n 3、显示插入： \n除了上面两种方式创建缓存外，还可以显示的使用put(K,V)方法，将值放入缓存中。但是这种方法没有“ get-if-absent-compute”语义。 \n 二、回收（逐出）策略 \n 由于guava是本地缓存，所以需要一个回收策略。guava提供了三种回收策略。 \n 1、基于size回收： \n 通过CacheBuilder.maximumSize(long)设置缓存项的 最大数目 ，当达到最大数目后，继续添加缓存项，Guava 默认会 根据LRU策略回收缓存项来保证不超过最大数目 ； 另外，可以通过CacheBuilder.weigher(Weigher)设置不同缓存项的权重，Guava Cache根据权重来回收缓存项。 \n LoadingCache < Key ,   Graph >  graphs  =   CacheBuilder . newBuilder ( )   \n\t . maximumWeight ( 100000 ) \n\t . weigher ( new   Weigher < Key ,   Graph > ( )   { \n\t\t public   int   weigh ( Key  k ,   Graph  g )   { \n\t\t\t return  g . vertices ( ) . size ( ) ; \n\t\t } \n\t } ) \n\t . build ( new   CacheLoader < Key ,   Graph > ( )   { \n\t\t\t public   Graph   load ( Key  key )   {   // no checked exception \n\t\t\t\t return   createExpensiveGraph ( key ) ; \n\t\t\t } \n\t } ) ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 #  2、定时回收： \n guava Cache提供两种定时回收的方法： \n expireAfterAccess(long, TimeUnit)：缓存项（Key）在给定时间范围内没有读/写访问，那么下次访问时，会回收该Key，然后同步load()，这种方式类似于基于size的LRU回收。（） \nexpireAfterWrite(long, TimeUnit)：缓存项（Key）在给定时间范围内没有写访问，那么下次访问时，会回收该Key，然后同步load()。 \n注：Guava Cache不会专门维护一个线程来回收这些过期的缓存项，只有在读/写访问时，才去判断该缓存项是否过期，如果过期，则会回收。而且注意，回收后会同步调用load方法来加载新值到cache中。 \n 3、基于引用回收： \n 通过使用弱引用的键、或弱引用的值、或软引用的值，Guava Cache可以把缓存设置为允许垃圾回收： \n CacheBuilder.weakKeys()：使用弱引用存储键。当键没有其它（强或软）引用时，缓存项可以被垃圾回收。 \nCacheBuilder.weakValues()：使用弱引用存储值。当值没有其它（强或软）引用时，缓存项可以被垃圾回收 \nCacheBuilder.softValues()：使用软引用存储值。软引用只有在响应内存需要时，才按照全局最近最少使用的顺序回收。考虑到使用软引用的性能影响，我们通常建议使用更有性能预测性的缓存大小限定（见上文，基于容量回收） \n以上是guava自动维护的，当然我们也可以手动将缓存值清理出cache，见下面。 \n 三、删除缓存和删除监听器 \n 1、删除缓存Key方法： \n在任何时候，可以通过一下方法将Key从缓存中移除，而不用等guava的回收。 \n 清除单个Key：Cache.invalidate(key) \n批量清除：Cache.invalidateAll(keys) \n清除所有缓存项：Cache.invalidateAll() \n2、删除监听器： \n通过CacheBuilder.removalListener(RemovalListener)，你可以声明一个监听器，以便缓存项被移除时做一些额外操作。缓存项被移除时，RemovalListener会获取移除通知RemovalNotification，其中包含移除原因RemovalCause、键和值。 \n 四、刷新策略 \n 1、什么时候进行清理？ \n使用CacheBuilder构建的缓存不会“自动”执行清理和逐出值，也不会在值过期后立即执行清理或逐出值，或类似的任何操作。取而代之的是，如果写操作很少，它会在写操作期间或偶尔的读操作期间执行少量维护。 \n 原因如下：如果我们要连续执行Cache维护，则需要创建一个线程，并且该线程的操作将与用户操作争夺共享锁。此外，某些环境限制了线程的创建，这会使CacheBuilder在该环境中无法使用。 \n 相反，我们会将选择权交给您。如果您的缓存是高吞吐量的，那么您不必担心执行缓存维护以清理过期的条目等。如果您的缓存确实很少写入，并且您不想清理来阻止缓存读取，则您可能希望创建自己的维护线程，该线程定期调用Cache.cleanUp（）。 \n 最后一句话怎么理解：如果流量很大，每时每刻都在访问cache，那么guava会自动根据回收策略进行清理数据。如果量很小，由于guava的惰性原理，不会及时回收，用户可以自己定时清理。 \n 2、刷新缓存策略： \n1）刷新和回收区别： \n 刷新策略和上面说的回收策略不太一样：刷新表示为key加载新值，这个过程可以是异步的（需要重写CacheLoader的reload方法，否则仍然是同步的调用load），而回收Key的时候，会调用load方法加载值，这个过程是同步的。二者的相同点是：都是在访问缓存项（Key）的时候才会触发。 \n 2）刷新方法： \n 可以调用LoadingCache.refresh(K) 来刷新某个Key。（ 注 ：只有LoadingCache类才有refresh方法） \n public   static   ThreadPoolExecutor  threadPool  =   new   ThreadPoolExecutor ( 5 ,   50 ,   300 ,   TimeUnit . SECONDS ,  \n\t new   ArrayBlockingQueue < Runnable > ( 50 ) ,   \n\t new   ThreadFactory ( ) {   public   Thread   newThread ( Runnable  r )   { \n\t\t return   new   Thread ( r ,   "pool_"   +  r . hashCode ( ) ) ; \n\t } } ,   new   ThreadPoolExecutor . DiscardOldestPolicy ( ) ) ; \n \n \n public   static   LoadingCache < String ,   String >  cache  =   CacheBuilder \n\t . newBuilder ( ) . maximumSize ( 100 ) \n\t . expireAfterWrite ( 100 ,   TimeUnit . SECONDS )   // 根据写入时间过期 \n\t . build ( new   CacheLoader < String ,   String > ( )   { \n\t\t @Override \n\t\t public   String   load ( String  key )   { \n\t\t\t return   getSchema ( key ) ; \n\t\t } \n\t\t\n\t\t public   ListenableFuture < String >   reload ( String  key ,   String  oldValue )   throws   Exception   { \n\t\t\t ListenableFutureTask < String >  task  =    ListenableFutureTask . create ( new   Callable < String > ( )   { \n\t\t\t\t @Override \n\t\t\t\t public   String   call ( )   throws   Exception   { \n\t\t\t\t\t Thread . sleep ( 1000 ) ; \n\t\t\t\t\t System . out . println ( "async...." ) ; \n\t\t\t\t\t return   getSchema ( key ) ; \n\t\t\t\t } \n\t\t\t } ) ; \n\t\t\tthreadPool . submit ( task ) ; \n\t\t\t return  task ; \n\t\t } \n\t } ) ; \n \n //调用 \ncache . refresh ( key1 ) ; //void \n System . out . println ( "after refresh" ) ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 输出： \n after refresh \n async... \n 证明refresh方法是一个异步的。如果我们没有重写reload方法，那么看reload源码，就是默认的同步调用load，如： \n \n 3、刷新+回收策略： \n 我们对比一下几种策略： \n 1）定时过期回收： \n 前面我们知道可以配置expireAfterWrite或expireAfterAccess来设置定期回收，那我们现在来看下这种策略在高并发情况下是否存在“缓存击穿”问题？当高并发条件下同时进行get操作，而此时缓存值已过期时，会导致大量线程都调用生成缓存值的方法，比如从数据库读取。这时候就容易造成大量请求同时查询数据库中该条记录，也就是“缓存击穿”。 \n Guava cache则对此种情况有一定控制。当大量线程用相同的key获取缓存值时，只会有一个线程进入load方法，而其他线程则等待，直到缓存值被生成。这样也就避免了缓存击穿的危险。 \n 2）定时刷新： \n guava虽然不会有缓存击穿的情况，但是每当某个缓存值过期时，老是会导致大量的请求线程被阻塞。而Guava则提供了另一种缓存策略，缓存值定时刷新：更新线程调用load方法更新该缓存，其他请求线程返回该缓存的旧值。这样对于某个key的缓存来说，只会有一个线程被阻塞，用来生成缓存值，而其他的线程都返回旧的缓存值，不会被阻塞。 \n 这里就需要用到Guava cache的refreshAfterWrite方法。例如： \n LoadingCache < String ,   Object >  caches  =   CacheBuilder . newBuilder ( ) \n                 . maximumSize ( 100 ) \n                 . refreshAfterWrite ( 10 ,   TimeUnit . MINUTES ) \n                 . build ( new   CacheLoader < String ,   Object > ( )   { \n                     @Override \n                     public   Object   load ( String  key )   throws   Exception   { \n                         return   generateValueByKey ( key ) ; \n                     } \n                 } ) ; \n try   { \n     System . out . println ( caches . get ( "key-zorro" ) ) ; \n }   catch   ( ExecutionException  e )   { \n    e . printStackTrace ( ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 注：前面两种策略中的定时，不是真正意义上的定时。Guava cache的刷新和回收都是需要依靠用户请求触发的。 \n 3）异步刷新策略： \n 上面解决了同一个key的缓存过期时会让多个线程阻塞的问题，只会让用来执行刷新缓存操作的一个用户线程会被阻塞。由此可以想到另一个问题，当缓存的key很多时，高并发条件下大量线程同时获取不同key对应的缓存，此时依然会造成大量线程阻塞，并且给数据库带来很大压力。这个问题的解决办法就是将刷新缓存值的任务交给后台线程，所有的用户请求线程均返回旧的缓存值，这样就不会有用户线程被阻塞了。 \n ListeningExecutorService  backgroundRefreshPools  =  \n\t\t\t\t MoreExecutors . listeningDecorator ( Executors . newFixedThreadPool ( 20 ) ) ; \n         LoadingCache < String ,   Object >  caches  =   CacheBuilder . newBuilder ( ) \n                 . maximumSize ( 100 ) \n                 . refreshAfterWrite ( 10 ,   TimeUnit . MINUTES ) \n                 . build ( new   CacheLoader < String ,   Object > ( )   { \n                     @Override \n                     public   Object   load ( String  key )   throws   Exception   { \n                         return   generateValueByKey ( key ) ; \n                     } \n                    \n                     @Override \n                     public   ListenableFuture < Object >   reload ( String  key , \n                    \t\t Object  oldValue )   throws   Exception   { \n                    \t return  backgroundRefreshPools . submit ( new   Callable < Object > ( )   { \n \n\t\t\t\t\t\t\t @Override \n\t\t\t\t\t\t\t public   Object   call ( )   throws   Exception   { \n\t\t\t\t\t\t\t\t return   generateValueByKey ( key ) ; \n\t\t\t\t\t\t\t } \n\t\t\t\t\t\t } ) ; \n                     } \n                 } ) ; \n try   { \n     System . out . println ( caches . get ( "key-zorro" ) ) ; \n }   catch   ( ExecutionException  e )   { \n    e . printStackTrace ( ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 重写了CacheLoader的reload方法，在该方法中建立缓存刷新的任务并提交到线程池。 \n 注意：因为刷新动作和回收一样，都是在检索的时候才会触发，所以当你的缓存配置了CacheBuilder.refreshAfterWrite(long, TimeUnit)时，如果部分缓存项很久没有被访问，那么再次被访问时，可能会获得过期很久的数据，这显然是不行的。而单独配置expireAfterWrite(long, TimeUnit)也是有问题的，如果热点数据突然过期，因为同步load()必然会影响读效率。 \n 所以，通常我们都是CacheBuilder.refreshAfterWrite(long, TimeUnit)和expireAfterWrite(long, TimeUnit) 同时配置，并且刷新的时间间隔要比过期的时间间隔短！这样当较长时间没有被访问的缓存项突然被访问时，会触发过期回收而不是刷新，后面会分析这一块的源码，而热点数据只会触发刷新操作不会触发回收操作。 \n 本地缓存字典，要注意的问题： \n 1.缓存不能太大 \n 设置最大容量，lru驱逐； \n 2.字典内容需要及时更新，不然会映射到旧值 \n 增加过期expireAfterWrite(long, TimeUnit) \n 3.如果热点数据突然过期，因为同步load()必然会影响读效率。 \n CacheBuilder.refreshAfterWrite(long, TimeUnit)刷新的时间间隔要比过期的时间间隔短， \n 这样真正数据不更新只出现在refresh之后到过期的这段时间。 \n Caffeine Cache \n Caffeine 是基于 Google Guava Cache 设计经验改进的结果，相较于 Guava 在性能和命中率上更具有效率，你可以认为其是 Guava Plus。 \n guava 和caffeine对比： \n 一、性能 \n Guava 中其读写操作夹杂着过期时间的处理，也就是你在一次 put 操作中有可能会做淘汰操作，所以其读写性能会受到一定影响。 \n Caffeine 在读写操作方面完爆 Guava，主要是因为 Caffeine 对这些事件的操作是异步的，将事件提交至队列（使用 Disruptor RingBuffer），然后会通过默认的 ForkJoinPool.commonPool()，或自己配置的线程池，进行取队列操作，然后再进行后续的淘汰、过期操作。 \n 以下是对比情况： \n （1）在此基准测试中，从配置了最大大小的缓存中，8 个线程并发读： \n \n （2）在此基准测试中，从配置了最大大小的缓存中，6个线程并发读、2个线程并发写： \n \n （3）在此基准测试中，从配置了最大大小的缓存中，8 个线程并发写： \n 二、 命中率 \n 缓存的淘汰策略是为了预测哪些数据在短期内最可能被再次用到,从而提升缓存的命中率。Guava 使用 S-LRU 分段的最近最少未使用算法，Caffeine 采用了一种结合 LRU、LFU 优点的算法： W-TinyLFU，其特点是：高命中率、低内存占用。 \n 2.1 LRU \n Least Recently Used：如果数据最近被访问过，将来被访问的概率也更高。**每次访问就把这个元素放到队列的头部，队列满了就淘汰队列尾部的数据，**即淘汰最长时间没有被访问的。 \n 其缺点是， 如果某一时刻大量数据到来，很容易将热点数据挤出缓存 ，留下来的很可能是只访问一次，今后不会再访问的或频率极低的数据。比如外卖中午时候访问量突增、微博爆出某明星糗事就是一个突发性热点事件。当事件结束后，可能没有啥访问量了，但是由于其极高的访问频率，导致其在未来很长一段时间内都不会被淘汰掉。 \n 2.2 LFU \n Least Frequently Used（最不经常使用）：如果数据最近被访问过，那么将来被访问的概率也更高。也就是淘汰一定时间内被访问次数最少的数据（时间局部性原理）。 \n 需要维护每个数据项的访问频率信息，每次访问都需要更新，这个开销是非常大的。 \n 其优点是， 避免了 LRU 的缺点，因为根据频率淘汰，不会出现大量进来的挤压掉 老的，如果在数据的访问的模式不随时间变化时候，LFU 能够提供绝佳的命中率。 \n 其缺点是，LFU算法根据次数进行缓存淘汰，还是以热点数据为例，某天有明星XXX出轨，XXX这个词被搜索了十万次， 过了一个月后 ， 热度过去了 ，大家搜索量少了，但XXX明星出轨的 相关数据依然在缓存中 ，这份数据可能需要很久才能被淘汰掉。 \n 另外，LFU 算法由于 需要额外的存储空间记录访问次数 ，数据量非常大的情况下对于存储的消耗也是很大的。 \n 2.3 TinyLFU \n TinyLFU 顾名思义，轻量级LFU，相比于 LFU 算法用更小的内存空间来记录访问频率。 \n TinyLFU  维护了近期访问记录的频率信息 ，不同于传统的 LFU 维护整个生命周期的访问记录，所以他可以很好地应对 突发性的热点事件（超过一定时间，这些记录不再被维护） 。这些访问记录会作为一个过滤器， 当新加入的记录（New Item）访问频率高于将被淘汰的缓存记录（Cache Victim）时才会被替换 。流程如下： \n \n tiny-lfu-arch \n 尽管维护的是近期的访问记录， 但仍然是非常昂贵的 ，TinyLFU 通过 Count-Min Sketch 算法（  布隆过滤器  的一种变种）来记录频率信息，它占用空间小且误报率低，关于 Count-Min Sketch 算法可以参考论文：pproximating Data with the Count-Min Data Structure。 \n TinyLFU采用了一种基于 滑动窗口的时间衰减 设计机制，借助于一种简易的reset操作：每次添加一条记录到Sketch的时候，都会给一个计数器上加1，当计数器达到一个尺寸W的时候，把所有记录的Sketch数值都除以2，该 reset 操作可以起到衰减的作用 。 \n 缺点：在一些数目很少但突发访问量很大的场景下，因为它们 无法在短时间内积累到足够高的频率 ，从而被过滤器过滤掉。 \n 2.4 W-TinyLFU \n W-TinyLFU 是 Caffeine 提出的一种全新算法，它可以解决频率统计不准确以及访问频率衰减的问题。这个方法让我们从空间、效率、以及适配举证的长宽引起的哈希碰撞的错误率上做均衡。 \n W-TinyLFU 算法是对 TinyLFU算法的优化，能够很好地解决一些稀疏的突发访问元素。在一些数目很少但突发访问量很大的场景下，TinyLFU将无法保存这类元素，因为它们无法在短时间内积累到足够高的频率，从而被过滤器过滤掉。W-TinyLFU 将 新记录暂时放入 Window Cache  里面，只有通过 TinLFU 考察才能进入 Main Cache。大致流程如下图：（先采用LRU得到准备淘汰的数据--\x3e再经过LFU才是真正淘汰） \n 三、运用 \n 场景一 \n 配置方式 ：设置 maxSize、refreshAfterWrite，不设置 expireAfterWrite \n 存在问题 ：get 缓存间隔超过 refreshAfterWrite 后，触发缓存异步刷新，此时会获取缓存中的旧值 \n 适用场景 ： \n \n 缓存数据量大，限制缓存占用的内存容量* 缓存值会变，需要刷新缓存* 可以接受任何时间缓存中存在旧数据 \n 场景二 \n 配置方式 ：设置 maxSize、expireAfterWrite，不设置 refreshAfterWrite \n 存在问题 ：get 缓存间隔超过 expireAfterWrite 后，针对该 key，获取到锁的线程会同步执行 load，其他未获得锁的线程会阻塞等待，获取锁线程执行延时过长会导致其他线程阻塞时间过长 \n 适用场景 ： \n \n 缓存数据量大，限制缓存占用的内存容量* 缓存值会变，需要刷新缓存* 不可以接受缓存中存在旧数据* 同步加载数据延迟小（使用 redis 等） \n 场景三 \n 配置方式 ：设置 maxSize、refreshAfterWrite、expireAfterWrite，refreshAfterWrite < expireAfterWrite \n 存在问题 ： \n \n get 缓存间隔在 refreshAfterWrite 和 expireAfterWrite 之间，触发缓存异步刷新，此时会获取缓存中的旧值* get 缓存间隔大于 expireAfterWrite，针对该 key，获取到锁的线程会同步执行 load，其他未获得锁的线程会阻塞等待，获取锁线程执行延时过长会导致其他线程阻塞时间过长 \n \n 适用场景 ： \n \n 缓存数据量大，限制缓存占用的内存容量* 缓存值会变，需要刷新缓存* 可以接受有限时间缓存中存在旧数据* 同步加载数据延迟小（使用 redis 等） \n \n \n 由此可见，caffeine在使用方式上与guava一样。 \n Caffeine 兼容 Guava API，从 Guava 切换到 Caffeine，仅需要把 CacheBuilder.newBuilder()改成 Caffeine.newBuilder() 即可。 \n < dependency > \n     < groupId > com.github.ben-manes.caffeine </ groupId > \n     < artifactId > caffeine </ artifactId > \n </ dependency > \n \n 1 2 3 4 需要注意的是，在使用 Guava 的 get()方法时，当缓存的 load()方法返回 null 时，会抛出 ExecutionException。切换到 Caffeine 后，get()方法不会抛出异常，但允许返回为 null，需要做好判空处理。 \n 四、Coding \n Caffeine提供了四种缓存添加策略：手动加载，自动加载，手动异步加载和自动异步加载。 \n 1.创建 \n 「1.手动加载」 \n 在每次get key的时候指定一个同步的函数，如果key不存在就调用这个函数生成一个值。 \n /**\n     * 手动加载\n     * @param key\n     * @return\n     */\npublic Object manulOperator(String key) {\n    Cache&lt;String, Object> cache = Caffeine.newBuilder()\n        .expireAfterWrite(1, TimeUnit.SECONDS)\n        .expireAfterAccess(1, TimeUnit.SECONDS)\n        .maximumSize(10)\n        .build();\n    //如果一个key不存在，那么会进入指定的函数生成value\n    Object value = cache.get(key, t -> setValue(key).apply(key));\n    cache.put("hello",value);\n\n    //判断是否存在如果不存返回null\n    Object ifPresent = cache.getIfPresent(key);\n    //移除一个key\n    cache.invalidate(key);\n    return value;\n}\n\npublic Function&lt;String, Object> setValue(String key){\n    return t -> key + "value";\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 「2. 同步加载」 \n 构造Cache时候，build方法传入一个CacheLoader实现类。实现load方法，通过key加载value。 \n /**\n     * 同步加载\n     * @param key\n     * @return\n     */\npublic Object syncOperator(String key){\n    LoadingCache&lt;String, Object> cache = Caffeine.newBuilder()\n        .maximumSize(100)\n        .expireAfterWrite(1, TimeUnit.MINUTES)\n        .build(k -> setValue(key).apply(key));\n    return cache.get(key);\n}\n\npublic Function&lt;String, Object> setValue(String key){\n    return t -> key + "value";\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 「3. 异步加载」 \n AsyncLoadingCache是继承自LoadingCache类的，异步加载使用Executor去调用方法并返回一个CompletableFuture。异步加载缓存使用了响应式编程模型。 \n 如果要以同步方式调用时，应提供CacheLoader。要以异步表示时，应该提供一个AsyncCacheLoader，并返回一个CompletableFuture。 \n /**\n     * 异步加载\n     *\n     * @param key\n     * @return\n     */\npublic Object asyncOperator(String key){\n    AsyncLoadingCache&lt;String, Object> cache = Caffeine.newBuilder()\n        .maximumSize(100)\n        .expireAfterWrite(1, TimeUnit.MINUTES)\n        .buildAsync(k -> setAsyncValue(key).get());\n\n    return cache.get(key);\n}\n\npublic CompletableFuture&lt;Object> setAsyncValue(String key){\n    return CompletableFuture.supplyAsync(() -> {\n        return key + "value";\n    });\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #  2 回收策略 \n Caffeine提供了3种回收策略：基于大小回收，基于时间回收，基于引用回收。 \n 「1. 基于大小的过期方式」 \n 基于大小的回收策略有两种方式：一种是基于缓存大小，一种是基于权重。 \n // 根据缓存的计数进行驱逐\nLoadingCache&lt;String, Object> cache = Caffeine.newBuilder()\n    .maximumSize(10000)\n    .build(key -> function(key));\n\n// 根据缓存的权重来进行驱逐（权重只是用于确定缓存大小，不会用于决定该缓存是否被驱逐）\nLoadingCache&lt;String, Object> cache1 = Caffeine.newBuilder()\n    .maximumWeight(10000)\n    .weigher(key -> function1(key))\n    .build(key -> function(key));\nmaximumWeight与maximumSize不可以同时使用。\n \n 1 2 3 4 5 6 7 8 9 10 11 「2.基于时间的过期方式」 \n // 基于固定的到期策略进行退出\nLoadingCache&lt;String, Object> cache = Caffeine.newBuilder()\n    .expireAfterAccess(5, TimeUnit.MINUTES)\n    .build(key -> function(key));\nLoadingCache&lt;String, Object> cache1 = Caffeine.newBuilder()\n    .expireAfterWrite(10, TimeUnit.MINUTES)\n    .build(key -> function(key));\n\n// 基于不同的到期策略进行退出\nLoadingCache&lt;String, Object> cache2 = Caffeine.newBuilder()\n    .expireAfter(new Expiry&lt;String, Object>() {\n        @Override\n        public long expireAfterCreate(String key, Object value, long currentTime) {\n            return TimeUnit.SECONDS.toNanos(seconds);\n        }\n\n        @Override\n        public long expireAfterUpdate(@Nonnull String s, @Nonnull Object o, long l, long l1) {\n            return 0;\n        }\n\n        @Override\n        public long expireAfterRead(@Nonnull String s, @Nonnull Object o, long l, long l1) {\n            return 0;\n        }\n    }).build(key -> function(key));\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Caffeine提供了三种定时驱逐策略： \n \n expireAfterAccess(long, TimeUnit):在最后一次访问或者写入后开始计时，在指定的时间后过期。假如一直有请求访问该key，那么这个缓存将一直不会过期。 \n expireAfterWrite(long, TimeUnit): 在最后一次写入缓存后开始计时，在指定的时间后过期。 \n expireAfter(Expiry): 自定义策略，过期时间由Expiry实现独自计算。 \n \n 缓存的删除策略使用的是惰性删除和定时删除。这两个删除策略的时间复杂度都是O(1)。 \n 「3. 基于引用的过期方式」 \n Java中四种引用类型 \n \n // 当key和value都没有引用时驱逐缓存\nLoadingCache&lt;String, Object> cache = Caffeine.newBuilder()\n    .weakKeys()\n    .weakValues()\n    .build(key -> function(key));\n\n// 当垃圾收集器需要释放内存时驱逐\nLoadingCache&lt;String, Object> cache1 = Caffeine.newBuilder()\n    .softValues()\n    .build(key -> function(key));\n \n 1 2 3 4 5 6 7 8 9 10 注意：AsyncLoadingCache不支持弱引用和软引用。 \n \n Caffeine.weakKeys()：使用弱引用存储key。如果没有其他地方对该key有强引用，那么该缓存就会被垃圾回收器回收。由于垃圾回收器只依赖于身份(identity)相等，因此这会导致整个缓存使用身份 (==) 相等来比较 key，而不是使用 equals()。 \n Caffeine.weakValues() ：使用弱引用存储value。如果没有其他地方对该value有强引用，那么该缓存就会被垃圾回收器回收。由于垃圾回收器只依赖于身份(identity)相等，因此这会导致整个缓存使用身份 (==) 相等来比较 key，而不是使用 equals()。 \n Caffeine.softValues() ：使用软引用存储value。当内存满了过后，软引用的对象以将使用最近最少使用(least-recently-used ) 的方式进行垃圾回收。由于使用软引用是需要等到内存满了才进行回收，所以我们通常建议给缓存配置一个使用内存的最大值。softValues() 将使用身份相等(identity) (==) 而不是equals() 来比较值。 \n \n Caffeine.weakValues()和Caffeine.softValues()不可以一起使用。 \n  3. 移除事件监听 \n Cache&lt;String, Object> cache = Caffeine.newBuilder()\n    .removalListener((String key, Object value, RemovalCause cause) ->\n                     System.out.printf("Key %s was removed (%s)%n", key, cause))\n    .build();\n \n 1 2 3 4 #   4. 写入外部存储 \n CacheWriter 方法可以将缓存中所有的数据写入到第三方。 \n LoadingCache&lt;String, Object> cache2 = Caffeine.newBuilder()\n    .writer(new CacheWriter&lt;String, Object>() {\n        @Override public void write(String key, Object value) {\n            // 写入到外部存储\n        }\n        @Override public void delete(String key, Object value, RemovalCause cause) {\n            // 删除外部存储\n        }\n    })\n    .build(key -> function(key));\n \n 1 2 3 4 5 6 7 8 9 10 如果你有多级缓存的情况下，这个方法还是很实用。 \n 注意：CacheWriter不能与弱键或AsyncLoadingCache一起使用。 \n  5. 统计 \n 与Guava Cache的统计一样。 \n Cache&lt;String, Object> cache = Caffeine.newBuilder()\n    .maximumSize(10_000)\n    .recordStats()\n    .build();\n \n 1 2 3 4 通过使用Caffeine.recordStats(), 可以转化成一个统计的集合. 通过 Cache.stats() 返回一个CacheStats。CacheStats提供以下统计方法： \n \n hitRate(): 返回缓存命中率 \n evictionCount(): 缓存回收数量 \n averageLoadPenalty(): 加载新值的平均时间 \n \n'},{title:"雪花算法",frontmatter:{title:"雪花算法",date:"2022-1-08",author:"Gordon",sidebar:"auto",categories:["算法"],tags:["分布式id生成"]},regularPath:"/%E5%85%B6%E4%BB%96/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95.html",relativePath:"其他/雪花算法.md",key:"v-2e09059c",path:"/2022/01/08/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/",headers:[{level:2,title:"简介",slug:"简介"},{level:2,title:"原理",slug:"原理"},{level:2,title:"代码实现",slug:"代码实现"},{level:2,title:"优缺点",slug:"优缺点"},{level:2,title:"时钟回拨的解决方案",slug:"时钟回拨的解决方案"},{level:3,title:"问题阐述",slug:"问题阐述"},{level:3,title:"解决方案",slug:"解决方案"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:' 简介 \n 雪花算法，英文名为snowflake，翻译过来就是是雪花，所以叫雪花算法。 \n 在大自然雪花形成过程中，会形成不同的结构分支，所以说不存在两片完全一样的雪花，表示生成的id如雪花般独一无二。 \n \n 雪花算法，它最早是twitter内部使用的 分布式环境下的唯一分布式ID生成算法 。 \n 原理 \n 详细的雪花算法构造如下图所示： \n \n 雪花算法的原理：就是生成一个的 64 位的 long 类型的唯一 id， 主要分为如下4个部分组成： \n 1）1位保留 (基本不用) \n 1位标识：由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0，所以这第一位都是0。 \n 2）41位时间戳 \n 接下来 41 位存储毫秒级时间戳，41位可以表示2^41-1个毫秒的值，转化成单位年则是:(2^41−1)/(1000∗60∗60∗24∗365)=69年 。 \n 41位时间戳 ：也就是说这个时间戳可以使用69年不重复，大概可以使用 69 年。 \n 注意：41位时间截不是存储当前时间的时间截，而是存储时间截的差值“ 当前时间截 – 开始时间截 ”得到的值。 \n 这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的， 一般设置好后就不要去改变了，切记！！！ \n 因为，雪花算法有如下缺点：依赖服务器时间， 服务器时钟回拨时可能会生成重复 id 。 \n 3）10位机器 \n 10位的数据机器位，可以部署在1024个节点，包括5位datacenterId和5位workerId，最多可以部署 2^10=1024 台机器。 \n 这里的5位可以表示的最大正整数是2^5−1=31，即可以用0、1、2、3、….31这32个数字，来表示不同的datecenterId，或workerId。 \n 4） 12bit序列号 \n 用来记录同毫秒内产生的不同id，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号。 \n 理论上雪花算法方案的QPS约为409.6w/s，这种分配方式可以保证在任何一个IDC的任何一台机器在任意毫秒内生成的ID都是不同的。 \n 代码实现 \n public   class   IdWorker   { \n \n\t //因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。 \n\n\t //设置一个时间初始值    2^41 - 1   差不多可以用69年 \n\t private   long  twepoch  =   1585644268888L ; \n\n\t //机器ID  2进制5位  32位减掉1位 31个 \n\t private   long  workerId ; \n\t //机房ID 2进制5位  32位减掉1位 31个 \n\t private   long  datacenterId ; \n\t //代表一毫秒内生成的多个id的最新序号  12位 4096 -1 = 4095 个 \n\t private   long  sequence ; \n\t //5位的机器id \n\t private   long  workerIdBits  =   5L ; \n\t //5位的机房id \n\t private   long  datacenterIdBits  =   5L ; \n\t /**\n\t *\n\t *         -1的源码   10000001\n\t *         -1的反码   11111110\n\t *         -1的补码   11111111\n\t *         -1左移12位= 1111 1111 0000 0000 0000\n\t *         -1       = 1111 1111 1111 1111 1111\n\t *         异或运算  = 0000 0000 1111 1111 1111=4095\n\t *         因此sequenceMask的值为4095\n\t *\n\t */ \n\t // 这个是二进制运算，就是5 bit最多只能有31个数字，也就是说机器id最多只能是32以内 \n\t private   long  maxWorkerId  =   - 1L   ^   ( - 1L   <<  workerIdBits ) ; \n\t // 这个是一个意思，就是5 bit最多只能有31个数字，机房id最多只能是32以内 \n\t private   long  maxDatacenterId  =   - 1L   ^   ( - 1L   <<  datacenterIdBits ) ; \n\n\t //每毫秒内产生的id数 2 的 12次方 \n\t private   long  sequenceBits  =   12L ; \n\t private   long  sequenceMask  =   - 1L   ^   ( - 1L   <<  sequenceBits ) ; \n \n\t private   long  workerIdShift  =  sequenceBits ; \n\t private   long  datacenterIdShift  =  sequenceBits  +  workerIdBits ; \n\t private   long  timestampLeftShift  =  sequenceBits  +  workerIdBits  +  datacenterIdBits ; \n\n\t //记录产生时间毫秒数，判断是否是同1毫秒 \n\t private   long  lastTimestamp  =   - 1L ; \n\t public   long   getWorkerId ( ) { \n\t\t return  workerId ; \n\t } \n\t public   long   getDatacenterId ( )   { \n\t\t return  datacenterId ; \n\t } \n\t public   long   getTimestamp ( )   { \n\t\t return   System . currentTimeMillis ( ) ; \n\t } \n \n \n \n\t public   IdWorker ( long  workerId ,   long  datacenterId ,   long  sequence )   { \n \n\t\t // 检查机房id和机器id是否超过31 不能小于0 \n\t\t if   ( workerId  >  maxWorkerId  ||  workerId  <   0 )   { \n\t\t\t throw   new   IllegalArgumentException ( \n\t\t\t\t\t String . format ( "worker Id can\'t be greater than %d or less than 0" , maxWorkerId ) ) ; \n\t\t } \n \n\t\t if   ( datacenterId  >  maxDatacenterId  ||  datacenterId  <   0 )   { \n \n\t\t\t throw   new   IllegalArgumentException ( \n\t\t\t\t\t String . format ( "datacenter Id can\'t be greater than %d or less than 0" , maxDatacenterId ) ) ; \n\t\t } \n\t\t this . workerId  =  workerId ; \n\t\t this . datacenterId  =  datacenterId ; \n\t\t this . sequence  =  sequence ; \n\t } \n \n\t // 这个是核心方法，通过调用nextId()方法，让当前这台机器上的snowflake算法程序生成一个全局唯一的id \n\t public   synchronized   long   nextId ( )   { \n\t\t // 这儿就是获取当前时间戳，单位是毫秒 \n\t\t long  timestamp  =   timeGen ( ) ; \n\t\t if   ( timestamp  <  lastTimestamp )   { //时间回调抛出异常 \n \n\t\t\t System . err . printf ( \n\t\t\t\t\t "clock is moving backwards. Rejecting requests until %d." ,  lastTimestamp ) ; \n\t\t\t throw   new   RuntimeException ( \n\t\t\t\t\t String . format ( "Clock moved backwards. Refusing to generate id for %d milliseconds" , \n\t\t\t\t\t\t\tlastTimestamp  -  timestamp ) ) ; \n\t\t } \n \n\t\t // 下面是说假设在同一个毫秒内，又发送了一个请求生成一个id \n\t\t // 这个时候就得把seqence序号给递增1，最多就是4096 \n\t\t if   ( lastTimestamp  ==  timestamp )   { \n \n\t\t\t // 这个意思是说一个毫秒内最多只能有4096个数字，无论你传递多少进来， \n\t\t\t //这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围 \n\t\t\tsequence  =   ( sequence  +   1 )   &  sequenceMask ; \n\t\t\t //当某一毫秒的时间，产生的id数 超过4095，系统会进入等待，直到下一毫秒，系统继续产生ID \n\t\t\t if   ( sequence  ==   0 )   { \n\t\t\t\ttimestamp  =   tilNextMillis ( lastTimestamp ) ; \n\t\t\t } \n \n\t\t }   else   { \n\t\t\tsequence  =   0 ; \n\t\t } \n\t\t // 这儿记录一下最近一次生成id的时间戳，单位是毫秒 \n\t\tlastTimestamp  =  timestamp ; \n\t\t // 这儿就是最核心的二进制位运算操作，生成一个64bit的id \n\t\t // 先将当前时间戳左移，放到41 bit那儿；将机房id左移放到5 bit那儿；将机器id左移放到5 bit那儿；将序号放最后12 bit \n\t\t // 最后拼接起来成一个64 bit的二进制数字，转换成10进制就是个long型 \n\t\t return   ( ( timestamp  -  twepoch )   <<  timestampLeftShift )   | \n\t\t\t\t ( datacenterId  <<  datacenterIdShift )   | \n\t\t\t\t ( workerId  <<  workerIdShift )   |  sequence ; \n\t } \n \n\t /**\n\t * 当某一毫秒的时间，产生的id数 超过4095，系统会进入等待，直到下一毫秒，系统继续产生ID\n\t * @param lastTimestamp\n\t * @return\n\t */ \n\t private   long   tilNextMillis ( long  lastTimestamp )   { \n \n\t\t long  timestamp  =   timeGen ( ) ; \n \n\t\t while   ( timestamp  <=  lastTimestamp )   { \n\t\t\ttimestamp  =   timeGen ( ) ; \n\t\t } \n\t\t return  timestamp ; \n\t } \n\t //获取当前时间戳 \n\t private   long   timeGen ( ) { \n\t\t return   System . currentTimeMillis ( ) ; \n\t } \n \n\t /**\n\t *  main 测试类\n\t * @param args\n\t */ \n\t public   static   void   main ( String [ ]  args )   { \n\t\t System . out . println ( 1 & 4596 ) ; \n\t\t System . out . println ( 2 & 4596 ) ; \n\t\t System . out . println ( 6 & 4596 ) ; \n\t\t System . out . println ( 6 & 4596 ) ; \n\t\t System . out . println ( 6 & 4596 ) ; \n\t\t System . out . println ( 6 & 4596 ) ; \n //\t\tIdWorker worker = new IdWorker(1,1,1); \n //\t\tfor (int i = 0; i < 22; i++) { \n //\t\t\tSystem.out.println(worker.nextId()); \n //\t\t} \n\t } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 #  优缺点 \n 雪花算法，它至少有如下4个优点： \n 1.系统环境ID不重复 \n 能满足高并发分布式系统环境ID不重复，比如大家熟知的分布式场景下的数据库表的ID生成。 \n 2.生成效率极高 \n 在高并发，以及分布式环境下，除了生成不重复 id，每秒可生成百万个不重复 id，生成效率极高。 \n 3.保证基本有序递增 \n 基于时间戳，可以保证基本有序递增，很多业务场景都有这个需求。 \n 4.不依赖第三方库 \n 不依赖第三方的库，或者中间件，算法简单，在内存中进行。 \n 雪花算法，有一个比较大的缺点： \n 依赖服务器时间，服务器时钟回拨时可能会生成重复 id。 \n 时钟回拨的解决方案 \n 问题阐述 \n 既然是雪花算法的问题，那我们就来看下雪花算法出了什么问题： \n （1）What：雪花算法生成了重复的 ID，这些 ID 是什么样的？ \n （2）Why：雪花算法为什么生成了重复的 key \n 第一个问题，我们可以通过报错信息发现，这个重复的 ID 是  -1 ，这个就很奇怪了。一般雪花算法生成的唯一 ID 如下所示，我分别用二进制和十进制来表示： \n 十进制表示：2097167233578045440\n\n二进制表示：0001 1101 0001 1010 1010 0010 0111 1100 1101 1000 0000 0010 0001 0000 0000 0000\n \n 1 2 3 找到项目中使用雪花算法的工具类，生成 ID 的时候有个判断逻辑： \n \n 当 当前时间 小于 上次的生成时间 就会返回  -1 ，所以问题就出在这个逻辑上面。(上述代码的雪花算法是直接抛异常) \n \n if (timestamp < this.lastTimestamp) {\n   return -1;\n}\n \n 1 2 3 \n 由于每次  timestamp  都是小于 lastTimeStamp ，所以 每次都返回了 -1 ，这也解释了为什么生成了重复的 key。 \n 时钟回拨或跳跃 \n 那么问题就 聚焦 在为什么 当前时间 还会小于 上次的生成时间 。 \n 下面有种场景可能发生这种情况： \n 首先假定当前的北京时间是 9:00:00。另外上次生成 ID 的时候，服务器获取的时间 lastTimestamp=10:00:00，而现在服务器获取的当前时间 timestamp=09:00:00，这就相当于服务器之前是获取了一个未来时间，现在突然 跳跃 到当前时间。 \n 而这种场景我们称之为 时钟回拨 或 时钟跳跃 。 \n 时钟回拨 ：服务器时钟可能会因为各种原因发生不准，而网络中会提供 NTP 服务来做时间校准，因此在做校准的时候，服务器时钟就会发生时钟的跳跃或者回拨问题。 \n 时钟同步 \n 那么服务器为什么会发生时钟回拨或跳跃呢？ \n \n 我们猜测是不是服务器上的时钟不同步后，又自动进行同步了，前后时间不一致。 \n \n 首先我们的每台服务器上都安装了  ntpdate  软件，作为 NTP 客户端，会每隔  10 分钟 向  NTP 时间服务器 同步一次时间。 \n 如下图所示，服务器 1 和 服务器 2 部署了应用服务，每隔 10 分钟向 时间服务器 同步一次时间，来保证服务器 1 和服务器 2 的时间和 时间服务器 的时间一致。 \n \n 每隔 10 分钟同步的设置： \n */10 * * * * /usr/sbin/ntpdate <ip>\n \n 1 另外时间服务器会向  NTP Pool 同步时间，NTP Pool 正在为世界各地成百上千万的系统提供服务。它是绝大多数主流Linux发行版和许多网络设备的默认“时间服务器”。（参考ntppool.org） \n 那问题就是 NTP 同步出了问题？？ \n 时钟不同步 \n 我们到服务器上查看了下时间， 确实和时钟服务器不同步，早了几分钟。 \n 当我们执行 NTP 同步的命令后，时钟又同步了，也就是说时间回拨了。同步的命令如下： \n ntpdate  <时钟服务器 IP>\n \n 1 在产生事故之前，我们重启过服务器 1。我们推测服务器重启后，服务器因网络问题没有正常同步 。而在下一次定时同步操作到来之前的这个时间段，我们的后端服务已经出现了因 ID 重复导致的大量异常问题。 \n 这个 NTP 时钟回拨的偶发现象并不常见，但时钟回拨确实会带了很多问题，比如 润秒  问题也会带来 1s 时间的回拨。 \n 为了预防这种情况的发生，网上也有一些开源解决方案。 \n 解决方案 \n （1）方式一：使用美团 Leaf方案，基于雪花算法。 \n （2）方式二：使用百度 UidGenerator，基于雪花算法。 \n （3）方式三：用 Redis 生成自增的分布式 ID。弊端是 ID 容易被猜到，有安全风险。 \n 美团的Leaf方案 \n 美团的开源项目  Leaf  的方案：采用依赖  ZooKeeper  的数据存储。如果时钟回拨的时间超过最大容忍的毫秒数阈值，则程序报错；如果在可容忍的范围内，Leaf 会 等待时钟同步到最后一次主键生成的时间后再继续工作 。 \n 重点就是需要等待时钟同步！ \n 百度 UidGenerator 方案 \n 百度 UidGenerator 方案不在每次获取 ID 时都实时计算分布式 ID，而是利用 RingBuffer 数据结构，通过缓存的方式预生成一批唯一 ID 列表，然后通过 incrementAndGet() 方法获取下一次的时间，从而脱离了对服务器时间的依赖，也就不会有时钟回拨的问题。 \n 重点就是预生成一批 ID！ \n Github地址： \n https://github.com/baidu/uid-generator\n \n 1 '},{frontmatter:{},regularPath:"/%E5%85%B6%E4%BB%96/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84.html",relativePath:"其他/常见的索引树结构.md",key:"v-7ea9e72a",path:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84/",lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:""},{title:"Spring",frontmatter:{},regularPath:"/%E5%85%B6%E4%BB%96/spring.html",relativePath:"其他/spring.md",key:"v-63f06f0b",path:"/1970/01/01/spring/",headers:[{level:2,title:"Spring",slug:"spring-2"},{level:3,title:"简介",slug:"简介"},{level:3,title:"优点",slug:"优点"},{level:3,title:"组成",slug:"组成"},{level:2,title:"IOC",slug:"ioc"},{level:3,title:"IOC组成理论推导",slug:"ioc组成理论推导"},{level:3,title:"IOC本质",slug:"ioc本质"},{level:3,title:"IOC创建对象的方式【☆】",slug:"ioc创建对象的方式【☆】"},{level:2,title:"Spring的基础配置",slug:"spring的基础配置"},{level:3,title:"别名",slug:"别名"},{level:3,title:"Bean的配置",slug:"bean的配置"},{level:3,title:"import【☆】",slug:"import【☆】"},{level:2,title:"DI依赖注入【☆】",slug:"di依赖注入【☆】"},{level:3,title:"构造器注入",slug:"构造器注入"},{level:3,title:"set方式注入 【重点】",slug:"set方式注入-【重点】"},{level:3,title:"拓展：p、c 标签注入",slug:"拓展-p、c-标签注入"},{level:3,title:"Bean的作用域",slug:"bean的作用域"},{level:2,title:"Bean的自动装配",slug:"bean的自动装配"},{level:3,title:"byName与byType自动装配",slug:"byname与bytype自动装配"},{level:3,title:"使用注解实现自动装配",slug:"使用注解实现自动装配"},{level:3,title:"@Autowired与@Resource",slug:"autowired与-resource"},{level:3,title:"5.4 使用注解开发",slug:"_5-4-使用注解开发"},{level:3,title:"5.5 使用java的方式配置Spring",slug:"_5-5-使用java的方式配置spring"},{level:2,title:"6 代理模式",slug:"_6-代理模式"},{level:3,title:"6.1静态代理",slug:"_6-1静态代理"},{level:3,title:"6.2 动态代理",slug:"_6-2-动态代理"},{level:2,title:"7.AOP",slug:"_7-aop"},{level:3,title:"7.1 什么是AOP",slug:"_7-1-什么是aop"},{level:3,title:"7.2 Aop在Spring中的作用",slug:"_7-2-aop在spring中的作用"},{level:3,title:"7.3 使用Spring实现Aop",slug:"_7-3-使用spring实现aop"},{level:2,title:"8.整合Mybatis",slug:"_8-整合mybatis"},{level:3,title:"8.1整合Mybatis方式一",slug:"_8-1整合mybatis方式一"},{level:3,title:"8.1整合Mybatis方式二",slug:"_8-1整合mybatis方式二"},{level:2,title:"9.Spring配置声明事务注入",slug:"_9-spring配置声明事务注入"},{level:2,title:"10.致谢",slug:"_10-致谢"},{level:2,title:"11.多模块项目打包",slug:"_11-多模块项目打包"},{level:3,title:"一，创建Maven多模块项目",slug:"一-创建maven多模块项目"},{level:3,title:"entity 的 pom.xml 内容",slug:"entity-的-pom-xml-内容"},{level:3,title:"dao 的 pom.xml 内容",slug:"dao-的-pom-xml-内容"},{level:3,title:"service 模块的 pom.xml 内容",slug:"service-模块的-pom-xml-内容"},{level:3,title:"web模块的 pom.xml 内容",slug:"web模块的-pom-xml-内容"},{level:3,title:"三、代码测试",slug:"三、代码测试"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:' Spring \n Spring \n 简介 \n ·     Spring：春天 给软件行业带来了春天 \n ·     2002，首次推出了Spring框架的雏形：interface21框架 \n ·     Spring框架即是以interface21框架为基础经过重新设计，并不断丰富其内涵，于2004年3月24日发布了1.0正式版 \n ·      Rod Johnson  Spring Framework创始人，著名作者。 Rod在悉尼大学不仅获得了计算机学位，同时还获得了音乐学位。更令人吃惊的是在回到软件开发领域之前，他还获得了音乐学的博士学位。 有着相当丰富的C/C++技术背景的Rod早在1996年就开始了对Java服务器端技术的研究 \n ·     Spring理念：使现有的技术更加容易使用，本身是一个大杂烩，整合了现有的技术框架！ \n ·     SSH :Struct2 + Spring + Hibernate \n ·     SSM：SpringMVC + SPring +Mybatis \n ·     官网：https://spring.io/ \n ·     官方下载地址：http://repo.spring.io/release/org/springframework/spring \n ·     GitHub：https://github.com/spring-projects/spring-framework \n ·     Spring核心技术地址：https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#beans-annotation-config \n 优点 \n \n Spring是一个开源的免费的框架（容器）！ \n Spring是一个轻量级的，非入侵式的框架 \n 控制反转（IOC），面向切面编程（AOP） \n 支持事务的处理，对框架整合的支持！ \n \n Spring就是一个轻量级的控制反转（IOC）和切面编程（AOP）的框架！ \n 组成 \n IOC \n IOC组成理论推导 \n 新建一个普通maven项目spring-study,删除src,作为父项目,在项目下新建一个子module \n 在父项目导包 \n < dependencies > \n         < dependency > \n             < groupId > org.springframework </ groupId > \n             < artifactId > spring-webmvc </ artifactId > \n             < version > 5.2.0.RELEASE </ version > \n         </ dependency > \n     </ dependencies > \n \n 1 2 3 4 5 6 7 在spring-01-ioc1下 \n 新建dao层，service层 \n \n 原来的实现方式 \n1.UserDao接口 \n public   interface   UserDao   { \n     void   getUser ( ) ; \n } \n \n 1 2 3 2.UserDaoImpl实现类 \n public   class   UserDaoImpl   implements   UserDao { \n     public   void   getUser ( )   { \n         System . out . println ( "默认获取用户的数据" ) ; \n     } \n } \n \n 1 2 3 4 5 3.UserService业务接口 \n public interface UserService {\n    void getUser();\n}\n \n 1 2 3 4.UserServiceImpl实现类 \n public class UserServiceImpl implements UserService{\n \n    private UserDao userDao = new UserDaoImpl();\n \n    public void getUser() {\n        userDao.getUser();\n    }\n}\n \n 1 2 3 4 5 6 7 8 测试 \n public   class   MyTest   { \n     public   static   void   main ( String [ ]  args )   { \n         UserService  userService  =   new   UserServiceImpl ( ) ; \n        userService . getUser ( ) ; \n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 把Userdao的实现类增加一个 \n public   class   UserDaoMySqlImpl   implements   UserDao   { \n    @Override \n    public   void   getUser ( )   { \n        System . out . println ( "MySql获取用户数据" ) ; \n   } \n } \n \n 1 2 3 4 5 6 紧接着我们要去使用MySql的话 , 我们就需要去service实现类里面修改对应的实现 \n public   class   UserServiceImpl   implements   UserService { \n \n     //private UserDao userDao = new UserDaoImpl(); \n     private   UserDao  userDao  =   new   UserDaoMySqlImpl ( ) ; \n\n     public   void   getUser ( )   { \n        userDao . getUser ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 再假设, 我们再增加一个Userdao的实现类 \n public   class   UserDaoOracleImpl   implements   UserDao   { \n    @Override \n    public   void   getUser ( )   { \n        System . out . println ( "Oracle获取用户数据" ) ; \n   } \n } \n \n 1 2 3 4 5 6 如果想要改变，就需要每次更改UserDao \n public   class   UserServiceImpl   implements   UserService { \n \n     //private UserDao userDao = new UserDaoImpl(); \n     //private UserDao userDao = new UserDaoMySqlImpl(); \n     private   UserDao  userDao  =   new   UserDaoOracleImpl ( ) ; \n\n     public   void   getUser ( )   { \n        userDao . getUser ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 若将UesrDao使用Set接口实现 \n public   class   UserServiceImpl   implements   UserService { \n \n     //private UserDao userDao = new UserDaoImpl(); \n     //private UserDao userDao = new UserDaoMySqlImpl(); \n     //private UserDao userDao = new UserDaoOracleImpl(); \n     private   UserDao  userDao ; \n     // 利用set实现 \n     public   void   setUserDao ( UserDao  userDao )   { \n         this . userDao  =  userDao ; \n     } \n\n     public   void   getUser ( )   { \n        userDao . getUser ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 测试 \n public   class   MyTest   { \n     public   static   void   main ( String [ ]  args )   { \n        /* UserService userService = new UserServiceImpl();\n        userService.getUser();*/ \n         UserServiceImpl  userService  =   new   UserServiceImpl ( ) ; \n         //默认用户数据 \n        userService . setUserDao ( new   UserDaoImpl ( ) ) ; \n        userService . getUser ( ) ; \n         //mysql用户数据 \n        userService . setUserDao ( new   UserDaoMySqlImpl ( ) ) ; \n        userService . getUser ( ) ; \n         //oracle用户数据 \n        userService . setUserDao ( new   UserDaoOracleImpl ( ) ) ; \n        userService . getUser ( ) ; \n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \n 之前，程序通过new主动创建对象！控制权在程序猿手上(UserServiceImpl) \n 使用set注入后，程序不再具有主动性，而是变成了被动的接受对象！ \n 这种思想，从本质上解决了问题，程序员不用再去管理对象的创建了，降低了耦合性！ \n IOC本质 \n 控制反转IOC（Inversion of Control），是一种设计思想，DI（依赖注入）是实现IOC的一种方法，  也有人认为DI只是IOC的另一种说法。没有IOC的程序中，我们使用面向对象编程，对象的创建与对象间的依赖关系完全硬编码在程序中，对象的创建由程序自己控制，控制反转后将对象的创建转移给第三方，个人认为所谓的控制反转就是：获得依赖的方式反转了。 \n \n IoC是Spring框架的核心内容 ，使用多种方式完美的实现了IoC，可以使用XML配置，也可以使用注解，新版本的Spring也可以零配置实现IoC。 \n Spring容器在初始化时先读取配置文件，根据配置文件或元数据创建与组织对象存入容器中，程序使用时再从Ioc容器中取出需要的对象。 \n \n 采用XML方式配置Bean的时候，Bean的定义信息是和实现分离的，而采用注解的方式可以把两者合为一体，Bean的定义信息直接以注解的形式定义在实现类中，从而达到了零配置的目的。 \n 控制反转是一种通过描述（xml或注解）并通过第三方去生产或获取特定对象的方式。在spring中实现控制反转的是IOC容器，其实现方法是依赖注入（Dependency Injection，DI） \n 新建spring-02-hellospring module \n \n 新建实体类 \n public   class   Hello   { \n     private   String  name ; \n \n     public   String   getName ( )   { \n         return  name ; \n     } \n \n     public   void   setName ( String  name )   { \n         this . name  =  name ; \n     } \n \n     @Override \n     public   String   toString ( )   { \n         return   "Hello{"   + \n                 "name=\'"   +  name  +   \'\\\'\'   + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 新建beans.xml文件 \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     \x3c!--使用Spring来创建对象，在Spring这些都称为Bean--\x3e \n     < bean   id = " hello "   class = " com.learning.pojo.Hello " > \n         < property   name = " name "   value = " spring " /> \n     </ bean > \n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 测试 \n public   class   MyTest   { \n \n     public   static   void   main ( String [ ]  args )   { \n         //获取spring的上下文对象 \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         //我们的对象现在都在spring中管理，我们要使用，直接去里面取出来 \n         //Hello hello = (Hello) context.getBean("hello"); \n         //getBean的另一种用法 \n         Hello  hello  =  context . getBean ( "hello" ,   Hello . class ) ; \n         System . out . println ( hello . toString ( ) ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 会发现，已经不需要手动new对象，对象是在xml文件中配置。或者通俗来讲，不需要改底层代码，而xml文件不算底层代码。 \n 修改案例一，我们在案spring-01-ioc1中， 新增一个Spring配置文件beans.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " MysqlImpl "   class = " com.learning.dao.UserDaoMySqlImpl " /> \n     < bean   id = " OracleImpl "   class = " com.learning.dao.UserDaoOracleImpl " /> \n\n     < bean   id = " ServiceImpl "   class = " com.learning.service.UserServiceImpl " > \n         \x3c!--注意: 这里的name并不是属性 , 而是set方法后面的那部分 , 首字母小写--\x3e \n         \x3c!--引用另外一个bean , 不是用value 而是用 ref--\x3e \n         < property   name = " userDao "   ref = " OracleImpl " /> \n     </ bean > \n\n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 测试 \n public   class   MyTest   { \n     public   static   void   main ( String [ ]  args )   { \n         //原生的方式 \n        /* UserService userService = new UserServiceImpl();\n        userService.getUser();*/ \n\n        //ioc理论推导 \n         /*UserServiceImpl userService = new UserServiceImpl();\n        //默认用户数据\n        userService.setUserDao(new UserDaoImpl());\n        userService.getUser();\n        //mysql用户数据\n        userService.setUserDao(new UserDaoMySqlImpl());\n        userService.getUser();\n        //oracle用户数据\n        userService.setUserDao(new UserDaoOracleImpl());\n        userService.getUser();*/ \n\n         //使用xml方式 \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         UserServiceImpl  serviceImpl  =  context . getBean ( "ServiceImpl" ,   UserServiceImpl . class ) ; \n        serviceImpl . getUser ( ) ; \n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 控制反转 \n \n 控制：  谁来控制对象的创建，传统应用程序的对象是由程序本身控制创建的，使用Spring后，对象是由Spring来创建的 \n 反转 **😗* 程序本身不创建对象，而变成被动的接收对象。 \n 依赖注入：  就是利用set方法来进行注入的 \n IOC 是一种编程思想，由主动的编程变为被动的接收，所谓的 IOC **，即对象由 Spring 来创建，管理，装配** \n IOC创建对象的方式【☆】 \n 新建 spring-03-ioc2 module \n \n \n 默认使用无参构造创建对象 \n 新建实体类 User \n public   class   User   { \n\n    private   String  name ; \n\n    public   User ( )   { \n        System . out . println ( "user无参构造方法" ) ; \n   } \n\n     public   User ( String  name )   { \n         this . name  =  name ; \n         System . out . println ( "user有参构造方法" ) ; \n     } \n\n     public   void   setName ( String  name )   { \n        this . name  =  name ; \n   } \n\n    public   void   show ( ) { \n        System . out . println ( "name=" +  name  ) ; \n   } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " user "   class = " com.learning.pojo.User " > \n         < property   name = " name "   value = " gordon " /> \n     </ bean > \n </ beans >  \n \n 1 2 3 4 5 6 7 8 9 10 测试 \n public   class   Mytest   { \n     public   static   void   main ( String [ ]  args )   { \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         //在执行getBean的时候, user已经创建好了 , 通过无参构造 \n         User  user  =  context . getBean ( "user" ,   User . class ) ; \n        user . show ( ) ; \n     } \n } \n\n //结果打印  \nuser无参构造方法\nname = gordon\n \n 1 2 3 4 5 6 7 8 9 10 11 12 结果可以发现，在调用show方法之前，User对象已经通过无参构造初始化了！ \n \n \n 使用有参构造创建对象的三种方式 \n \n \n public   class   UserT   { \n \n     private   String  name ; \n     private   int  age ; \n\n     public   UserT ( )   { \n         System . out . println ( "user无参构造方法" ) ; \n     } \n     public   UserT ( String  name ,   int  age )   { this . name  =  name ; this . age  =  age ; \n         System . out . println ( "user有参构造方法" ) ; } \n     public   String   getName ( )   { return  name ; } \n     public   void   setName ( String  name )   { this . name  =  name ; } \n     public   int   getAge ( )   { return  age ; } \n     public   void   setAge ( int  age )   { this . age  =  age ; } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 有参构造的三种创建方式 \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " user "   class = " com.learning.pojo.User " > \n         < property   name = " name "   value = " gordon " /> \n     </ bean > \n     \x3c!-- 第一种根据index参数下标设置 --\x3e \n     < bean   id = " user1 "   class = " com.learning.pojo.UserT " > \n         \x3c!-- index指构造方法 , 下标从0开始 --\x3e \n         < constructor-arg   index = " 0 "   value = " 张三 " /> \n         < constructor-arg   index = " 1 "   value = " 18 " /> \n     </ bean > \n     \x3c!-- 第二种根据参数类型设置 --\x3e \n     < bean   id = " user2 "   class = " com.learning.pojo.UserT " > \n         < constructor-arg   type = " int "   value = " 18 " /> \n         < constructor-arg   type = " java.lang.String "   value = " 张三 " /> \n     </ bean > \n     \x3c!-- 第三种根据参数名字设置 --\x3e \n     < bean   id = " user3 "   class = " com.learning.pojo.UserT " > \n         \x3c!-- name指参数名 --\x3e \n         < constructor-arg   name = " name "   value = " 张三 " /> \n         < constructor-arg   name = " age "   value = " 18 " /> \n     </ bean > \n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 测试 \n public   class   Mytest   { \n     public   static   void   main ( String [ ]  args )   { \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         //在执行getBean的时候, user已经创建好了 , 通过无参构造 \n        /* User user = context.getBean("user", User.class);\n        user.show();*/ \n\n        //有参构造 \n         //第一种根据index参数下标设置 \n         UserT  user1  =  context . getBean ( "user1" ,   UserT . class ) ; \n         System . out . println ( "user1 = "   +  user1 ) ; \n         //第二种根据参数类型设置 \n         UserT  user2  =  context . getBean ( "user2" ,   UserT . class ) ; \n         System . out . println ( "user2 = "   +  user2 ) ; \n         //第三种根据参数名字设置 \n         UserT  user3  =  context . getBean ( "user3" ,   UserT . class ) ; \n         System . out . println ( "user3 = "   +  user3 ) ; \n\n     } \n } \n\n\n //结果 \nuser无参构造方法   -- 配置加载的时候初始化无参构造\nuser有参构造方法  \nuser有参构造方法\nuser有参构造方法\nuser1  =   UserT { name = \'张三\' ,  age = 18 } \nuser2  =   UserT { name = \'张三\' ,  age = 18 } \nuser3  =   UserT { name = \'张三\' ,  age = 18 } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 在获取spring的上下文对象（ new ClassPathXmlApplicationContext(“beans.xml”); ）时，spring容器中的所有的对象就已经被创建了。 \n Spring的基础配置 \n 别名 \n \x3c!--如果添加了别名，通过别名也可以获取对象--\x3e \n     < alias   name = " user "   alias = " userAlias " /> \n \n 1 2 #  Bean的配置 \n \n         \x3c!--bean就是java对象,由Spring创建和管理--\x3e \n\n \x3c!--\n   id 是bean的标识符,要唯一,如果没有配置id,name就是默认标识符\n   如果配置id,又配置了name,那么name是别名\n   name可以设置多个别名,可以用逗号,分号,空格隔开\n   如果不配置id和name,可以根据applicationContext.getBean(.class)获取对象;\n\nclass是bean的全限定名=包名+类名\n--\x3e \n     < bean   id = " user4 "   class = " com.learning.pojo.UserT "   name = " userAlias4 userAlias5,userAlias6;userAlias7 " > \n         \x3c!-- name指参数名 --\x3e \n         < constructor-arg   name = " name "   value = " 张三 " /> \n         < constructor-arg   name = " age "   value = " 18 " /> \n     </ bean > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 测试 \n public   class   Mytest   { \n     public   static   void   main ( String [ ]  args )   { \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         //别名 \n         //通过alias \n         UserT  userAlias3  =  context . getBean ( "userAlias3" ,   UserT . class ) ; \n         System . out . println ( "userAlias3 = "   +  userAlias3 ) ; \n         //通过bean name \n         Object  userAlias4  =  context . getBean ( "userAlias4" ) ; \n         System . out . println ( "userAlias4 = "   +  userAlias4 ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 #  import【☆】 \n import，一般用于团队开发使用，他可以将多个配置文件，导入合并为1个 \n假设，现在项目中又多个人开发，这三个人负责不同的类开发，不同的类需要注册在不同的bean中，我们可以用import将所有人的beans.xml合并为一个总的！ \n 修改案例一，不同dao实现类由不同的成员开发注入配置。 \n \n \n beans.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " defaultImpl "   class = " com.learning.pojo.UserDaoImpl " /> \n\n     < bean   id = " default "   class = " com.learning.service.UserServiceImpl " > \n         \x3c!--注意: 这里的name并不是属性 , 而是set方法后面的那部分 , 首字母小写--\x3e \n         \x3c!--引用另外一个bean , 不是用value 而是用 ref--\x3e \n         < property   name = " userDao "   ref = " defaultImpl " /> \n     </ bean > \n\n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n \n beans1.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " OracleImpl "   class = " com.learning.pojo.UserDaoOracleImpl " /> \n\n     < bean   id = " ServiceImpl "   class = " com.learning.service.UserServiceImpl " > \n         \x3c!--注意: 这里的name并不是属性 , 而是set方法后面的那部分 , 首字母小写--\x3e \n         \x3c!--引用另外一个bean , 不是用value 而是用 ref--\x3e \n         < property   name = " userDao "   ref = " OracleImpl " /> \n     </ bean > \n\n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n \n beans2.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " mysqlImpl "   class = " com.learning.pojo.UserDaoMySqlImpl " /> \n\n     < bean   id = " mysql "   class = " com.learning.service.UserServiceImpl " > \n         \x3c!--注意: 这里的name并不是属性 , 而是set方法后面的那部分 , 首字母小写--\x3e \n         \x3c!--引用另外一个bean , 不是用value 而是用 ref--\x3e \n         < property   name = " userDao "   ref = " mysqlImpl " /> \n     </ bean > \n\n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n \n 合并applicationContext.xml \n \n \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     \x3c!-- <import resource="{path}/beans.xml"/> --\x3e \n     < import   resource = " beans.xml " /> \n     < import   resource = " beans1.xml " /> \n     < import   resource = " beans2.xml " /> \n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 测试 \n public   class   MyTest   { \n     public   static   void   main ( String [ ]  args )   { \n\n         //import 合并成员xml \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "applicationContext.xml" ) ; \n         UserServiceImpl  aDefault  =  context . getBean ( "default" ,   UserServiceImpl . class ) ; \n        aDefault . getUser ( ) ; \n         UserServiceImpl  oracle  =  context . getBean ( "oracle" ,   UserServiceImpl . class ) ; \n        oracle . getUser ( ) ; \n         UserServiceImpl  mysql  =  context . getBean ( "mysql" ,   UserServiceImpl . class ) ; \n        mysql . getUser ( ) ; \n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #  DI依赖注入【☆】 \n 构造器注入 \n 默认无参，有参有三种。 \n set方式注入 【重点】 \n 依赖注入 \n依赖：bean对象的创建依赖于容器 \n注入：bean对象中的所有属性，由容器来注入 \n 【环境搭建】 \n 新建子module spring-04-di \n 1.复杂类型 \n public   class   Address   { \n \n     private   String  address ; \n \n     public   String   getAddress ( )   { \n         return  address ; \n     } \n \n     public   void   setAddress ( String  address )   { \n         this . address  =  address ; \n     } \n     @Override \n     public   String   toString ( )   { \n         return   "Address{"   + \n                 "address=\'"   +  address  +   \'\\\'\'   + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 2.真实测试对象 \n package   com . learning . pojo ; \n\n import   java . util . * ; \n\n public   class   Student   { \n \n     private   String  name ; \n     private   Address  address ; \n     private   String   [ ]  books ; \n     private   List < String >  hobbies ; \n     private   Map < String ,   String >  card ; \n     private   Set < String >  games ; \n     private   String  wife ; \n     private   Properties  info ; \n\n     public   String   getName ( )   { \n         return  name ; \n     } \n\n     public   void   setName ( String  name )   { \n         this . name  =  name ; \n     } \n\n     public   Address   getAddress ( )   { \n         return  address ; \n     } \n\n     public   void   setAddress ( Address  address )   { \n         this . address  =  address ; \n     } \n\n     public   String [ ]   getBooks ( )   { \n         return  books ; \n     } \n\n     public   void   setBooks ( String [ ]  books )   { \n         this . books  =  books ; \n     } \n\n     public   List < String >   getHobbies ( )   { \n         return  hobbies ; \n     } \n\n     public   void   setHobbies ( List < String >  hobbies )   { \n         this . hobbies  =  hobbies ; \n     } \n\n     public   Map < String ,   String >   getCard ( )   { \n         return  card ; \n     } \n\n     public   void   setCard ( Map < String ,   String >  card )   { \n         this . card  =  card ; \n     } \n\n     public   Set < String >   getGames ( )   { \n         return  games ; \n     } \n\n     public   void   setGames ( Set < String >  games )   { \n         this . games  =  games ; \n     } \n\n     public   String   getWife ( )   { \n         return  wife ; \n     } \n\n     public   void   setWife ( String  wife )   { \n         this . wife  =  wife ; \n     } \n\n     public   Properties   getInfo ( )   { \n         return  info ; \n     } \n\n     public   void   setInfo ( Properties  info )   { \n         this . info  =  info ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "Student{"   + \n                 "name=\'"   +  name  +   \'\\\'\'   + \n                 ", address="   +  address . toString ( )   + \n                 ", books="   +   Arrays . toString ( books )   + \n                 ", hobbies="   +  hobbies  + \n                 ", card="   +  card  + \n                 ", games="   +  games  + \n                 ", wife=\'"   +  wife  +   \'\\\'\'   + \n                 ", info="   +  info  + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 3.beans.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " addr "   class = " com.learning.pojo.Address " > \n         < property   name = " address "   value = " 深圳 " /> \n     </ bean > \n\n\n     < bean   id = " student "   class = " com.learning.pojo.Student " > \n         \x3c!--第一种 常量注入 value--\x3e \n         < property   name = " name "   value = " gordon " /> \n         \x3c!--第二种 Bean注入， 注意点：这里的值是一个引用，ref--\x3e \n         < property   name = " address "   ref = " addr " /> \n         \x3c!--数组注入--\x3e \n         < property   name = " books " > \n             < array > \n                 \x3c!--value 中什么加引号，什么时候不加--\x3e \n                 < value > 红楼梦 </ value > \n                 < value > 三国演义 </ value > \n                 < value > 水浒传 </ value > \n                 < value > 西游记 </ value > \n             </ array > \n         </ property > \n         \x3c!--List注入--\x3e \n         < property   name = " hobbies " > \n             < list > \n                 < value > 听歌 </ value > \n                 < value > 看电影 </ value > \n                 < value > 爬山 </ value > \n             </ list > \n         </ property > \n         \x3c!--map 注入--\x3e \n         < property   name = " card " > \n             < map > \n                 < entry   key = " 中国邮政 "   value = " 456456456465456 " /> \n                 < entry   key = " 建设 "   value = " 1456682255511 " /> \n             </ map > \n         </ property > \n         \x3c!--set 注入--\x3e \n         < property   name = " games " > \n             < set > \n                 < value > LOL </ value > \n                 < value > COC </ value > \n                 < value > BOB </ value > \n             </ set > \n         </ property > \n         \x3c!--Null注入--\x3e \n         < property   name = " wife " > < null /> </ property > \n         \x3c!--Properties注入--\x3e \n         < property   name = " info " > \n             < props > \n                 < prop   key = " 学号 " > 20190604 </ prop > \n                 < prop   key = " 性别 " > 男 </ prop > \n                 < prop   key = " 姓名 " > 小明 </ prop > \n             </ props > \n         </ property > \n\n     </ bean > \n\n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 4.测试类 \n public   class   MyTest   { \n     public   static   void   main ( String [ ]  args )   { \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         Student  student  =  context . getBean ( "student" ,   Student . class ) ; \n         System . out . println ( student ) ; \n     } \n } \n /*\nStudent{name=\'gordon\', address=Address{address=\'深圳\'}, books=[红楼梦, 三国演义, 水浒传, 西游记], hobbies=[听歌, 看电影, 爬山], card={中国邮政=456456456465456, 建设=1456682255511}, games=[LOL, COC, BOB], wife=\'null\', info={学号=20190604, 性别=男, 姓名=小明}}\n*/ \n\n \n 1 2 3 4 5 6 7 8 9 10 11 #  拓展：p、c 标签注入 \n 官方 \n \n p****标签注入，须在beans中引入 xmlns:p="http://www.springframework.org/schema/p" \n c****标签注入，需在实体中增加有参构造方法，并引入 xmlns:c="http://www.springframework.org/schema/c" \n 修改案例三，新建一个xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xmlns: p = " http://www.springframework.org/schema/p " \n        xmlns: c = " http://www.springframework.org/schema/c " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     \x3c!--c命名空间注入，通过构造器注入：construct-args--\x3e \n     \x3c!--【注意：需有参构造器！】--\x3e \n     < bean   id = " userc "   class = " com.learning.pojo.UserT "   c: name = " gordon "   c: age = " 18 " /> \n     \x3c!--P(属性: properties)命名空间 , 属性依然要设置set方法--\x3e \n     < bean   id = " userp "   class = " com.learning.pojo.UserT "   p: name = " gordon "   p: age = " 18 " /> \n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 测试 \n public   class   Mytest   { \n     public   static   void   main ( String [ ]  args )   { \n         //p c命名空间方式注入 \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans1.xml" ) ; \n         UserT  userc  =  context . getBean ( "userc" ,   UserT . class ) ; \n         System . out . println ( "userc = "   +  userc ) ; \n         UserT  userp  =  context . getBean ( "userp" ,   UserT . class ) ; \n         System . out . println ( "userp = "   +  userp ) ; \n     } \n } \n //打印结果 \n /*\nuser有参构造方法\nuser无参构造方法\nuserc = UserT{name=\'gordon\', age=18}\nuserp = UserT{name=\'gordon\', age=18}\n*/ \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #  Bean的作用域 \n **1.** 单例模式（Spring默认机制） \n <bean id="accountService" class="com.something.DefaultAccountService" scope="singleton"/>\n \n 1 \n**2.** 原型模式：每次从容器中get对象时，都重新创建 \n <bean id="accountService" class="com.something.DefaultAccountService" scope="prototype"/>\n \n 1 \n**3.** 其余的request、session、application、websocket这些只能在web开发中使用 \n Bean的自动装配 \n \n 自动装配是spring满足bean依赖的一种方式 \n Spring会在上下文中自动寻找，并自动给bean装配属性 \n \n 在Spring中由三种装配方式 \n \n 在xml中显式配置 \n 在java中显式配置 \n 隐式的自动装配bean \n \n Spring的自动装配需要从两个角度来实现，或者说是两个操作： \n 1.组件扫描(component scanning)：spring会自动发现应用上下文中所创建的bean； \n 2.自动装配(autowiring)：spring自动满足bean之间的依赖，也就是我们说的IoC/DI； \n 组件扫描和自动装配组合发挥巨大威力，使得显示的配置降低到最少。 \n 推荐不使用自动装配xml配置 , 而使用注解 . \n 【环境搭建】 \n 1.新建一个项目spring-05-autowired \n 2.新建两个实体类，Cat  Dog  都有一个叫的方法 \n public   class   Cat   { \n     public   void   shout ( )   { \n         System . out . println ( "miao~" ) ; \n     } \n } \n \n 1 2 3 4 5 public   class   Dog   { \n     public   void   shout ( )   { \n         System . out . println ( "wang~" ) ; \n     } \n } \n \n 1 2 3 4 5 3.新建一个用户类 \n public   class   User   { \n     private   Cat  cat ; \n     private   Dog  dog ; \n     private   String  str ; \n\n     public   Cat   getCat ( )   { \n         return  cat ; \n     } \n\n     public   void   setCat ( Cat  cat )   { \n         this . cat  =  cat ; \n     } \n\n     public   Dog   getDog ( )   { \n         return  dog ; \n     } \n\n     public   void   setDog ( Dog  dog )   { \n         this . dog  =  dog ; \n     } \n\n     public   String   getStr ( )   { \n         return  str ; \n     } \n\n     public   void   setStr ( String  str )   { \n         this . str  =  str ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "User{"   + \n                 "cat="   +  cat  + \n                 ", dog="   +  dog  + \n                 ", str=\'"   +  str  +   \'\\\'\'   + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 4.编写beans.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n        http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " dog "   class = " com.learning.pojo.Dog " /> \n     < bean   id = " cat "   class = " com.learning.pojo.Cat " /> \n     < bean   id = " user "   class = " com.learning.pojo.User " > \n         < property   name = " cat "   ref = " cat " /> \n         < property   name = " dog "   ref = " dog " /> \n         < property   name = " str "   value = " gordon " /> \n     </ bean > \n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #  byName与byType自动装配 \n autowire byName (按名称自动装配) \n 由于在手动配置xml过程中，常常发生字母缺漏和大小写等错误，而无法对其进行检查，使得开发效率降低。 \n 采用自动装配将避免这些错误，并且使配置简单化。 \n 测试： \n 1、修改bean配置，增加一个属性  autowire="byName" \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n        http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " dog "   class = " com.learning.pojo.Dog " /> \n     < bean   id = " cat0 "   class = " com.learning.pojo.Cat " /> \n     < bean   id = " user "   class = " com.learning.pojo.User " > \n         < property   name = " cat "   ref = " cat0 " /> \n         < property   name = " dog "   ref = " dog " /> \n         < property   name = " str "   value = " gordon " /> \n     </ bean > \n     \x3c!--byName 没有cat 会报空指针异常--\x3e \n     < bean   id = " cat "   class = " com.learning.pojo.Cat " /> \n     < bean   id = " userByName "   class = " com.learning.pojo.User "   autowire = " byName " > \n         < property   name = " str "   value = " gordon " /> \n     </ bean > \n     </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 2、再次测试，结果依旧成功输出！ \n public   class   MyTest   { \n     @Test \n     public   void   testMethodAutowire ( )   { \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         User  user  =  context . getBean ( "userByName" ,   User . class ) ; \n        user . getCat ( ) . shout ( ) ; \n        user . getDog ( ) . shout ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 3、我们将 cat 的bean id修改为 catXXX \n 4、再次测试， 执行时报空指针java.lang.NullPointerException。因为按byName规则找不对应set方法，真正的setCat就没执行，对象就没有初始化，所以调用时就会报空指针错误。 \n 小结： \n 当一个bean节点带有 autowire byName的属性时。 \n 将查找其类中所有的set方法名，例如setCat，获得将set去掉并且首字母小写的字符串，即cat。 \n 去spring容器中寻找是否有此字符串名称id的对象。 \n 如果有，就取出注入；如果没有，就报空指针异常。 \n autowire byType (按类型自动装配) \n 使用autowire byType首先需要保证：同一类型的对象，在spring容器中唯一。如果不唯一，会报不唯一NoUniqueBeanDefinitionException的异常。 \n 测试： \n 1、将user的bean配置修改一下 ： autowire="byType" \n 2、测试，正常输出 \n 3、在注册一个cat 的bean对象！ \n < bean   id = " dog "   class = " com.learning.pojo.Dog " /> \n < bean   id = " cat "   class = " com.learning.pojo.Cat " /> \n < bean   id = " cat2 "   class = " com.learning.pojo.Cat " /> \n < bean   id = " user "   class = " com.learning.pojo.User "   autowire = " byType " > \n     < property   name = " str "   value = " gordon " /> \n </ bean > \n \n 1 2 3 4 5 6 4、测试，报错：NoUniqueBeanDefinitionException \n 5、删掉cat2，将cat的bean名称改掉！测试！因为是按类型装配，所以并不会报异常，也不影响最后的结果。甚至将id属性去掉，也不影响结果。 \n 这就是按照类型自动装配！ \n 小结： \nbyName的时候，需要保证所有bean的id唯一，并且这个bean需要和自动注入的属性的set方法的值一致 \nbyType的时候，需要保证所有bean的class唯一，并且这个bean需要和自动注入的属性的类型一致 \n 使用注解实现自动装配 \n jdk1.5支持的注解 Spring2.5支持的注解 \nThe introduction of annotation-based configuration raised the question of whether this approach is “better” than XML \n 【项目环境搭建】 \n 修改案例五，新建beans2.xml \n 使用注解须知： \n2.1.导入约束：context约束 \n（xmlns:context=“http://www.springframework.org/schema/context” \nhttp://www.springframework.org/schema/context \nhttps://www.springframework.org/schema/context/spring-context.xsd） \n2.2.配置注解的支持context:annotation-config \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n       xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n       xmlns: context = " http://www.springframework.org/schema/context " \n       xmlns: aop = " http://www.springframework.org/schema/aop " \n       xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       https://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context\n       https://www.springframework.org/schema/context/spring-context.xsd\n       http://www.springframework.org/schema/aop\n       https://www.springframework.org/schema/aop/spring-aop.xsd " > \n \n    \x3c!--开启注解支持--\x3e \n    < context: annotation-config /> \n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Autowired \n \n @Autowired是按类型自动转配的，不支持id匹配。 \n 需要导入 spring-aop的包！ \n \n 测试： \n 1、将User类中的set方法去掉，使用@Autowired注解 \n public   class   User   { \n     @Autowired \n     private   Cat  cat ; \n     @Autowired \n     private   Dog  dog ; \n     private   String  str ; \n\n     public   Cat   getCat ( )   { \n         return  cat ; \n     } \n\n     public   Dog   getDog ( )   { \n         return  dog ; \n     } \n\n     public   String   getStr ( )   { \n         return  str ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "User{"   + \n                 "cat="   +  cat  + \n                 ", dog="   +  dog  + \n                 ", str=\'"   +  str  +   \'\\\'\'   + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 2、此时配置文件内容 \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xmlns: context = " http://www.springframework.org/schema/context " \n        xmlns: aop = " http://www.springframework.org/schema/aop " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       https://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context\n       https://www.springframework.org/schema/context/spring-context.xsd\n       http://www.springframework.org/schema/aop\n       https://www.springframework.org/schema/aop/spring-aop.xsd " > \n\n     \x3c!--开启注解支持--\x3e \n     < context: annotation-config /> \n     \x3c!--没开启注解，没使用autowire自动装配--\x3e \n     \x3c!--\n    <bean id="dog" class="com.learning.pojo.Dog"/>\n    <bean id="cat" class="com.learning.pojo.Cat"/>\n    <bean id="user" class="com.learning.pojo.User">\n        <property name="cat" ref="cat"/>\n        <property name="dog" ref="dog"/>\n        <property name="str" value="gordon"/>\n    </bean>\n    --\x3e \n     \x3c!--没开启注解，使用autowire自动装配--\x3e \n     \x3c!--\n     byName 没有cat 会报空指针异常\n    <bean id="userByName" class="com.learning.pojo.User" autowire="byName">\n        <property name="str" value="gordon"/>\n    </bean>\n\n    byType\n    <bean id="userByType" class="com.learning.pojo.User" autowire="byType">\n        <property name="str" value="gordon"/>\n    </bean>\n    --\x3e \n     < bean   id = " dog "   class = " com.learning.pojo.Dog " /> \n     < bean   id = " cat "   class = " com.learning.pojo.Cat " /> \n     < bean   id = " user "   class = " com.learning.pojo.User " /> \n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 3、测试 \n public   class   MyTest   { \n     @Test \n     public   void   annoTest ( ) { \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         User  user  =  context . getBean ( "user" ,   User . class ) ; \n        user . getCat ( ) . shout ( ) ; \n        user . getDog ( ) . shout ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 #  @Autowired与@Resource \n @Autowired \n直接在属性上使用即可！也可以在set方式上使用 \n使用Autowired我们可以不用编写set方法了，前提是你这个自动装配的属性在IOC（Spring）容器中存在，且符合名字byName \n 科普: \n @Autowired(required=false)  说明：false，对象可以为null；true，对象必须存对象，不能为null。； \n      //如果定义了Autowired的required属性为false，说明这个对象可以为null，否则不允许为空 \n     @Autowired ( required  =   false ) \n     private   Cat  cat ; \n     @Autowired \n     private   Dog  dog ; \n     private   String  name ; \n \n 1 2 3 4 5 6 如果@Autowired自动装配的环境比较复杂，自动装配无法通过一个注解【@Autowired】完成的时候，我们可以使用@Qualifier（value = “xxx”）去配置@Autowired的使用，指定一个唯一的bean对象注入！ \n public   class   User   { \n     @Autowired ( required  =   false ) \n     private   Cat  cat ; \n     @Autowired \n     @Qualifier ( value  =   "dog2" ) \n     private   Dog  dog ; \n     private   String  str ; \n\n     public   Cat   getCat ( )   { \n         return  cat ; \n     } \n\n     public   Dog   getDog ( )   { \n         return  dog ; \n     } \n\n     public   String   getStr ( )   { \n         return  str ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "User{"   + \n                 "cat="   +  cat  + \n                 ", dog="   +  dog  + \n                 ", str=\'"   +  str  +   \'\\\'\'   + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 < bean   id = " dog "   class = " com.learning.pojo.Dog " /> \n     < bean   id = " cat "   class = " com.learning.pojo.Cat " /> \n     < bean   id = " user "   class = " com.learning.pojo.User " /> \n     \x3c!--dog type不唯一 使用@Qualifier（value = “xxx”）去配置@Autowired的使用，指定一个唯一的bean对象注入！--\x3e \n     < bean   id = " dog2 "   class = " com.learning.pojo.Dog " /> \n \n 1 2 3 4 5 @Resource****注解，不指定name值，先去判断byName和byType，有一个能注入即成功 \n public   class   People   { \n     @Resource ( name  =   "xxxx" ) \n     private   Cat  cat ; \n \n 1 2 3 小结：@Resource和@Autowired的区别 \n \n 都是用来自动装配的，都可以放在属性字段上 \n @Autowired通过byType的方式实现，而且必须要求这个对象存在！ \n @Resource默认通过byName的方式实现，如果找不到名字，则通过byType实现！如果两个都找不到的情况下，就报错！ \n 执行顺序不同：@Autowired通过byType的方式实现。@Resource默认通过byName的方式实现。 \n 5.4 使用注解开发 \n 在Spring4之后，要使用注解开发，必须保证aop的包导入了 \n \n使用注解需要导入context约束，增加注解的支持！ \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n \n    \x3c!--开启注解支持--\x3e\n    <context:annotation-config/>\n    \x3c!--指定要扫描的包，这个包下的注解就会生效--\x3e\n    <context:component-scan base-package="com.yang"/>\n</beans>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1. bean****注入使用@Componet注解 \n //@Component 等价于<bean id="user" class="com.yang.entity.User"/>\n@Component\npublic class User {\n    String name;\n}\n \n 1 2 3 4 5 2.   属性注入使用@Value注解 \n //@Component 等价于<bean id="user" class="com.yang.entity.User"/>\n@Component\npublic class User {\n \n    String name;\n    //@Value("yang") 相当于<property name="name" value="yang"/>\n    @Value("yang")\n    public void setName(String name) {\n        this.name = name;\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 3.   衍生注解 \n@Componet有几个衍生注解，我们在web开发中，会按照mvc三层架构分层！ \n \n dao层 【@Repository】 \n service层 【@Service】 \n controller层 【@Controller】 \n这四个注解功能都是一样的，都是代表将某个类注册到Spring中，装配Bean \n \n **4.** 自动装配 \n @Autowired  自动装配通过类型、名字\n                  如果Autowired不能唯一自动装配上属性，则需要通过@Qualifier(value="xxx")\n@Nullable   字段标记了这个注解，说明这个字段可以为null\n@Resource 自动装配通过名字，类型\n \n 1 2 3 4 5.   作用域 \n@Scope(“singleton”)单例 \n 6.   小结 \nXML 与 注解 \n \n xml更加万能，适用于任何场合！维护简单方便 \n 注解不是自己类使用不了， 维护相对复杂 \n \n XML 与 注解最佳实践 \n \n xml用来管理bean \n 注解只负责完成属性的注入 \n 我们在使用过程中，只需要注意一个问题：必须让注解生效，就需要开启注解的支持 \n \n \x3c!--开启注解支持--\x3e\n   <context:annotation-config/>\n   \x3c!--指定要扫描的包，这个包下的注解就会生效--\x3e\n   <context:component-scan base-package="com.yang"/>\n \n 1 2 3 4 #  5.5 使用java的方式配置Spring \n \n实体类 \n package com.yang.entity;\n \nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.stereotype.Component;\n \n@Component\npublic class User {\n \n    private String name;\n \n    public String getName() {\n        return name;\n    }\n    @Value("Yang")\n    public void setName(String name) {\n        this.name = name;\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 配置类 \n import org.springframework.context.annotation.Configuration;\nimport org.springframework.context.annotation.Import;\n \n//@Configuration,这个也会被Spring容器托管，注册到容器中，因为打开注解，它本身就被定义为组件了@Component\n//@Configuration该注解代表了这是一个配置类，与applicationContext.xml一样\n@Configuration\n@ComponentScan("com.yang.entity")\n@Import(YangConfig2.class)\npublic class YangConfig {\n \n    //注册一个Bean，就相当于我们之前写的一个bean标签\n    //方法名字 == bean标签的id\n    //方法的返回值 == bean标签中的class属性\n \n    @Bean\n    public User getUser () {\n        return new User();//就是返回要注入到bean的对象\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 测试类 \n public class MyTest {\n \n    public static void main(String[] args) {\n \n        ApplicationContext annotationConfigApplicationContext = new AnnotationConfigApplicationContext(YangConfig.class);\n        User getUser = (User)annotationConfigApplicationContext.getBean("getUser");\n        System.out.println(getUser.getName());\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 #  6 代理模式 \n 6.1静态代理 \n 角色分析： \n \n 抽象角色：一般会使用接口或者抽象类来解决 \n 真实角色：被代理的角色 \n 代理角色：代理真实角色，代理真实角色后，我们一般会做一些附属操作 \n 客户：访问代理对象的人 \n \n 代码步骤： \n \n 接口 \n 真实角色 \n 代理角色 \n 客户端访问代理角色 \n 具体代码可参考： 多线程中的静态代理 \n \n 代理模式的好处： \n \n 可以使真实角色的操作更加存粹！不用去关注一些公共的业务 \n 公共交给了代理角色，实现了业务的分工 \n 公共业务发生扩展的时候，方便集中管理 \n \n 缺点： \n \n 一个真实角色就会产生一个代理角色，代码量会翻倍 开发效率变低 \n 6.2 动态代理 \n \n 动态代理和静态代理角色一样 \n 动态代理的代理类是动态生成的，不是我们直接写好的 \n 动态代理分为两大类：基于接口的动态代理，基于类的动态代理 \n基于接口：JDK动态代理 \n基于类： cglib \njava字节码实现： javasist \n \n 需要了解两个类：Proxy：代理 InvocationHandler：调用处理程序 \n Proxy \n java.lang.reflect.Proxy \nProxy提供了创建动态代理类和实例的静态方法，它也是由这些方法创建的所有动态代理类的超类。(大白话：这是一个静态类，类里边有方法得到代理类) \n 动态代理类 （以下简称为代理类 ）是一个实现在类创建时在运行时指定的接口列表的类，具有如下所述的行为。 代理接口是由代理类实现的接口。 代理实例是代理类的一个实例。 每个代理实例都有一个关联的调用处理程序对象，它实现了接口InvocationHandler 。 通过其代理接口之一的代理实例上的方法调用将被分派到实例调用处理程序的invoke方法，传递代理实例， java.lang.reflect.Method被调用方法的java.lang.reflect.Method对象以及包含参数的类型Object Object的数组。 调用处理程序适当地处理编码方法调用，并且返回的结果将作为方法在代理实例上调用的结果返回。 \n 代理类具有以下属性： \n \n 代理类是公共的，最终的，而不是抽象的，如果所有代理接口都是公共的。 \n 如果任何代理接口是非公开的，代理类是非公开的，最终的，而不是抽象的 。 \n 代理类的不合格名称未指定。 然而，以字符串"$Proxy"开头的类名空间应该保留给代理类。 \n 一个代理类扩展了java.lang.reflect.Proxy 。 \n 代理类完全按照相同的顺序实现其创建时指定的接口。 \n 如果一个代理类实现一个非公共接口，那么它将被定义在与该接口相同的包中。 否则，代理类的包也是未指定的。 请注意，程序包密封不会阻止在运行时在特定程序包中成功定义代理类，并且类也不会由同一类加载器定义，并且与特定签名者具有相同的包。 \n 由于代理类实现了在其创建时指定的所有接口， getInterfaces在其类对象上调用getInterfaces将返回一个包含相同列表接口的数组（按其创建时指定的顺序），在其类对象上调用getMethods将返回一个数组的方法对象，其中包括这些接口中的所有方法，并调用getMethod将在代理接口中找到可以预期的方法。 \nProxy.isProxyClass方法将返回true，如果它通过代理类 - 由Proxy.getProxyClass返回的类或由Proxy.newProxyInstance返回的对象的类 - 否则为false。 \n 所述java.security.ProtectionDomain代理类的是相同由引导类装载程序装载系统类，如java.lang.Object ，因为是由受信任的系统代码生成代理类的代码。 此保护域通常将被授予java.security.AllPermission 。 \n 每个代理类有一个公共构造一个参数，该接口的实现InvocationHandler ，设置调用处理程序的代理实例。 而不必使用反射API来访问公共构造函数，也可以通过调用Proxy.newProxyInstance方法来创建代理实例，该方法将调用Proxy.getProxyClass的操作与调用处理程序一起调用构造函数。 \n \n //为某个接口创建代理Foo ： \n  InvocationHandler handler = new MyInvocationHandler(...);\n     Class<?> proxyClass = Proxy.getProxyClass(Foo.class.getClassLoader(), Foo.class);\n     Foo f = (Foo) proxyClass.getConstructor(InvocationHandler.class).\n                     newInstance(handler); \n  //或更简单地： \n  Foo f = (Foo) Proxy.newProxyInstance(Foo.class.getClassLoader(),\n                                          new Class<?>[] { Foo.class },\n                                          handler);\npublic static 类<?> getProxyClass(ClassLoader loader,\n                                     类<?>... interfaces)\n                              throws IllegalArgumentException\n/*给定类加载器和接口数组的代理类的java.lang.Class对象。 代理类将由指定的类加载器定义，并将实现所有提供的接口。 如果任何给定的接口是非公开的，则代理类将是非公开的。 如果类加载器已经定义了接口相同置换的代理类，那么将返回现有的代理类; 否则，这些接口的代理类将被动态生成并由类加载器定义。 \n对可能传递给Proxy.getProxyClass的参数有几个Proxy.getProxyClass ： \n \ninterfaces数组中的所有类对象都必须表示接口，而不是类或原始类型。 \ninterfaces数组中没有两个元素可能是指相同的类对象。 \n所有的接口类型必须通过指定的类加载器的名称可见。 换句话说，对于类加载器cl和每个接口i ，以下表达式必须为真： \n  Class.forName(i.getName(), false, cl) == i 所有非公共接口必须在同一个包中; 否则代理类将不可能实现所有接口，而不管其中定义了什么包。 \n对于具有相同签名的指定接口的任何成员方法集合： \n如果任何方法的返回类型是原始类型或void，则所有方法必须具有相同的返回类型。 \n否则，其中一个方法必须具有一个返回类型，该类型可以分配给其余方法的所有返回类型。 \n生成的代理类不能超过虚拟机对类施加的任何限制。 例如，VM可以将类可以实现的接口数量限制为65535; 在这种情况下， interfaces阵列的大小不得超过65535。 \n如果任何这些限制被违反， Proxy.getProxyClass将抛出一个IllegalArgumentException 。 如果interfaces数组参数或其任何元素为null ，则将抛出一个NullPointerException 。 \n \n请注意，指定的代理接口的顺序是重要的：具有相同组合的接口但不同顺序的代理类的两个请求将导致两个不同的代理类。 \n \n参数 \nloader - 类加载器来定义代理类 \ninterfaces - 要实现的代理类的接口列表 \n结果 \n在指定的类加载器中定义并实现指定接口的代理类 */\n \npublic static Object newProxyInstance(ClassLoader loader,\n                                      类<?>[] interfaces,\n                                      InvocationHandler h)\n                               throws IllegalArgumentException\n/*返回指定接口的代理类的实例，该接口将方法调用分派给指定的调用处理程序。 \nProxy.newProxyInstance因为与IllegalArgumentException相同的原因而Proxy.getProxyClass 。 \n \n参数 \nloader - 类加载器来定义代理类 \ninterfaces - 代理类实现的接口列表 \nh - 调度方法调用的调用处理函数 \n结果 \n具有由指定的类加载器定义并实现指定接口的代理类的指定调用处理程序的代理实例 */\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 InvocationHandler \n InvocationHandler是由代理实例的调用处理程序实现的接口 。 \n每个代理实例都有一个关联的调用处理程序。 当在代理实例上调用方法时，方法调用将被编码并分派到其调用处理程序的invoke方法。 \ninvoke(Object proxy, 方法 method, Object[] args) 处理代理实例上的方法调用并返回结果。 \n Object invoke(Object proxy,\n              方法 method,\n              Object[] args)\n       throws Throwable处理代理实例上的方法调用并返回结果。 当在与之关联的代理实例上调用方法时，将在调用处理程序中调用此方法。 \n/*参数 \nproxy - 调用该方法的代理实例 \nmethod -所述方法对应于调用代理实例上的接口方法的实例。 方法对象的声明类将是该方法声明的接口，它可以是代理类继承该方法的代理接口的超级接口。 \nargs -包含的方法调用传递代理实例的参数值的对象的阵列，或null如果接口方法没有参数。 原始类型的参数包含在适当的原始包装器类的实例中，例如java.lang.Integer或java.lang.Boolean 。 \n结果 \n从代理实例上的方法调用返回的值。 如果接口方法的声明返回类型是原始类型，则此方法返回的值必须是对应的基本包装类的实例; 否则，它必须是可声明返回类型的类型。 如果此方法返回的值是null和接口方法的返回类型是基本类型，那么NullPointerException将由代理实例的方法调用抛出。 如上所述，如果此方法返回的值，否则不会与接口方法的声明的返回类型兼容，一个ClassCastException将代理实例的方法调用将抛出。 \n异常 \nThrowable - 从代理实例上的方法调用抛出的异常。 异常类型必须可以分配给接口方法的throws子句中声明的任何异常类型java.lang.RuntimeException检查的异常类型java.lang.RuntimeException或java.lang.Error 。 如果检查的异常是由这种方法是不分配给任何的中声明的异常类型throws接口方法的子句，则一个UndeclaredThrowableException包含有由该方法抛出的异常将通过在方法调用抛出代理实例。 */\n \n 1 2 3 4 5 6 7 8 9 10 11 12 编写实例 \n 接口 \n package com.yang.demo;\npublic interface UserServiceInterface {\n    public void add();\n    public void delete();\n    public void update();\n    public void select();\n}\n \n 1 2 3 4 5 6 7 接口实现类 \n package com.yang.demo;\npublic class UserServiceImpl implements UserServiceInterface{\n    public void add() {\n        System.out.println("增加一个用户");\n    }\n    public void delete() {\n        System.out.println("删除一个用户");\n    }\n    public void update() {\n        System.out.println("更新一个用户");\n    }\n    public void select() {\n        System.out.println("检索一个用户");\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 创建代理工具类 \n package com.yang.demo;\nimport java.lang.reflect.InvocationHandler;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Proxy;\n//自动生成代理类的类\npublic class ProxyInvocationHandler implements InvocationHandler {\n \n    //被代理的接口\n    private Object target;\n \n    public void setTarget (Object target) {\n        this.target = target;\n    }\n    //重写的InvocationHandler中的invoke方法\n    //处理代理实例，并返回结果\n    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n        log(method.getName());\n        Object invoke = method.invoke(target, args);\n        return invoke;\n    }\n    //生成得到代理类\n    public Object getProxy () {\n        return Proxy.newProxyInstance(this.getClass().getClassLoader(),\n                target.getClass().getInterfaces(), this);\n    }\n    public void log (String msg) {\n        System.out.println("执行了" + msg + "方法");\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 测试类 \n package com.yang.demo;\n \npublic class Client {\n    public static void main(String[] args) {\n        //真实角色\n        UserServiceImpl userService = new UserServiceImpl();\n        //代理角色，不存在\n        ProxyInvocationHandler proxyInvocationHandler = new ProxyInvocationHandler();\n        //设置要代理的对象\n        proxyInvocationHandler.setTarget(userService);\n        //注意要用接口强转，否则会报异常\n        UserServiceInterface proxy = (UserServiceInterface)proxyInvocationHandler.getProxy();\n        proxy.add();\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #  7.AOP \n 7.1 什么是AOP \n AOP（Aspect Oriented Programming）意为：面向切面编程，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术，AOP是OOP的延续，是软件开发中的一个热点，也是Spring框架中的一个重要内容，是函数式编程的一种衍生泛型。利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。 \n 7.2 Aop在Spring中的作用 \n 提供生命事务：允许用户自定义切面 \n \n 横切关注点：跨越应用程序多个模块的方法或功能。即与我们的业务逻辑无关的，但是我们需要关注的部分，就是横切关注点。如日志，安全，缓存，事务等等。。。 \n 切面（ASPECT）：横切关注点 被模块化的特殊对象。即 它是一个类 \n 通知（Advice）：切面必须要完成的工作，即 他是类中的一个方法 \n 目标（target）：被通知的对象 \n 代理（Proxy）：向目标对象应用通知之后创建的对象 \n 切入点（PointCut）：切面通知 执行的"地点"的定义 \n 连接点（jointPoint）：与切入点匹配的执行点 \n \n ![在这里插入图片描述](file:///C:/Users/GORDON~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.png) \nSpringAop中，通过Advice定义横切逻辑，Spring中支持的5种类型的Advice \n 7.3 使用Spring实现Aop \n 【重点】使用AOP织入，需要依赖包 \n <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.9.4</version>\n</dependency>\n \n 1 2 3 4 5 方式一：使用Spring的API接口 \neg:在执行UserService实现类的所有方法时，增加日志功能 \n UserServer接口 \n package com.yang.service;\npublic interface UserService {\n    public void add();\n    public void update();\n    public void delete();\n    public void select();\n}\n \n 1 2 3 4 5 6 7 UserServer实现类 \n package com.yang.service;\npublic class UserServiceImpl implements UserService{\n    public void add() {\n        System.out.println("增加了一个用户");\n    }\n    public void update() {\n        System.out.println("更新了一个用户");\n    }\n    public void delete() {\n        System.out.println("删除了一个用户");\n    }\n    public void select() {\n        System.out.println("检索了一个用户");\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Log类 \n import org.springframework.aop.AfterReturningAdvice;\nimport org.springframework.aop.MethodBeforeAdvice;\nimport java.lang.reflect.Method;\npublic class Log implements MethodBeforeAdvice, AfterReturningAdvice {\n    //method:要执行的目标对象的方法（method being invoked）\n    //object:参数（args: arguments to the method）\n    //o:目标对象 （target：target of the method invocation）\n    public void before(Method method, Object[] args, Object target) throws Throwable {\n        System.out.println(target.getClass().getName() + "的" + method.getName() + "被执行了");\n    }\n    //returnValue:返回值\n    public void afterReturning(Object returnValue, Method method, Object[] args, Object target) throws Throwable {\n        System.out.println("执行了" + method.getName() + "方法，返回值为" + returnValue);\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 配置文件 \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n \n    \x3c!--注册bean--\x3e\n    <bean id="userService" class="com.yang.service.UserServiceImpl"/>\n    <bean id="log" class="com.yang.log.Log"/>\n    \x3c!--方式：使用原生Spring Api接口--\x3e\n    \x3c!--配置aop--\x3e\n    <aop:config>\n        \x3c!--切入点：execution:表达式，execution(*(修饰词) *(返回值) *(类名) *(方法名) *(参数))  ..任意参数--\x3e\n        <aop:pointcut id="pointcut" expression="execution(* com.yang.service.UserServiceImpl.*(..))"/>\n \n        \x3c!--执行环绕增加--\x3e\n        <aop:advisor advice-ref="log" pointcut-ref="pointcut"/>\n    </aop:config>\n</beans>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 测试类 \n import com.yang.service.UserService;\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.support.ClassPathXmlApplicationContext;\npublic class MyTest {\n    public static void main(String[] args) {\n        ApplicationContext classPathXmlApplicationContext = new ClassPathXmlApplicationContext("applicationContext.xml");\n        UserService userService = classPathXmlApplicationContext.getBean("userService", UserService.class);\n        userService.add();\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 方式二：自定义来实现AOP【主要是切面定义】 \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n \n    \x3c!--注册bean--\x3e\n    <bean id="userService" class="com.yang.service.UserServiceImpl"/>\n    <bean id="log" class="com.yang.log.Log"/>\n    <bean id="diy" class="com.yang.diy.DiyPointCut"/>\n    \x3c!--方式2：自定义类--\x3e\n    <aop:config>\n       \x3c!--<aop:aspect ref="diy"> : 标注这个类为切面--\x3e\n        <aop:aspect ref="diy">\n            \x3c!--切入点--\x3e\n            <aop:pointcut id="point" expression="execution(* com.yang.service.UserServiceImpl.*(..))"/>\n            \x3c!--通知--\x3e\n            <aop:before method="beforeMethod" pointcut-ref="point"/>\n        </aop:aspect>\n    </aop:config>\n</beans>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 自定义类 \n package com.yang.diy;\npublic class DiyPointCut {\n    public void beforeMethod () {\n        System.out.println("方法执行之前");\n    }\n}\n \n 1 2 3 4 5 6 方式三：使用注解实现AOP \nXML文件 \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n \n    \x3c!--注册bean--\x3e\n    <bean id="userService" class="com.yang.service.UserServiceImpl"/>\n    \x3c!--方式3：使用注解--\x3e\n    \x3c!--开启注解支持  JDK（默认proxy-target-class="false"）cglib默认proxy-target-class="true"）--\x3e\n    <aop:aspectj-autoproxy proxy-target-class="false"/>\n    <bean id="annotationPointCut" class="com.yang.diy.AnnotationPointcut"/>\n<beans/>\npackage com.yang.diy;\n \nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.Signature;\nimport org.aspectj.lang.annotation.After;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Before;\n \n@Aspect\npublic class AnnotationPointcut {\n    @Before("execution(* com.yang.service.UserServiceImpl.*(..))")\n    public void before () {\n        System.out.println("====方法执行前====");\n    }\n    @After("execution(* com.yang.service.UserServiceImpl.*(..))")\n    public void after () {\n        System.out.println("====方法执行后====");\n    }\n    //在环绕增强中，我们可以给定一个参数，代表我们要获取处理切入的点\n    @Around("execution(* com.yang.service.UserServiceImpl.*(..))")\n    public void around (ProceedingJoinPoint pjp) throws Throwable {\n        System.out.println("环绕前");\n        Signature signature = pjp.getSignature();//获得签名\n        System.out.println("signature" + signature);\n \n        Object proceed = pjp.proceed();//执行方法\n        System.out.println("环绕后");\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \n 8.整合Mybatis \n 8.1整合Mybatis方式一 \n 步骤： \n \n 导入相关 jar 包 \njunit \nmybatis \nmysql数据库 \nspring相关的 \naop织入 \nmybatis-spring \n \n <?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <parent>\n        <artifactId>spring-study</artifactId>\n        <groupId>com.yang</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n \n    <artifactId>spring-10-mybatis</artifactId>\n \n    <dependencies>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.13</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>5.1.47</version>\n        </dependency>\n        <dependency>\n            <groupId>org.mybatis</groupId>\n            <artifactId>mybatis</artifactId>\n            <version>3.5.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-webmvc</artifactId>\n            <version>5.2.0.RELEASE</version>\n        </dependency>\n        \x3c!--Spring操作数据库的话，还需要一个spring-jdbc--\x3e\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-jdbc</artifactId>\n            <version>5.1.9.RELEASE</version>\n        </dependency>\n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.9.4</version>\n        </dependency>\n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjrt</artifactId>\n            <version>1.8.13</version>\n        </dependency>\n        <dependency>\n            <groupId>org.mybatis</groupId>\n            <artifactId>mybatis-spring</artifactId>\n            <version>2.0.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.projectlombok</groupId>\n            <artifactId>lombok</artifactId>\n            <version>1.16.10</version>\n        </dependency>\n    </dependencies>\n \n    <build>\n        <resources>\n            <resource>\n                <directory>src/main/java</directory>\n                <includes>\n                    <include>**/*.xml</include>\n                </includes>\n                <filtering>true</filtering>\n            </resource>\n        </resources>\n    </build>\n</project>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 \n 编写配置文件 \nspring-dao.xml配置数据源DataSource与sqlSession \n \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n    \x3c!--DataSource：使用Spring的数据源替换Mybatis的配置  c3p0  dbcp  druid\n    我们这里使用Spring提供的JDBC：org.springframework.jdbc.datasource--\x3e\n    <bean id="dataSource" class="org.springframework.jdbc.datasource.DriverManagerDataSource">\n        <property name="driverClassName" value="com.mysql.jdbc.Driver"/>\n        <property name="url" value="jdbc:mysql://localhost:3306/mybatis?useSSL=true&amp;useUnicode=false&amp;characterEncoding=utf-8"/>\n        <property name="username" value="root"/>\n        <property name="password" value="root"/>\n    </bean>\n    \x3c!--sqlSessionFactory--\x3e\n    <bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean">\n        <property name="dataSource" ref="dataSource" />\n        \x3c!--绑定Mybatis配置文件--\x3e\n        <property name="configLocation" value="classpath:mybatis-config.xml"/>\n        <property name="mapperLocations" value="classpath:com/yang/mapper/*.xml"/>\n    </bean>\n \n    <bean id="sqlSession" class="org.mybatis.spring.SqlSessionTemplate">\n        \x3c!--只能使用构造器注入sqlSessionFactory，因为没有set方法--\x3e\n        <constructor-arg index="0" ref="sqlSessionFactory"/>\n    </bean>\n</beans>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 mybatis-config.xml配置一些mybatis专属配置 \n <?xml version="1.0" encoding="UTF-8" ?>\n<!DOCTYPE configuration\n        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"\n        "http://mybatis.org/dtd/mybatis-3-config.dtd">\n<configuration>\n    <typeAliases>\n        <package name="com.yang.entity"/>\n    </typeAliases>\n</configuration>\n \n 1 2 3 4 5 6 7 8 9 applicationContext.xml整合与注册bean等 \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n    <import resource="spring-dao.xml"/>\n    <bean id="userMapper" class="com.yang.mapper.UserMapperImpl">\n        <property name="sqlSessionTemplate" ref="sqlSession"/>\n    </bean>\n</beans>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 \n 测试 \n实体类 \n \n package com.yang.entity;\nimport lombok.Data;\n@Data\npublic class User {\n    private int id;\n    private String name;\n    private int pwd;\n}\n \n 1 2 3 4 5 6 7 8 mapper接口 \n package com.yang.mapper;\nimport com.yang.entity.User;\nimport java.util.List;\n \npublic interface UserMapper {\n    public List<User> selectUser();\n}\n \n 1 2 3 4 5 6 7 mapper实现类 \n package com.yang.mapper;\n \nimport com.yang.entity.User;\nimport org.mybatis.spring.SqlSessionTemplate;\n \nimport java.util.List;\n \npublic class UserMapperImpl implements UserMapper{\n \n    //我们的所有操作，原来都是用sqlSession来执行，现在都是用sqlSessionTemplate\n    private SqlSessionTemplate sqlSessionTemplate;\n \n    public void setSqlSessionTemplate (SqlSessionTemplate sqlSessionTemplate) {\n        this.sqlSessionTemplate = sqlSessionTemplate;\n    }\n    public List<User> selectUser() {\n        UserMapper mapper = sqlSessionTemplate.getMapper(UserMapper.class);\n        return mapper.selectUser();\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mapper映射文件 \n <?xml version="1.0" encoding="UTF-8" ?>\n<!DOCTYPE mapper\n        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"\n        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">\n<mapper namespace="com.yang.mapper.UserMapper">\n    <select id="selectUser" resultType="user">\n        select * from mybatis.user;\n    </select>\n</mapper>\n \n 1 2 3 4 5 6 7 8 9 测试类 \n import com.yang.entity.User;\nimport com.yang.mapper.UserMapper;\nimport org.junit.Test;\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.support.ClassPathXmlApplicationContext;\n \nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.util.List;\n \npublic class MyTest {\n    @Test\n    public void test () throws IOException {\n        ApplicationContext classPathXmlApplicationContext = new ClassPathXmlApplicationContext("applicationContext.xml");\n \n        UserMapper userMapper = classPathXmlApplicationContext.getBean("userMapper", UserMapper.class);\n        List<User> userList = userMapper.selectUser();\n \n        for (User user : userList) {\n            System.out.println(user);\n        }\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #  8.1整合Mybatis方式二 \n SqlSessionDaoSupport \n SqlSessionDaoSupport 是一个抽象的支持类，用来为你提供 SqlSession。调用 getSqlSession() 方法你会得到一个 SqlSessionTemplate，之后可以用于执行 SQL 方法，就像下面这样: \n package com.yang.mapper;\n \nimport com.yang.entity.User;\nimport org.apache.ibatis.session.SqlSession;\nimport org.mybatis.spring.support.SqlSessionDaoSupport;\n \nimport java.util.List;\npublic class UserMapperImpl2 extends SqlSessionDaoSupport implements UserMapper {\n    public List<User> selectUser() {\n        SqlSession sqlSession = getSqlSession();\n        UserMapper mapper = sqlSession.getMapper(UserMapper.class);\n        List<User> users = mapper.selectUser();\n        //return getSqlSession().getMapper(UserMapper.class).selectUser();\n        return users;\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 实际上是整合mybatis一与整合mybatis二是一样的方法，只不过二继承了SqlSessionDaoSupport ，在getSqlSession()，做的也是setSqlSessionTemplate \n 9.Spring配置声明事务注入 \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xmlns:tx="http://www.springframework.org/schema/tx"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd\n        http://www.springframework.org/schema/tx\n        http://www.springframework.org/schema/tx/spring-tx.xsd">\n \n    <import resource="spring-dao.xml"/>\n \n    <bean id="userMapper" class="com.yang.mapper.UserMapperImpl2">\n        <property name="sqlSessionTemplate" ref="sqlSession"/>\n    </bean>\n    \x3c!--配置声明事务注入--\x3e\n    \x3c!--要开启 Spring 的事务处理功能，在 Spring 的配置文件中创建一个 DataSourceTransactionManager 对象：--\x3e\n    <bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">\n        <property name="dataSource" ref="dataSource"/>\n        \x3c!--或者使用构造注入--\x3e\n        \x3c!--<constructor-arg ref="dataSource" />--\x3e\n    </bean>\n \n    \x3c!--结合AOP实现事务的织入--\x3e\n    \x3c!--配置事务通知--\x3e\n    <tx:advice id="txAdvice" transaction-manager="transactionManager">\n        \x3c!--给哪些方法配置事务--\x3e\n        \x3c!--配置事务的传播特性 propagation\n                PROPAGATION_REQUIRED:如果当前没有事务，就新建一个事务，如果已存在一个事务中，加入到这个事务中，这是最常见的选择。\n                PROPAGATION_SUPPORTS:支持当前事务，如果没有当前事务，就以非事务方法执行。\n                PROPAGATION_MANDATORY:使用当前事务，如果没有当前事务，就抛出异常。\n                PROPAGATION_REQUIRES_NEW:新建事务，如果当前存在事务，把当前事务挂起。\n                PROPAGATION_NOT_SUPPORTED:以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。\n                PROPAGATION_NEVER:以非事务方式执行操作，如果当前事务存在则抛出异常。\n                PROPAGATION_NESTED:      如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED 类似的操作\n        --\x3e\n        <tx:attributes>\n            <tx:method name="add" propagation="REQUIRED"/>\n            <tx:method name="delete" propagation="REQUIRED"/>\n            <tx:method name="update" propagation="REQUIRED"/>\n            <tx:method name="select" read-only="true"/>\n            \x3c!--全部方法--\x3e\n            <tx:method name="*" propagation="REQUIRED"/>\n        </tx:attributes>\n    </tx:advice>\n \n    \x3c!--配置事务切入--\x3e\n    <aop:config>\n       \x3c!--该包下的所有方法--\x3e\n        <aop:pointcut id="txPointCut" expression="execution(* com.yang.mapper.*.*(..))"/>\n        <aop:advisor advice-ref="txAdvice" pointcut-ref="txPointCut"/>\n    </aop:config>\n</beans>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #  10.致谢 \n \n 感谢狂神，本编文章均为狂神说Spring5视频中总结 \n 狂神哔哩哔哩账号地址：https://space.bilibili.com/95256449?from=search&seid=18072733147691362652 \n 真的讲的超级好，什么springboot、springcloud等等都有 \n 11.多模块项目打包 \n 开发环境：IDEA， \n ​         SprngBoot 2.0.4， \n ​          Maven  2.19.1 \n 工程结构： \n ​               父工程father \n ​                          子模块 dao    （用于持久化数据跟数据库交互） \n ​                          子模块 entity   （实体类） \n ​                          子模块 service （处理业务逻辑） \n ​                          子模块 web    （页面交互接收、传递数据，唯一有启动类的模块） \n ​                关系：     web依赖 service、dao、entity \n ​                          service依赖 dao、entity \n ​                          dao依赖 entity \n ​                          entity谁都不依赖，独立的 \n 一，创建Maven多模块项目 \n 先建立外层父工程     File →new →project 选择Spring Initializr      Next下一步到以下页面 \n \n \n \n 工程结构如下 \n \n 接下来，把src整个删掉，父工程不需要，因为父工程你就当它只有一个外壳就完了 \n 接下来创建子模块 工程上右键 → new →  Module  选择Spring Initaializr 下一步 \n \n \n 重复以上动作，创建dao模块，service模块，web模块 \n service模块和entity模块一样什么都不需要引入 \n dao模块和web模块可以根据实际需求选择引入mysql，mybatis，redis，web这些，我把我的贴出来 \n \n \n 删除每个子模块中没用的文件，.mvn、.gitignore、daoiml、mvnw、mvnw.cmd文件只留下pom.xml \n 删除除了web模块以外其它模块中的Applicatin启动项，和resources目录下的application.properties配置文件 \n \n 以上动作操作完成以后如果你发现你的子模块变成了文件夹，没关系，找到Maven Projects刷新一下就好了 \n \n 整理过后的项目结构是这样的 \n 以上项目的基本结构就完成了，接下来建立各自依赖 \n 二、依赖关系 \n打开父pom.xml修改打包方式jar为pom，注意：build内容也需要做替换，因为默认的spring-boot-maven-plugin这种方式，等到后期打包的时候他会一直提示你，你引入的依赖不存在！代码如下 \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n     \x3c!--父pom.xml--\x3e \n     < groupId > com.miu </ groupId > \n     < artifactId > father </ artifactId > \n     < version > 0.0.1-SNAPSHOT </ version > \n     < packaging > pom </ packaging > \n \n     < name > father </ name > \n     < description > Demo project for Spring Boot </ description > \n \n     < parent > \n         < groupId > org.springframework.boot </ groupId > \n         < artifactId > spring-boot-starter-parent </ artifactId > \n         < version > 2.0.4.RELEASE </ version > \n         < relativePath />   \x3c!-- lookup parent from repository --\x3e \n     </ parent > \n \n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n     </ properties > \n \n     \x3c!--声明你有四个儿子 --\x3e \n     < modules > \n         < module > entity </ module > \n         < module > dao </ module > \n         < module > service </ module > \n         < module > web </ module > \n     </ modules > \n \n     < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter </ artifactId > \n         </ dependency > \n \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n     </ dependencies > \n \n     < build > \n         < plugins > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.1 </ version > \n                 < configuration > \n                     < source > ${java.version} </ source > \n                     < target > ${java.version} </ target > \n                 </ configuration > \n             </ plugin > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-surefire-plugin </ artifactId > \n                 < version > 2.19.1 </ version > \n                 < configuration > \n                     < skipTests > true </ skipTests >      \x3c!--默认关掉单元测试 --\x3e \n                 </ configuration > \n             </ plugin > \n         </ plugins > \n     </ build > \n \n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 这里有个坑需要注意，dao、service、entity这三个模块的pom.xml文件中不需要build 内容，直接干掉 \n entity 的 pom.xml 内容 \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n     < groupId > com.miu </ groupId > \n     < artifactId > entity </ artifactId > \n     < version > 0.0.1-SNAPSHOT </ version > \n     < packaging > jar </ packaging > \n     < name > entity </ name > \n     < description > Demo project for Spring Boot </ description > \n     \x3c!--声明父模块--\x3e \n     < parent > \n         < groupId > com.miu </ groupId > \n         < artifactId > father </ artifactId > \n         < version > 0.0.1-SNAPSHOT </ version > \n         < relativePath > ../pom.xml </ relativePath > \n     </ parent > \n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n     </ properties > \n     < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter </ artifactId > \n         </ dependency > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n     </ dependencies > \n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #  dao 的 pom.xml 内容 \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n     \x3c!--dao 模块 pom.xml--\x3e \n     < groupId > com.miu </ groupId > \n     < artifactId > dao </ artifactId > \n     < version > 0.0.1-SNAPSHOT </ version > \n     < packaging > jar </ packaging > \n     < name > dao </ name > \n     < description > Demo project for Spring Boot </ description > \n     \x3c!--声明父模块--\x3e \n     < parent > \n         < groupId > com.miu </ groupId > \n         < artifactId > father </ artifactId > \n         < version > 0.0.1-SNAPSHOT </ version > \n         < relativePath > ../pom.xml </ relativePath > \n     </ parent > \n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n     </ properties > \n     < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-data-redis </ artifactId > \n         </ dependency > \n         < dependency > \n             < groupId > org.mybatis.spring.boot </ groupId > \n             < artifactId > mybatis-spring-boot-starter </ artifactId > \n             < version > 1.3.2 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > mysql </ groupId > \n             < artifactId > mysql-connector-java </ artifactId > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n         \x3c!--dao 模块 引入entity模块--\x3e \n         < dependency > \n             < groupId > com.miu </ groupId > \n             < artifactId > entity </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n     </ dependencies > \n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #  service 模块的 pom.xml 内容 \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n \n     < groupId > com.miu </ groupId > \n     < artifactId > service </ artifactId > \n     < version > 0.0.1-SNAPSHOT </ version > \n     < packaging > jar </ packaging > \n \n     < name > service </ name > \n     < description > Demo project for Spring Boot </ description > \n     \x3c!--声明父模块--\x3e \n     < parent > \n         < groupId > com.miu </ groupId > \n         < artifactId > father </ artifactId > \n         < version > 0.0.1-SNAPSHOT </ version > \n         < relativePath > ../pom.xml </ relativePath > \n     </ parent > \n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n     </ properties > \n     < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter </ artifactId > \n         </ dependency > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n         \x3c!--service模块 引入entity模块--\x3e \n         < dependency > \n             < groupId > com.miu </ groupId > \n             < artifactId > entity </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n         \x3c!--service模块 引入dao模块--\x3e \n         < dependency > \n             < groupId > com.miu </ groupId > \n             < artifactId > dao </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n     </ dependencies > \n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #  web模块的 pom.xml 内容 \n ​     注意build部分，因为web模块作为程序的入口启动，所以它需要打包，并且要指定Main Class \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n     < groupId > com.miu </ groupId > \n     < artifactId > web </ artifactId > \n     < version > 0.0.1-SNAPSHOT </ version > \n     < packaging > jar </ packaging > \n     < name > web </ name > \n     < description > Demo project for Spring Boot </ description > \n     \x3c!--声明父模块--\x3e \n     < parent > \n         < groupId > com.miu </ groupId > \n         < artifactId > father </ artifactId > \n         < version > 0.0.1-SNAPSHOT </ version > \n         < relativePath > ../pom.xml </ relativePath > \n     </ parent > \n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n     </ properties > \n     < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-data-redis </ artifactId > \n         </ dependency > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-web </ artifactId > \n         </ dependency > \n         < dependency > \n             < groupId > org.mybatis.spring.boot </ groupId > \n             < artifactId > mybatis-spring-boot-starter </ artifactId > \n             < version > 1.3.2 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > mysql </ groupId > \n             < artifactId > mysql-connector-java </ artifactId > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n         \x3c!--web模块 引入entity模块--\x3e \n         < dependency > \n             < groupId > com.miu </ groupId > \n             < artifactId > entity </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n         \x3c!--web模块 引入service模块--\x3e \n         < dependency > \n             < groupId > com.miu </ groupId > \n             < artifactId > service </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n         \x3c!--web模块 引入dao模块--\x3e \n         < dependency > \n             < groupId > com.miu </ groupId > \n             < artifactId > dao </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n     </ dependencies > \n     < build > \n         < plugins > \n             < plugin > \n                 < groupId > org.springframework.boot </ groupId > \n                 < artifactId > spring-boot-maven-plugin </ artifactId > \n                 < configuration > \n                     \x3c!-- 指定该Main Class为全局的唯一入口 --\x3e \n                     < mainClass > com.miu.web.WebApplication </ mainClass > \n                     < layout > ZIP </ layout > \n                 </ configuration > \n                 < executions > \n                     < execution > \n                         < goals > \n                             < goal > repackage </ goal > \x3c!--可以把依赖的包都打包到生成的Jar包中--\x3e \n                         </ goals > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n         </ plugins > \n     </ build > \n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 到此为止所有的依赖全部完成！接下来就是测试！这里只用简单的测试来实验！ \n 三、代码测试 \n entity模块中创建 EntiyTest类 \n \n dao模块中创建 DaoTest类 \n \n service模块中创建ServiceTest类 \n \n Web模块中创建WebTest类 \n \n 最后把web模块中的application.properties文件补充一下就OK了，因为引入了mysql，redis等配置，所以数据源是要配的，不然运行起来会报错找不到数据源！ \n server.port = 8080 \n #-----------------------------------数据库配置---------------------------------------- \nspring.datasource.driver-class-name = com.mysql.jdbc.Driver\n spring.datasource.url = jdbc:mysql://127.0.0.1:3306/test?characterEncoding = utf8\n spring.datasource.username = root\n spring.datasource.password = 123 \n #------------------------------------redis配置--------------------------------------- \n spring.redis.database = 0 \n spring.redis.host = 127.0 .0.1\n spring.redis.port = 6379 \n spring.redis.password = \nspring.redis.jedis.pool.max-active = 8 \nspring.redis.jedis.pool.max-idle = 8 \nspring.redis.jedis.pool.max-wait = -1ms\nspring.redis.jedis.pool.min-idle = 0 \n spring.redis.timeout = 10000ms\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 一切准备就绪，开始运行web模块下的启动类进行测试 \n \n 四、打包可执行jar \n看到上面的页面就证明模块之间的依赖没有问题，调用正常，我这里是用简单的创建对象的这种方式来操作的，实际开发并不是这种操作，大部分都是通过 @Autowired 注解 来实现的注入，这里我就不做演示了，只要模块之间调用没问题，剩下的就是铺代码的事了，接下来还有最后一个打包问题，为什么要啰嗦那么多还要说打包问题呢，因为我建议在项目架构之初，除了搭框架以外，最好是在最开始的时候就测试一下打包，尤其是这种多模块项目之间各种依赖的这种工程的打包，如果等你代码写的铺天盖地的时候你在去想怎么打包，到时候有你头疼的！如果你是按照我本章的流程一步步下来的话，那么你完全不用担心打包问题，因为所有的pom.xml有已经配置好了，只需要动手运行 package打包动作就行了，第一次打包不需要clean，记住以后每次打包之前clean一下，关于为什么打jar包，不打war包这个问题，还有其它会遇到的问题，在文章最后会做说明！ \n \n 双击运行package，看到BUILD SUCCESS 就证明打包成功了，如此简单？告诉你就是这么简单，前提是你的每一个模块下的pom.xml要配置好，谁需要打包，谁不需要打包，谁依赖谁，父工程是否声明了子模块，子模块是否声明了父工程是谁，这些是重点！ \n \n 接下来去找你工程目录，web文件夹下的target文件夹，刚才打包好的jar文件，就放在这里了 \n \n 然后我把这个jar文件上传到我的测试服务器，使用 java -jar web-0.0.1-SNAPSHOT.jar 命令来测试运行打包的可执行jar文件到底行不行！ \n \n 运行成功，输入我测试服务器地址测试也没问题，到此为止全部搞定 \n \n 聚合工程举一个简单的例子， \n 整个工程你就当作一个公司，父工程（退休了什么也不干）只需要声明有几个儿子（子模块）就完事了， \n 子模块web声明父工程是谁，就当他是大儿子，公司他管事，pom.xml文件需要打包，需要build配置，需要其它三个兄弟帮助 \n 其它子模块声明父工程是谁，之间关系都是兄弟，不需要打包，哪里需要去哪里！ \n 在此我说一下重点和需要注意的地方！ \n1.父pom.xml 打包方式，jar要更改为pom，build 需要更改 \n 2.不需要打包的模块pom.xml文件中不要写 ，全删掉，例如有些工程中的common模块，utils模块，entity模块，service模  块都不需要打包 \n 3.声明父工程时，填写父工程位置 ../pom.xml \n 4.关于applicatin.properties配置文件，只需要在启动的模块中配置就可以了， \n 5.关于打包为什么打包jar包，不打war包，打war包目的是war包可以运行在tomcat下，但是SpringBoot是内置tomcat，如果你打war包，前提是干掉内置的tomcat，然后才能打包，各种麻烦，直接打包可执行jar包，使用java -jar 命令就可以完美的运行起来很方便！ \n 6.真实开发中使用@Autowired 注解 来实现注入，而不是new对象这种方式，所以可能会产生注入以后报错，是因为你的启动类上没有配置扫描，使用 \n @ComponentScan(basePackages = "你的路径")注解来解决，如果你使用的持久层是Mybatis，那么你的mapper也需要扫描，在启动类上使用 \n@MapperScan("你的mapper文件地址")注解来解决，算了还是贴个图片吧 \n \n'},{title:"gitlab通过CICD流水线部署",frontmatter:{title:"gitlab通过CICD流水线部署",date:"2022-10-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["其他"],tags:["kafka的高性能原理","服务器小文件传输"]},regularPath:"/%E5%85%B6%E4%BB%96/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86.html",relativePath:"其他/零拷贝原理.md",key:"v-61c5b94b",path:"/2022/10/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86/",headers:[{level:3,title:"前言",slug:"前言"},{level:3,title:"为什么要有 DMA 技术?",slug:"为什么要有-dma-技术"},{level:3,title:"传统的文件传输有多糟糕？",slug:"传统的文件传输有多糟糕"},{level:3,title:"如何减少「上下文切换」和「数据拷贝」的次数？",slug:"如何减少「上下文切换」和「数据拷贝」的次数"},{level:3,title:"PageCache 有什么作用？",slug:"pagecache-有什么作用"},{level:3,title:"大文件传输用什么方式实现？",slug:"大文件传输用什么方式实现"},{level:3,title:"总结",slug:"总结"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:" 前言 \n 磁盘可以说是计算机系统最慢的硬件之一，读写速度相差内存 10 倍以上，所以针对优化磁盘的技术非常的多，比如零拷贝、直接 I/O、异步 I/O 等等，这些优化的目的就是为了提高系统的吞吐量，另外操作系统内核中的磁盘高速缓存区，可以有效的减少磁盘的访问次数。 \n 这次，我们就以「文件传输」作为切入点，来分析 I/O 工作方式，以及如何优化传输文件的性能。 \n 为什么要有 DMA 技术? \n 在没有 DMA 技术前，I/O 的过程是这样的： \n \n CPU 发出对应的指令给磁盘控制器，然后返回； \n 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个 中断 ； \n CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。 \n \n 为了方便你理解，我画了一副图： \n \n 可以看到，整个数据的传输过程，都要需要 CPU 亲自参与搬运数据的过程，而且这个过程，CPU 是不能做其他事情的。 \n 简单的搬运几个字符数据那没问题，但是如果我们用千兆网卡或者硬盘传输大量数据的时候，都用 CPU 来搬运的话，肯定忙不过来。 \n 计算机科学家们发现了事情的严重性后，于是就发明了 DMA 技术，也就是 直接内存访问（*Direct Memory Access*）  技术。 \n 什么是 DMA 技术？简单理解就是， 在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务 。 \n 那使用 DMA 控制器进行数据传输的过程究竟是什么样的呢？下面我们来具体看看。 \n \n 具体过程： \n \n 用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态； \n 操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务； \n DMA 进一步将 I/O 请求发送给磁盘； \n 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满； \n DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务 ； \n 当 DMA 读取了足够多的数据，就会发送中断信号给 CPU； \n CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回； \n \n 可以看到， 整个数据传输的过程，CPU 不再参与数据搬运的工作，而是全程由 DMA 完成，但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪里传输到哪里，都需要 CPU 来告诉 DMA 控制器。 \n 早期 DMA 只存在在主板上，如今由于 I/O 设备越来越多，数据传输的需求也不尽相同，所以每个 I/O 设备里面都有自己的 DMA 控制器。 \n 传统的文件传输有多糟糕？ \n 如果服务端要提供文件传输的功能，我们能想到的最简单的方式是：将磁盘上的文件读取出来，然后通过网络协议发送给客户端。 \n 传统 I/O 的工作方式是，数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的 I/O 接口从磁盘读取或写入。 \n 代码通常如下，一般会需要两个系统调用： \n read ( file ,  tmp_buf ,  len ) ; \n write ( socket ,  tmp_buf ,  len ) ; \n \n 1 2 代码很简单，虽然就两行代码，但是这里面发生了不少的事情。 \n \n 首先，期间共 发生了 4 次用户态与内核态的上下文切换 ，因为发生了两次系统调用，一次是  read()  ，一次是  write() ，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。 \n 上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。 \n 其次，还 发生了 4 次数据拷贝 ，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程： \n \n 第一次拷贝 ，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。 \n 第二次拷贝 ，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。 \n 第三次拷贝 ，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。 \n 第四次拷贝 ，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。 \n \n 我们回过头看这个文件传输的过程，我们只是搬运一份数据，结果却搬运了 4 次，过多的数据拷贝无疑会消耗 CPU 资源，大大降低了系统性能。 \n 这种简单又传统的文件传输方式，存在冗余的上文切换和数据拷贝，在高并发系统里是非常糟糕的，多了很多不必要的开销，会严重影响系统性能。 \n 所以， 要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数 。 \n 如何减少「上下文切换」和「数据拷贝」的次数？ \n 零拷贝技术实现的方式通常有 2 种： \n \n mmap + write \n sendfile \n \n 虚拟内存 \n 在了解零拷贝技术之前，先了解虚拟内存的概念。 \n所有现代操作系统都使用虚拟内存，使用虚拟地址取代物理地址，主要有以下几点好处： \n \n 多个虚拟内存可以指向同一个物理地址。 \n 虚拟内存空间可以远远大于物理内存空间。 \n \n 利用上述的第一条特性可以优化，可以把内核空间和用户空间的虚拟地址映射到同一个物理地址，这样在 I/O 操作时就不需要来回复制了。 \n 如下图展示了虚拟内存的原理。 \n mmap + write \n 在前面我们知道， read()  系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用  mmap()  替换  read()  系统调用函数。 \n buf  =   mmap ( file ,  len ) ; \n write ( sockfd ,  buf ,  len ) ; \n \n 1 2 mmap()  系统调用函数会直接把内核缓冲区里的数据「 映射 」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。 \n \n 具体过程如下： \n \n 应用进程调用了  mmap()  后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区； \n 应用进程再调用  write() ，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据； \n 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。 \n \n 我们可以得知，通过使用  mmap()  来代替  read() ， 可以减少一次数据拷贝的过程。 \n 但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。 \n sendfile \n 在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数  sendfile() ，函数形式如下： \ninclude   <sys/socket.h> \n ssize_t   sendfile ( int  out_fd ,   int  in_fd ,   off_t   * offset ,   size_t  count ) ; \n \n 1 2 它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。 \n 首先，它可以替代前面的  read()  和  write()  这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。 \n 其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图： \n \n 但是这还不是真正的零拷贝，那能不能把 CPU COPY 减少到没有呢？ \n 带 sg 的sendfile方式 \n Linux 2.4 内核进行了优化，提供了带有 scatter/gather 的 sendfile 操作，这个操作可以把最后一次 CPU COPY 去除。其原理就是在内核空间 Read BUffer 和 Socket Buffer 不做数据复制，而是将 Read Buffer 的内存地址、偏移量记录到相应的 Socket Buffer 中，这样就不需要复制。其本质和虚拟内存的解决方法思路一致，就是内存地址的记录。 \n \n 这就是所谓的 零拷贝（*Zero-copy*）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。 。 \n 零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数， 只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。 \n 所以，总体来看， 零拷贝技术可以把文件传输的性能提高至少一倍以上 。 \n 使用零拷贝技术的项目 \n 事实上，Kafka 这个开源项目，就利用了「零拷贝」技术，从而大幅提升了 I/O 的吞吐率，这也是 Kafka 在处理海量数据为什么这么快的原因之一。 \n 如果你追溯 Kafka 文件传输的代码，你会发现，最终它调用了 Java NIO 库里的  transferTo  方法： \n @Overridepublic  \n long   transferFrom ( FileChannel  fileChannel ,   long  position ,   long  count )   throws   IOException   {  \n     return  fileChannel . transferTo ( position ,  count ,  socketChannel ) ; \n } \n \n 1 2 3 4 如果 Linux 系统支持  sendfile()  系统调用，那么  transferTo()  实际上最后就会使用到  sendfile()  系统调用函数。 \n 曾经有大佬专门写过程序测试过，在同样的硬件条件下，传统文件传输和零拷拷贝文件传输的性能差异，你可以看到下面这张测试数据图，使用了零拷贝能够缩短  65%  的时间，大幅度提升了机器传输数据的吞吐量。 \n \n 数据来源于：https://developer.ibm.com/articles/j-zerocopy/ \n 另外，Nginx 也支持零拷贝技术，一般默认是开启零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置如下： \n http  { \n ... \n    sendfile on\n ... \n } \n \n 1 2 3 4 5 sendfile 配置的具体意思: \n \n 设置为 on 表示，使用零拷贝技术来传输文件：sendfile ，这样只需要 2 次上下文切换，和 2 次数据拷贝。 \n 设置为 off 表示，使用传统的文件传输技术：read + write，这时就需要 4 次上下文切换，和 4 次数据拷贝。 \n \n 当然，要使用 sendfile，Linux 内核版本必须要 2.1 以上的版本。 \n PageCache 有什么作用？ \n 回顾前面说道文件传输过程，其中第一步都是先需要先把磁盘文件数据拷贝「内核缓冲区」里，这个「内核缓冲区」实际上是 磁盘高速缓存（*PageCache*） 。 \n 由于零拷贝使用了 PageCache 技术，可以使得零拷贝进一步提升了性能，我们接下来看看 PageCache 是如何做到这一点的。 \n 读写磁盘相比读写内存的速度慢太多了，所以我们应该想办法把「读写磁盘」替换成「读写内存」。于是，我们会通过 DMA 把磁盘里的数据搬运到内存里，这样就可以用读内存替换读磁盘。 \n 但是，内存空间远比磁盘要小，内存注定只能拷贝磁盘里的一小部分数据。 \n 那问题来了，选择哪些磁盘数据拷贝到内存呢？ \n 我们都知道程序运行的时候，具有「局部性」，所以通常，刚被访问的数据在短时间内再次被访问的概率很高，于是我们可以用  PageCache 来缓存最近被访问的数据 ，当空间不足时淘汰最久未被访问的缓存。 \n 所以，读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存 PageCache 中。 \n 还有一点，读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是非常耗时的，为了降低它的影响， PageCache 使用了「预读功能」 。 \n 比如，假设 read 方法每次只会读  32 KB  的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会把其后面的 32～64 KB 也读取到 PageCache，这样后面读取 32～64 KB 的成本就很低，如果在 32～64 KB 淘汰出 PageCache 前，进程读取到它了，收益就非常大。 \n 所以，PageCache 的优点主要是两个： \n \n 缓存最近被访问的数据； \n 预读功能； \n \n 这两个做法，将大大提高读写磁盘的性能。 \n 但是，在传输大文件（GB 级别的文件）的时候，PageCache 会不起作用，那就白白浪费 DMA 多做的一次数据拷贝，造成性能的降低，即使使用了 PageCache 的零拷贝也会损失性能 \n 这是因为如果你有很多 GB 级别文件需要传输，每当用户访问这些大文件的时候，内核就会把它们载入 PageCache 中，于是 PageCache 空间很快被这些大文件占满。 \n 另外，由于文件太大，可能某些部分的文件数据被再次访问的概率比较低，这样就会带来 2 个问题： \n \n PageCache 由于长时间被大文件占据，其他「热点」的小文件可能就无法充分使用到 PageCache，于是这样磁盘读写的性能就会下降了； \n PageCache 中的大文件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷贝到 PageCache 一次； \n \n 所以，针对大文件的传输，不应该使用 PageCache，也就是说不应该使用零拷贝技术，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，这样在高并发的环境下，会带来严重的性能问题。 \n 大文件传输用什么方式实现？ \n 那针对大文件的传输，我们应该使用什么方式呢？ \n 我们先来看看最初的例子，当调用 read 方法读取文件时，进程实际上会阻塞在 read 方法调用，因为要等待磁盘数据的返回，如下图： \n \n 具体过程： \n \n 当调用 read 方法时，会阻塞着，此时内核会向磁盘发起 I/O 请求，磁盘收到请求后，便会寻址，当磁盘数据准备好后，就会向内核发起 I/O 中断，告知内核磁盘数据已经准备好； \n 内核收到 I/O 中断后，就将数据从磁盘控制器缓冲区拷贝到 PageCache 里； \n 最后，内核再把 PageCache 中的数据拷贝到用户缓冲区，于是 read 调用就正常返回了。 \n \n 对于阻塞的问题，可以用异步 I/O 来解决，它工作方式如下图： \n \n 它把读操作分为两部分： \n \n 前半部分，内核向磁盘发起读请求，但是可以 不等待数据就位就可以返回 ，于是进程此时可以处理其他任务； \n 后半部分，当内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的 通知 ，再去处理数据； \n \n 而且，我们可以发现，异步 I/O 并没有涉及到 PageCache，所以使用异步 I/O 就意味着要绕开 PageCache。 \n 绕开 PageCache 的 I/O 叫直接 I/O，使用 PageCache 的 I/O 则叫缓存 I/O。通常，对于磁盘，异步 I/O 只支持直接 I/O。 \n 前面也提到，大文件的传输不应该使用 PageCache，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache。 \n 于是， 在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术 。 \n 直接 I/O 应用场景常见的两种： \n \n 应用程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启； \n 传输大文件的时候，由于大文件难以命中 PageCache 缓存，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用直接 I/O。 \n \n 另外，由于直接 I/O 绕过了 PageCache，就无法享受内核的这两点的优化： \n \n 内核的 I/O 调度算法会缓存尽可能多的 I/O 请求在 PageCache 中，最后「 合并 」成一个更大的 I/O 请求再发给磁盘，这样做是为了减少磁盘的寻址操作； \n 内核也会「 预读 」后续的 I/O 请求放在 PageCache 中，一样是为了减少对磁盘的操作； \n \n 于是，传输大文件的时候，使用「异步 I/O + 直接 I/O」了，就可以无阻塞地读取文件了。 \n 所以，传输文件的时候，我们要根据文件的大小来使用不同的方式： \n \n 传输大文件的时候，使用「异步 I/O + 直接 I/O」； \n 传输小文件的时候，则使用「零拷贝技术」； \n \n 在 nginx 中，我们可以用如下配置，来根据文件的大小来使用不同的方式： \n location  / video /   {  \n     sendfile   on ;  \n     aio   on ;  \n    directio  1024m ;  \n } \n \n 1 2 3 4 5 当文件大小大于  directio  值后，使用「异步 I/O + 直接 I/O」，否则使用「零拷贝技术」。 \n 总结 \n 早期 I/O 操作，内存与磁盘的数据传输的工作都是由 CPU 完成的，而此时 CPU 不能执行其他任务，会特别浪费 CPU 资源。 \n 于是，为了解决这一问题，DMA 技术就出现了，每个 I/O 设备都有自己的 DMA 控制器，通过这个 DMA 控制器，CPU 只需要告诉 DMA 控制器，我们要传输什么数据，从哪里来，到哪里去，就可以放心离开了。后续的实际数据传输工作，都会由 DMA 控制器来完成，CPU 不需要参与数据传输的工作。 \n 传统 IO 的工作方式，从硬盘读取数据，然后再通过网卡向外发送，我们需要进行 4 上下文切换，和 4 次数据拷贝，其中 2 次数据拷贝发生在内存里的缓冲区和对应的硬件设备之间，这个是由 DMA 完成，另外 2 次则发生在内核态和用户态之间，这个数据搬移工作是由 CPU 完成的。 \n 为了提高文件传输的性能，于是就出现了零拷贝技术，它通过一次系统调用（ sendfile  方法）合并了磁盘读取与网络发送两个操作，降低了上下文切换次数。另外，拷贝数据都是发生在内核中的，天然就降低了数据拷贝的次数。 \n Kafka 和 Nginx 都有实现零拷贝技术，这将大大提高文件传输的性能。 \n 零拷贝技术是基于 PageCache 的，PageCache 会缓存最近访问的数据，提升了访问缓存数据的性能，同时，为了解决机械硬盘寻址慢的问题，它还协助 I/O 调度算法实现了 IO 合并与预读，这也是顺序读比随机读性能好的原因。这些优势，进一步提升了零拷贝的性能。 \n 需要注意的是，零拷贝技术是不允许进程对文件内容作进一步的加工的，比如压缩数据再发送。 \n 另外，当传输大文件时，不能使用零拷贝，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，并且大文件的缓存命中率不高，这时就需要使用「异步 IO + 直接 IO 」的方式。 \n 在 Nginx 里，可以通过配置，设定一个文件大小阈值，针对大文件使用异步 IO 和直接 IO，而对小文件使用零拷贝。 \n"},{title:"美团开源动态线程池",frontmatter:{title:"美团开源动态线程池",date:"2023-05-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["其他"],tags:["线程池设计","spring"]},regularPath:"/%E5%85%B6%E4%BB%96/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0.html",relativePath:"其他/美团开源动态线程池.md",key:"v-5692179e",path:"/2023/05/08/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0/",headers:[{level:2,title:"由来",slug:"由来"},{level:2,title:"轻松入门",slug:"轻松入门"},{level:2,title:"线程池动态化设计思路演进",slug:"线程池动态化设计思路演进"},{level:3,title:"1.普通线程池",slug:"_1-普通线程池"},{level:3,title:"2.集成nacos动态监听单个线程池",slug:"_2-集成nacos动态监听单个线程池"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:' 由来 \n 没有一个合适的公式，可以帮助我们合理的设定线程池参数，故而需要在实际运行过程中动态调整参数，缩短程序恢复的时间。 \n \n 官网 ：https://dynamictp.cn \n gitee地址 ：https://gitee.com/dromara/dynamic-tp \n github地址 ：https://github.com/dromara/dynamic-tp \n 轻松入门 \n \n 引入相应配置中心的依赖，maven 依赖见下述 \n 配置中心配置线程池实例，配置文件见下述 \n 启动类加 @EnableDynamicTp 注解 \n 使用 @Resource 或 @Autowired 进行依赖注入，或通过 DtpRegistry.getDtpExecutor("name") 获取 \n 通过以上 4 步就可以使用了，是不是感觉超级简单呀 \n  线程池动态化设计思路演进 \n 1.普通线程池 \n 1.创建一个user的module，引入依赖 \n          < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-web </ artifactId > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n     </ dependencies > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 \n 2.创建启动应用，并创建线程池，固定参数，作为bean注解 \n import   org . springframework . boot . SpringApplication ; \n import   org . springframework . boot . autoconfigure . SpringBootApplication ; \n import   org . springframework . context . annotation . Bean ; \n\n import   java . util . concurrent . * ; \n\n @SpringBootApplication \n public   class   MyApplication   { \n\n     @Bean   //作为一个bean初始化 \n     public   ExecutorService   executorService ( ) { \n         return   new   ThreadPoolExecutor ( 10 ,   100 , \n                 0L ,   TimeUnit . MILLISECONDS , \n                 new   LinkedBlockingQueue < Runnable > ( ) ) ; \n     } \n\n     public   static   void   main ( String [ ]  args )   { \n         SpringApplication . run ( MyApplication . class , args ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 3.创建Controller，引用线程池执行任务 \n import   org . springframework . web . bind . annotation . GetMapping ; \n import   org . springframework . web . bind . annotation . RestController ; \n\n import   javax . annotation . Resource ; \n import   java . util . concurrent . ExecutorService ; \n\n @RestController \n public   class   CommonExecutorsController   { \n\n     @Resource   //注入引用 \n     private   ExecutorService  executorService ; \n\n     @GetMapping ( "/test" ) \n     public   Integer   test ( ) { \n        executorService . execute ( ( ) -> doTask ( ) ) ; \n         return   1 ; \n     } \n\n     public   void   doTask ( ) { \n\n\n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 如果线程池的参数采用配置注入，需要增加配置，并在应用中引入 \n 增加application.yml配置 \n dtp : \n   core-pool-size :   10 \n   maximum-pool-size :   100 \n \n 1 2 3 在应用中调用参数 \n package   com . gordon . common ; \n\n import   org . springframework . beans . factory . annotation . Autowired ; \n import   org . springframework . boot . SpringApplication ; \n import   org . springframework . boot . autoconfigure . SpringBootApplication ; \n import   org . springframework . context . annotation . Bean ; \n import   org . springframework . core . env . Environment ; \n\n import   java . util . concurrent . ExecutorService ; \n import   java . util . concurrent . LinkedBlockingQueue ; \n import   java . util . concurrent . ThreadPoolExecutor ; \n import   java . util . concurrent . TimeUnit ; \n\n @SpringBootApplication \n public   class   MyApplication   { \n\n     @Autowired   //引用配置的环境参数 \n     public   Environment  environment ; \n\n     // CommonExecutorsController 需要提前创建注入 \n     @Bean   //作为一个bean初始化 \n     public   ExecutorService   executorService ( ) { \n         Integer  corePoolSize  =   Integer . valueOf ( environment . getProperty ( "dtp.core-pool-size" ) ) ; \n         Integer  maximumPoolSize  =   Integer . valueOf ( environment . getProperty ( "dtp.maximum-pool-size" ) ) ; \n\n         //打印确认配置是否正确注入 \n         System . out . println ( "corePoolSize = "   +  corePoolSize ) ; \n\n         return   new   ThreadPoolExecutor ( corePoolSize ,  maximumPoolSize , \n                 0L ,   TimeUnit . MILLISECONDS , \n                 new   LinkedBlockingQueue < Runnable > ( ) ) ; \n     } \n\n     public   static   void   main ( String [ ]  args )   { \n         SpringApplication . run ( MyApplication . class , args ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #  2.集成nacos动态监听单个线程池 \n 注意nacos和springboot的集成有版本匹配问题 \n 自定义一个starter集成nacos注册器进行监听配置的更新 \n 相关依赖 \n < dependency > \n             < groupId > com.alibaba.boot </ groupId > \n             < artifactId > nacos-config-spring-boot-starter </ artifactId > \n             < version > 0.2.7 </ version > \n         </ dependency > \n \n 1 2 3 4 5 创建一个动态线程池 \n package   com . gordon . singleDtp ; \n\n import   java . util . concurrent . LinkedBlockingQueue ; \n import   java . util . concurrent . ThreadPoolExecutor ; \n import   java . util . concurrent . TimeUnit ; \n\n public   class   DtpExecutor   extends   ThreadPoolExecutor   { \n\n     public   DtpExecutor ( int  corePoolSize ,   int  maximumPoolSize )   { \n         super ( corePoolSize ,  maximumPoolSize , 0L ,   TimeUnit . MILLISECONDS , \n                 new   LinkedBlockingQueue < Runnable > ( ) ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 创建一个配置类，并监听 \n package   com . gordon . singleDtp ; \n\n import   org . springframework . beans . factory . annotation . Autowired ; \n import   org . springframework . context . annotation . Bean ; \n import   org . springframework . context . annotation . Configuration ; \n import   org . springframework . core . env . Environment ; \n\n @Configuration \n public   class   AutoSingleDtpConfiguration    { //AutoSingleDtpConfiguration 这个要生效还要配置spring.factories \n\n     @Autowired   //引用配置的环境参数 \n     public   Environment  environment ; \n\n     // CommonExecutorsController 需要提前创建注入 \n     @Bean   //作为一个bean初始化 \n     public   DtpExecutor   executor ( ) { \n         Integer  corePoolSize  =   Integer . valueOf ( environment . getProperty ( "dtp.core-pool-size" ) ) ; \n         Integer  maximumPoolSize  =   Integer . valueOf ( environment . getProperty ( "dtp.maximum-pool-size" ) ) ; \n\n         return   new   DtpExecutor ( corePoolSize , maximumPoolSize ) ; \n     } \n\n     @Bean \n     public   NacosListener   nacosListener ( ) { \n         return   new   NacosListener ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 如何去生效一个配置类？ \n 1.添加注解@Configuration \n 2.在META-INF文件夹下的spring.factories进行配置 \n org.springframework.boot.autoconfigure.EnableAutoConfiguration = com.gordon.singleDtp.AutoSingleDtpConfiguration \n \n 1 创建监听器 \n package   com . gordon . singleDtp ; \n\n import   com . alibaba . nacos . api . annotation . NacosInjected ; \n import   com . alibaba . nacos . api . config . ConfigService ; \n import   com . alibaba . nacos . api . config . listener . Listener ; \n import   org . springframework . beans . factory . InitializingBean ; \n import   org . springframework . beans . factory . annotation . Autowired ; \n import   org . springframework . beans . factory . config . YamlPropertiesFactoryBean ; \n import   org . springframework . core . io . ByteArrayResource ; \n\n import   java . util . Properties ; \n import   java . util . concurrent . Executor ; \n import   java . util . concurrent . Executors ; \n\n public   class   NacosListener   implements   Listener ,   InitializingBean   { \n\n     @NacosInjected \n     private   ConfigService  configService ; \n\n     @Autowired \n     private   DtpExecutor  executor ; \n\n     @Override   //启动一个后台进程去执行内容的更新 \n     public   Executor   getExecutor ( )   { \n         return   Executors . newFixedThreadPool ( 1 ) ; \n     } \n\n     @Override \n     public   void   receiveConfigInfo ( String  content )   { \n\n         YamlPropertiesFactoryBean  bean  =   new   YamlPropertiesFactoryBean ( ) ; \n        bean . setResources ( new   ByteArrayResource ( content . getBytes ( ) ) ) ; \n         Properties  properties  =  bean . getObject ( ) ; \n         Integer  corePoolSize  =   Integer . valueOf ( properties . getProperty ( "dtp.core-pool-size" ) ) ; \n         Integer  maximumPoolSize  =   Integer . valueOf ( properties . getProperty ( "dtp.maximum-pool-size" ) ) ; \n        executor . setCorePoolSize ( corePoolSize ) ; \n        executor . setMaximumPoolSize ( maximumPoolSize ) ; \n\n     } \n\n     @Override \n     public   void   afterPropertiesSet ( )   throws   Exception   { \n        configService . addListener ( "dtp.yaml" , "DEFAULT_GROUP" , this ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 配置nacos的参数 \n nacos : \n   config : \n     server-addr :  127.0.0.1 : 8848 \n     data-id :  dtp.yaml\n     type :  yaml\n     auto-refresh :   true \n     bootstrap : \n       enable :   true \n \n 1 2 3 4 5 6 7 8 \n user模块引入监听starter \n < dependency > \n             < groupId > com.example </ groupId > \n             < artifactId > dtp-spring-boot-stater-nacos </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n \n 1 2 3 4 5 在user模块下创建singleDtp包 \n 创建启动容器应用 \n package   com . gordon . singleDtp ; \n\n import   org . springframework . boot . SpringApplication ; \n import   org . springframework . boot . autoconfigure . SpringBootApplication ; \n\n @SpringBootApplication \n public   class   MyApplication   { \n\n     public   static   void   main ( String [ ]  args )   { \n         ConfigurableApplicationContext  ct  =   SpringApplication . run ( MyApplication . class ,  args ) ; \n         DtpExecutor  executor  =  ct . getBean ( DtpExecutor . class ) ; \n         //只打印启动时的参数，动态修改配置，不会再打印 \n         System . out . println ( "SpringBootApplication = "   +  executor . getCorePoolSize ( ) ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 创建controller类接收请求 \n package   com . gordon . singleDtp . controller ; \n\n import   com . gordon . singleDtp . DtpExecutor ; \n import   org . springframework . beans . factory . annotation . Autowired ; \n import   org . springframework . web . bind . annotation . GetMapping ; \n import   org . springframework . web . bind . annotation . RestController ; \n\n @RestController \n public   class   SingleDtpExecutorsController   { \n     @Autowired \n     private   DtpExecutor  executor ; \n\n     @GetMapping ( "/test" ) \n     public   String   test ( ) { \n        executor . execute ( ( ) -> doTask ( ) ) ; \n\n         return  executor . getCorePoolSize ( ) + "" ; \n     } \n\n     public   void   doTask ( ) { \n\n\n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 以上就是大概的实现思路，美团开源的动态化线程池有更丰富 \n'},{title:"网络IO模型",frontmatter:{title:"网络IO模型",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["其他"],tags:["网络IO"]},regularPath:"/%E5%85%B6%E4%BB%96/%E7%BD%91%E7%BB%9CIO%E6%A8%A1%E5%9E%8B.html",relativePath:"其他/网络IO模型.md",key:"v-227baaf0",path:"/2023/06/10/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/",headers:[{level:3,title:"前言",slug:"前言"},{level:3,title:"IO模型的分类",slug:"io模型的分类"},{level:3,title:"传统的IO即BIO",slug:"传统的io即bio"},{level:3,title:"同步非阻塞IO即NIO",slug:"同步非阻塞io即nio"},{level:3,title:"IO多路复用（难点）",slug:"io多路复用-难点"},{level:3,title:"IO模型之信号驱动模型",slug:"io模型之信号驱动模型"},{level:3,title:"IO 模型之异步IO(AIO)",slug:"io-模型之异步io-aio"},{level:3,title:"Reactor三种模型",slug:"reactor三种模型"},{level:3,title:"代码",slug:"代码"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' 前言 \n 掌握IO模型的演进，代码可以先忽略。 \n IO模型的分类 \n IO模型给BIO、NIO、多路复用、信号驱动IO、异步IO五种IO模型 \n 大白话理解： \n 举个生活中简单的例子，你妈妈让你烧水，小时候你比较笨啊，在哪里傻等着水开（ 同步阻塞 ）。等你稍微再长大一点，你知道每次烧水的空隙可以去干点其他事，然后只需要时不时来看看水开了没有（ 同步非阻塞 ）。后来，你们家用上了水开了会发出声音的壶，这样你就只需要听到响声后就知道水开了，在这期间你可以随便干自己的事情，你需要去倒水了（ 异步非阻塞 ）。 \n 传统的IO即BIO \n \n 内核空间需要释放不能阻塞 \n 同步非阻塞IO即NIO \n \n 很多空轮询对CPU的占用 \n IO多路复用（难点） \n \n IO复用模型核心思路：系统给我们提供 一类函数 （如我们耳濡目染的 select、poll、epoll函数 ），它们可以同时监控多个 fd 的操作，任何一个返回内核数据就绪，应用进程再发起 recvfrom 系统调用。 \n 文件描述符fd(File Descriptor),它是计算机科学中的一个术语，形式上是一个非负整数。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。\n \n 1 #  IO多路复用之select \n 应用进程通过调用select函数，可以同时监控多个 fd ，在 select 函数监控的 fd 中，只要有任何一个数据状态准备就绪了， select 函数就会返回可读状态，这时应用进程再发起 recvfrom 请求去读取数据。 \n 但是呢， select 有几个缺点： \n \n 监听的IO最大连接数有限，在Linux系统上一般为1024。 \n select函数返回后，是通过 遍历 fdset ，找到就绪的描述符 fd 。（仅知道有I/O事件发生，却不知是哪几个流，所以遍历所有流） \n IO多路复用之 poll \n 因为 存在连接数限制 ，所以后来又提出了 poll 。与select相比， poll 解决了 连接数限制问题 。但是呢，select和poll一样，还是需要通过遍历文件描述符来获取已经就绪的 socket 。如果同时连接的大量客户端在一时刻可能只有极少处于就绪状态，伴随着监视的描述符数量的增长，（时间花在 遍历 描述符就会增加） 效率也会线性下降 。 \n 因此经典的多路复用模型 epoll 诞生。 \n IO多路复用之epoll \n 小白也看得懂的 I/O 多路复用解析（超详细案例）_哔哩哔哩_bilibili \n 为了解决 select/poll 存在的问题，多路复用模型 epoll 诞生，它采用事件驱动来实现。 \n epoll 先通过 epoll_ctl() 来注册一个 fd （文件描述符），一旦基于某个 fd 就绪时，内核会采用回调机制，迅速激活这个 fd ，当进程调用 epoll_wait() 时便得到通知。这里去掉了 遍历文件描述符 的坑爹操作，而是采用 监听事件回调 的的机制，唤醒等待队列，检查就绪事件。 \n \n \n 大致流程如下： \n 1）用户空间调用 epoll_create ，内核新建 epoll 对象，返回 epoll 的 fd，用于后续操作 \n 2）用户空间反复调用 epoll_ctl 将我们要监听的 fd 维护到 epoll，底层通过红黑树来高效的维护 fd 集合 \n 3）用户空间调用 epoll_wait 获取就绪事件，内核检查 epoll 的就绪列表，如果就绪列表为空则会进入阻塞 \n 4）客户端向服务端发送数据，数据通过网络传输到服务端的网卡 \n 5）网卡通过 DMA 的方式将数据包写入到指定内存中（ring_buffer），处理完成后通过中断信号告诉 CPU 有新的数据包到达 \n 6）CPU 收到中断信号后，进行响应中断，首先保存当前执行程序的上下文环境，然后调用中断处理程序（网卡驱动程序）进行处理： \n \n \n 根据数据包的ip和port找到对应的socket，将数据放到socket的接收队列； \n \n \n 执行 socket 对应的回调函数：将当前 socket 添加到 eventpoll 的就绪列表、唤醒 eventpoll 等待队列里的用户进程（设置为RUNNING状态） \n \n \n 7）用户进程恢复运行后，调用 epoll_wait 检查 eventpoll 里的就绪列表，不是该进程数据添加到等待队列里面，再取下一个进程，检查不为空，则将就绪事件填充到入参中的 events 里，然后返回。 \n 8）用户进程收到返回的事件后，执行 events 里的事件处理，例如读事件则将数据从内核缓冲区拷贝到应用程序缓冲区 \n 9）最后执行逻辑处理 \n 我们一起来总结一下select、poll、epoll的区别 \n \n \n \n \n select \n poll \n epoll \n \n \n \n \n 底层数据结构 \n 数组 \n 链表 \n 红黑树和双链表 \n \n \n 获取就绪的fd \n 遍历 \n 遍历 \n 事件回调 \n \n \n 事件复杂度 \n O(n) \n O(n) \n O(1) \n \n \n 最大连接数 \n 1024 \n 无限制 \n 无限制 \n \n \n fd数据拷贝 \n 每次调用select，需要将fd数据从用户空间拷贝到内核空间 \n 每次调用poll，需要将fd数据从用户空间拷贝到内核空间 \n 不需要从用户空间频繁拷贝fd数据到内核空间 \n \n \n \n epoll 明显优化了IO的执行效率，但在进程调用 epoll_wait() 时，仍然可能被阻塞的。能不能酱紫：不用我老是去问你数据是否准备就绪，等我发出请求后，你数据准备好了通知我就行了，这就诞生了 信号驱动IO模型 。 \n IO模型之信号驱动模型 \n 信号驱动IO不再用主动询问的方式去确认数据是否就绪，而是向内核发送一个信号（调用 sigaction 的时候建立一个 SIGIO 的信号），然后应用用户进程可以去做别的事，不用阻塞。当内核数据准备好后，再通过 SIGIO 信号通知应用进程，数据准备好后的可读状态。应用用户进程收到信号之后，立即调用 recvfrom ，去读取数据。 \n \n 信号驱动IO模型，在应用进程发出信号后，是立即返回的，不会阻塞进程。它已经有异步操作的感觉了。但是你细看上面的流程图， 发现数据复制到应用缓冲的时候 ，应用进程还是阻塞的。回过头来看下，不管是BIO，还是NIO，还是信号驱动，在数据从内核复制到应用缓冲的时候，都是阻塞的。还有没有优化方案呢？ AIO （真正的异步IO）！ \n IO 模型之异步IO(AIO) \n 前面讲的 BIO，NIO和信号驱动 ，在数据从内核复制到应用缓冲的时候，都是 阻塞 的，因此都不是真正的异步。 AIO 实现了IO全流程的非阻塞，就是应用进程发出系统调用后，是立即返回的，但是 立即返回的不是处理结果，而是表示提交成功类似的意思 。等内核数据准备好，将数据拷贝到用户进程缓冲区，发送信号通知用户进程IO操作执行完毕。 \n 流程如下： \n \n 异步IO的优化思路很简单，只需要向内核发送一次请求，就可以完成数据状态询问和数据拷贝的所有操作，并且不用阻塞等待结果。 \n  Reactor三种模型 \n 单线程模型 \n 模型图如下： \n \n 上图描述了 Reactor 的单线程模型结构，在 Reactor 单线程模型中，所有 I/O 操作（包括连接建立、数据读写、事件分发等）、业务处理，都是由一个线程完成的。单线程模型逻辑简单，缺陷也十分明显： \n \n 一个线程支持处理的连接数非常有限，CPU 很容易打满，性能方面有明显瓶颈； \n 当多个事件被同时触发时，只要有一个事件没有处理完，其他后面的事件就无法执行，这就会造成消息积压及请求超时； \n 线程在处理 I/O 事件时，Select 无法同时处理连接建立、事件分发等操作； \n 如果 I/O 线程一直处于满负荷状态，很可能造成服务端节点不可用。 \n \n 在单线程 Reactor 模式中，Reactor 和 Handler 都在同一条线程中执行。这样，带来了一个问题：当其中某个 Handler 阻塞时，会导致其他所有的 Handler 都得不到执行。 \n 在这种场景下，被阻塞的 Handler 不仅仅负责输入和输出处理的传输处理器，还包括负责新连接监听的 Acceptor 处理器，可能导致服务器无响应。这是一个非常严重的缺陷，导致单线程反应器模型在生产场景中使用得比较少。 \n 单Reactor多线程模型 \n 1.1 大部分网络服务包括以下处理步骤 \nread request（读取客户端发送过来的byte数据） \ndecode request（把byte数据解码成特定类型的数据） \nprocess (compute) service（根据请求数据进行业务处理） \nencode reply（把处理结果转换成byte数据） \nsend reply（发送byte数据给客户端） \n 1.2 Reactor多线程模型图 （来自于Doug Lea的文章） \n \n Reactor - 负责响应IO事件，把事件分发给相应的处理代码。Reactor运行在一个独立的线程中（非Thread Pool中的线程）。具体来说，Reactor主要有两个职责，一个是处理来自客户端的连接事件，处理代码由acceptor实现；另一个是处理读取和发送数据的事件，处理代码由Handler实现。 \nAcceptor - 用以接受客户端的连接请求，然后创建Handler对连接进行后续的处理（读取，处理，发送数据）。 \nHandler - 事件处理类，用以实现具体的业务逻辑。图中read，decode，compute，encode和send都是由handler实现的。 \nThread Pool - Thread Pool中的thread被称作worker thread。Handler中的decode，compute和encode是用worker thread执行的。值得注意的是Handler中的read和send方法是在Reactor线程而不是worker thread中执行的。这意味着对socket数据的读取发送数据和对数据的处理是在不同的线程中进行的. \n 1.3 Reactor多线程模型的主要问题 \n \n read和send会影响接受客户端连接的性能 \n前面分析过read和send是在Reactor线程中执行的，接受客户端的连接请求也是在Reactor线程中执行。这使得如果有read或者send耗时较长，会影响其他客户端连接的速度。 \n Read和send性能不够高效 \n网络服务对于来自同一客户端的read和send是串行的，但是对于不同客户端之间的read和send是可以并行进行的。由于read和send运行在Reactor单线程中，不能充分发挥硬件能力。 \n 线程上下文切换带来额外开销 \n前面提到的处理客户端请求的步骤依次是read，decode，process，encode，send。由于read和send是在Reactor线程中执行，而decode，process和encode是在worker thread线程中执行，引入了额外的线程切换开销，这种开销在高并发的时候会体现出来。 \n Reactor主从多线程模型 \n \n 主从 Reactor 模型要想解决这个问题，同样需要从我们前面介绍的几个阶段中的某一个或者多个进行优化处理。 \n 既然是主从模式，那谁主谁从呢？哪个模块使用主从呢？ \n 在多线程模型中，我们提到，其主要缺陷在于同一时间无法处理 大量新连接 、 IO就绪事件 ；因此，将主从模式应用到这一块，就可以解决这个问题。 \n 主从 Reactor 模式中，分为了主 Reactor 和 从 Reactor，分别处理  新建立的连接 、 IO读写事件/事件分发 。 \n \n 一来，主 Reactor 可以解决同一时间大量新连接，将其注册到从 Reactor 上进行IO事件监听处理 \n 二来，IO事件监听相对新连接处理更加耗时，此处我们可以考虑使用线程池来处理。这样能充分利用多核 CPU 的特性，能使更多就绪的IO事件及时处理。 \n \n 简言之，主从多线程模型由多个 Reactor 线程组成，每个 Reactor 线程都有独立的 Selector 对象。MainReactor 仅负责处理客户端连接的 Accept 事件，连接建立成功后将新创建的连接对象注册至 SubReactor。再由 SubReactor 分配线程池中的 I/O 线程与其连接绑定，它将负责连接生命周期内所有的 I/O 事件。 \n 在海量客户端并发请求的场景下，主从多线程模式甚至可以适当增加 SubReactor 线程的数量，从而利用多核能力提升系统的吞吐量。 \n 实际应用中的多线程模型 \n Doug Lea文章中的”主从Reactor“模式可以解决上述第一个和第二个问题。它把接受客户端连接的 \n“主Reactor”单独运行在一个线程中，“从Reactor”有多个，组成一个Reactor Pool，每个”从Reactor“都运行在一个独立的线程上，具有自己的selector和dispatch loop。但第三个问题还是没有解决，read和send仍然是运行在”从Reactor“线程上，而decode，process和encode运行在worker thread上。 \n 要解决第三个问题，可以采用下面的方式。Reactor线程专门用于接受客户端连接（通过acceptor）；创建多个Event Loop ，组成Event Loop Pool，每个Event Loop都有自己的Selector，并且运行在独立的线程上；Acceptor对于每一个客户端的连接从EventLoopPool中选择一个Event Loop进行处理，并且保证每个客户端连接在整个生命周期中都是由同一个Event Loop线程来处理，从而使得Handler中的实现-read，decode，process，encode，send-都在同一个线程中执行。整个线程模型除了高效的性能，还有非常重要的一点是Handler的实现不需要加锁，一方面对性能有帮助，另一方面避免多线程编程的复杂度。 \n 2.1 改进后的模型图 \n \n reactor是高并发编程的基础模型，比如netty，kafka都是以它为模型构建。 \n 代码 \n BIO（Blocking IO） \n 1、客户端 \n @Slf4j \n public   class   BioClient   { \n\n\t public   static   void   main ( String [ ]  args )   throws   IOException   { \n\t\t Socket  socket  =   new   Socket ( "127.0.0.1" ,   9099 ) ; \n\t\t // 向服务端发送数据 \n\t\tsocket . getOutputStream ( ) . write ( "Hello BioServer" . getBytes ( ) ) ; \n\t\tsocket . getOutputStream ( ) . flush ( ) ; \n\t\tlog . info ( "向服务端发送数据结束" ) ; \n\t\t byte [ ]  bytes  =   new   byte [ 1024 ] ; \n\t\t //接收服务端回传的数据 \n\t\tsocket . getInputStream ( ) . read ( bytes ) ; \n\t\tlog . info ( "接收到服务端返回的数据：{}" ,   new   String ( bytes ) ) ; \n\t\tsocket . close ( ) ; \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 2、服务端 \n @Slf4j \n public   class   BioServer   { \n\t public   static   void   main ( String [ ]  args )   throws   IOException   { \n\t\t ServerSocket  serverSocket  =   new   ServerSocket ( 9099 ) ; \n\t\t while   ( true )   { \n\t\t\tlog . info ( "等待客户端连接" ) ; \n\t\t\t //阻塞方法 \n\t\t\t Socket  socket  =  serverSocket . accept ( ) ; \n\t\t\tlog . info ( "客户端连接成功" ) ; \n\t\t\t new   Thread ( ( )   ->   { \n\t\t\t\t try   { \n\t\t\t\t\t handler ( socket ) ; \n\t\t\t\t }   catch   ( IOException  e )   { \n\t\t\t\t\tlog . error ( e . getMessage ( ) ,  e ) ; \n\t\t\t\t } \n\t\t\t } ) . start ( ) ; \n\n\t\t } \n\t } \n\n\t private   static   void   handler ( Socket  socket )   throws   IOException   { \n\t\t byte [ ]  bytes  =   new   byte [ 1024 ] ; \n\n\t\tlog . info ( "开始读取数据" ) ; \n\t\t // 接收客户端的数据，阻塞方法，没有数据可读时就阻塞 \n\t\t int  read  =  socket . getInputStream ( ) . read ( bytes ) ; \n\t\tlog . info ( "读取数据完毕" ) ; \n\t\t if   ( read  !=   - 1 )   { \n\t\t\tlog . info ( "接收到客户端的数据：{}" ,   new   String ( bytes ,   0 ,  read ) ) ; \n\t\t } \n\t\t // 向outputStream中回写数据 \n\t\tsocket . getOutputStream ( ) . write ( "Hello BioClient" . getBytes ( ) ) ; \n\t\tsocket . getOutputStream ( ) . flush ( ) ; \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 3、效果展示 \n根据测试结果，我们不难发现，BIO的服务端是阻塞获取新请求。结合代码，没接收到一个新请求，就开启一个线程读取对应的数据。 \n 服务端： \n \n 客户端： \n \n 4、总结 \n在BIO模式下，会存在一下问题： \n 问题一：只能以阻塞的形式读取每一次的请求。 \n 问题二：如果客户端发送的数据为0，那么服务端在读取数据的时候也会阻塞读取，直到获取到数据才会向下执行，即下面这行代码也是阻塞的。 \n int  read  =  socket . getInputStream ( ) . read ( bytes ) ; \n \n 1 BIO升级为每接收到一个连接，就开启一个线程 \n 问题三：并发请求10W个连接，此时就会开启10W个线程，资源耗费极大 \n 问题四：和问题二相同，如果其中一个线程迟迟没有获取到数据，这就会导致部分线程卡死 \n BIO再次升级，升级为一个线程池的使用 \n 问题五：虽然线程池能够解决线程开启和释放的性能损耗，但是依然会出现问题四 \n BIO 方式适用于连接数目比较小且固定的架构， 这种方式对服务器资源要求比较高， 但程序简单易理解 \n NIO（NonBlocking IO） \n 1、客户端 \n @Slf4j \n public   class   NioClient   { \n\t public   static   void   main ( String [ ]  args )   throws   IOException   { \n\n\n\t\t SocketChannel  sc  =   SocketChannel . open ( ) ; \n\t\tsc . connect ( new   InetSocketAddress ( "localhost" ,   8099 ) ) ; \n\n\t\tsc . write ( Charset . defaultCharset ( ) . encode ( "Hello NioServer" ) ) ; \n\t\tlog . info ( "发送数据成功" ) ; \n\t\t ByteBuffer  buffer  =   ByteBuffer . allocate ( 16 ) ; \n\n\t\t int  read  =  sc . read ( buffer ) ; \n\t\t if   ( read  >   0 )   { \n\t\t\tlog . info ( "读取到服务端返回的 {} 长度的数据" ,  read ) ; \n\t\t } \n\n\t\tsc . write ( Charset . defaultCharset ( ) . encode ( "Hello NioServer2" ) ) ; \n\t\tlog . info ( "再次发送数据成功" ) ; \n\n\t\t System . in . read ( ) ;   // 阻塞线程 \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 2、服务端 \n @Slf4j \n public   class   NioServer   { \n\n\t public   static   void   main ( String [ ]  args )   throws   IOException ,   InterruptedException   { \n\t\t ByteBuffer  buffer  =   ByteBuffer . allocate ( 16 ) ; \n\t\t ServerSocketChannel  ssc  =   ServerSocketChannel . open ( ) ; \n\t\tssc . configureBlocking ( false ) ; // accept不会阻塞 \n\t\tssc . bind ( new   InetSocketAddress ( 8099 ) ) ; \n\t\t List < SocketChannel >  channelList  =   new   ArrayList < > ( ) ; \n\t\t while   ( true )   { \n\t\t\tlog . info ( "开始接收请求" ) ; \n\t\t\t SocketChannel  accept  =  ssc . accept ( ) ; \n\t\t\t if   ( accept  !=   null )   { \n\t\t\t\tlog . info ( "获取到对应的连接"   +  accept ) ; \n\t\t\t\taccept . configureBlocking ( false ) ;   // read不会阻塞 \n\t\t\t\tchannelList . add ( accept ) ; \n\t\t\t } \n\n\t\t\t for   ( SocketChannel  channel  :  channelList )   { \n\t\t\t\tlog . info ( "开始读取客户端的数据" ,  channel ) ; \n\t\t\t\t int  read  =  channel . read ( buffer ) ; \n\t\t\t\t if   ( read  >   0 )   { \n\t\t\t\t\tbuffer . flip ( ) ; \n\t\t\t\t\tlog . info ( "读取到 {} 长度的数据" ,  read ) ; \n\t\t\t\t\t ByteBuffer  retBuf  =   ByteBuffer . wrap ( "Hi NioClient" . getBytes ( ) ) ; \n\t\t\t\t\t TimeUnit . SECONDS . sleep ( 5 ) ;   // 阻塞线程，便于观察控制台 \n\t\t\t\t\tchannel . write ( retBuf ) ; \n\t\t\t\t\tbuffer . clear ( ) ; \n\t\t\t\t\tlog . info ( "数据读取完成" ,  channel ) ; \n\t\t\t\t } \n\t\t\t } \n\t\t } \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 3、效果展示 \nNIO区别于BIO最大的特点就是是否是阻塞获取请求（一定要区分开Java的NIO包和NIO这个概念）。NIO会不断的轮询获取请求，即控制台中不断的轮询打印开始接受请求，如果接收到数据，就会像控制台那样，直接读取对应的数据。结尾的报错是因为我关闭了客户端的连接 \n 服务端： \n \n 客户端： \n \n 4、总结 \n 对然NIO不再像BIO那样会阻塞线程，造成网络堵塞，但是它不停的去轮询线程，会造成CPU空转，这也是我们不想看到的。如果大量连接不涉及写数据，但此时这个线程轮询获取写入数据的操作就是无意义的。 \n 多路复用（Multiplexing） \n 1、客户端 \n 多路复用客户端代码结构为： \n 初始化对应的与服务端的连接，这里的初始化包含初始化channel和Selector，并且完成对应事件和channel的绑定关系 \n开启连接，内部逻辑为依靠selector进行监听对应的事件（这就是我们所说的Reactor事件处理机制），如果监听到对应的连接信息，就根据连接中的事件进行判定，然后再执行不同的业务逻辑 \n @Slf4j \n public   class   MultioClient   { \n\n\t private   Selector  selector ; \n\n\t public   static   void   main ( String [ ]  args )   throws   IOException   { \n\t\t MultioClient  client  =   new   MultioClient ( ) ; \n\t\tclient . initClient ( "127.0.0.1" ,   9099 ) ; \n\t\tclient . connect ( ) ; \n\t } \n\n\t /**\n\t * 获得一个Socket通道，并对该通道做一些初始化的工作\n\t *\n\t * @param ip   连接的服务器的ip\n\t * @param port 连接的服务器的端口号\n\t */ \n\t public   void   initClient ( String  ip ,   int  port )   throws   IOException   { \n\t\t // 获得一个Socket通道 \n\t\t SocketChannel  channel  =   SocketChannel . open ( ) ; \n\t\t // 设置通道为非阻塞 \n\t\tchannel . configureBlocking ( false ) ; \n\t\t // 获得一个通道管理器 \n\t\t this . selector  =   Selector . open ( ) ; \n\t\t // 客户端连接服务器,其实方法执行并没有实现连接，需要在listen（）方法中调 \n\t\t // 用channel.finishConnect() 才能完成连接 \n\t\tlog . info ( "第1步：客户端初始化，发起连接，触发连接事件请求" ) ; \n\t\tchannel . connect ( new   InetSocketAddress ( ip ,  port ) ) ; \n\t\t // 将通道管理器和该通道绑定，并为该通道注册SelectionKey.OP_CONNECT事件（连接事件）。 \n\n\t\tchannel . register ( selector ,   SelectionKey . OP_CONNECT ) ; \n\t } \n\n\t /**\n\t * 采用轮询的方式监听selector上是否有需要处理的事件，如果有，则进行处理\n\t */ \n\t public   void   connect ( )   throws   IOException   { \n\t\t // 轮询访问selector \n\t\t while   ( true )   { \n\t\t\tselector . select ( ) ; \n\t\t\t // 获得selector中选中的项的迭代器 \n\t\t\t Iterator < SelectionKey >  it  =   this . selector . selectedKeys ( ) . iterator ( ) ; \n\t\t\t while   ( it . hasNext ( ) )   { \n\t\t\t\t SelectionKey  key  =  it . next ( ) ; \n\t\t\t\t // 删除已选的key,以防重复处理 \n\t\t\t\tit . remove ( ) ; \n\t\t\t\t // 连接事件发生 \n\t\t\t\t if   ( key . isConnectable ( ) )   { \n\t\t\t\t\t SocketChannel  channel  =   ( SocketChannel )  key . channel ( ) ; \n\t\t\t\t\t // 如果正在连接，则完成连接 \n\t\t\t\t\t if   ( channel . isConnectionPending ( ) )   { \n\t\t\t\t\t\tchannel . finishConnect ( ) ; \n\t\t\t\t\t } \n\t\t\t\t\t // 设置成非阻塞 \n\t\t\t\t\tchannel . configureBlocking ( false ) ; \n\t\t\t\t\t // 在这里可以给服务端发送信息哦 \n\t\t\t\t\t ByteBuffer  buffer  =   ByteBuffer . wrap ( "Hello MultServer" . getBytes ( ) ) ; \n\n\t\t\t\t\tlog . info ( "第3步：向服务端发送数据" ) ; \n\t\t\t\t\tchannel . write ( buffer ) ; \n\t\t\t\t\t // 在和服务端连接成功之后，为了可以接收到服务端的信息，需要给通道设置读的权限。 \n\t\t\t\t\tchannel . register ( this . selector ,   SelectionKey . OP_READ ) ;    // 获得了可读的事件 \n\t\t\t\t }   else   if   ( key . isReadable ( ) )   { \n\t\t\t\t\tlog . info ( "第6步：读取返回的数据" ) ; \n\t\t\t\t\t read ( key ) ; \n\t\t\t\t } \n\n\t\t\t } \n\t\t } \n\t } \n\n\t /**\n\t * 处理读取服务端发来的信息事件\n\t *\n\t * @param key\n\t */ \n\t public   void   read ( SelectionKey  key )   throws   IOException   { \n\t\t //和服务端的read方法一样 \n\t\t // 服务器可读取消息:得到事件发生的Socket通道 \n\t\t SocketChannel  channel  =   ( SocketChannel )  key . channel ( ) ; \n\t\t // 创建读取的缓冲区 \n\t\t ByteBuffer  buffer  =   ByteBuffer . allocate ( 1024 ) ; \n\t\t int  len  =  channel . read ( buffer ) ; \n\t\t if   ( len  !=   - 1 )   { \n\t\t\tlog . info ( "客户端收到信息："   +   new   String ( buffer . array ( ) ,   0 ,  len ) ) ; \n\t\t } \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 服务端 \n多路复用服务端代码逻辑： \n 同样的先初始化出channel和selector，然后完成事件的注册。需要区分的是客户端的channel是SocketChannel，服务端的channel是ServerSocketChannel。 \n同样的依靠Reactor事件处理机制去监听对应的请求中的事件，然后根据事件的类型做出不同的判定 \n @Slf4j \n public   class   MultioServer   { \n\n\n\t public   static   void   main ( String [ ]  args )   throws   IOException   { \n\t\t // 创建一个在本地端口进行监听的服务Socket通道.并设置为非阻塞方式 \n\t\t ServerSocketChannel  ssc  =   ServerSocketChannel . open ( ) ; \n\t\t // 必须配置为非阻塞才能往selector上注册，否则会报错，selector模式本身就是非阻塞模式 \n\t\tssc . configureBlocking ( false ) ; \n\t\tssc . socket ( ) . bind ( new   InetSocketAddress ( 9099 ) ) ; \n\t\t // 创建一个选择器selector \n\t\t Selector  selector  =   Selector . open ( ) ; \n\t\t // 把ServerSocketChannel注册到selector上，并且selector对客户端accept连接操作感兴趣 \n\t\tssc . register ( selector ,   SelectionKey . OP_ACCEPT ) ; \n\n\t\t while   ( true )   { \n\t\t\tlog . info ( "等待事件发生——" ) ; \n\t\t\t // 轮询监听channel里的key，select是阻塞的，accept()也是阻塞的 \n\t\t\t int  select  =  selector . select ( ) ; \n\n\t\t\tlog . info ( "有事件发生——" ) ; \n\t\t\t // 有客户端请求，被轮询监听到 \n\t\t\t Iterator < SelectionKey >  it  =  selector . selectedKeys ( ) . iterator ( ) ; \n\t\t\t while   ( it . hasNext ( ) )   { \n\t\t\t\t SelectionKey  key  =  it . next ( ) ; \n\t\t\t\t //删除本次已处理的key，防止下次select重复处理 \n\t\t\t\tit . remove ( ) ; \n\t\t\t\t handle ( key ) ; \n\t\t\t } \n\t\t } \n\t } \n\n\t /**\n\t * @param key 对应的事件key\n\t */ \n\t private   static   void   handle ( SelectionKey  key )   throws   IOException   { \n\t\t if   ( key . isAcceptable ( ) )   { \n\n\t\t\tlog . info ( "第2步：有客户端[连接事件]发生" ) ; \n\t\t\t ServerSocketChannel  ssc  =   ( ServerSocketChannel )  key . channel ( ) ; \n\t\t\t // NIO非阻塞体现：此处accept方法是阻塞的，但是这里因为是发生了连接事件，所以这个方法会马上执行完，不会阻塞 \n\t\t\t // 处理完连接请求不会继续等待客户端的数据发送 \n\t\t\t SocketChannel  sc  =  ssc . accept ( ) ; \n\t\t\tsc . configureBlocking ( false ) ; \n\t\t\t // 通过Selector监听Channel时对读事件感兴趣 \n\t\t\tsc . register ( key . selector ( ) ,   SelectionKey . OP_READ ) ; \n\t\t }   else   if   ( key . isReadable ( ) )   { \n\n\t\t\tlog . info ( "第4步：有客户端数据[可读事件]发生" ) ; \n\t\t\t SocketChannel  sc  =   ( SocketChannel )  key . channel ( ) ; \n\t\t\t ByteBuffer  buffer  =   ByteBuffer . allocate ( 1024 ) ; \n\t\t\t // NIO非阻塞体现：首先read方法不会阻塞，其次这种事件响应模型，当调用到read方法时肯定是发生了客户端发送数据的事件 \n\t\t\t int  len  =  sc . read ( buffer ) ; \n\t\t\t if   ( len  !=   - 1 )   { \n\t\t\t\tlog . info ( "读取到客户端发送的数据，可进行相应的业务处理：" , new   String ( buffer . array ( ) ,   0 ,  len ) ) ; \n\t\t\t } \n\t\t\t // 向客户端写入数据 \n\t\t\t ByteBuffer  bufferToWrite  =   ByteBuffer . wrap ( "Hello MultClient" . getBytes ( ) ) ; \n\t\t\tsc . write ( bufferToWrite ) ; \n\t\t\t // 会触发key.isWritable() 或 key.isReadable()的条件 \n\t\t\tkey . interestOps ( SelectionKey . OP_READ   |   SelectionKey . OP_WRITE ) ; \n\t\t }   else   if   ( key . isWritable ( ) )   { \n\n\t\t\tlog . info ( "第5步：有客户端数据[写入事件]发生了" ) ; \n\t\t\t SocketChannel  sc  =   ( SocketChannel )  key . channel ( ) ; \n\t\t\t // NIO事件触发是水平触发 \n\t\t\t // 使用Java的NIO编程的时候，在没有数据可以往外写的时候要取消写事件， \n\t\t\t // 在有数据往外写的时候再注册写事件 \n\t\t\tkey . interestOps ( SelectionKey . OP_READ ) ; \n\t\t } \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 3、效果展示 \n代码执行逻辑为： \n 首先客户端会初始化，然后去连接服务端 \n服务端接收到对应的连接事件后 \n然后客户端向服务端写入数据 \n此时服务端接收到客户端发送过来的连接，触发可读事件（key.isReadable()） \n读取完数据之后，服务端准备向客户端写入数据，此时触发服务端的可写事件，没有数据可以写入的时候取消写事件 \n最终客户端收到服务端写会的数据 \n客户端： \n \n 服务端： \n \n 4、总结 \n 多路复用其实是一个相对完美的解决方案，即采用了Reactor模型，当对应的读、写、连接等事件触发的时候，再触发对应的接收请求的逻辑，然后完成后续的业务代码。既不会造成空轮询无效请求，也不会阻塞网络。 \n 正式基于种种特点，Netty就在该IO模型的基础上进行封装。 \n Netty（基于多路复用IO） \n 1、客户端 \nNetty客户端代码如下，代码基本骨架都差不多 \n 1.船舰对应的group，你可以理解为用来接收请求和分配工作线程的线程池组 \n2.创建对应的channel \n3.创建需要对数据进行处理的handler。handler分为两大类：ChannelInboundHandler和4.ChannelOutboundHandler，你可以理解为是对管道中数据入口方向和出口方向数据的处理。 \n5.连接对应的服务端地址 \n6.sync阻塞住线程，连接好后再继续向下执行，否则就是异步进行连接 \n @Slf4j \n public   class  netty客户端  { \n\t public   static   void   main ( String [ ]  args )   throws   InterruptedException ,   IOException   { \n\n\t\t new   Bootstrap ( ) \n\t\t\t\t . group ( new   NioEventLoopGroup ( ) ) \n\t\t\t\t . channel ( NioSocketChannel . class ) \n\t\t\t\t . handler ( new   ChannelInitializer < NioSocketChannel > ( )   { \n\t\t\t\t\t @Override \n\t\t\t\t\t protected   void   initChannel ( NioSocketChannel  ch )   throws   Exception   { \n\t\t\t\t\t\tch . pipeline ( ) . addLast ( new   StringEncoder ( ) ) ; \n\t\t\t\t\t\tch . pipeline ( ) . addLast ( new   StringDecoder ( ) ) ;                          // inputStream \n\t\t\t\t\t\tch . pipeline ( ) . addLast ( new   ChannelInboundHandlerAdapter ( )   { \n\t\t\t\t\t\t\t // 接收响应消息 \n\t\t\t\t\t\t\t @Override \n\t\t\t\t\t\t\t public   void   channelRead ( ChannelHandlerContext  ctx ,   Object  msg )   { \n\t\t\t\t\t\t\t\tlog . info ( "第4步：netty客户端接收到服务端返回的数据为：{}" ,  msg ) ; \n\t\t\t\t\t\t\t } \n\n\t\t\t\t\t\t\t @Override \n\t\t\t\t\t\t\t public   void   channelActive ( ChannelHandlerContext  ctx )   { \n\t\t\t\t\t\t\t\tlog . info ( "第1步：客户端启动触发事件" ) ; \n\t\t\t\t\t\t\t\tctx . writeAndFlush ( "Hello NettyServer" ) ; \n\t\t\t\t\t\t\t } \n\t\t\t\t\t\t } ) ; \n\t\t\t\t\t } \n\t\t\t\t } ) \n\t\t\t\t . connect ( new   InetSocketAddress ( "localhost" ,   8099 ) ) \n\t\t\t\t . sync ( ) ; \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 服务端 \n服务端代码格式和客户端差不多，最开始创建的Bootstrap对象变成了ServerBootstrap。同样的，核心逻辑在initChannel方法里面。 \n StringEncoder和StringDecoder为一个编解码器，我调用的writeAndFlush方法是直接写入的字节，需要在服务端和客户端分别对该字节进行相同格式的编解码工作，才能够收到对应的数据信息 \n @Slf4j \n public   class  netty服务端  { \n\t public   static   void   main ( String [ ]  args )   { \n\n\t\t new   ServerBootstrap ( ) \n\t\t\t\t . group ( new   NioEventLoopGroup ( ) ) \n\t\t\t\t . channel ( NioServerSocketChannel . class ) \n\t\t\t\t . childHandler ( new   ChannelInitializer < NioSocketChannel > ( )   { \n\t\t\t\t\t @Override \n\t\t\t\t\t protected   void   initChannel ( NioSocketChannel  ch )   throws   Exception   { \n\t\t\t\t\t\tch . pipeline ( ) . addLast ( new   StringEncoder ( ) ) ; \n\t\t\t\t\t\tch . pipeline ( ) . addLast ( new   StringDecoder ( ) ) ; \n\t\t\t\t\t\tch . pipeline ( ) . addLast ( new   ChannelInboundHandlerAdapter ( )   { \n\n\t\t\t\t\t\t\t @Override \n\t\t\t\t\t\t\t public   void   channelRead ( ChannelHandlerContext  ctx ,   Object  msg )   throws   Exception   { \n\t\t\t\t\t\t\t\tlog . info ( "第2步：服务端接收消息：{}" ,  msg ) ; \n\t\t\t\t\t\t\t } \n\n\t\t\t\t\t\t\t @Override \n\t\t\t\t\t\t\t public   void   channelReadComplete ( ChannelHandlerContext  ctx )   throws   Exception   { \n\t\t\t\t\t\t\t\tlog . info ( "第3步：服务端接收到消息后进行业务处理" ) ; \n\t\t\t\t\t\t\t\tctx . channel ( ) . writeAndFlush ( "Hello NettyClient" ) ; \n\t\t\t\t\t\t\t } \n\t\t\t\t\t\t } ) ; \n\t\t\t\t\t } \n\t\t\t\t } ) \n\t\t\t\t . bind ( 8099 ) ; \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 3、效果展示 \n 服务端： \n \n 客户端： \n \n 4、总结 \n 当你熟悉了Netty代码的编写风格之后，你会发现，我们想要对数据进行处理，只需要编写对应的ChannelHandler，其他的额外的关于数据的解析、读取等繁琐操作都被Netty封装了，即开发人员能够更加专注于业务开发，而非投身于繁琐的字节解析工作中。 \n AIO（Asynchronous IO） \n 1、客户端 \n \n 对应的Channel变成了AsynchronousSocketChannel \n \n @Slf4j \n public   class   AIOClient   { \n\n\t public   static   void   main ( String . . .  args )   throws   Exception   { \n\t\t AsynchronousSocketChannel  socketChannel  =   AsynchronousSocketChannel . open ( ) ; \n\t\tsocketChannel . connect ( new   InetSocketAddress ( "127.0.0.1" ,   9000 ) ) . get ( ) ; \n\t\tsocketChannel . write ( ByteBuffer . wrap ( "Hello AioServer" . getBytes ( ) ) ) ; \n\t\t ByteBuffer  buffer  =   ByteBuffer . allocate ( 512 ) ; \n\t\t Integer  len  =  socketChannel . read ( buffer ) . get ( ) ; \n\t\t if   ( len  !=   - 1 )   { \n\t\t\tlog . info ( "客户端收到信息："   +   new   String ( buffer . array ( ) ,   0 ,  len ) ) ; \n\t\t } \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 2.服务端 \n @Slf4j \n public   class   AIOServer   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         final   AsynchronousServerSocketChannel  serverChannel  = \n                 AsynchronousServerSocketChannel . open ( ) . bind ( new   InetSocketAddress ( 9000 ) ) ; \n\n        serverChannel . accept ( null ,   new   CompletionHandler < AsynchronousSocketChannel ,   Object > ( )   { \n             @Override \n             public   void   completed ( AsynchronousSocketChannel  socketChannel ,   Object  attachment )   { \n                 try   { \n                     // 再此接收客户端连接，如果不写这行代码后面的客户端连接连不上服务端 \n                    serverChannel . accept ( attachment ,   this ) ; \n                    log . info ( socketChannel . getRemoteAddress ( ) . toString ( ) ) ; \n                     ByteBuffer  buffer  =   ByteBuffer . allocate ( 1024 ) ; \n                    socketChannel . read ( buffer ,  buffer ,   new   CompletionHandler < Integer ,   ByteBuffer > ( )   { \n                         @Override \n                         public   void   completed ( Integer  result ,   ByteBuffer  buffer )   { \n                            buffer . flip ( ) ; \n                            log . info ( new   String ( buffer . array ( ) ,   0 ,  result ) ) ; \n                            socketChannel . write ( ByteBuffer . wrap ( "Hello AioClient" . getBytes ( ) ) ) ; \n                         } \n\n                         @Override \n                         public   void   failed ( Throwable  exc ,   ByteBuffer  buffer )   { \n                            exc . printStackTrace ( ) ; \n                         } \n                     } ) ; \n                 }   catch   ( IOException  e )   { \n                    e . printStackTrace ( ) ; \n                 } \n             } \n\n             @Override \n             public   void   failed ( Throwable  exc ,   Object  attachment )   { \n                exc . printStackTrace ( ) ; \n             } \n         } ) ; \n\n         Thread . sleep ( Integer . MAX_VALUE ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 3、效果展示 \n 演示的效果也很好理解，就是我们常规的理解的异步获取到数据。 \n 客户端： \n \n 服务端： \n \n 4、总结 \n异步非阻塞， 由操作系统完成后回调通知服务端程序启动线程去处理， 一般适用于连接数较多且连接时间较长的应用，JDK7 开始支持。 \n 在Linux系统上，AIO的底层实现仍使用Epoll，没有很好实现AIO，因此在性能上没有明显的优势，而且被JDK封装了一层不容易深度优化，Linux上AIO还不够成熟。Netty是异步非阻塞框架，Netty在JDK的NIO上做了很多异步的封装。 \n \n'},{frontmatter:{},regularPath:"/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%84%9A%E6%9C%AC/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html",relativePath:"常用命令脚本/git常用命令.md",key:"v-e7d27b94",path:"/1970/01/01/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/",headers:[{level:2,title:"git config",slug:"git-config"},{level:2,title:"git clone",slug:"git-clone"},{level:2,title:"git init",slug:"git-init"},{level:2,title:"git status",slug:"git-status"},{level:2,title:"git remote",slug:"git-remote"},{level:2,title:"git branch",slug:"git-branch"},{level:2,title:"git checkout",slug:"git-checkout"},{level:2,title:"git cherry-pick",slug:"git-cherry-pick"},{level:2,title:"git add",slug:"git-add"},{level:2,title:"git commit",slug:"git-commit"},{level:2,title:"git fetch",slug:"git-fetch"},{level:2,title:"git merge",slug:"git-merge"},{level:2,title:"git diff",slug:"git-diff"},{level:2,title:"git pull",slug:"git-pull"},{level:2,title:"git push",slug:"git-push"},{level:2,title:"git log",slug:"git-log"},{level:2,title:"git reset",slug:"git-reset"},{level:2,title:"git revert",slug:"git-revert"},{level:2,title:"git tag",slug:"git-tag"},{level:2,title:"git mv",slug:"git-mv"},{level:2,title:"git rm",slug:"git-rm"},{level:2,title:"Git操作场景示例",slug:"git操作场景示例"},{level:3,title:"1. 删除掉本地不存在的远程分支",slug:"_1-删除掉本地不存在的远程分支"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' git  init   #版本库初始化 \n git  clone https://gitlab.com/GordonChanFZ/recommendsystem.git  #克隆 \n git  remote  add  origin https://gitlab.com/GordonChanFZ/recommendsystem.git    #增加远程仓库 \n git  branch  # 列出本地的所有分支，当前所在分支以 "*" 标出 \n git  branch chenguobin  #创建新分支 \n git   add  \n git  commit\n git  push origin chenguobin:chenguobin  #推送到远程仓库 \n \n 1 2 3 4 5 6 7 8 #  git config \n查看配置信息 \n--local：仓库级，--global：全局级，--system：系统级 \n$ git config  < - - local  |   - - global  |   - - system >   - l\n查看当前生效的配置信息 \n$ git config  - l\n编辑配置文件 \n--local：仓库级，--global：全局级，--system：系统级 \n$ git config  < - - local  |   - - global  |   - - system >   - e\n添加配置项 \n--local：仓库级，--global：全局级，--system：系统级 \n$ git config  < - - local  |   - - global  |   - - system >   - - add  < name >   < value > \n获取配置项 \n$ git config  < - - local  |   - - global  |   - - system >   - - get  < name > \n删除配置项 \n$ git config  < - - local  |   - - global  |   - - system >   - - unset  < name > \n配置提交记录中的用户信息 \n$ git config  - - global user . name  < 用户名 > \n$ git config  - - global user . email  < 邮箱地址 > \n更改Git缓存区的大小 \n如果提交的内容较大，默认缓存较小，提交会失败 \n缓存大小单位：B，例如：524288000（500MB） \n$ git config  - - global http . postBuffer  < 缓存大小 > \n调用 git status/git diff 命令时以高亮或彩色方式显示改动状态 \n$ git config  - - global color . ui  true \n配置可以缓存密码，默认缓存时间15分钟 \n$ git config  - - global credential . helper cache\n配置密码的缓存时间 \n缓存时间单位：秒 \n$ git config  - - global credential . helper  \'cache --timeout=<缓存时间>\' \n配置长期存储密码 \n$ git config  - - global credential . helper store\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 #  git clone \n 从远程仓库克隆一个版本库到本地。 \n默认在当前目录下创建和版本库名相同的文件夹并下载版本到该文件夹下 \n$  git  clone  < 远程仓库的网址 > \n指定本地仓库的目录 \n$  git  clone  < 远程仓库的网址 >   < 本地目录 > \n-b 指定要克隆的分支，默认是master分支 \n$  git  clone  < 远程仓库的网址 >   -b   < 分支名称 >   < 本地目录 > \n \n 1 2 3 4 5 6 7 8 #  git init \n 初始化项目所在目录，初始化后会在当前目录下出现一个名为 .git 的目录。 \n初始化本地仓库，在当前目录下生成 .git 文件夹 \n$ git init\n \n 1 2 #  git status \n 查看本地仓库的状态。 \n查看本地仓库的状态 \n$ git status\n以简短模式查看本地仓库的状态 \n会显示两列，第一列是文件的状态，第二列是对应的文件 \n文件状态：A 新增，M 修改，D 删除，?? 未添加到Git中 \n$ git status  - s\n \n 1 2 3 4 5 6 7 #  git remote \n 操作远程库。 \n列出已经存在的远程仓库 \n$ git remote\n列出远程仓库的详细信息，在别名后面列出URL地址 \n$ git remote  - v\n$ git remote  - - verbose\n添加远程仓库 \n$ git remote add  < 远程仓库的别名 >   < 远程仓库的 URL 地址 > \n修改远程仓库的别名 \n$ git remote rename  < 原远程仓库的别名 >   < 新的别名 > \n删除指定名称的远程仓库 \n$ git remote remove  < 远程仓库的别名 > \n修改远程仓库的 URL 地址 \n$ git remote set - url  < 远程仓库的别名 >   < 新的远程仓库 URL 地址 > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #  git branch \n 操作 Git 的分支命令。 \n列出本地的所有分支，当前所在分支以 "*" 标出 \n$ git branch\n列出本地的所有分支并显示最后一次提交，当前所在分支以 "*" 标出 \n$ git branch  - v\n创建新分支，新的分支基于上一次提交建立 \n$ git branch  < 分支名 > \n修改分支名称 \n如果不指定原分支名称则为当前所在分支 \n$ git branch  - m  [ < 原分支名称 > ]   < 新的分支名称 > \n强制修改分支名称 \n$ git branch  - M   [ < 原分支名称 > ]   < 新的分支名称 > \n删除指定的本地分支 \n$ git branch  - d  < 分支名称 > \n强制删除指定的本地分支 \n$ git branch  - D   < 分支名称 > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #  git checkout \n 检出命令，用于创建、切换分支等。 \n切换到已存在的指定分支 \n$ git checkout  < 分支名称 > \n创建并切换到指定的分支，保留所有的提交记录 \n等同于 "git branch" 和 "git checkout" 两个命令合并 \n$ git checkout  - b  < 分支名称 > \n创建并切换到指定的分支，删除所有的提交记录 \n$ git checkout  - - orphan  < 分支名称 > \n替换掉本地的改动，新增的文件和已经添加到暂存区的内容不受影响 \n$ git checkout  < 文件路径 > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 #  git cherry-pick \n 把已经提交的记录合并到当前分支。 \n把已经提交的记录合并到当前分支 \n$ git cherry - pick  < commit  ID > \n \n 1 2 #  git add \n 把要提交的文件的信息添加到暂存区中。当使用 git commit 时，将依据暂存区中的内容来进行文件的提交。 \n把指定的文件添加到暂存区中 \n$ git  add   < 文件路径 > \n添加所有修改、已删除的文件到暂存区中 \n$ git  add   - u  [ < 文件路径 > ] \n$ git  add   -- update  [ < 文件路径 > ] \n添加所有修改、已删除、新增的文件到暂存区中，省略 <文件路径> 即为当前目录 \n$ git  add   - A  [ < 文件路径 > ] \n$ git  add   -- all  [ < 文件路径 > ] \n查看所有修改、已删除但没有提交的文件，进入一个子命令系统 \n$ git  add   - i  [ < 文件路径 > ] \n$ git  add   -- interactive  [ < 文件路径 > ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #  git commit \n 将暂存区中的文件提交到本地仓库中。 \n把暂存区中的文件提交到本地仓库，调用文本编辑器输入该次提交的描述信息 \n$ git commit\n把暂存区中的文件提交到本地仓库中并添加描述信息 \n$ git commit  - m  "<提交的描述信息>" \n把所有修改、已删除的文件提交到本地仓库中 \n不包括未被版本库跟踪的文件，等同于先调用了 "git add -u" \n$ git commit  - a  - m  "<提交的描述信息>" \n修改上次提交的描述信息 \n$ git commit  - - amend\n \n 1 2 3 4 5 6 7 8 9 10 11 12 #  git fetch \n 从远程仓库获取最新的版本到本地的 tmp 分支上。 \n将远程仓库所有分支的最新版本全部取回到本地 \n$ git fetch  < 远程仓库的别名 > \n将远程仓库指定分支的最新版本取回到本地 \n$ git fetch  < 远程主机名 >   < 分支名 > \n \n 1 2 3 4 5 #  git merge \n 合并分支。 \n把指定的分支合并到当前所在的分支下 \n$ git merge  < 分支名称 > \n \n 1 2 #  git diff \n 比较版本之间的差异。 \n比较当前文件和暂存区中文件的差异，显示没有暂存起来的更改 \n$ git diff\n比较暂存区中的文件和上次提交时的差异 \n$ git diff  - - cached\n$ git diff  - - staged\n比较当前文件和上次提交时的差异 \n$ git diff  HEAD \n查看从指定的版本之后改动的内容 \n$ git diff  < commit  ID > \n比较两个分支之间的差异 \n$ git diff  < 分支名称 >   < 分支名称 > \n查看两个分支分开后各自的改动内容 \n$ git diff  < 分支名称 > ... < 分支名称 > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #  git pull \n 从远程仓库获取最新版本并合并到本地。 \n首先会执行  git fetch ，然后执行  git merge ，把获取的分支的 HEAD 合并到当前分支。 \n从远程仓库获取最新版本。 \n$ git pull\n \n 1 2 #  git push \n 把本地仓库的提交推送到远程仓库。 \n把本地仓库的分支推送到远程仓库的指定分支 \n$ git push  < 远程仓库的别名 >   < 本地分支名 > : < 远程分支名 > \n 87 cb0b967095a6c07d49505768137528\n删除指定的远程仓库的分支 \n$ git push  < 远程仓库的别名 >   : < 远程分支名 > \n$ git push  < 远程仓库的别名 >   - - delete  < 远程分支名 > \n \n 1 2 3 4 5 6 #  git log \n 显示提交的记录。 \n打印所有的提交记录 \n$  git  log\n打印从第一次提交到指定的提交的记录 \n$  git  log  < commit ID > \n打印指定数量的最新提交的记录 \n$  git  log - < 指定的数量 > \n \n 1 2 3 4 5 6 7 8 #  git reset \n 还原提交记录。 \n重置暂存区，但文件不受影响 \n相当于将用 "git add" 命令更新到暂存区的内容撤出暂存区，可以指定文件 \n没有指定 commit ID 则默认为当前 HEAD \n$ git reset  [ < 文件路径 > ] \n$ git reset  - - mixed  [ < 文件路径 > ] \n将 HEAD 的指向改变，撤销到指定的提交记录，文件未修改 \n$ git reset  < commit  ID > \n$ git reset  - - mixed  < commit  ID > \n将 HEAD 的指向改变，撤销到指定的提交记录，文件未修改 \n相当于调用 "git reset --mixed" 命令后又做了一次 "git add" \n$ git reset  - - soft  < commit  ID > \n将 HEAD 的指向改变，撤销到指定的提交记录，文件也修改了 \n$ git reset  - - hard  < commit  ID > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #  git revert \n 生成一个新的提交来撤销某次提交，此次提交之前的所有提交都会被保留。 \n生成一个新的提交来撤销某次提交 \n$ git revert  < commit  ID > \n \n 1 2 #  git tag \n 操作标签的命令。 \n打印所有的标签 \n$ git tag\n添加轻量标签，指向提交对象的引用，可以指定之前的提交记录 \n$ git tag  < 标签名称 >   [ < commit  ID > ] \n添加带有描述信息的附注标签，可以指定之前的提交记录 \n$ git tag  - a  < 标签名称 >   - m  < 标签描述信息 >   [ < commit  ID > ] \n切换到指定的标签 \n$ git checkout  < 标签名称 > \n查看标签的信息 \n$ git show  < 标签名称 > \n删除指定的标签 \n$ git tag  - d  < 标签名称 > \n将指定的标签提交到远程仓库 \n$ git push  < 远程仓库的别名 >   < 标签名称 > \n将本地所有的标签全部提交到远程仓库 \n$ git push  < 远程仓库的别名 >  –tags\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #  git mv \n 重命名文件或者文件夹。 \n重命名指定的文件或者文件夹 \n$ git mv  < 源文件 / 文件夹 >   < 目标文件 / 文件夹 > \n \n 1 2 #  git rm \n 删除文件或者文件夹。 \n移除跟踪指定的文件，并从本地仓库的文件夹中删除 \n$ git rm  < 文件路径 > \n移除跟踪指定的文件夹，并从本地仓库的文件夹中删除 \n$ git rm  - r  < 文件夹路径 > \n移除跟踪指定的文件，在本地仓库的文件夹中保留该文件 \n$ git rm  - - cached\n \n 1 2 3 4 5 6 7 8 #  Git操作场景示例 \n 1. 删除掉本地不存在的远程分支 \n 多人合作开发时，如果远程的分支被其他开发删除掉，在本地执行  git branch --all  依然会显示该远程分支，可使用下列的命令进行删除： \n使用 pull 命令，添加 -p 参数 \n$ git pull  - p\n等同于下面的命令 \n$ git fetch  - p\n$ git fetch  - - prune origin\n \n 1 2 3 4 5 6 df -h 查看磁盘占用情况 \n du -h --max-depth=1查看目录大小 \n yum clean all 清除yum缓存 \n find -type f -size +50M \n conda clean -p \n'},{frontmatter:{},regularPath:"/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/redis.html",relativePath:"存储引擎/redis.md",key:"v-7f30b557",path:"/1970/01/01/redis/",headers:[{level:2,title:"前言",slug:"前言"},{level:2,title:"redis基本数据结构",slug:"redis基本数据结构"},{level:3,title:"Redis核心对象",slug:"redis核心对象"},{level:3,title:"String类型",slug:"string类型"},{level:3,title:"Hash类型",slug:"hash类型"},{level:3,title:"List类型",slug:"list类型"},{level:3,title:"Set集合",slug:"set集合"},{level:3,title:"ZSet集合",slug:"zset集合"},{level:2,title:"Redis内存管理",slug:"redis内存管理"},{level:3,title:"淘汰策略",slug:"淘汰策略"},{level:3,title:"删除过期键策略",slug:"删除过期键策略"},{level:2,title:"Redis缓存三大问题",slug:"redis缓存三大问题"},{level:3,title:"缓存击穿",slug:"缓存击穿"},{level:3,title:"缓存穿透",slug:"缓存穿透"},{level:3,title:"缓存雪崩",slug:"缓存雪崩"},{level:2,title:"redis持久化",slug:"redis持久化"},{level:3,title:"RDB持久化机制",slug:"rdb持久化机制"},{level:3,title:"AOF持久化机制",slug:"aof持久化机制"},{level:3,title:"混合持久化",slug:"混合持久化"},{level:2,title:"redis事务",slug:"redis事务"},{level:3,title:"Redis事务的应用场景",slug:"redis事务的应用场景"},{level:3,title:"基本操作",slug:"基本操作"},{level:3,title:"错误处理",slug:"错误处理"},{level:3,title:"Redis事务的注意事项与局限性",slug:"redis事务的注意事项与局限性"},{level:3,title:"使用Lua脚本优化Redis事务",slug:"使用lua脚本优化redis事务"},{level:2,title:"Redis的部署模式",slug:"redis的部署模式"},{level:2,title:"生产中的问题",slug:"生产中的问题"},{level:3,title:"1.big key",slug:"_1-big-key"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' title: redis \ndate: 2022-09-08 \nauthor: Gordon \nsidebar: \'auto\' \ncategories: \n \n 存储引擎 \n nosql \ntags: \n kv存储 \n 实时 \n 前言 \n Redis是基于c语言编写的开源非关系型内存数据库，可以用作数据库、缓存、消息中间件。 \n Redis（全称：Remote Dictionary Server，即远程字典服务器）是一个开源的高性能键值数据库和缓存系统。Redis 的数据结构支持字符串、哈希表、列表、集合和有序集合等类型。同时，Redis 还提供了丰富的操作指令，例如 GET/SET、INCR/DECR、HGET/HSET、LPUSH/RPUSH、SADD/SMEMBERS、ZADD/ZRANGE 等。除此之外，Redis 还支持事务、过期时间、发布/订阅等特性，能够方便地实现各种高效的数据存储和读取方案。 \n 脑图 \n redis基本数据结构 \n  Redis 核心对象 \n 在Redis中有一个 核心的对象 叫做 redisObject  ，是用来表示所有的key和value的，用redisObject结构体来表示 String、Hash、List、Set、ZSet 五种数据类型。 \n redisObject 的源代码在 redis.h 中，使用c语言写的，感兴趣的可以自行查看，关于redisObject我这里画了一张图，表示redisObject的结构如下所示： \n \n在redisObject中type表示属于哪种数据类型，encoding表示该数据的存储方式，也就是底层的实现的该数据类型的数据结构。因此这篇文章具体介绍的也是encoding对应的部分。 \n那么encoding中的存储类型又分别表示什么意思呢？具体数据类型所表示的含义，如下图所示： \n \n 图片截图出自《Redis设计与实现第二版》 \n 可能看完这图，还是觉得一脸懵。不慌，会进行五种数据结构的详细介绍，这张图只是让你找到每种中数据结构对应的储存类型有哪些，大概脑子里有个印象。 \n 举一个简单的例子，你在Redis中设置一个字符串 key 234 ，然后查看这个字符串的存储类型就会看到为int类型，非整数型的使用的是embstr储存类型，具体操作如下图所示： \n String类型 \n String是Redis最基本的数据类型，上面的简介中也说到Redis是用c语言开发的。但是Redis中的字符串和c语言中的字符串类型却是有明显的区别。 \n String类型的数据结构存储方式有三种 int、raw、embstr 。那么这三种存储方式有什么区别呢？ \n int \n Redis中规定假如存储的是 整数型值 ，比如 set num 123 这样的类型，就会使用 int的存储方式进行存储，在redisObject的 ptr属性 中就会保存该值。 \n \n SDS（embstr->raw） \n 假如存储的 字符串是一个字符串值并且长度大于44个字节 就会使用 SDS（simple dynamic string） 方式进行存储，并且encoding设置为raw；若是 字符串长度小于等于44个字节 就会将encoding改为embstr来保存字符串。 \n (3.2之前是39字节) \n SDS称为 简单动态字符串 ，对于SDS中的定义在Redis的源码中有的三个属性 int len、int free、char buf[] 。 \n len保存了字符串的长度，free表示buf数组中未使用的字节数量，buf数组则是保存字符串的每一个字符元素。 \n 因此当你在Redsi中存储一个字符串Hello时，根据Redis的源代码的描述可以画出SDS的形式的redisObject结构图如下图所示： \n \n SDS还提供 空间预分配 和 惰性空间释放 两种策略。在为字符串分配空间时，分配的空间比实际要多，这样就能 减少连续的执行字符串增长带来内存重新分配的次数 。 \n 当字符串被缩短的时候，SDS也不会立即回收不适用的空间，而是通过 free 属性将不使用的空间记录下来，等后面使用的时候再释放。 \n 具体的空间预分配原则是： 当修改字符串后的长度len小于1MB，就会预分配和len一样长度的空间，即len=free；若是len大于1MB，free分配的空间大小就为1MB 。 \n SDS是二进制安全的，除了可以储存字符串以外还可以储存二进制文件（如图片、音频，视频等文件的二进制数据）；而c语言中的字符串是以空字符串作为结束符，一些图片中含有结束符，因此不是二进制安全的。 \n String类型应用 \n 说到这里我相信很多人可以说已经精通Redis的String类型了，但是纯理论的精通，理论还是得应用实践，上面说到String可以用来存储图片，现在就以图片存储作为案例实现。 \n （1）首先要把上传得图片进行编码，这里写了一个工具类把 图片处理成了Base64 得编码形式，具体得实现代码如下： \n /**\n\n   * 将图片内容处理成Base64编码格式\n\n   * @param file\n\n   * @return\n\n   */ \n\n   public   static   String   encodeImg ( MultipartFile  file )   { \n\n     byte [ ]  imgBytes  =   null ; \n\n    try   { \n\n      imgBytes  =  file . getBytes ( ) ; \n\n     }   catch   ( IOException  e )   { \n\n      e . printStackTrace ( ) ; \n\n    } \n\n     BASE64Encoder  encoder  =   new   BASE64Encoder ( ) ; \n\n     return  imgBytes == null ? null : encoder . encode ( imgBytes  ) ; \n\n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 （2）第二步就是把处理后的图片字符串格式存储进Redis中，实现得代码如下所示： \n /**\n\n   * Redis存储图片\n\n   * @param file\n\n   * @return\n\n   */ \n\n   public   void   uploadImageServiceImpl ( MultipartFile  image )   { \n\n     String  imgId  =   UUID . randomUUID ( ) . toString ( ) ; \n\n    String  imgStr =   ImageUtils . encodeImg ( image ) ; \n\n    redisUtils . set ( imgId  ,  imgStr ) ; \n\n     // 后续操作可以把imgId存进数据库对应的字段，如果需要从redis中取出，只要获取到这个字段后从redis中取出即可。 \n\n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 这样就是实现了图片得二进制存储，当然String类型得数据结构得应用也还有常规计数： 统计微博数、统计粉丝数 等。 \n  Hash 类型 \n Hash对象的实现方式有两种分别是 ziplist、hashtable ，其中hashtable的存储方式key是String类型的，value也是以 key value 的形式进行存储。 \n 在redis.conf的默认配置为： \n \n 当hash对象同时满足以下两个条件的时候，使用ziplist编码否则使用hashtable： \n a、所有的键值对的键和值的字符串长度都不超过64byte（一个英文字母一个字节）； \n b、哈希对象保存的键值对数量不超过512个。 \n 字典类型的底层就是hashtable实现的，明白了字典的底层实现原理也就是明白了hashtable的实现原理，hashtable的实现原理可以于HashMap的是底层原理相类比。 \n 字典 \n 两者在新增时都会 通过key计算出数组下标，不同的是计算法方式不同 ， HashMap中是以hash函数 的方式，而 hashtable中计算出hash值后，还要通过sizemask 属性和哈希值再次得到数组下标 。 \n 我们知道hash表最大的问题就是hash冲突，为了解决hash冲突，假如hashtable中不同的key通过计算得到同一个index，就会形成单向链表（ 链地址法 ），如下图所示： \n \n rehash \n 在字典的底层实现中，value对象以每一个dictEntry的对象进行存储，当hash表中的存放的键值对不断的增加或者减少时，需要对hash表进行一个扩展或者收缩。 \n 这里就会和HashMap一样也会就进行rehash操作，进行重新散列排布。从上图中可以看到有 ht[0] 和 ht[1] 两个对象，先来看看对象中的属性是干嘛用的。 \n 在hash表结构定义中有四个属性分别是 dictEntry **table、unsigned long size、unsigned long sizemask、unsigned long used ，分别表示的含义就是 哈希表数组、hash表大小、用于计算索引值，总是等于size-1、hash表中已有的节点数 。 \n ht[0]是用来最开始存储数据的，当要进行扩展或者收缩时，ht[0]的大小就决定了ht[1]的大小，ht[0]中的所有的键值对就会重新散列到ht[1]中。 \n 扩展操作：ht[1]扩展的大小是比当前 ht[0].used 值的二倍大的第一个 2 的整数幂；收缩操作：ht[0].used 的第一个大于等于的 2 的整数幂。 \n 当ht[0]上的所有的键值对都rehash到ht[1]中，会重新计算所有的数组下标值，当数据迁移完后ht[0]就会被释放，然后将ht[1]改为ht[0]，并新创建ht[1]，为下一次的扩展和收缩做准备。 \n 渐进式rehash \n 假如在rehash的过程中数据量非常大，Redis不是一次性把全部数据rehash成功，这样会导致Redis对外服务停止，Redis内部为了处理这种情况采用 渐进式的rehash 。 \n Redis将所有的rehash的操作分成多步进行，直到都rehash完成，具体的实现与对象中的 rehashindex 属性相关， 若是rehashindex 表示为-1表示没有rehash操作 。 \n 当rehash操作开始时会将该值改成0，在渐进式rehash的过程 更新、删除、查询会在ht[0]和ht[1]中都进行 ，比如更新一个值先更新ht[0]，然后再更新ht[1]。 \n 而新增操作直接就新增到ht[1]表中，ht[0]不会新增任何的数据，这样保证 ht[0]只减不增，直到最后的某一个时刻变成空表 ，这样rehash操作完成。 \n ziplist \n 压缩列表 （ziplist） 是一组连续内存块组成的顺序的数据结构，压缩列表能够节省空间，压缩列表中使用多个节点来存储数据。 \n 压缩列表是列表键和哈希键底层实现的原理之一， 压缩列表并不是以某种压缩算法进行压缩存储数据，而是它表示一组连续的内存空间的使用，节省空间 ，压缩列表的内存结构图如下： \n \n 压缩列表中每一个节点表示的含义如下所示： \n \n \n zlbytes ：4个字节的大小，记录压缩列表占用内存的字节数。 \n \n \n zltail ：4个字节大小，记录表尾节点距离起始地址的偏移量，用于快速定位到尾节点的地址。 \n \n \n zllen ：2个字节的大小，记录压缩列表中的节点数。 \n \n \n entry ：表示列表中的每一个节点。 \n \n \n zlend ：表示压缩列表的特殊结束符号 \'0xFF\' 。 \n \n \n 在压缩列表中每一个entry节点又有三部分组成，包括 previous_entry_length、encoding、content 。 \n \n \n previous_entry_length 表示前一个节点entry的长度，可用于计算前一个节点的真实地址，因为他们的地址是连续的。 \n \n \n encoding：这里保存的是content的内容类型和长度。 \n \n \n content：content保存的是每一个节点的内容。 \n \n \n hash的应用场景 \n 存储用户数据 （ht） \n 第一个场景比如我们要 储存用户信息 ，一般使用用户的ID作为key值，保持唯一性，用户的其他信息（地址、年龄、生日、电话号码等）作为value值存储。 \n 若是传统的实现就是将用户的信息封装成为一个对象，通过序列化存储数据，当需要获取用户信息的时候，就会通过反序列化得到用户信息。 \n \n 但是这样必然会造成 序列化和反序列化的性能的开销 ，并且若是只修改其中的一个属性值，就需要把整个对象序列化出来，操作的动作太大，造成不必要的性能开销。 \n 若是使用Redis的hash来存储用户数据，就会将原来的value值又看成了一个k v形式的存储容器，这样就不会带来序列化的性能开销的问题。 \n \n 分布式生成唯一ID \n 第二个场景就是生成分布式的唯一ID，这个场景下就是把redis封装成了一个工具类进行实现，实现的代码如下： \n   // offset表示的是id的递增梯度值 \n\n   public   Long   getId ( String  key , String  hashKey , Long  offset )   throws   BusinessException { \n\n     try   { \n\n       if   ( null   ==  offset )   { \n\n        offset = 1L ; \n\n       } \n\n       // 生成唯一id \n\n       return  redisUtil . increment ( key ,  hashKey ,  offset ) ; \n\n     }   catch   ( Exception  e )   { \n\n       //若是出现异常就是用uuid来生成唯一的id值 \n\n       int  randNo = UUID . randomUUID ( ) . toString ( ) . hashCode ( ) ; \n\n       if   ( randNo  <   0 )   { \n\n        randNo = - randNo ; \n\n       } \n\n      return   Long . valueOf ( String . format ( "%16d" ,  randNo ) ) ; \n\n     } \n\n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 UUID \n 算法的核心思想是结合机器的网卡、当地时间、一个随记数来生成UUID。 \n \n 优点：本地生成，生成简单，性能好，没有高可用风险 \n 缺点：长度过长，存储冗余，且无序不可读，查询效率低 \n \n Redis生成方案 \n 利用redis的incr原子性操作自增，一般算法为：  年份 + 当天距当年第多少天 + 天数 + 小时 + redis自增 \n 优点： \n \n 有序递增，可读性强 \n \n 缺点： \n \n 占用带宽，每次要向redis进行请求 \n 存在单点问题，如果集群部署，复杂而如果单单只为了生成ID，得不偿失 \n \n 算法可以调整为 就一个 redis自增，不需要什么年份，多少天等。 \n  List 类型 \n Redis中的列表在3.2之前的版本是使用 ziplist 和 linkedlist 进行实现的。在3.2之后的版本就是引入了 quicklist 替代了ziplist+linkedlist，不需要再进行编码转换。 \n 在redis.conf的默认配置为： \n \n 列表类型 (List) 是⼀个使用线性结构存储的结构，它的元素插入会按照先后顺序存储到链表结构中。 \n列表类型的底层数据结构可以是压缩列表（ZipList)或者链表（LinkedList） \n 同时满足下述两个条件使用ziplist，否则使用链表 \n 1.当列表对象的所有字符串元素长度都不超过64字节 \n 2.保存的元素数量不超过512个 \n ziplist压缩列表上面已经讲过了，我们来看看linkedlist和quicklist的结构是怎么样的。 \n linkedlist是一个双向链表，他和普通的链表一样都是由指向前后节点的指针。插入、修改、更新的时间复杂度尾O(1)，但是查询的时间复杂度确实O(n)。 \n linkedlist和quicklist的底层实现是采用链表进行实现，在c语言中并没有内置的链表这种数据结构，Redis实现了自己的链表结构。 \n \n Redis中链表的特性： \n \n 每一个节点都有指向前一个节点和后一个节点的指针。 \n 头节点和尾节点的prev和next指针指向为null，所以链表是无环的。 \n 链表有自己长度的信息，获取长度的时间复杂度为O(1)。 \n \n 应用：消息队列 \n Set集合 \n Redis中列表和集合都可以用来存储字符串，但是 Set是不可重复的集合，而List列表可以存储相同的字符串 ，Set集合是无序的这个和后面讲的ZSet有序集合相对。 \n Set的底层实现是 ht和intset 。 \n 同时满足以下两个条件使用intset，否则使用ht： \n 1.所有的元素均为整数； \n 2.元素个数不超过512 。 \n ht（哈希表）前面已经详细了解过，下面我们来看看inset类型的存储结构。 \n inset也叫做整数集合，用于保存整数值的数据结构类型，它可以保存 int16_t 、 int32_t  或者 int64_t  的整数值。 \n 在整数集合中，有三个属性值 encoding、length、contents[] ，分别表示编码方式、整数集合的长度、以及元素内容，length就是记录contents里面的大小。 \n 在整数集合新增元素的时候，若是超出了原集合的长度大小，就会对集合进行升级，具体的升级过程如下： \n \n \n 首先扩展底层数组的大小，并且数组的类型为新元素的类型。 \n \n \n 然后将原来的数组中的元素转为新元素的类型，并放到扩展后数组对应的位置。 \n \n \n 整数集合升级后就不会再降级，编码会一直保持升级后的状态。 \n \n \n SRANDMEMBER key [count] \n返回集合中一个或多个随机数 \n SINTER key1 [key2] \n返回给定所有集合的交集 \n 应用场景 \n Set集合的应用场景可以用来 去重、抽奖、共同好友 等业务类型。 \n  ZSet 集合 \n ZSet是有序集合，从上面的图中可以看到ZSet的底层实现是 ziplist 和 skiplist 实现的，ziplist上面已经详细讲过，这里来讲解skiplist的结构实现。 \n 在redis.conf的默认配置为： \n \n 同时满足以下两个条件使用ziplist，否则使用skiplist： \n 1.有序集合保存的元素数量不超过128个 \n 2.有序集合保存的所有元素的长度不超过64字节 \n skiplist 也叫做 跳跃表 ，跳跃表是一种有序的数据结构，它通过每一个节点维持多个指向其它节点的指针，从而达到快速访问的目的。 \n skiplist由如下几个特点： \n \n \n 有很多层组成，由上到下节点数逐渐密集，最上层的节点最稀疏，跨度也最大。 \n \n \n 每一层都是一个有序链表，只扫包含两个节点，头节点和尾节点。 \n \n \n 每一层的每一个每一个节点都含有指向同一层下一个节点和下一层同一个位置节点的指针。 \n \n \n 如果一个节点在某一层出现，那么该以下的所有链表同一个位置都会出现该节点。 \n \n \n 具体实现的结构图如下所示： \n \n 在跳跃表的结构中有head和tail表示指向头节点和尾节点的指针，能后快速的实现定位。level表示层数，len表示跳跃表的长度，BW表示后退指针，在从尾向前遍历的时候使用。 \n BW下面还有两个值分别表示分值（score）和成员对象（各个节点保存的成员对象）。 \n 跳跃表的实现中，除了最底层的一层保存的是原始链表的完整数据，上层的节点数会越来越少，并且跨度会越来越大。 \n 跳跃表的上面层就相当于索引层，都是为了找到最后的数据而服务的，数据量越大，跳表所体现的查询的效率就越高，和平衡树的查询效率相差无几。 \n \n \n \n \n \n \n \n \n \n ZREVRANGE key start stop [WITHSCORES]    返回有序集中指定区间内的成员，通过索引，分数从高到低 \n 按照PV降序获取页面  ZREVRANGE pv_zset 0 -1 \n \n \n \n 应用场景 \n 因为ZSet是有序的集合（每个元素都带有score（权重），以此来对元素进行排序），因此ZSet在实现排序类型的业务是比较常见的，比如在首页推荐10个最热门的帖子，也就是阅读量由高到低，排行榜的实现等业务。 \n  Redis 内存管理 \n 假如你的Redis内存满了怎么办？ 在Redis中有配置参数 maxmemory 可以 设置Redis内存的大小 。 \n 倘若实际的存储中超出了Redis的配置参数的大小时，Redis中有 淘汰策略 ，把 需要淘汰的key给淘汰掉，整理出干净的一块内存给新的key值使用 。 \n 接下来我们就详细的聊一聊Redis中的淘汰策略，并且深入的理解每个淘汰策略的原理和应用的场景。 \n  淘汰策略 \n Redis提供了 8种的淘汰策略 ，其中默认的是 noeviction ，这6中淘汰策略如下： \n \n noeviction ( 默认策略 )：若是内存的大小达到阀值的时候，所有申请内存的指令都会报错。 \n allkeys-lru ：所有key都是使用 LRU算法 进行淘汰。 \n volatile-lru ：所有 设置了过期时间的key使用LRU算法 进行淘汰。 \n volatile-lfu：在设置了过期时间的键空间中，将访问频率最少的键值对淘汰 \n allkeys-lfu：在所有主键空间中，将访问频率最少的键值对淘汰 \n allkeys-random ：所有的key使用 随机淘汰 的方式进行淘汰。 \n volatile-random ：所有 设置了过期时间的key使用随机淘汰 的方式进行淘汰。 \n volatile-ttl ：所有设置了过期时间的key 根据过期时间进行淘汰，越早过期就越快被淘汰 。 \n \n 假如在Redis中的数据有 一部分是热点数据，而剩下的数据是冷门数据 ，或者 我们不太清楚我们应用的缓存访问分布状况 ，这时可以使用 allkeys-lru 。 \n 假如所有的数据访问的频率大概一样，就可以使用 allkeys-random 的淘汰策略。 \n 假如要配置具体的淘汰策略，可以在 redis.conf 配置文件中配置，具体配置如下所示： \n \n 这只需要把注释给打开就可以，并且配置指定的策略方式，另一种的配置方式就是命令的方式进行配置，具体的执行命令如下所示： \n // 获取maxmemory-policy配置 \n 127.0.0.1:6379> config get maxmemory-policy \n // 设置maxmemory-policy配置为allkeys-lru \n 127.0.0.1:6379> config set maxmemory-policy allkeys-lru \n 在介绍8种的淘汰策略方式的时候，说到了LRU算法， 那么什么是LRU算法呢？ \n LRU算法 \n LRU(Least Recently Used) 即表示最近最少使用，也就是在最近的时间内最少被访问的key，算法根据数据的历史访问记录来进行淘汰数据。 \n 它的核心的思想就是： 假如一个key值在最近很少被使用到，那么在将来也很少会被访问 。 \n 实际上Redis实现的LRU并不是真正的LRU算法，也就是名义上我们使用LRU算法淘汰键，但是实际上被淘汰的键并不一定是真正的最久没用的。 \n Redis使用的是近似的LRU算法， 通过随机采集法淘汰key，每次都会随机选出5个key，然后淘汰里面最近最少使用的key 。 \n 这里的5个key只是默认的个数，具体的个数也可以在配置文件中进行配置，在配置文件中的配置如下图所示： \n \n 当近似LRU算法取值越大的时候就会越接近真实的LRU算法，可以这样理解，因为 取值越大那么获取的数据就越全，淘汰中的数据的就越接近最近最少使用的数据 。 \n 那么为了实现根据时间实现LRU算法，Redis必须为每个key中额外的增加一个内存空间用于存储每个key的时间，大小是3字节。 \n 在Redis 3.0中对近似的LRU算法做了一些优化，Redis中会维护大小是 16 的一个候选池的内存。 \n 当第一次随机选取的采样数据，数据都会被放进候选池中，并且候选池中的数据会根据时间进行排序。 \n 当第二次以后选取的数据，只有 小于候选池内的最小时间 的才会被放进候选池中。 \n 当某一时刻候选池的数据满了，那么时间最大的key就会被挤出候选池。当执行淘汰时，直接从候选池中选取最近访问时间最小的key进行淘汰。 \n 这样做的目的就是选取出最近似符合最近最少被访问的key值，能够正确的淘汰key值，因为随机选取的样本中的最小时间可能不是真正意义上的最小时间。 \n 但是LRU算法有如下弊端： \n 1.假如一个key值在以前都没有被访问到，然而最近一次被访问到了，那么就会认为它是热点数据，不会被淘汰； \n 2.有些数据以前经常被访问到，只是最近的时间内没有被访问到，这样就导致这些数据很可能被淘汰掉，这样一来就会出现误判而淘汰热点数据。 \n 于是在Redis 4.0的时候除了LRU算法，新加了一种LFU算法， 那么什么是LFU算法呢？ \n LFU算法 \n LFU(Least Frequently Used) 即表示最近频繁被使用，也就是最近的时间段内，频繁被访问的key，它以最近的时间段的被访问次数的频率作为一种判断标准。 \n 它的核心思想就是：根据key最近被访问的频率进行淘汰，比较少被访问的key优先淘汰，反之则优先保留。 \n LFU算法反映了一个key的热度情况，不会因为LRU算法的偶尔一次被访问被认为是热点数据。 \n 在LFU算法中支持 volatile-lfu 策略和 allkeys-lfu 策略。 \n 以上介绍了Redis的8种淘汰策略，这8种淘汰策略旨在告诉我们怎么做，但是什么时候做？这个还没说，下面我们就来详细的了解Redis什么时候执行淘汰策略。 \n  删除过期键策略 \n 在Redis种有三种删除的操作此策略，分别是： \n \n \n 定时删除 ：创建一个定时器，定时的执行对key的删除操作。 \n \n \n 惰性删除 ：每次只有再访问key的时候，才会检查key的过期时间，若是已经过期了就执行删除。 \n \n \n 定期删除 ：每隔一段时间，就会检查删除掉过期的key。 \n \n \n 定时删除 对于 内存来说是友好的 ，定时清理出干净的空间，但是对于 cpu来说并不是友好的 ，程序需要维护一个定时器，这就会占用cpu资源。 \n 惰性的删除 对于 cpu来说是友好的 ，cpu不需要维护其它额外的操作，但是对于 内存来说是不友好的 ，因为要是有些key一直没有被访问到，就会一直占用着内存。 \n 定期删除是上面两种方案的折中方案**，每隔一段时间删除过期的key，也就是根据具体的业务，合理的取一个时间定期的删除key**。 \n 通过 最合理控制删除的时间间隔 来删除key，减 少对cpu的资源的占用消耗 ，使删除操作合理化。 \n RDB和AOF 的淘汰处理 \n 在Redis中持久化的方式有两种 RDB 和 AOF 。 \n 在RDB中是以快照的形式获取内存中某一时间点的数据副本，在创建RDB文件的时候可以通过 save 和 bgsave 命令执行创建RDB文件。 这两个命令都不会把过期的key保存到RDB文件中，这样也能达到删除过期key的效果。 \n 当在启动Redis载入RDB文件的时候， Master 不会把过期的key载入，而 Slave 会把过期的key载入。 \n 在AOF模式下，Redis提供了Rewrite的优化措施，执行的命令分别是 REWRITEAOF 和 BGREWRITEAOF ， 这两个命令都不会把过期的key写入到AOF文件中，也能删除过期key 。 \n  Redis 缓存三大问题 \n 日常的开发中，无不都是使用数据库来进行数据的存储，由于一般的系统任务中通常不会存在高并发的情况，所以这样看起来并没有什么问题。 \n 一旦涉及大数据量的需求，如一些 商品抢购 的情景，或者 主页访问量 瞬间较大的时候，单一使用数据库来保存数据的系统会因为 面向磁盘 ， 磁盘读/写 速度问题有严重的性能弊端，详细的 磁盘读写原理 请参考这一片[]。 \n 在这一瞬间成千上万的请求到来，需要系统在 极短的时间 内完成成 千上万 次的 读/写操作 ，这个时候往往不是数据库能够承受的，极其容易造成数据库系统瘫痪，最终导致服务宕机的严重生产问题。 \n 为了克服上述的问题，项目通常会引入 NoSQL 技术，这是一种 基于内存 的 数据库 ，并且提供一定的 持久化 功能。 \n Redis 技术就是 NoSQL 技术中的一种。 Redis 缓存的使用，极大的提升了应用程序的性能和效率，特别是 数据查询 方面。 \n 但同时，它也带来了一些问题。其中，最要害的问题，就是数据的一致性问题，从严格意义上讲，这个问题无解。如果对 数据的一致性 要求很高，那么就不能使用 缓存 。 \n 另外的一些典型问题就是， 缓存击穿 、 缓存穿透 和 缓存雪崩 。 \n  缓存击穿 \n 缓存击穿 是指一个 key 非常热点，在不停的扛着大并发， 大并发 集中对这一个点进行访问，当这个key在失效的瞬间，持续的 大并发 就穿破缓存，直接请求数据库，瞬间对数据库的访问压力增大。 \n 缓存击穿这里强调的是 并发 ，造成缓存击穿的原因有以下两个： \n \n \n 该数据没有人查询过 ，第一次就大并发的访问。（冷门数据） \n \n \n 添加到了缓存，reids有设置数据失效的时间 ，这条数据刚好失效，大并发访问（热点数据） \n \n \n 对于缓存击穿的解决方案就是加锁，具体实现的原理图如下： \n \n 当用户出现 大并发 访问的时候，在查询缓存的时候和查询数据库的过程加锁，只能第一个进来的请求进行执行，当第一个请求把该数据放进缓存中，接下来的访问就会直接集中缓存，防止了 缓存击穿 。 \n 业界比价普遍的一种做法，即根据key获取value值为空时，锁上，从数据库中 load 数据后再释放锁。若其它线程获取锁失败，则等待一段时间后重试。这里要注意，分布式环境中要使用 分布式锁 ， 单机 的话用普通的锁（ synchronized 、 Lock ）就够了。 \n 利用redis本身实现： \n //分布式的锁实现具体实现的代码如下： \n public   String   getProduceNum ( String  key )   { \n     // 获取分布式锁 \n     RLock  lock  =  redissonClient . getLock ( key ) ; \n     try   { \n         // 获取库存数 \n         int  num =   Integer . parseInt ( redisTemplate . opsForValue ( ) . get ( key ) ) ;   \n         // 上锁            \n        lock . lock ( ) ; \n         if   ( num >   0 )   { \n             //减少库存，并存入缓存中 \n            redisTemplate . opsForValue ( ) . set ( key ,   ( num  -   1 )   +   "" ) ; \n             System . out . println ( "剩余库存为num："   +   ( num -   1 ) ) ; \n         }   else   { \n             System . out . println ( "库存已经为0" ) ; \n         } \n     }   catch   ( NumberFormatException  e )   { \n        e . printStackTrace ( ) ; \n     }   finally   { \n         //解锁 \n        lock . unlock ( ) ; \n     } \n     return   "OK" ; \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #   缓存穿透 \n 缓存穿透是指查询一条数据库和缓存都没有的一条数据，就会一直查询数据库，对数据库的访问压力就会增大，缓存穿透的解决方案，有以下两种， 推荐使用布隆过滤器 ： \n \n \n 缓存空对象 ：代码维护较简单，但是效果不好。 \n \n \n 布隆过滤器 ：代码维护复杂，效果很好。 \n \n \n 缓存空对象 \n 缓存空对象是指当一个请求过来缓存中和数据库中都不存在该请求的数据，第一次请求就会跳过缓存进行数据库的访问，并且访问数据库后返回为空，此时也将该空对象进行缓存。 \n 若是再次进行访问该空对象的时候，就会直接 击中缓存 ，而不是再次 数据库 ： \n //缓存空对象的实现代码如下： \n public   class   UserServiceImpl   { \n      @Autowired \n      UserDAO  userDAO ; \n      @Autowired \n      RedisCache  redisCache ; \n \n      public   User   findUser ( Integer  id )   { \n           Object  object  =  redisCache . get ( Integer . toString ( id ) ) ; \n           // 缓存中存在，直接返回 \n           if ( object  !=   null )   { \n                // 检验该对象是否为缓存空对象，是则直接返回null \n                if ( object  instanceof   NullValueResultDO )   { \n                     return   null ; \n                } \n                return   ( User ) object ; \n           }   else   {   \n                // 缓存中不存在，查询数据库 \n                User  user  =  userDAO . getUser ( id ) ; \n                // 存入缓存 \n                if ( user  !=   null )   { \n                    redisCache . put ( Integer . toString ( id ) , user ) ; \n                }   else   { \n                     // 将空对象存进缓存 \n                     //redisCache.put(Integer.toString(id), new NullValueResultDO()); \n                    //缓存空对象的实现代码很简单，但是缓存空对象会带来比较大的问题，就是缓存中会存在很多空对象，占用内存的空间，浪费资源，一个解决的办法就是设置空对象的较短的过期时间： \n // 再缓存的时候，添加多一个该空对象的过期时间60秒 \nredisCache . put ( Integer . toString ( id ) ,   new   NullValueResultDO ( ) , 60 ) ; \n                } \n                return  user ; \n           } \n      }           \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 布隆过滤器 \n 布隆过滤器是一种基于 概率 的 数据结构 ，主要用来 判断 某个元素 是否在集合内 ，它具有 运行速度快 （时间效率）， 占用内存小 的优点（空间效率），但是有一定的 误识别率 和 删除困难 的问题。它只能告诉你某个元素一定不在集合内或可能在集合内。 \n 在计算机科学中有一种思想： 空间换时间，时间换空间 。一般两者是不可兼得，而布隆过滤器运行效率和空间大小都兼得，它是怎么做到的呢？ \n 在布隆过滤器中引用了一个 误判率 的概念，即它可能会把不属于这个集合的元素认为可能属于这个集合，但是不会把属于这个集合的认为不属于这个集合，布隆过滤器的特点如下： \n \n \n 一个非常大 的二进制位数组  （数组里只有0和1） \n \n \n 若干个 哈希函数 \n \n \n 空间效率 和 查询效率高 \n \n \n 不存在 漏报 （False Negative）：某个元素在某个集合中，肯定能报出来。 \n \n \n 可能存在 误报 （False Positive）：某个元素不在某个集合中，可能也被爆出来。 \n \n \n 不提供删除方法，代码维护困难。 \n \n \n 位数组初始化都为0，它不存元素的具体值，当元素经过哈希函数哈希后的值（也就是数组下标）对应的数组位置值改为1。 \n \n \n 实际布隆过滤器存储数据和查询数据的原理图如下： \n \n 可能很多读者看完上面的特点和原理图，还是看不懂，别急下面通过图解一步一步的讲解布隆过滤器，总而言之一句简单的话概括就是布隆过滤器是一个 很大二进制 的 位数组 ，数组里面 只存0和1 。 \n 初始化的布隆过滤器的结构图如下： \n \n 以上只是画了布隆过滤器的很小很小的一部分，实际布隆过滤器是非常大的数组（这里的大是指它的 长度大 ，并不是指它所占的 内存空间大 ）。 \n 那么一个数据是怎么存进布隆过滤器的呢？ \n 当一个数据进行存入布隆过滤器的时候，会经过如干个哈希函数进行哈希（若是对哈希函数还不懂的请参考这一片[]），得到对应的哈希值作为数组的下标，然后将初始化的位数组对应的下标的值修改为1，结果图如下： \n \n 当再次进行存入第二个值的时候，修改后的结果的原理图如下： \n \n 所以每次存入一个数据，就会哈希函数的计算，计算的结果就会作为下标，在布隆过滤器中有多少个哈希函数就会计算出多少个下标，布隆过滤器插入的流程如下： \n \n \n 将要添加的元素给m个哈希函数 \n \n \n 得到对应于位数组上的m个位置 \n \n \n 将这m个位置设为1 \n \n \n 那么为什么会有误判率呢？ \n 假设在我们多次存入值后，在布隆过滤器中存在x、y、z这三个值，布隆过滤器的存储结构图如下所示： \n \n 当我们要查询的时候，比如查询a这个数，实际中a这个数是不存在布隆过滤器中的，经过2哥哈希函数计算后得到a的哈希值分别为2和13，结构原理图如下： \n \n 经过查询后，发现2和13位置所存储的值都为1，但是2和13的下标分别是x和z经过计算后的下标位置的修改，该布隆过滤器中实际不存在a，那么布隆过滤器就会误判改值可能存在，因为布隆过滤器不存 元素值 ，所以存在 误判率 。 \n 那么具体布隆过布隆过滤的判断的准确率和一下 两个因素 有关： \n \n \n 布隆过滤器大小 ：越大，误判率就越小，所以说布隆过滤器一般长度都是非常大的。 \n \n \n 哈希函数的个数 ：哈希函数的个数越多，那么误判率就越小。 \n \n \n 那么为什么不能删除元素呢？ \n 原因很简单，因为删除元素后，将对应元素的下标设置为零，可能别的元素的下标也引用改下标，这样别的元素的判断就会收到影响，原理图如下： \n \n 当你删除z元素之后，将对应的下标10和13设置为0，这样导致x和y元素的下标受到影响，导致数据的判断不准确. \n 布隆过滤器的缺点就是要维持容器中的数据，实时的要更新布隆过滤器中的数据为最新。 \n \x3c!--引入谷歌guava依赖--\x3e \n         < dependency > \n             < groupId > com.google.guava </ groupId > \n             < artifactId > guava </ artifactId > \n             < version > 31.0.1-jre </ version > \n         </ dependency > \n \n 1 2 3 4 5 6 //创建一个测试类，存入100w个数据到布隆过滤器，同时用10w个不存在的数据测试误判率。 \n import   com . google . common . hash . BloomFilter ; \n import   com . google . common . hash . Funnels ; \n import   org . junit . jupiter . api . Test ; \n import   org . springframework . boot . test . context . SpringBootTest ; \n \n import   java . math . BigDecimal ; \n \n @SpringBootTest \n class   RetailUserApplicationTests   { \n \n     @Test \n     void   contextLoads ( )   { \n         this . BloomTest ( ) ; \n     } \n \n     public   void   BloomTest ( )   { \n         // 开始时间 \n         long  startTime  =   System . currentTimeMillis ( ) ; \n         // 初始化误判个数 \n         BigDecimal  count  =   new   BigDecimal ( "0" ) ; \n         // 相当于一个常量 \n         BigDecimal  one  =   new   BigDecimal ( "1" ) ; \n         // 测试的10W个数据 也是常量 用于计算误判率 \n         BigDecimal  testCount  =   new   BigDecimal ( "100000" ) ; \n         // 百分比换算，还是常量 \n         BigDecimal  mult  =   new   BigDecimal ( "100" ) ; \n \n         // 第一个参数为数据类型，第二个数组长度，第三个误判率 \n         BloomFilter < Integer >  bloomFilter  =   BloomFilter . create ( Funnels . integerFunnel ( ) ,   1000000L ,   0.01 ) ; \n \n         // 插入100w个数据 \n         for   ( int  i  =   1 ;  i  <=   1000000 ;  i ++ )   { \n            bloomFilter . put ( i ) ; \n         } \n \n         // 测试10W个不存在的数据 \n         for   ( int  i  =   2000000 ;  i  <=   2100000 ;  i ++ )   { \n             boolean  mightContain  =  bloomFilter . mightContain ( i ) ; \n             if   ( mightContain )   { \n                count  =  count . add ( one ) ; \n             } \n         } \n         System . out . println ( "总耗时"   +   ( System . currentTimeMillis ( )   -  startTime )   +   "MS" ) ; \n         System . out . println ( "误判个数:"   +  count ) ; \n         System . out . println ( "误判率:"   +   ( count . divide ( testCount ) ) . multiply ( mult )   +   "%" ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 \n 但是，误判率并不是设置的越小越好。设置的越小，进行的哈希次数就越多。要取一个适当的值来确定误差值。就和hashmap的负载因子是0.75一样 为1哈希冲突太大，为0.5冲突是少了，但是空间利用率下降了。 \n  缓存雪崩 \n 缓存雪崩 是指在某一个时间段，缓存集中过期失效。此刻无数的请求直接绕开缓存，直接请求数据库。 \n 造成缓存雪崩的原因，有以下两种： \n \\1.  reids宕机 \n \\2.  大部分数据失效 \n 比如天猫双11，马上就要到双11零点，很快就会迎来一波抢购，这波商品在23点集中的放入了缓存，假设缓存一个小时，那么到了凌晨24点的时候，这批商品的缓存就都过期了。 \n 而对这批商品的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰，对数据库造成压力，甚至压垮数据库。 \n 缓存雪崩的原理图如下， \n 当正常的情况下，key没有大量失效的用户访问原理图如下： \n \n 当某一时间点，key大量失效，造成的缓存雪崩的原理图如下： \n \n 我们可以在事故前中后三个方面来思考解决方案 \n \n 事故前：redis高可用方案，主从+哨兵，集群方案，避免全盘崩溃 \n 事故中：较少数据库的压力，本地Ehcache缓存+限流及降级，避免超过数据库承受压力 \n 事故后：做redis持久化，一旦Redis重启，可从磁盘中快速恢复数据 \n \n 针对大部分的数据失效，可采用过期时间随机，不要集中过期。 \n redis持久化 \n \n Redis 是一个基于内存的非关系型的数据库，数据保存在内存中，但是内存中的数据也容易发生丢失。这里Redis就为我们提供了持久化的机制，分别是 RDB(Redis DataBase) 和 AOF(Append Only File) 。 \n Redis在以前的版本中是单线程的，而在6.0后对Redis的io模型做了优化， io Thread 为多线程的，但是 worker Thread 仍然是单线程。 \n 在Redis启动的时候就会去加载持久化的文件，如果没有就直接启动 ，在启动后的某一时刻由继续持久化内存中产生的数据。 \n 接下来我们就来详细了解Redis的两种持久化机制 RDB(Redis DataBase) 和 AOF(Append Only File) 。 \n  RDB 持久化机制 \n 什么是RDB持久化呢？RDB持久化就是将当前进程的数据以生成快照的形式持久化到磁盘中。对于快照的理解，我们可以理解为将当前线程的数据以拍照的形式保存下来。 \n RDB持久化的时候会单独fork一个与当前进程一摸一样的子进程来进行持久化，因此RDB持久化有如下特点： \n \n \n 开机恢复数据快。 \n \n \n 写入持久化文件快。 \n \n \n RDB的持久化也是Redis默认的持久化机制，它会把内存中的数据以快照的形式写入默认文件名为 dump.rdb 中保存。 \n 在安装后的Redis中，Redis的配置都在 redis.conf 文件中，如下图所示， dbfilename 就是配置RDB的持久化文件名. \n \n 持久化触发时机 \n 在RDB机制中触发内存中的数据进行持久化，有以下三种方式： \n （1） save命令： \n save命令不会fork子进程，通过 阻塞当前Redis服务器 ，直到RDB完成为止，所以该命令在生产中一般不会使用。 \n 在redis.conf的配置中 dir 的配置就是RDB持久化后生成rdb二进制文件所在的位置，默认的位置是 ./ ，表示当前位置，哪里启动redis，就会在哪里生成持久化文件，如下图所示： \n \n \n （2） bgsave命令： \n bgsave 命令会在 后台 fork一个与Redis主线程一摸一样的子线程，由子线程负责内存中的数据持久化。 \n 这样fork与主线程一样的子线程消耗了内存，但是不会阻塞主线程处理客户端请求，是 以空间换时间的方式快照内存中的数据到到文件中 。 \n bgsave 命令阻塞只会发生在fork子线程的时候，这段时间发生的非常短，可以忽略不计，如下图是 bgsave执行的流程图： \n \n dbfilename 是配置生成的文件名，也可以通过命令行使用命令来动态的设置这两个配置，命令如下： \n config set dir{newDir} \n config set dbfilename{newFileName} \n （3） 自动化 \n 除了上面在命令行使用save和bgsave命令触发持久化，也可以在 redis.conf 配置文件中，完成配置，如下图所示： \n \n 在新安装的redis中由默认的以上三个save配置， save 900 1 表示900秒内如果至少有1个key值变化，则进行持久化保存数据； \n save 300 10 则表示300秒内如果至少有10个key值发生变化，则进行持久化， save 60 10000 以此类推。 \n save和bgsave的对比区别： \n \n \n save是 同步 持久化数据，而bgsave是 异步 持久化数据。 \n \n \n save 不会fork子进程，通过 主进程 持久化数据，会 阻塞 处理客户端的请求，而 bdsave 会 fork 子进程持久化数据，同时还可以处理客户端请求，高效。 \n \n \n save 不会消耗内存 ，而bgsave 会消耗内存 。 \n \n \n RDB的优缺点 \n 缺点：  RDB持久化后的文件是紧凑的二进制文件，适合于备份、全量复制、大规模数据恢复的场景，对数据完整性和一致性要求不高， RDB会丢失最后一次快照的数据 。 \n 优点：  开机的恢复数据快，写入持久化文件快。 \n  AOF 持久化机制 \n AOF持久化机制是以日志的形式记录Redis中的每一次的 增删改操作 ，不会记录查询操作，以文本的形式记录，打开记录的日志文件就可以查看操作记录。 \n AOF是默认不开启的，若是像开启AOF，在如下图的配置修改即可： \n \n 只需要把 appendonly no 修改为 appendonly yes 即可开启，在AOF中通过 appendfilename 配置生成的文件名，该文件名默认为 appendonly.aof ，路径也是通过dir配置的，这个于RDB的一样，具体的配置信息如下图所示： \n \n AOF****触发机制 \n AOF带来的持久化更加安全可靠，默认提供 三种 触发机制，如下所示： \n \n \n no ：表示等操作系统等数据缓存同步到磁盘中（快、持久化没保证）。 \n \n \n always ：同步持久化，每次发生数据变更时，就会立即记录到磁盘中（慢，安全）。 \n \n \n everysec ：表示每秒同步一次（默认值，很快，但是会丢失一秒内的数据）。 \n \n \n AOF中每秒同步也是异步完成的， 效率是非常高 的，由于该机制对日志文件的写入操作是采用 append 的形式。 \n 因此在写入的过程即使宕机，也不会丢失已经存入日志文件的数据，数据的完整性是非常高的。 \n 在新安装的Redis的配置文件中，AOF的配置如下所示： \n AOF重写机制 \n 但是，在写入所有的操作到日志文件中时，就会出现日志文件很多重复的操作，甚至是无效的操作，导致日志文件越来越大。 \n 所谓的无效的的操作，举个例子，比如某一时刻对一个k++，然后后面的某一时刻k--，这样k的值是保持不变的，那么这两次的操作就是无效的。 \n 如果像这样的无效操作很多，记录的文件臃肿，就浪费了资源空间，所以在Redis中出现了 rewrite 机制。 \n redis提供了bgrewriteaof命令。将内存中的数据以命令的方式保存到临时文件中，同时会fork出一条新进程来将文件重写。 \n 重写AOF的日志文件不是读取旧的日志文件瘦身，而是将内存中的数据用命令的方式重写一个AOF文件，重新保存替换原来旧的日志文件，因此内存中的数据才是最新的。 \n 重写操作也会 fork 一个子进程来处理重写操作，重写以内存中的数据作为重写的源，避免了操作的冗余性，保证了数据的最新。 \n 在Redis以append的形式将修改的数据写入老的磁盘中 ，同时Redis也会创建一个新的文件用于记录此期间有哪些命令被执行。 \n 当AOF的日志文件增长到一定大小的时候Redis就能够bgrewriteaof对日志文件进行重写瘦身。当AOF配置文件大于改配置项时自动开启重写（这里指超过原大小的100%）。 \n 该配置可以通过如下的配置项进行配置： \n \n AOF 的优缺点 \n 优点：  AOF更好保证数据不会被丢失，最多只丢失一秒内的数据，通过fork一个子进程处理持久化操作，保证了主进程不会阻塞io操作，能高效的处理客户端的请求。 \n 另外重写操作保证了数据的有效性，即使日志文件过大也会进行重写。 \n AOF的日志文件的记录可读性非常的高，即使某一时刻有人执行 flushall 清空了所有数据，只需要拿到aof的日志文件，然后把最后一条的flushall给删除掉，就可以恢复数据。 \n 缺点：   对于相同数量的数据集而言， AOF文件通常要大于RDB文件 。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。AOF在运行效率上往往会慢于RDB。 \n  混合持久化 \n 在redis4.0后混合持久化（RDB+AOF）对重写的优化，4.0版本的混合持久化默认是关闭的，可以通过以下的配置开启混合持久化： \n \n 混合持久化也是通过 bgrewriteaof 来完成的，不同的是当开启混合持久化时，fork出的子进程先将共享内存的数据以RDB方式写入aof文件中，然后再将重写缓冲区的增量命令以AOF方式写入文件中。 \n 写入完成后通知主进程统计信息，并将新的含有RDB格式和AOF格式的AOF文件替换旧的AOF文件。简单的说：新的AOF文件前半段是以RDB格式的全量数据后半段是AOF格式的增量数据。 \n 优点：  混合持久化结合 RDB持久化 和 AOF持久化 的优点，由于绝大部分的格式是RDB格式，加载速度快，增量数据以AOF方式保存，数据更少的丢失。 \n RDB 和AOF优势和劣势 \n rdb适合大规模的数据恢复，由于rdb时异快照的形式持久化数据，恢复的数据快，在一定的时间备份一次，而aof的保证数据更加完整，损失的数据只在秒内。 \n 具体哪种更适合生产，在官方的建议中两种持久化机制同时开启，如果两种机制同时开启，优先使用aof持久化机制。 \n  redis事务 \n Redis事务是一组命令的集合，将多个命令进行打包，然后这些命令会被顺序的添加到队列中，并且按顺序的执行这些命令。 \n  Redis事务的应用场景 \n 在分布式系统和高并发场景下，事务处理具有重要意义。Redis事务可以确保数据的一致性，避免并发操作导致的数据不一致问题。以下是一些Redis事务的应用场景： \n \n 批量操作：Redis 事务可以将多个命令打包成一个单元来执行，可以减少与 Redis 服务器的通信次数，从而提高性能。 \n 数据库迁移：在迁移数据时，需要保证数据一致性。通过Redis事务，可以确保数据在迁移过程中不会出现不一致的情况。 \n 分布式锁：在分布式系统中，为了保证数据的一致性，需要实现分布式锁。通过Redis事务，可以在同一个事务中执行锁定、解锁等操作，确保锁的原子性。 \n 基本操作 \n 这个与Redis的特点：快速、高效有着密切的关联，因为一些列回滚操作、像事务隔离级别那这样加锁、解锁，是非常消耗性能的。所以，Redis中执行事务的流程只需要简单的下面三个步骤： \n \n \n 开始事务（MULTI） \n \n \n 命令入队 \n \n \n 执行事务（EXEC）、撤销事务（DISCARD ） \n \n \n 在Redis中事务的实现主要是通过如下的命令实现的： \n \n \n \n 命令 \n 功能描述 \n \n \n \n \n MULTI \n 事务开始的命令 ，执行该命令后，后面执行的对Redis数据类型的 操作命令都会顺序的放进队列中 ，等待执行EXEC命令后队列中的命令才会被执行 \n \n \n DISCARD \n 放弃执行队列中的命令 ，你可以理解为Mysql的回滚操作， 并且将当前的状态从事务状态改为非事务状态 。 \n \n \n EXEC \n 执行该命令后 表示顺序执行队列中的命令 ，执行完后并将结果显示在客户端， 将当前状态从事务状态改为非事务状态 。若是执行该命令之前有key被执行WATCH命令并且又被其它客户端修改，那么就会放弃执行队列中的所有命令，在客户端显示报错信息，若是没有修改就会执行队列中的所有命令。 \n \n \n WATCH key \n 表示指定监视某个key， 该命令只能在 MULTI 命令之前执行 ，如果监视的key被其他客户端修改， EXEC****将会放弃执行队列中的所有命令 \n \n \n UNWATCH \n 取消监视之前通过****WATCH   命令监视的****key ，通过执行EXEC 、DISCARD 两个命令之前监视的key也会被取消监视 \n  错误处理 \n 在事务执行过程中，可能会遇到命令执行失败的情况。对于错误的处理，Redis采用的策略是：即使某个命令执行失败，事务中的其他命令仍然会继续执行。然而，整个事务的返回结果会包含错误信息，以便客户端了解事务执行过程中发生的错误。 \n  Redis事务的注意事项与局限性 \n 虽然Redis事务具有一定的功能，但在使用过程中需要注意以下事项： \n \n 无回滚机制 \n \n 与传统 关系型数据库 不同，Redis事务不支持回滚（Rollback）。当事务中的某个命令执行失败时，Redis不会回滚已执行的命令。因此，在使用Redis事务时，需要确保事务中的每个命令都能正确执行，以避免数据不一致的问题。 \n \n 事务内的命令不支持条件判断 \n \n Redis事务不支持在事务内进行条件判断。这意味着，事务中的所有命令都会被执行，无论前面的命令是否执行成功。这可能导致数据的不一致性。想要解决这个问题，可以使用 Lua脚本 来实现条件判断。 \n \n 性能影响 \n \n 由于Redis使用 单线程模型 来执行事务，因此，在事务执行期间，服务器无法处理其他客户端的请求。这可能对Redis的性能产生影响。为了降低事务对性能的影响，建议将事务中的命令数量控制在一个合理的范围内。 \n  使用Lua脚本优化Redis事务 \n 在某些场景下，Redis事务可能无法满足应用的需求，例如需要在事务中进行条件判断或循环。在这种情况下，可以使用Redis的Lua脚本功能来优化事务。Lua脚本可以在Redis服务器端原子性地执行一系列命令，并支持条件判断和循环，从而提供更强大的事务处理能力。 \n \n Lua脚本与Redis事务的比较 \n \n 与Redis事务相比，Lua脚本具有以下优势： \n \n 更强大的逻辑处理能力：Lua脚本 支持条件判断、循环 等复杂逻辑，而Redis事务只能顺序执行命令。 \n 更好的性能：由于Lua脚本 在服务器端执行，避免了多次往返通信 带来的延迟，因此性能通常优于Redis事务。 \n 更高的可维护性：将业务逻辑封装在Lua脚本中，可以提高代码的可读性和可维护性。 \n \n 然而，使用Lua脚本也有一些局限性： \n \n 学习成本：使用Lua脚本需要学习Lua语言及其在Redis中的使用方法。 \n 脚本管理：当业务逻辑变得复杂时，需要对多个Lua脚本进行维护和管理。 \n 脚本执行的限制：为了避免 长时间执行的脚本阻塞Redis服务器 ，Redis对Lua脚本执行时间有一定的限制。如果脚本执行时间过长，可能会被强制终止。 \n \n \n redis 使用lua脚本的语法 \n \n Redis Eval 命令 - 执行 Lua 脚本 \n redis 127.0.0.1:6379> eval "return {KEYS[1],KEYS[2],ARGV[1],ARGV[2]}" 2 key1 key2 first second\n1) "key1"\n2) "key2"\n3) "first"\n4) "second"\n \n 1 2 3 4 5 其中 \nscript： 参数是一段 Lua 5.1 脚本程序。脚本不必(也不应该)定义为一个 Lua 函数。 \nnumkeys： 用于指定键名参数的个数。 \nkey [key …]： 从 EVAL 的第三个参数开始算起，表示在脚本中所用到的那些 Redis 键(key)，这些键名参数可以在 Lua 中通过全局变量 KEYS 数组，用 1 为基址的形式访问( KEYS[1] ， KEYS[2] ，以此类推)。 \narg [arg …]： 附加参数，在 Lua 中通过全局变量 ARGV 数组访问，访问的形式和 KEYS 变量类似( ARGV[1] 、 ARGV[2] ，诸如此类)。 \n 在Lua中，可以通过内置的函数redis.call()(某个命令失败会整个中断)和redis.pcall()(某个命令失败，会继续执行)来执行redis命令。 \n redis.call()和redis.pcall()区别：前者过程中某个命令失败会整个事务中断，或者不会，继续执行。 \n redis-master:637 9 >   eval   "return redis.call(\'set\',\'foo\',\'bar\')"   0 \nOK\nredis-master:637 9 >   eval   "return redis.call(\'get\',KEYS[1])"   1  foo\n "bar" \n \n 1 2 3 4 可以直接通过 redis-cli --eval执行写好的test.lua脚本： \n 执行命令： redis-cli -a 密码 --eval Lua 脚本路径 key [key ...] , arg [arg ...] 。 \n脚本路径后紧跟 key [key …] ，相比命令行模式， 少了 numkeys 这个 key 的数量值 。key [key …] 和 arg [arg …]  之间的英文逗号前后必须有空格 ，否则报错。 \n \n 模拟cas命令,cas.lua \n local   old = ARGV [ 1 ] \n local   new = ARGV [ 2 ] \n local   expire = ARGV [ 3 ] \n local   current = redis.call ( \'get\' ,KEYS [ 1 ] ) \n if  current  then \n\t if  current~ = ARGV [ 1 ]   then \n\t\t return  -1 ; \n\tend ; \n\tredis.call ( \'setex\' ,KEYS [ 1 ] ,ARGV [ 2 ] ,ARGV [ 3 ] ) \n\t return   1 ; \nend ; \n return   0 ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 redis-cli  -h  redis-master  -a   123456   --eval  cas.lua cas , old  60  new\n \n 1 \n Java开发中的使用 \n \x3c!--redis--\x3e \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-data-redis </ artifactId > \n             < version > 2.3.1.RELEASE </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.commons </ groupId > \n             < artifactId > commons-pool2 </ artifactId > \n             < version > 2.8.1 </ version > \n         </ dependency > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 import   org . springframework . data . redis . core . RedisTemplate ; \n import   org . springframework . data . redis . core . script . RedisScript ; \n import   org . springframework . data . redis . serializer . GenericToStringSerializer ; \n import   org . springframework . data . redis . serializer . StringRedisSerializer ; \n import   org . springframework . stereotype . Component ; \n\n import   javax . annotation . Resource ; \n import   java . util . Arrays ; \n import   java . util . List ; \n import   java . util . Objects ; \n import   java . util . stream . Collectors ; \n\n @Component \n public   class   RedisUtil   { \n     @Resource \n     private   RedisTemplate < String ,   Object >  redisTemplate ; \n\n     /**\n     * 执行 lua 脚本\n     * @author hengyumo\n     * @since 2021-06-05\n     *\n     * @param luaScript  lua 脚本\n     * @param returnType 返回的结构类型\n     * @param keys       KEYS\n     * @param argv       ARGV\n     * @param <T>        泛型\n     *\n     * @return 执行的结果\n     */ \n     public   < T >   T   executeLuaScript ( String  luaScript ,   Class < T >  returnType ,   String [ ]  keys ,   Object . . .  argv )   { \n         Object  execute  =  redisTemplate . execute ( RedisScript . of ( luaScript ,  returnType ) , \n                 new   StringRedisSerializer ( ) , \n                 new   GenericToStringSerializer < > ( returnType ) , \n                 Arrays . asList ( keys ) , \n                 ( Object [ ] )  argv ) ; \n         return   ( T )  execute ; \n     } \n\n\n     // 以下命令删除xxx*格式的所有key值 \n     private   final   static   String   LUA_SCRIPT_CLEAR_WITH_KEY_PRE   = \n             "local redisKeys = redis.call(\'keys\',KEYS[1]..\'*\');"   + \n                     "for i,k in pairs(redisKeys) do redis.call(\'del\',k);end;"   + \n                     "return redisKeys;" ; \n\n     /**\n     * @author hengyumo\n     * @since 2021-06-05\n     *\n     * 删除以key为前缀的所有键值\n     * @param keyPre 前缀\n     * @return  返回删除掉的所有key\n     */ \n     public   List < String >   deleteKeysWithPre ( String  keyPre )   { \n         @SuppressWarnings ( "unchecked" ) \n         List < Object >  result  =   executeLuaScript ( LUA_SCRIPT_CLEAR_WITH_KEY_PRE ,   List . class ,   new   String [ ]   { keyPre } ) ; \n         return  result . stream ( ) . map ( x  ->   { \n             if   ( x  instanceof   List )   { \n                 @SuppressWarnings ( "unchecked" ) \n                 List < String >  list  =   ( List < String > )  x ; \n                 if   ( list . size ( )   >   0 )   { \n                     return  list . get ( 0 ) ; \n                 } \n             } \n             return   null ; \n         } ) . filter ( Objects :: nonNull ) . collect ( Collectors . toList ( ) ) ; \n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 \n import   org . junit . Test ; \n import   org . junit . runner . RunWith ; \n import   org . springframework . boot . test . context . SpringBootTest ; \n import   org . springframework . test . context . junit4 . SpringRunner ; \n\n import   javax . annotation . Resource ; \n import   java . util . List ; \n\n\n @SpringBootTest \n @RunWith ( SpringRunner . class ) \n public   class   RedisUtilTest   { \n     @Resource \n     private   RedisUtil  redisUtil ; \n\n     @Test \n     @SuppressWarnings ( "unchecked" ) \n     public   void   executeLuaScript ( )   { \n         String  script_get  =   "redis.call(\'get\',KEYS[1])" ; \n         List < Object >  list  =  redisUtil . executeLuaScript ( script_get , \n                 List . class ,   new   String [ ]   { "cas" } ) ; \n            list . forEach ( x  ->   { \n                 if ( x == null ) { \n                     System . out . println ( "key的值为nil不存在" ) ; \n                 } else { \n                     System . out . println ( x . toString ( ) ) ; \n                 } \n\n             } ) ; \n\n         String  script_cas = "local old=ARGV[1]\\n"   + \n                 "local new=ARGV[2]\\n"   + \n                 "local expire=ARGV[3]\\n"   + \n                 "local current=redis.call(\'get\',KEYS[1])\\n"   + \n                 "if current then\\n"   + \n                 "\\tif current~=ARGV[1] then\\n"   + \n                 "\\t\\treturn -1;\\n"   + \n                 "\\tend;\\n"   + \n                 "\\tredis.call(\'setex\',KEYS[1],60,ARGV[2])\\n"   + \n                 "\\treturn 1;\\n"   + \n                 "end;\\n"   + \n                 "return 0;\\n" ; \n\n        list  =  redisUtil . executeLuaScript ( script_cas , \n                 List . class ,   new   String [ ]   { "cas" } , "old" , "new" ) ; \n         System . out . println ( "不存在时，返回：" ) ; \n        list . forEach ( System . out :: println ) ; \n\n         String  script_set  =   "redis.call(\'set\',KEYS[1],ARGV[1]);return redis.call(\'get\',KEYS[1]);" ; \n        list  =  redisUtil . executeLuaScript ( script_set , \n                 List . class ,   new   String [ ]   { "cas" } , "old" ) ; \n         System . out . println ( "初始化key的值为old" ) ; \n\n        list  =  redisUtil . executeLuaScript ( script_cas , \n                 List . class ,   new   String [ ]   { "cas" } , "old" , "new" ) ; \n         System . out . println ( "新旧一致时，返回：" ) ; \n        list . forEach ( System . out :: println ) ; \n        list  =  redisUtil . executeLuaScript ( script_cas , \n                 List . class ,   new   String [ ]   { "cas" } , "old" , "new" ) ; \n         System . out . println ( "新旧不一致时，返回：" ) ; \n        list . forEach ( System . out :: println ) ; \n     } \n\n     @Test \n     public   void   deleteKeysWithPre ( )   { \n         List < String >  list  =  redisUtil . deleteKeysWithPre ( "ca" ) ; \n        list . forEach ( System . out :: println ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 \n 注意脚本可能存在卡住情况，可以限制卡住的最长时间。 \n Redis的部署模式 \n Redis作为缓存的高效中间件，在我们日常的开发中被频繁的使用，今天就来说一说Redis的四种模式，分别是 单机版、主从复制、哨兵、以及集群模式 。 \n 主从模式 \n 原理 \n 主从的原理还算是比较简单的，一主多从， 主数据库（master）可以读也可以写（read/write），从数据库仅读（only read） 。 \n 但是，主从模式一般实现 读写分离 ， 主数据库仅写（only write） ，减轻主数据库的压力，下面一张图搞懂主从模式的原理 \n 一般在读写分离通过客户端实现,可以由主节点提供写服务,由从节点提供读服务(即 写Redis 数据时应用连接主节点, 读Redis 数据时应用连接从节点)。 \n 服务端也可以实现， 先决条件 HAProxy，或者其他任何能够自定义TCP健康检查策略的负载均衡服务都行（比如Nginx，F5等等），本文将以HAProxy为例。 \n 实现思路 \n 在Redis服务前面搭建L4负载均衡服务，做下面两件事： \n \n 把写请求定向到当前的Master节点上 \n 把读请求在当前的Slave节点间负载均衡 \n \n 这就要求负载均衡服务不仅要知道当前哪些Redis节点是可用的，还要知道这些Redis节点的身份。 \n通过自定义TCP健康检查策略可以很容易实现： \n \n 哨兵模式 \n 原理 \n 哨兵模式是主从的升级版，因为主从的出现故障后，不会自动恢复，需要人为干预，这就很蛋疼啊。 \n 在主从的基础上，实现哨兵模式就是为了监控主从的运行状况，对主从的健壮进行监控，就好像哨兵一样，只要有异常就发出警告，对异常状况进行处理。 \n \n 所以，总的概括来说，哨兵模式有以下的优点（功能点）： \n \\1.   监控 ：监控master和slave是否正常运行，以及哨兵之间也会相互监控 \n \\2.   自动故障恢复 ：当master出现故障的时候，会自动选举一个slave作为master顶上去。（过半选举机制） \n 优点 \n 哨兵模式是主从模式的升级版，所以在系统层面提高了系统的可用性和性能、稳定性。当master宕机的时候，能够自动进行故障恢复，需不要人为的干预。 \n 哨兵于哨兵之间、哨兵与master之间能够进行及时的监控，心跳检测，及时发现系统的问题，这都是弥补了主从的缺点。 \n 缺点 \n 哨兵一主多从的模式同样也会遇到写的瓶颈，已经 存储瓶颈 ，若是master宕机了，故障 恢复的时间比较长 ，写的业务就会受到影响。 \n 增加了哨兵也增加了系统的复杂度，需要同时维护哨兵模式。 \n Cluster模式 \n 最后，Cluster是真正的集群模式了，哨兵解决和主从不能自动故障恢复的问题，但是同时也存在难以扩容以及单机存储、读写能力受限的问题。 \n 集群模式实现了Redis数据的分布式存储，实现数据的分片，每个redis节点存储不同的内容，并且解决了在线的节点收缩（下线）和扩容（上线）问题。 \n 集群模式真正意义上实现了系统的高可用和高性能，但是集群同时进一步使系统变得越来越复杂，接下来我们来详细的了解集群的运作原理。 \n 数据分区原理 \n 集群的原理图还是很好理解的，在Redis集群中采用的使虚拟槽分区算法，会把redis集群分成16384 个槽（0 -16383）。 \n 比如：下图所示三个master，会把0 -16383范围的槽可能分成三部分（0-5000）、（5001-11000）、（11001-16383）分别数据三个缓存节点的槽范围。 \n \n 当客户端请求过来，会首先通过对key进行CRC16 校验并对 16384 取模（CRC16(key)%16383）计算出key所在的槽，然后再到对应的槽上进行取数据或者存数据，这样就实现了数据的访问更新。 \n \n 之所以进行分槽存储，是将一整堆的数据进行分片，防止单台的redis数据量过大，影响性能的问题。 \n 生产中的问题 \n 1.big key \n 1.1 big key的简介 \n big keys指的是key对应的value值比较大， 判断标准并不是唯一。在实际业务开发中，对 big keys 的判断是需要根据具体的使用场景做不同的判断。比如操作某个 key 导致请求响应时间变慢，那么这个 key 就可以判定成 big keys。 \n 1.2 big key的危害 \n 在系统中如果存在 big keys，会导致请求数据响应变慢、请求超时或者系统不稳定。 \n 响应变慢、超时阻塞 \n Redis 是单线程工作的，同一时间只能处理一个请求，操作 big keys 时比较耗时，请求响应也变慢。其他请求也处于阻塞状态，导致请求超时。除了查询 big keys 比较耗时，删除 big keys 也会导致一样的问题。 \n 网络拥塞 \n 请求单个 big keys 产生的网络流量比较大，假设一个 big keys 为 1MB，客户端每秒访问量是 1000，那么每秒产生 1000MB 的流量，普通的千兆网卡承受不了这么大的流量。而且一般会在单机部署多个Redis实例，一个 big keys 可能也会影响其他实例。 \n 内存分布不均 \n Redis 集群模式中，key根据不同的hash嘈分配到不同的节点上，当大部分的 big keys 分布在同一个节点，导致内存倾斜在同一个节点上，内存分布不均。在水平扩容时，需要以最大容量的节为准，浪费内存。 \n 1.3 big key的分析 \n big key的分析可分为在线分析和离线分析。 \n \n \n 在线分析 \n 使用命令 --bigkeys \n --bigkeys  是 redis 自带的命令，对整个 Key 进行扫描，统计 string，list，set， zset ，hash 这几个常见数据类型中每种类型里的最大的 key。 \n string 类型统计的是 value 的字节数；另外 4 种复杂结构的类型统计的是元素个数，不能直观的看出 value 占用字节数，所以 --bigkeys  对分析 string 类型的大 key 是有用的 ，而复杂结构的类型还需要一些第三方工具。 \n \n 注：元素个数少，不一定 value 不大；元素个数多，也不一定 value 就大 \n \n redis - cli  - h  127.0 .0 .1   - p  6379   - a  "password"   -- bigkeys\n \n 1 --bigkeys  是以 scan 延迟计算的方式扫描所有 key，因此执行过程中不会阻塞 redis，但实例存在大量的 keys 时，命令执行的时间会很长，这种情况建议在 slave 上扫描。 \n –-bigkeys  其实就是找出类型中最大的 key，最大的 key 不一定是大 key，最大的 key 都不超过 10kb 的话，说明不存在大 key。 \n 但某种类型如果存在较多的大key (>10kb)，只会统计 top1 的那个 key，如果要统计所有大于 10kb 的 key，需要用第三方工具扫描 rdb 持久化文件。 \n \n \n 离线分析 \n 使用 Rdbtools 工具包 \n Rdbtools 是 python写的 一个第三方开源工具，用来解析 Redis 快照文件。除了解析 rdb 文件，还提供了统计单个 key 大小的工具。 \n 1、安装 \n git clone https : / / github . com / sripathikrishnan / redis - rdb - tools\n\n\n\ncd redis - rdb - tools sudo  &&  python setup . py install\n \n 1 2 3 4 5 2、使用 \n 从  dump.rdb  快照文件统计, 将所有 > 10kb 的 key 输出到一个 csv 文件 \n rdb dump . rdb  - c memory  -- bytes  10240   - f live_redis . csv\n \n 1 使用 rdb_bigkeys 工具 \n （不支持redis 3.2之后的rdb版本，需要重新编译，需要趟坑） \n 线上遇到redis CPU高与网卡带宽跑满的情况， 很明显的bigkey问题， 但使用一个开源的以python编写的redis RDB分析工具来分析big key， 分析150MB的RDB文件花了一个小时， 这太慢了， 这是一个使用go重新写了个分析RDB文件来找出big key的工具rdb_bigkeys速度很快， 同样分析150MB的RDB文件， 只要1分2秒。 \n #使用很简单，全部就下面提到的5个参数：  \n./rdb_bigkeys  --bytes   1024   --file  bigkeys_6379.csv  --sep   0   --sorted   --threads   4  dump6379.rdb \n #上述命令分析dump6379.rdb文件中大于1024bytes的KEY， 由大到小排好序， 以CSV格式把结果输出到bigkeys_6379.csv的文件中 \n \n 1 2 3 \n 安装过程以及遇到的问题解决 \n yum  install  go  -y  \n export   GOBIN = $GOPATH /bin\n export   PATH = $PATH : $GOBIN \n mkdir  /home/gocode/\n export   GOPATH = /home/gocode/\n cd   $GOPATH \n git  clone https://github.com/weiyanwei412/rdb_bigkeys.git\n cd  rdb_bigkeys\ngo  install \ngo mod init rdb_bigkeys\ngo mod tidy\n #这里会遇到dial tcp 142.251.42.241:443: connect: connection refused \ngo  env   -w   GOPROXY = https://goproxy.cn,direct\ngo get \ngo build\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 为了能支持高版本的rdb，暂时把版本检查给注释了 \n \n 使用RDR工具 \n （不支持redis 3.2之后的rdb版本，需要重新编译，一样需要趟坑） \n go clean  --modcache \ngo mod init rdr-linux\ngo mod tidy\ngo mod vendor \n #添加完依赖包之后,可以到vendor下check是否版本检查注释掉了 \ngo build\n \n 1 2 3 4 5 6 RDR 是解析 redis rdbfile 工具。与redis-rdb-tools相比，RDR 是由golang 实现的，速度更快。 \n \n 分析 Redis 内存中那个 Key 值占用的内存最多 \n 分析出 Redis 内存中那一类开头的 Key 占用最多，有利于内存优化 \n Redis Key 值以 Dashboard 展示，这样更直观 \n \n 网页查看 \n ./rdr-linux show  -p   8080  dump.rdb\n \n 1 \n 网页查看如下： \n \n 查看redis keys情况（没有排序，不是top10的意思，从头抽了） \n ./rdr-linux keys dump.rdb  |   head   -n   10 \n \n 1 \n redis-rdb-tools用于分析所有key及占用空间；rdr能够分析出所有key但是没法计算key占用空间，不过额外提供图形化界面。 \n \n \n \n rdr和rdb_bigkeys分析耗时都很快474MB都只有不到1min就处理完了，区别在于：rdb_bigkeys只是给出排完序的key的大小清单，若要看占比，还需要另外统计：rdr给出了不同前缀key的个数，所占用内存大小以及top100的key。 \n 1.4 big key的解决 \n 异步删除 big keys \n 找到 big keys 之后，首先需要删除对应的big keys，但是使用 del 命令删除 big keys 是比较耗时的。Redis4.0 后可以使用 unlink 删除，和 del 命令相比，unlink 是非阻塞的异步删除。 \n 非字符串的 big keys，使用 hscan、sscan、zscan 方式渐进式删除，同时要注意防止big keys 过期时间自动删除问题(例如一个 200 万的 zset 设置1小时过期，会触发del操作，造成阻塞)。 \n big key 拆分 \n 字符串类型的数据是减少字符串的长度，将一个字符串拆成几个小的字符串。非字符串的是减少元素数量。这些都是讲一个 key 拆成多个 key，比如： \n \n 字符串类型的数据，根据数据的属性拆分。比如商品信息，根据的类别拆分 key。 \n 非字符串类型的数据，根据数据的属性拆分，可以按照日期拆分，比如每天登录人的集合，按照日期拆分，key20220101、key20220102. \n \n 如果 big keys 无法避免，那获取数据尽量不要把所有的数据都取出来，就使用分段的方式取出数据。删除的方式也类似，分段删除数据。 \n'},{title:"k8s常用命令",frontmatter:{title:"k8s常用命令",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["云原生"],tags:["云原生","容器技术"]},regularPath:"/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%84%9A%E6%9C%AC/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html",relativePath:"常用命令脚本/k8s常用命令.md",key:"v-4f7f9be4",path:"/2023/06/10/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/",headers:[{level:2,title:"一、Kubernetes基础对象清理",slug:"一、kubernetes基础对象清理"},{level:2,title:"二、k8s磁盘清理（服务器磁盘清理）",slug:"二、k8s磁盘清理-服务器磁盘清理"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:'  一、Kubernetes基础对象清理 \n 1.1 清理Evicted 状态的 Pod \n kubectl get pods --all-namespaces  -o  wide  |   grep  Evicted  |   awk   \'{print $1,$2}\'   |   xargs   -L1  kubectl delete pod  -n \n \n 1 1.2 清理Error状态的Pod \n kubectl get pods --all-namespaces  -o  wide  |   grep  Error  |   awk   \'{print $1,$2}\'   |   xargs   -L1  kubectl delete pod  -n \n \n 1 1.3 清理 Completed 状态的 Pod \n kubectl get pods --all-namespaces  -o  wide  |   grep  Completed  |   awk   \'{print $1,$2}\'   |   xargs   -L1  kubectl delete pod  -n \n \n 1 1.4 清理没有被使用的 PV \n kubectl describe  -A  pvc  |   grep   -E   "^Name:.*$|^Namespace:.*$|^Used By:.*$"   |   grep   -B   2   "<none>"   |   grep   -E   "^Name:.*$|^Namespace:.*$"   |   cut   -f2  -d:  |   paste   -d   " "  - -  |   xargs   -n2   bash   -c   \'kubectl -n ${1} delete pvc ${0}\' \n \n 1 1.5 清理没有被绑定的 PVC \n kubectl describe  -A  pvc  |   grep   -E   "^Name:.*$|^Namespace:.*$|^Used By:.*$"   |   grep   -B   2   "<none>"   |   grep   -E   "^Name:.*$|^Namespace:.*$"   |   cut   -f2  -d:  |   paste   -d   " "  - -  |   xargs   -n2   bash   -c   \'kubectl -n ${1} delete pvc ${0}\' \n \n 1 1.6 清理没有被绑定的 PV \n kubectl get  pv   |   tail   -n  +2  |   grep   -v  Bound  |   awk   \'{print $1}\'   |   xargs   -L1  kubectl delete  pv \n \n 1 #  二、k8s磁盘清理（服务器磁盘清理） \n 查看磁盘占比情况 \n df   -h \n \n 1 进入对应目录，查看各目录占用大小 \n du   -h  --max-depth = 1 \n \n 1 文件太多，进行排序，或直接找大小大于一定大小的文件 \n   #排名前10的文件夹和文件 \n du   -hm  *  |   sort  -rn | head   -10  \n #查找大于100M的文件 \n find  ./  -type  f  -size  +100M\n #这里用du -h只显示文件大小和路径，如需查看详情信息，将 du -h 改为 ls -l 即可 \n find  ./  -type  f  -size  +100M   -print0   |   xargs   -0   du   -h   |   sort   -nr \n \n 1 2 3 4 5 6 清理了没有释放空间 \n lsof   |   grep  deleted\n lsof   |   grep  deleted | awk   \'{print "kill -9",$2}\' | sh \n \n 1 2 常见的需清空的文件夹 \n /var/log/journal \n #journalctl 命令自动维护文件大小 \n #1）只保留近一周的日志 \njournalctl --vacuum-time = 1w\n #2）只保留500MB的日志 \njournalctl --vacuum-size = 500M\n \n 1 2 3 4 5 确定都不保留，直接删除  /var/log/journal/  目录下的日志文件rm -rf /var/log/journal/* \n /var/log/messages \n #/var/log/messages 日志默认保留4周，结果系统盘满了，所以要修改配置文件将4周改为1周 \n vim  /etc/logrotate.conf\n重启或者强制执行生效 \n systemctl restart rsyslog 重启或者强制执行生效logrotate  -f  /etc/logrotate.conf\n \n 1 2 3 4 \n 如果确认可以清空，执行 \n cat  /dev/null  >  /var/log/messages\n \n 1 清理yum缓存 \n 1 .清除缓存目录 ( /var/cache/yum ) 下的软件 包\n\n命令：yum clean packages\n\n 2 .清除缓存目录 ( /var/cache/yum ) 下的 headers\n\n命令：yum clean headers\n\n 3 .清除缓存目录 ( /var/cache/yum ) 下旧的 headers\n\n命令：yum clean oldheaders\n\n 4 .清除缓存目录 ( /var/cache/yum ) 下的软件 包及旧的headers\n\n命令：yum clean, yum clean all  ( =  yum clean packages ;  yum clean oldheaders ) \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 docker相关文件 \n /var/lib/containerd \n crictl拉取的镜像，无用的话视情况可以删除。\n \n 1 /var/lib/docker/overlay2 \n 查看节点使用况，包括cpu、内存占用情况 \n kubectl describe nodes node1\n \n 1 Kubernetes中etcd节点挂了，修复后重启失败 \n (210条消息) Kubernetes中etcd节点挂了，修复后重启失败_k8s etcd挂了_→人生过客的博客-CSDN博客 \n'},{title:"hbase",frontmatter:{title:"hbase",date:"2022-08-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["存储引擎","nosql"],tags:["列式存储","近实时"]},regularPath:"/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/hbase.html",relativePath:"存储引擎/hbase.md",key:"v-24ffb3db",path:"/2022/08/08/hbase/",headers:[{level:2,title:"hbase介绍",slug:"hbase介绍"},{level:2,title:"hbase部署",slug:"hbase部署"},{level:2,title:"hbase的表模型",slug:"hbase的表模型"},{level:2,title:"hbase常用操作",slug:"hbase常用操作"},{level:3,title:"hbase的相关操作_shell命令",slug:"hbase的相关操作-shell命令"},{level:3,title:"hbase的javaAPI的操作",slug:"hbase的javaapi的操作"},{level:2,title:"hbase的架构及原理",slug:"hbase的架构及原理"},{level:3,title:"hbase的集群架构图",slug:"hbase的集群架构图"},{level:3,title:"region server的上线和下线",slug:"region-server的上线和下线"},{level:3,title:"master的上线和下线",slug:"master的上线和下线"},{level:3,title:"HBase的读写原理",slug:"hbase的读写原理"},{level:2,title:"Hbase三大机制",slug:"hbase三大机制"},{level:3,title:"HBase的flush刷新机制",slug:"hbase的flush刷新机制"},{level:3,title:"HBase的storeFile的合并机制",slug:"hbase的storefile的合并机制"},{level:3,title:"Hbase的split机制(region分裂)",slug:"hbase的split机制-region分裂"},{level:2,title:"HBase的Bulk Load 批量加载操作",slug:"hbase的bulk-load-批量加载操作"},{level:3,title:"MR-bulkload",slug:"mr-bulkload"},{level:3,title:"Spark bulkLoad",slug:"spark-bulkload"},{level:2,title:"HBase和Hive的集成操作",slug:"hbase和hive的集成操作"},{level:2,title:"Apache Phoenix",slug:"apache-phoenix"},{level:3,title:"phoenix集成hbase与hive、impala集成的对比",slug:"phoenix集成hbase与hive、impala集成的对比"},{level:3,title:"HBase的协处理器",slug:"hbase的协处理器"},{level:3,title:"Apache Phoenix的基本入门操作",slug:"apache-phoenix的基本入门操作"},{level:3,title:"Apache Phoenix的预分区操作",slug:"apache-phoenix的预分区操作"},{level:3,title:"apache  Phoenix的视图",slug:"apache-phoenix的视图"},{level:3,title:"Apache Phoenix的二级索引",slug:"apache-phoenix的二级索引"},{level:2,title:"hbase的表结构设计",slug:"hbase的表结构设计"},{level:3,title:"hbase的名称空间(命名空间)",slug:"hbase的名称空间-命名空间"},{level:3,title:"hbase表的列族的设计",slug:"hbase表的列族的设计"},{level:3,title:"hbase表的预分区",slug:"hbase表的预分区"},{level:3,title:"hbase的版本确界和TTL",slug:"hbase的版本确界和ttl"},{level:3,title:"hbase的中rowkey的设计原则",slug:"hbase的中rowkey的设计原则"},{level:3,title:"hbase的表的压缩方案的选择",slug:"hbase的表的压缩方案的选择"},{level:2,title:"HBase数据结构",slug:"hbase数据结构"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:" hbase介绍 \n HBase产生背景介绍: \n ​       由于 HAOOP 不支持随机读写的操作, 仅支持顺序性读写操作, 适合于进行批量化处理操作 \n HBase是采用 java 语言开发, HBase基于HDFS , 是一个 支持高效的 随机读写能力的noSQL型 数据库 \n HBase支持三种方式进行查询数据: \n \n 1) 支持主键查询 \n 2) 支持主键的范围查询 \n 3) 支持全表查询 \n \n HBase本质上就是一个存储容器, 不支持多行事务, 仅支持单行事务, 不支持SQL语句, 数据存储格式都是一切皆字节 \n HBase集群, 可以向hadoop一样, 通过横向扩展方式, 来提升HBase处理和存储的能力 \n HBase的表具有以下特征: \n \n \n \n 大: 表支持存储上十亿行数据, 上百万 列 \n \n \n \n \n 面向列: 数据是面向于列( 列族 )式的存储方案 \n \n \n \n \n 稀疏性: 在HBase的表存储数据, 如果某个字段为null, 并不会占用磁盘任何空间, 所以可以将表构建非常稀疏 \n \n \n \n 应用场景: \n \n \n \n 数据需要进行随机读写操作 \n \n \n \n \n 数据量比较的大 \n \n \n \n \n 数据比较的稀疏 \n \n \n \n hbase和其他软件的区别 \n hbase和RDBMS的区别 \n \n HBase:   以表形式存储数据, 不支持SQL 不支持事务, 仅支持单行事务操作, 数据存储是分布式存储 , 存储是结构化和半结构数据 ,不支持join \n RDBMS:以表形式存储数据, 支持SQL 支持多行事务操作, 数据存储中心化存储, 存储主要是以结构化数据为主,支持join \n \n hbase 和 HDFS的区别 \n \n HBASE:  强依赖于HDFS , 数据 最终 存储在HDFS之上的, 支持高效的 随机读写 操作 \n HDFS: 分布式存储容器, 适合于批量化数据存储, 不支持随机读写能力 \n \n 说明: \n \n 注意到 HBASE 和HDFS 既有联系 又有矛盾,  HBASE基于HDFS , 而HDFS不支持随机读写, 但是HBASE支持随机读写 \n \n hbase和hive的区别 \n \n \n HBASE: 基于hadoop的软件 , hbase是nosql存储容器, HBASE 延迟型较低, 接入在线业务 \n \n \n HIVE:  基于hadoop的软件, 数仓分析工具 , hive延迟较高,  接入离线业务 , 用于 OLAP操作 \n hbase部署 \n hbase的安装操作 \n 安装HBase的易错点: \n \n \n \n 修改hbase-site.xml的时候, 没有检查 zookeeper的目录位置 \n \n \n \n \n 没有将 htrace-core-3.1.0-incubating.jar 放置到hbase的lib目录下 \n \n \n \n \n 没有讲conf/regionserves中的localhost信息删除, 以及此文件存在空行 \n \n \n \n 如何启动HBase: \n \n \n \n 先去启动 zookeeper \n \n \n \n    #第一步: 三台节点都要执行:  \n   cd  /export/server/zookeeper-3.4.6/bin\n  ./zkServer.sh start \n  \n   #第二步: 通过 jps 查询 三个节点是否都出现了以下这个进程 \n     QuorumPeerMain\n   #第三步: 三台节点, 依次检查启动状态, 必须看到 两个 follower   一个 leader \n \n 1 2 3 4 5 6 7 \n \n \n \n 启动 hadoop集群: \n \n \n \n   1)   在node1的任意位置下执行: \n     start-all.sh\n   2)   检查 三个节点是否都启动 \n      node1 : \n         namenode\n         datanode\n         resourceManager\n         nodemanager\n     node2 : \n       seconderyNamenode\n       datanode\n       nodemanager\n     node3 : \n        datanode\n        nodemanager\n   3)   分别打开 50070 和  8088 的端口号 检查 \n       在50070   检查是否退出安全模式 \n      三个datanode是否存在\n      \n       在8088端口号,   检查 active node 是否为 3 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \n \n \n 启动 HBASE \n \n \n \n   1)   在node1节点上, 任意目录下执行: \n      start-hbase.sh\n  \n   2)   检查: 在三个节点依次 通过 jps查询 \n      node1 : \n         HMaster\n         HRegionServer\n      node2 : \n        HRegionServer\n      node3 : \n        HRegionServer\n        \n      此检查,   如果第三步正常, 不需要在检查, 如果第三步一直无法显示, 请在2分钟后, 在此查询, 是否有减少进程 \n     \n      如果那个节点没有启动,   请查询其日志文件: \n          日志存储的目录 :   /export/server/hbase-2.1.0/logs \n          查看两个文件 : \n             hbase-root-regionserver-node[N].log\n             hbase-root-regionserver-node[N].out\n          查看日志命令 : \n             tail   -200f  xxx.文件 \n  \n   3)   登录 hbase的管理界面: 端口号  16010 \n       访问 :   http://node1:16010 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \n  hbase的表模型 \n \n \n \n \n Table:  hbase的表, 在hbase中, 数据也是通过表形式组织在一起, hbase可以构建多张表 \n \n \n \n \n rowkey: 行键(主键) 类似于RDBMS中的PK , 保证唯一且非空  , rowkey仅会安装  字典序 方案进行排序 \n \n \n \n \n 列:  列族 + 列限定符(列名)组成的 \n \n \n 列族:   在一个表可以构建多个列族的, 每个列族下可以有多个列名, 支持有上百万个列\n \n 注意:\n \n 在建表的时候, 必须要指定列族 \n 在建表的时候, 建议列族越少越好, 能用一个解决的, 坚决不使用两个的 \n \n \n \n \n 列名:  一个列名必然属于某一个列族的, 列名不需要在建表的时候指定, 在后期添加数据的时候动态指定 \n \n \n \n \n 时间戳: \n \n \n 每一个数据都是由时间戳的概念, 默认为添加数据的时间, 当然也可以手动指定时间 \n \n \n \n \n 版本号:  是否需要保留每个数据的历史变更信息, 以及保留多少个 \n \n \n 默认值:  1  表示只保留最新的版本 \n 版本号是基于时间戳来记录各个历史版本的 \n \n \n \n \n 单元格:  如何确定一个单元格 \n \n \n 通过:  rowkey + 列族 + 列名 + 值 \n \n \n \n 说明: 在建表的时候,必须指定二个参数  一个 表名  一个是列族 \n hbase常用操作 \n hbase的相关操作_shell命令 \n hbase的基本shell操作 \n \n \n \n 如何进入HBase的命令行客户端 \n \n \n \n    [ root@node2 ~ ] # hbase shell \n \n 1 \n \n \n \n 查看帮助文档的命令 \n \n \n \n   hbase ( main ) :001: 0 >   help \n \n 1 \n   查看某一个命令如何使用:\n  \t格式: help  '命令名称'\n  \t\n  \thbase(main):012:0> help 'create'\n \n 1 2 3 4 \n \n \n \n 查询集群的状态: status \n \n \n \n \n \n 查询Hbase有那些表: list \n \n \n \n \n \n 如何创建一张表 \n \n \n \n   格式:\n       create  '表名'  ,  '列族名称1' , '列族名称2'   .. .. \n \n 1 2 \n ​ \n \n \n \n 如何向表中添加数据: put \n \n \n \n   格式:\n     put   '表名' , 'rowkey值' , '列族:列名' , '值' \n \n 1 2 \n \n \n \n 如何读取某一个rowkey的数据呢? \n \n \n \n   格式:\n    get '表名','rowkey',['列族1','列族2' ...],['列族1:列名'],['列族1','列族2:列名' ...]\n \n 1 2 \n \n \n \n 扫描查询 scan \n \n \n \n   格式:\n      +, [ { COLUMNS   = > [ '列族1' , '列族2' ] }   |   { COLUMNS   = > [ '列族1:列名' , '列族2' ] ,VERSIONS = > N }   ]  ,  [ { FORMATTER  = > 'toString' } ]  ,  [ { LIMIT  = > N } ] \n  \n  范围查询的格式:\n     scan  '表名' ,  { COLUNMS = > [ '列族1' , '列族2' ] }   |   { COLUMNS   = > [ '列族1:列名' , '列族2' ] , STARTROW  = > '起始rowkey的值' ,  ENDROW = > '结束rowkey' } \n     注意: 包头不包尾\n  \n  说明:\n       { FORMATTER  = > 'toString' }   :  用于显示中文\n       { LIMIT  = > N }   :  显示前 N条数据\n \n 1 2 3 4 5 6 7 8 9 10 \n \n \n \n \n \n \n 如何修改表中数据 \n \n \n \n   修改与添加数据的操作 是一致的, 只需要保证 rowkey相同 就是 修改操作\n \n 1 \n \n \n 删除数据: delete \n \n \n \n   格式:\n     delete  '表名' , 'rowkey' , '列族:列名' \n \n 1 2 \n   格式:\n      deleteall  '表名' , 'rowkey' , [ '列族:列名' ] \n \n 1 2 \n \n   delete   和 deleteall区别: \n     两个操作都是用来执行删除数据操作\n   区别点 : \n   1)   delete操作 只能删除表中某个列的数据  deleteall支持删除某行数据 \n   2)   通过delete删除某个列的时候, 默认只是删除其最新的版本, 而deleteall直接将其所有的版本数据全部都删除 \n \n 1 2 3 4 5 \n \n \n 如何清空表 \n \n \n \n   格式:\n    truncate  '表名' \n \n 1 2 \n \n \n \n 如何删除表 \n \n \n \n   格式:\n     drop  '表名' \n \n 1 2 \n 说明: \n ​     在删除表的时候, 必须先禁用表, 然后才能删除表 \n \n \n \n \n 如何查看表中有多少行数据 \n \n \n \n   格式:\n      count  '表名' \n \n 1 2 #  hbase的高级shell命令 \n \n \n \n HBase的过滤器查询操作 \n \n \n \n   格式 : \n       scan   '表名',{FILTER=>\"过滤器名称(比较运算符,比较器表达式)\" } \n   \n    常见的过滤器 : \n        rowkey过滤器 : \n            RowFilter   :   实现行键字符串的比较和过滤操作 \n            PrefixFilter   :     rowkey  前缀过滤器 \n        列族过滤器 : \n            FamilyFilter :   列族过滤器 \n        列名过滤器 : \n            QualifierFilter   :   列名过滤器, 中显示对应列名的数据 \n        列值过滤器 : \n           ValueFilter :    列值过滤器, 找到符合值的数据 \n           SingleColumnValueFilter :     在执行的列族和列名中进行比较具体的值, 将符合条数据整行数据全部都返回(包含条件的内容字段) \n              \n           SingleColumnVlaueExcludeFilter :   在执行的列族和列名中进行比较具体的值, 将符合条数据整行数据全部都返回(去除条件内容的字段) \n         其他过滤器 : \n              PageFilter   :   用来执行分页操作 \n             \n     \n   比较运算符    :    = >  < >= <= !=      \n  \n   比较器 : \n       BinaryComparator   :     完整匹配字节数组 \n       BinaryPrefixComparator   :   匹配字节数组前缀 \n       NullComparator   :     匹配Null \n       SubstringComparator   :     模糊匹配字符串 \n  \n   比较器表达式 : \n       BinaryComparator   :     binary:值 \n       BinaryPrefixComparator   :    binaryprefix:值 \n       NullComparator   :    null \n       SubstringComparator :   substring:值 \n      \n   如果不知道过滤器的构造参数,   可以查看此地址: \n      http : //hbase.apache.org/2.2/devapidocs/index.html \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 \n \n \n \n \n \n 高级的shell管理命令: \n \n \n \n 2.1:  whoami : 显示Hbase当前使用用户 \n \n \n \n 2.2:  describe: 展示表的结构信息 \n \n \n \n 2.3: exists  判断表是否存在 \n \n \n \n 2.4:  is_enabled  和 is_disabled  判断表是否启用和是否禁用 \n \n \n \n 2.5 : alter:  该命令可以改变表和列族信息 \n \n \n 如何增加列族: \n alter '表名' ,NAME=>'列族名',[VERSIONS=>N] \n \n \n \n 如何删除列族: \n alter '表名','delete'=>'列族名' \n  hbase的javaAPI的操作 \n \n \n \n 项目的准备工作 \n \n \n \n 创建maven项目: \n \n \n 1.1) 先构建一个父工程:  bigdata_parent_01 \n 1.2) 接着将父工程中 src 删除 \n 1.3) 创建一个子工程:  day01_hbase \n \n \n \n \n 导入相关的依赖:pom \n \n \n \n \n \n          < repositories > \x3c!--代码库--\x3e \n             < repository > \n                 < id > aliyun </ id > \n                 < url > http://maven.aliyun.com/nexus/content/groups/public/ </ url > \n                 < releases > < enabled > true </ enabled > </ releases > \n                 < snapshots > \n                     < enabled > false </ enabled > \n                     < updatePolicy > never </ updatePolicy > \n                 </ snapshots > \n             </ repository > \n         </ repositories > \n    \n         < dependencies > \n             < dependency > \n                 < groupId > org.apache.hbase </ groupId > \n                 < artifactId > hbase-client </ artifactId > \n                 < version > 2.1.0 </ version > \n             </ dependency > \n             < dependency > \n                 < groupId > commons-io </ groupId > \n                 < artifactId > commons-io </ artifactId > \n                 < version > 2.6 </ version > </ dependency > \n             < dependency > \n                 < groupId > junit </ groupId > \n                 < artifactId > junit </ artifactId > \n                 < version > 4.12 </ version > \n                 < scope > test </ scope > \n             </ dependency > \n             < dependency > \n                 < groupId > org.testng </ groupId > \n                 < artifactId > testng </ artifactId > \n                 < version > 6.14.3 </ version > \n                 < scope > test </ scope > \n             </ dependency > \n         </ dependencies > \n    \n         < build > \n             < plugins > \n                 < plugin > \n                     < groupId > org.apache.maven.plugins </ groupId > \n                     < artifactId > maven-compiler-plugin </ artifactId > \n                     < version > 3.1 </ version > \n                     < configuration > \n                         < target > 1.8 </ target > \n                         < source > 1.8 </ source > \n                     </ configuration > \n                 </ plugin > \n             </ plugins > \n         </ build > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \n \n \n 创建包结构: \n \n \n 名称: com.gordon.hbase \n \n \n \n 创建表 \n 实现步骤: \n 1)   创建HBase连接对象:  \n\n 2)   从连接对象中获取相关的管理对象: Admin(对表的操作) 和 Table(对表数据操作) \n\n 3)   执行相关的操作 \n\n 4)   处理结果集  -- 此步骤只有查询操作 \n\n 5)   释放资源 \n \n 1 2 3 4 5 6 7 8 9 \n 具体实现 \n \n   // 创建表 \n     @Test \n     public   void   test01 ( )   throws   Exception { \n\n         // 1) 创建HBase连接对象: \n\n         //Configuration conf = new Configuration(); \n         Configuration  conf  =   HBaseConfiguration . create ( ) ; \n        conf . set ( \"hbase.zookeeper.quorum\" , \"node1:2181,node2:2181,node3:2181\" ) ; \n         Connection  hbConn  =   ConnectionFactory . createConnection ( conf ) ; \n\n         // 2) 从连接对象中获取相关的管理对象: Admin(对表的操作) 和 Table(对表数据操作) \n\n         Admin  admin  =  hbConn . getAdmin ( ) ; \n\n         // 3) 执行相关的操作 \n\n         boolean  flag  =  admin . tableExists ( TableName . valueOf ( \"WATER_BILL\" ) ) ;   // 为true 表示存在, 否则为不存在 \n\n         if ( ! flag ) { \n             // 说明 表不存在 \n             //3.1: 创建 表的构建器对象 \n             TableDescriptorBuilder  descBuilder  =   TableDescriptorBuilder . newBuilder ( TableName . valueOf ( \"WATER_BILL\" ) ) ; \n\n\n             //3.2: 在构建器 设置表的列族信息 \n             ColumnFamilyDescriptorBuilder  familyDesc  =   ColumnFamilyDescriptorBuilder . newBuilder ( \"C1\" . getBytes ( ) ) ; \n            descBuilder . setColumnFamily ( familyDesc . build ( ) ) ; \n\n             //3.3: 基于表构建器  构建表基本信息封装对象 \n             TableDescriptor  desc  =  descBuilder . build ( ) ; \n\n            admin . createTable ( desc ) ; \n         } \n\n         // 4) 处理结果集  -- 此步骤只有查询操作 \n\n         // 5) 释放资源 \n        admin . close ( ) ; \n        hbConn . close ( ) ; \n\n     } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 添加数据 \n \n 操作步骤 \n \n 1)   创建Hbase的连接对象 \n\n 2)   通过连接对象 获取相关的管理对象: admin  和 table \n\n 3)   执行相关的操作:  添加数据 \n\n 4)   处理结果集:  -- 此步骤不需要 \n\n 5)   释放资源 \n \n 1 2 3 4 5 6 7 8 9 \n 具体操作代码 \n \n // 添加数据 \n     @Test \n     public   void   test02 ( )   throws   Exception { \n\n         //1) 创建Hbase的连接对象 \n         Configuration  conf  =   HBaseConfiguration . create ( ) ; \n\n        conf . set ( \"hbase.zookeeper.quorum\" , \"node1:2181,node2:2181,node3:2181\" ) ; \n\n         Connection  hbConn  =   ConnectionFactory . createConnection ( conf ) ; \n         //2) 通过连接对象 获取相关的管理对象: admin  和 table \n\n         Table  table  =  hbConn . getTable ( TableName . valueOf ( \"WATER_BILL\" ) ) ; \n         //3) 执行相关的操作:  添加数据 \n\n         Put  put  =   new   Put ( Bytes . toBytes ( 4944191 ) ) ; \n\n        put . addColumn ( \"C1\" . getBytes ( ) , \"NAME\" . getBytes ( ) , \"登卫红\" . getBytes ( ) ) ; \n        put . addColumn ( \"C1\" . getBytes ( ) , \"ADDRESS\" . getBytes ( ) , \"贵州省铜仁市德江县7单元267室\" . getBytes ( ) ) ; \n        put . addColumn ( \"C1\" . getBytes ( ) , \"SEX\" . getBytes ( ) , \"男\" . getBytes ( ) ) ; \n        put . addColumn ( \"C1\" . getBytes ( ) , \"PAY_TIME\" . getBytes ( ) , \"2020-05-10\" . getBytes ( ) ) ; \n       \n\n        table . put ( put ) ; \n\n         //4) 处理结果集:  -- 此步骤不需要 \n\n         //5) 释放资源 \n        table . close ( ) ; \n        hbConn . close ( ) ; \n\n\n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 抽取一些公共的方法 \n      private   Connection  hbConn ; \n     private   Table  table ; \n     private   Admin  admin ; \n     private   String  tableName  =   \"WATER_BILL\" ; \n     @Before \n     public   void   before ( )   throws   Exception { \n\n         //1) 创建Hbase的连接对象 \n         Configuration  conf  =   HBaseConfiguration . create ( ) ; \n\n        conf . set ( \"hbase.zookeeper.quorum\" , \"node1:2181,node2:2181,node3:2181\" ) ; \n\n        hbConn  =   ConnectionFactory . createConnection ( conf ) ; \n         //2) 通过连接对象 获取相关的管理对象: admin  和 table \n        admin  =  hbConn . getAdmin ( ) ; \n        table  =  hbConn . getTable ( TableName . valueOf ( tableName ) ) ; \n\n\n     } \n\n     @Test \n     public   void   test03 ( ) { \n        \n         // 3) 执行相关的操作 \n        \n        \n         //4) 处理结果集 \n        \n     } \n\n\n     @After \n     public    void   after ( )   throws   Exception { \n         //5. 释放资源 \n        admin . close ( ) ; \n        table . close ( ) ; \n        hbConn . close ( ) ; \n        \n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 查询某一条数据 \n \n 代码实现 \n \n // 根据rowkey 查询某一条数据 \n     @Test \n     public   void   test03 ( )   throws   Exception { \n\n         // 3) 执行相关的操作 get \n\n         Get  get  =   new   Get ( Bytes . toBytes ( 4944191 ) ) ; \n         Result  result  =  table . get ( get ) ;   // 一个Result 表示 就是一行数据 \n\n         //4) 处理结果集 \n\n         //4.1: 获取一行数据中每一个单元格 \n         List < Cell >  listCells  =  result . listCells ( ) ; \n\n         //4.2: 遍历单元格, 从单元格获取数据 \n         for   ( Cell  cell  :  listCells )   { \n\n             // 从单元格可以获取那些内容: rowkey  列族 列名 列值 \n\n             //4.2.1 获取rowkey \n             /*byte[] rowArray = cell.getRowArray();\n\n            int rowkey = Bytes.toInt(rowArray, cell.getRowOffset(), cell.getRowLength());*/ \n\n             byte [ ]  rowBytes  =   CellUtil . cloneRow ( cell ) ; \n             int  rowkey  =   Bytes . toInt ( rowBytes ) ; \n\n             // 4.2.2 获取 列族 \n             byte [ ]  familyBytes  =   CellUtil . cloneFamily ( cell ) ; \n             String  family  =   Bytes . toString ( familyBytes ) ; \n             // 4.2.3 获取 列名(列限定符) \n             byte [ ]  qualifierBytes  =   CellUtil . cloneQualifier ( cell ) ; \n             String  qualifier  =   Bytes . toString ( qualifierBytes ) ; \n             // 4.2.4 获取 列值 \n             byte [ ]  valueBytes  =   CellUtil . cloneValue ( cell ) ; \n             String  value  =   Bytes . toString ( valueBytes ) ; \n\n\n             System . out . println ( \"rowkey:\" + rowkey  + \"; 列族为:\" + family + \"; 列名为:\" + qualifier + \"; 列值为:\" + value ) ; \n\n         } \n\n\n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 删除数据 \n \n 代码实现 \n \n // 删除数据操作: \n     @Test \n     public    void    test04 ( )   throws   Exception { \n\n         // 3) 执行相关的操作 : delete  和 deleteall \n\n\n         Delete  delete  =   new   Delete ( Bytes . toBytes ( 4944191 ) ) ; \n\n         //delete.addColumn(\"C1\".getBytes(),\"NAME\".getBytes()); \n        delete . addFamily ( \"C1\" . getBytes ( ) ) ; \n        table . delete ( delete ) ; \n\n\n         //4) 处理结果集 -- 此步骤不需要 \n\n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 删除表 \n \n 代码实现 \n \n // 删除表操作: \n     @Test \n     public    void    test05 ( )   throws   Exception { \n         // 3) 执行相关的操作 \n\n         boolean  flag  =  admin . isTableEnabled ( TableName . valueOf ( tableName ) ) ;   // 判断是否启用表 \n         if ( flag ) { \n\n            admin . disableTable ( TableName . valueOf ( tableName ) ) ;   //如果启用, 将其先禁用 \n         } \n\n        admin . deleteTable ( TableName . valueOf ( tableName ) ) ; \n\n         //4) 处理结果集 -- 此步骤不需要 \n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 导入数据的操作 \n \n \n 如何导入数据操作 \n \n 语法格式: \n \n \n \n   hbase  org.apache.hadoop.hbase.mapreduce.Import 表名 HDFS数据文件路径\n \n 1 \n \n 注意: 此操作需要在shell命令窗口下执行, 不要在hbase的命令行窗口行 \n \n \n 操作步骤: \n \n \n \n 接下来即可执行导入命令 \n \n \n   hbase  org.apache.hadoop.hbase.mapreduce.Import WATER_BILL /water_bill/input\n \n 1 基于scan的扫描查询 \n //scan操作  \n @Test \n     public   void   scan ( )   throws   Exception { \n         Configuration  conf  =   HBaseConfiguration . create ( ) ; \n        conf . set ( \"hbase.zookeeper.quorum\" ,   \"node1:2181,node2:2181,node3:2181\" ) ; \n         Connection  hbConn  =   ConnectionFactory . createConnection ( conf ) ; \n         Table  table  =  hbConn . getTable ( TableName . valueOf ( \"WATER_BILL\" ) ) ; \n\n         //命令行的写法 scan 表名 ,{FILTER=>\"QualifierFilter(=,'substring:a')\"} \n\n         Scan  scan  =   new   Scan ( ) ; \n         FilterList  filter  =   new   FilterList ( ) ; \n         SingleColumnValueFilter  startValueFilter  =   new   SingleColumnValueFilter ( \"C1\" . getBytes ( ) , \"RECORD_DATE\" . getBytes ( ) ,   CompareOperator . GREATER_OR_EQUAL , \"2020-06-01\" . getBytes ( ) ) ; \n         SingleColumnValueFilter  endValueFilter  =   new   SingleColumnValueFilter ( \"C1\" . getBytes ( ) , \"RECORD_DATE\" . getBytes ( ) ,   CompareOperator . LESS_OR_EQUAL , \"2020-07-01\" . getBytes ( ) )   ; \n        filter . addFilter ( startValueFilter ) ; \n        filter . addFilter ( endValueFilter ) ; \n        scan . setFilter ( filter ) ; \n         ResultScanner  results  =  table . getScanner ( scan ) ; \n         for   ( Result  result  :  results )   { \n             List < Cell >  cells  =  result . listCells ( ) ; \n             for   ( Cell  cell  :  cells )   { \n                 String  qulifier  =   Bytes . toString ( CellUtil . cloneQualifier ( cell ) ) ; \n                 if ( \"NAME\" . equals ( qulifier ) ) { \n                     String  name  =   Bytes . toString ( CellUtil . cloneValue ( cell ) ) ; \n                     System . out . println ( name ) ; \n                 } \n             } \n\n         } \n        table . close ( ) ; \n        hbConn . close ( ) ; \n\n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #  hbase的架构及原理 \n hbase高可用架构部署 \n hbase的高可用, 主要指的是让HBase的主节点, 有多台, 当其中一台出现故障后, 可以让其他的节点顶上来 \n 如何配置呢? \n \n \n \n 在node1 进入 hbase的conf目录下, 创建一个 backup-masters \n \n \n \n    cd  /export/server/hbase-2.1.0/conf/\n   vim  backup-masters\n  \n  内容如下:\n  node2.itcast.cn\n  node3.itcast.cn\n \n 1 2 3 4 5 6 \n \n \n 将 backup-master 发送其他两台节点 \n \n \n \n    cd  /export/server/hbase-2.1.0/conf/\n   scp  backup-masters node2: $PWD \n   scp  backup-masters node3: $PWD \n \n 1 2 3 \n \n \n 重启 HBase的集群 \n \n \n \n   在 node1执行:  \n      stop-hbase.sh\n      start-hbase.sh\n \n 1 2 3 \n hbase的集群架构图 \n region server的上线和下线 \n region server的上线流程: \n \n 说明: \n ​       当regionServer上线后, master会立即感知到, 此时regionServer需要上master汇报自己当下管理那些region, 然后master会根据各个regionServer汇报的region的管理情况, 然后在读取meta表获取到所有的region, 与之比较, 查看是否还有未分配的region, 如果有, 将这个未分配的region, 均匀的分配给各个regionServer上, 保证负载均衡 \n regionServer的下线流程 \n \n ​\t   说明: 当HregionServer下线后, 对应这个regionServer所管理的region就处于无人管理的状态(无分配状态),此时master就需要将这些没有分配的region, 重新分配给其他的regionServer上 即可, 当regionServer有重新上线之后, 从之前regionServer上, 解除一些region, 将region分配给当前这个新启动regionServer上 (存在时间间隔, 不是立即执行) \n master的上线和下线 \n \n 说明: \n ​    master短暂的下线, 并不会太大的影响HBase的集群, 因为hbase的读写操作是不经过HMaster, 而大多数的请求都是读写的请求, \n ​     master下线主要是会影响到对元数据操作的请求, 比如说 创建表  删除表 修改表 \n HBase的读写原理 \n 读取数据的流程 \n Meat表内具体存放哪些信息： \nrowkey：由四个字段拼接起来，分别是 表名-StratRow-TimeStamp-EncodedName。\n\n数据分为4列：\n\ninfo：regioninfo：EncodedName、RegionName、Region的StartRow、Region的StopRow；\n\ninfo：seqnumDuringOpen：存储Region打开时的sequenceId；\n\ninfo：server：存储Region落在哪个RegionServer上；\n\ninfo：serverstartcode：存储所在的RegionServer启动时间戳；\n \n 1 2 3 4 5 6 7 8 9 10 11 12 \n HBase读取数据的流程 \n \n   由客户端发起读取数据的请求 :   scan '表名' \n  \n   1)   首先第一步连接zookeeper, 从zookeeper中获取 HBase:Meta 表的位置信息(meta被那个regionServer所管理) \n          HBase : Meta 表 是 hbase的管理表, 有且只有一个region 主要是用于存储 hbase的元数据信息, 比如 有那些表, 每个表有那些region, 每个region被那些regionServer管理 ......   \n   \n   2)    连接对应(hbase meta表的)regionServer, 获取meta表中数据, 从meta表获取对应要查询的表有几个region, 以及这些region被那些regionserver所管理, 与当下要读取表的元数据信息全部都获取到 \n  \n  \n   3)   并发的连接(查询表的)regionServer, 从各个regionServer的region中读取表的数据即可, 如果是基于scan扫描 此时会将所有的数据全部扫描的到客户端(边读边处理)), 如果是get操作, 此时从region中, 找到具体的rowkey \n       从region读取数据顺序 :     memStore  --\x3e blockCache ---\x3eStoreFile(小File) --\x3e 大HFile \n     \n \n 1 2 3 4 5 6 7 8 9 10 11 \n 数据的写入流程 \n \n HBase的数据写入流程 \n \n   客户端发起写入数据请求 :   put 'user' ,'rk001','C1:name','张三' \n  \n   1)   首先连接zookeeper, 获取HBase:Meta表所在的regionServer的地址 \n   2)   连接regionServer, 从meta表获取要写入数据的表,根据rowkey找到对应的region被那个regionServer所管理 \n   3)   连接对应regionServer, 开始进行数据写入操作 \n   4)   首先先将数据写入到这个regionServer的HLog(WAL)日志中, 然后再将数据写入到memStore(可能会写入到多个memStore)中 \n   5)   当HLog 和 memStore都成功将数据写入完成, 此时 客户端的写入流程就结束了, 客户端返回写入成功.... \n  \n  -------------------------以上为客户端流程------------------------------------\n   服务端流程 : \n  \n   6)   随着客户端不断的写入, memStore中数据越来越多, 当memStore数据达到一定的阈值(128M/1小时)后, 就会执行flush刷新机制, 将memStore数据 \"最终\" 刷新到HDFS上, 形成一个storeFile(小File)文件 \n  \n   7)   随着不断的flush的刷新操作, 在HDFS上, 会存储越来越多的小File文件, 当这些小的Hfile文件达到一定的阈值(3个及以上)后, 就会启动compact(合并压缩)机制, 将多个小Hfile \"最终\"  合并为一个大的HFile文件 \n  \n   8)   随着不断的compact的合并压缩, 这个大的Hfile 也会越来越大, 当这个大的Hfile达到一定的阈值(\"最终\"10GB)后, 就会触发 split机制, 将大的Hfile 进行一分为二, 形成两个新的Hfile, 此时对应region 也会进行一份为二, 形成两个新的region, 每个region管理其中一个新的大Hfile即可, 一旦split分裂完毕, 此时旧的region就会下线(注意: 在执行split分裂过程中, 当下分裂的表是不接受读写请求) \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n Hbase三大机制 \n  HBase的flush刷新机制 \n \n HBase的flush刷新机制: \n \n flush机制 :   刷新机制 \n       目的 : \n              将memStore中最终写入到HDFS上,   形成一个storeFile文件 \n       阈值 :   \n              大小阈值 :    128M \n              时间阈值 :     1小时 \n              注意 :   满足了那个阈值, 都会触发flush机制 \n\n       flush流程 :   hbase 2.0 flush \n               1)   关闭掉当前这个达到阈值的memStore空间, 然后开启一个新的memStore, 以便于正常写入 \n               2)   将这个关闭的memStore 首先放入一个 队列容器 (内存)中, 在HBase的2.0版本后, 会让这个队列容器,尽可能晚的刷新到磁盘上, 此时在队列的memStore变更为只读状态, 在读取数据时候, 依然可以从队列中进行读取操作, 以保证读取的效率 \n               3)   随着memStore不断的达到阈值 , 在队列容器中存储的memStore的数量也会越来越多, 当内存达到一定的阈值(?)后, 触发flush操作, 将队列中所有的数据 进行合并操作,然后 一次性刷新到磁盘上, 形成一个storeFile文件; \n\n 注意 :    此操作, 仅在hbase2.0版本后才支持, 在2.0版本下, 写入到队列之后, 直接将数据刷新HDFS上, 形成一个个storeFile, 即使刷新慢了, 导致队列中有了多个memStore, 依然一个memStore就是一个storeFile \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 说明: \n ​     内存合并操作, 在hbase2.0后就开始支持了, 但是hbase2.x版本, 默认是不开启内存合并的, 如果开启, 需要手动设置 \n 如何开启内存合并操作: \n \n \n 方式1: 全局配置 \n \n 配置在: hbase-site.xml ,建议配置 adaptive \n \n \n \n \n 方式2: 针对某一个表来设置 \n \n 配置在: 在建表的时候, 可以针对设置操作 \n \n \n \n \n 合并的方案: 共计有三种 \n \n basic(基础型):\n \n 作用: 在合并的过程中, 不关心是否有重复数据, 或者过期的版本数据, 直接进行合并即可, 效率最高的 \n \n \n eager(饥渴型):\n \n 作用: 在合并的过程中, 会积极的判断数据是否有重复, 以及是否有过期, 会将重复的 过期的版本, 全部清洗掉, 然后合并在一起 \n \n \n adaptive(适应型):\n \n 作用: 检查数据, 如果发现数据重复和过期版本 的比例以及达到 eager方案, 就采用饥渴型, 否则就采用基础型 \n HBase的storeFile的合并机制 \n \n compact机制流程说明 \n \n compact   合并压缩机制:   \n          目的 :    将多个storeFile(小Hfile) 合并为一个更大的HFile文件 \n          阈值 :     达到3个及以上 \n\n\n\n minor :  \n      目的 :    将多个小的storeFile 合并为一个较大的Hfile文件过程 \n\n      流程 : \n           1)   当storeFile文件数量达到阈值(3个及以上)后, 首先先将这几个小storeFile进行合并操作, 形成一个较大的storeFile文件 \n           2)   在合并过程中, minor 操作不会对数据进行任何的删除, 去掉重复操作, 仅仅做一个基本排序合并工作即可, 整体执行效率是非常快的 \n    \n\n major :  \n\n     目的 :   将这个较大的Hfile 和之前大的Hfile 进行合并形成一个最终的大Hfile操作 \n\n      流程 : \n            1)   默认情况下, 当达到一定的阈值(7天|手动)后触发major操作, 将磁盘中较大的HFile和之前大的Hfile进行合并, 形成一个最终的大Hfile文件即可 \n            2)   在合并的过程中, 打上重复标记的数据, 打上过期版本标记的数据, 在major执行过程中, 就会进行全部处理掉 \n\n     注意 :   由于major在合并的过程中, 需要对所有数据进行合并操作, 此时无法对数据进行相关的操作,  而且此操作由于数据量较大, 执行时间较大, 此操作会Hbase性能影响较大 \n          所以在实际生产中,   一般是关闭major, 改为手动执行 \n\n    合并做法 : \n            将HDFS数据读取出来,    边读边进行处理, 边将数据通过追加的形式添加到HDFS上 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #  Hbase的split机制(region分裂) \n \n 此公式主要是用于计算  表对应region, 何时执行分裂的操作 \n 比如说 : \n     当最初始的时候,   表只有一个Region ,   此时 1^2 * 128  与   10GB 做比较, 那个小, 我们就会在那个值上执行分裂, 此时第一次应该在 region的Hfile数据量达到 128M 的时候执行分裂 \n     当第二次分裂,   R=2, 经过计算后, 当region的Hfile数据量达到 512M的时候, 就会执行分裂 \n    以此类推\n    \n     直到表的region数量达到   9个及以上的时候, 此时region分裂按照 10GB 分裂一次 \n \n 1 2 3 4 5 6 #  HBase的Bulk Load 批量加载操作 \n 假设:  目前有一批数据, 此数据量比较大, 需要将这些数据写入到HBase中, 如果采用hbase的 普通的javaAPI操作, 会将数据正常的写入流程 进入到HBase中, 而正常数据流程: 先将数据写入到HLog 然后将数据写入memStore, 然后从memStore到storeFile, 再从storeFile到Hfile， 而且在整个写入流程中, 需要大量的占用服务器的资源 \n 如果这个时候, 还有大量的请求, 从Hbase的这个表中读取数据, 由于服务器的资源都被写入的请求占用了, 此时读取的请求可能无法实施, 或者返回结果会很慢, 此时对网络的带宽造成较大的影响 \n 思考 如何解决呢? \n 1) 将这一批数据 先转换为 HFile的文件\n2) 将HFile文件直接导入HBase中, 让Hbase直接加载即可\n\n此操作不需要先写入HLog 然后到内存, 然后HDFS过程, 直接将数据到达HDFS操作\n \n 1 2 3 4 \n 解决的应用场景 \n \n 需求一次性写入大量数据到达HBase的操作 \n \n 5.1 需求说明 \n ​       目前在HDFS上有一份 CSV文件, 此文件中记录大量的转账数据, 要求将这些转换数据 存储到Hbase中, 由于初始数据量过于庞大, 可以采用 bulk_load 将数据批量加载HBase中 \n ​       \n 准备工作 \n \n 在Hbase中创建目标表: \n \n create   'TRANSFER_RECORD' , 'C1' \n \n 1 \n 将数据 上传到 HDFS中 \n \n hdfs dfs  -mkdir   -p  /hbase/bulkload/input\n\nrz将数据上传到 Linux中\n\nhdfs dfs  -put  bank_record.csv /hbase/bulkload/input\n \n 1 2 3 4 5 \n \n \n 在IDEA中构建项目: day02_hbase_bulk_load \n \n \n \n \n 导入相关的pom依赖 \n \n \n \n        < repositories > \x3c!--代码库--\x3e \n           < repository > \n               < id > aliyun </ id > \n               < url > http://maven.aliyun.com/nexus/content/groups/public/ </ url > \n               < releases > < enabled > true </ enabled > </ releases > \n               < snapshots > \n                   < enabled > false </ enabled > \n                   < updatePolicy > never </ updatePolicy > \n               </ snapshots > \n           </ repository > \n       </ repositories > \n  \n       < dependencies > \n           < dependency > \n               < groupId > org.apache.hbase </ groupId > \n               < artifactId > hbase-client </ artifactId > \n               < version > 2.1.0 </ version > \n           </ dependency > \n           < dependency > \n               < groupId > commons-io </ groupId > \n               < artifactId > commons-io </ artifactId > \n               < version > 2.6 </ version > \n           </ dependency > \n  \n           < dependency > \n               < groupId > org.apache.hbase </ groupId > \n               < artifactId > hbase-mapreduce </ artifactId > \n               < version > 2.1.0 </ version > \n           </ dependency > \n  \n           < dependency > \n               < groupId > org.apache.hadoop </ groupId > \n               < artifactId > hadoop-mapreduce-client-jobclient </ artifactId > \n               < version > 2.7.5 </ version > \n           </ dependency > \n  \n           < dependency > \n               < groupId > org.apache.hadoop </ groupId > \n               < artifactId > hadoop-common </ artifactId > \n               < version > 2.7.5 </ version > \n           </ dependency > \n  \n           < dependency > \n               < groupId > org.apache.hadoop </ groupId > \n               < artifactId > hadoop-mapreduce-client-core </ artifactId > \n               < version > 2.7.5 </ version > \n           </ dependency > \n  \n           < dependency > \n               < groupId > org.apache.hadoop </ groupId > \n               < artifactId > hadoop-auth </ artifactId > \n               < version > 2.7.5 </ version > \n           </ dependency > \n  \n           < dependency > \n               < groupId > org.apache.hadoop </ groupId > \n               < artifactId > hadoop-hdfs </ artifactId > \n               < version > 2.7.5 </ version > \n           </ dependency > \n  \n       </ dependencies > \n  \n       < build > \n           < plugins > \n               < plugin > \n                   < groupId > org.apache.maven.plugins </ groupId > \n                   < artifactId > maven-compiler-plugin </ artifactId > \n                   < version > 3.1 </ version > \n                   < configuration > \n                       < target > 1.8 </ target > \n                       < source > 1.8 </ source > \n                   </ configuration > \n               </ plugin > \n           </ plugins > \n       </ build > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 \n \n \n 创建包结构 \n \n \n com.itheima.hbase.bulkLoad \n MR-bulkload \n 将CSV数据转换为HFile文件格式数据(天龙八部)==>内存多的话，需要改成spark来完成 \n \n \n \n 编写 Mapper程序 \n \n \n \n package   com . itheima . hbase . bulkLoad ; \n\n import   org . apache . hadoop . hbase . client . Put ; \n import   org . apache . hadoop . hbase . io . ImmutableBytesWritable ; \n import   org . apache . hadoop . io . LongWritable ; \n import   org . apache . hadoop . io . Text ; \n import   org . apache . hadoop . mapreduce . Mapper ; \n\n import   java . io . IOException ; \n\n public   class   BulkLoadMapper   extends   Mapper < LongWritable , Text , ImmutableBytesWritable ,   Put >   { \n     private   ImmutableBytesWritable  k2  =   new   ImmutableBytesWritable ( ) ; \n     @Override \n     protected   void   map ( LongWritable  key ,   Text  value ,   Context  context )   throws   IOException ,   InterruptedException   { \n\n         //1. 获取一行数据 \n         String  line  =  value . toString ( ) ; \n\n         //2. 判断是否接收到数据 \n         if ( line  !=   null   &&   ! \"\" . equals ( line . trim ( ) ) ) { \n\n             //3. 对这一行数据执行切割操作 \n\n             String [ ]  fields  =  line . split ( \",\" ) ; \n\n             //4. 封装 k2 和 v2的数据 \n             byte [ ]  rowkey  =  fields [ 0 ] . getBytes ( ) ; \n            k2 . set ( rowkey ) ; \n\n             Put  v2  =   new   Put ( rowkey ) ; \n\n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"code\" . getBytes ( ) , fields [ 1 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"rec_account\" . getBytes ( ) , fields [ 2 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"rec_bank_name\" . getBytes ( ) , fields [ 3 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"rec_name\" . getBytes ( ) , fields [ 4 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"pay_account\" . getBytes ( ) , fields [ 5 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"pay_name\" . getBytes ( ) , fields [ 6 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"pay_comments\" . getBytes ( ) , fields [ 7 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"pay_channel\" . getBytes ( ) , fields [ 8 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"pay_way\" . getBytes ( ) , fields [ 9 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"status\" . getBytes ( ) , fields [ 10 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"timestamp\" . getBytes ( ) , fields [ 11 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"money\" . getBytes ( ) , fields [ 12 ] . getBytes ( ) ) ; \n\n\n             //5. 写出去 \n            context . write ( k2 , v2 ) ; \n\n         } \n\n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 \n \n \n 编写 驱动类 \n \n \n \n package   com . itheima . hbase . bulkLoad ; \n\n import   org . apache . hadoop . conf . Configuration ; \n import   org . apache . hadoop . conf . Configured ; \n import   org . apache . hadoop . fs . Path ; \n import   org . apache . hadoop . hbase . HBaseConfiguration ; \n import   org . apache . hadoop . hbase . TableName ; \n import   org . apache . hadoop . hbase . client . Connection ; \n import   org . apache . hadoop . hbase . client . ConnectionFactory ; \n import   org . apache . hadoop . hbase . client . Put ; \n import   org . apache . hadoop . hbase . client . Table ; \n import   org . apache . hadoop . hbase . io . ImmutableBytesWritable ; \n import   org . apache . hadoop . hbase . mapreduce . HFileOutputFormat2 ; \n import   org . apache . hadoop . mapreduce . Job ; \n import   org . apache . hadoop . mapreduce . lib . input . TextInputFormat ; \n import   org . apache . hadoop . util . Tool ; \n import   org . apache . hadoop . util . ToolRunner ; \n\n public   class   BulkLoadDriver   extends   Configured   implements   Tool   { \n     @Override \n     public   int   run ( String [ ]  args )   throws   Exception   { \n\n         //1. 创建 Job对象 \n         Job  job  =   Job . getInstance ( super . getConf ( ) ,   \"BulkLoadDriver\" ) ; \n         // 设置提交yarn 必备参数 \n        job . setJarByClass ( BulkLoadDriver . class ) ; \n         //2. 设置 天龙八部 \n         //2.1: 设置 输入类 和输入路径 \n        job . setInputFormatClass ( TextInputFormat . class ) ; \n         TextInputFormat . addInputPath ( job , new   Path ( \"hdfs://node1:8020/hbase/bulkload/input\" ) ) ; \n         //2.2: 设置 mapper类和 map输出k2和v2的类型 \n        job . setMapperClass ( BulkLoadMapper . class ) ; \n        job . setMapOutputKeyClass ( ImmutableBytesWritable . class ) ; \n        job . setMapOutputValueClass ( Put . class ) ; \n         //2.3: 设置 shuffle操作: 分区 排序 规约  分组 \n\n         //2.7: 设置 reduce类  和 reduce 输出 k3和v3的类型 \n        job . setNumReduceTasks ( 0 ) ;   // 没有reduce \n\n        job . setOutputKeyClass ( ImmutableBytesWritable . class ) ; \n        job . setOutputValueClass ( Put . class ) ; \n\n         //2.8: 设置输出类, 及输出路径  Hfile \n        job . setOutputFormatClass ( HFileOutputFormat2 . class ) ; \n         // 获取 连接对象 和 table对象 \n         Connection  hbConn  =   ConnectionFactory . createConnection ( super . getConf ( ) ) ; \n         Table  table  =  hbConn . getTable ( TableName . valueOf ( \"TRANSFER_RECORD\" ) ) ; \n\n         HFileOutputFormat2 . configureIncrementalLoad ( job , table , hbConn . getRegionLocator ( TableName . valueOf ( \"TRANSFER_RECORD\" ) ) ) ; \n\n         HFileOutputFormat2 . setOutputPath ( job ,   new   Path ( \"hdfs://node1:8020/hbase/bulkload/output\" ) ) ; \n\n         //3. 提交任务 \n         boolean  flag  =  job . waitForCompletion ( true ) ; \n\n         return  flag  ?   0 : 1 ; \n     } \n\n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         Configuration  conf  =   HBaseConfiguration . create ( ) ; \n        conf . set ( \"hbase.zookeeper.quorum\" , \"node1:2181,node2:2181,node3:2181\" ) ; \n         int  i  =   ToolRunner . run ( conf ,   new   BulkLoadDriver ( ) ,  args ) ; \n\n         System . exit ( i ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \n \n \n 测试操作: \n \n \n \n \n 将Hfile文件格式数据加载HBase中 \n 语法格式要求 \n hbase org.apache.hadoop.hbase.tool.LoadIncrementalHFiles  MR输出路径  HBase表名\n \n 1 执行导入操作: \n hbase org.apache.hadoop.hbase.tool.LoadIncrementalHFiles  /hbase/bulkload/output  TRANSFER_RECORD\n \n 1 但是, 各位 你们今天执行可能会报错:  尝试了10次 依然无法导入错误 \n \n 检查: \n \n   查看 Hbase的日志: regionServer的日志 \n     优先查看 当下执行导入操作的这个regionServer的日志, 如果没有错误\n     在查询导入的表, 对应region属于哪个regionServer, 查询这个regionServer的日志\n \n 1 2 3 \n \n 错误原因: \n \n   hbase的采用 域名 和 hdfs的采用的域名不一致导致的,  hbase到导入数据的时候, 发现这个域名不一致, 以为不是同一个集群 导致失败\n \n 1 \n \n 解决方案: \n \n \n \n 查询 hbase-site.xml中  hbase.root.dir配置的hdfs的域名是什么? \n \n \n \n \n \n 查询 hdfs的 core-site.xml中  fs.defaultFS的配置的hdfs的域名是什么? \n \n \n \n \n \n 如果两个不一致, 建议大家修改 core-site.xml \n \n \n 修改后, 将这这个配置发送给 node2 和 node3 \n \n \n \n \n 重启 hadoop 和 hbase 即可,然后重新尝试导入操作 \n Spark bulkLoad \n /**\n  * @Author bigdatalearnshare\n  */ \nobject  App   { \n\n  def  main ( args :   Array [ String ] ) :   Unit   =   { \n     System . setProperty ( \"HADOOP_USER_NAME\" ,   \"root\" ) \n\n    val sparkSession  =   SparkSession \n       . builder ( ) \n       . config ( \"spark.serializer\" ,   \"org.apache.spark.serializer.KryoSerializer\" ) \n       . master ( \"local[*]\" ) \n       . getOrCreate ( ) \n    \n    val rowKeyField  =   \"id\" \n    \n    val df  =  sparkSession . read . format ( \"json\" ) . load ( \"/people.json\" ) \n\n    val fields  =  df . columns . filterNot ( _  ==   \"id\" ) . sorted\n\n    val data  =  df . rdd . map  {  row  = > \n      val rowKey  =   Bytes . toBytes ( row . getAs ( rowKeyField ) . toString ) \n\n      val kvs  =  fields . map  {  field  = > \n         new   KeyValue ( rowKey ,   Bytes . toBytes ( \"hfile-fy\" ) ,   Bytes . toBytes ( field ) ,   Bytes . toBytes ( row . getAs ( field ) . toString ) ) \n       } \n\n       ( new   ImmutableBytesWritable ( rowKey ) ,  kvs ) \n     } . flatMapValues ( x  = >  x ) . sortByKey ( ) \n    \n    val hbaseConf  =   HBaseConfiguration . create ( sparkSession . sessionState . newHadoopConf ( ) ) \n    hbaseConf . set ( \"hbase.zookeeper.quorum\" ,   \"linux-1:2181,linux-2:2181,linux-3:2181\" ) \n    hbaseConf . set ( TableOutputFormat . OUTPUT_TABLE ,   \"hfile\" ) \n    val connection  =   ConnectionFactory . createConnection ( hbaseConf ) \n\n    val tableName  =   TableName . valueOf ( \"hfile\" ) \n\n     //没有HBase表则创建 \n     creteHTable ( tableName ,  connection ) \n\n    val table  =  connection . getTable ( tableName ) \n\n     try   { \n      val regionLocator  =  connection . getRegionLocator ( tableName ) \n\n      val job  =   Job . getInstance ( hbaseConf ) \n\n      job . setMapOutputKeyClass ( classOf [ ImmutableBytesWritable ] ) \n      job . setMapOutputValueClass ( classOf [ KeyValue ] ) \n\n       HFileOutputFormat2 . configureIncrementalLoad ( job ,  table ,  regionLocator ) \n\n      val savePath  =   \"hdfs://linux-1:9000/hfile_save\" \n       delHdfsPath ( savePath ,  sparkSession ) \n\n      job . getConfiguration . set ( \"mapred.output.dir\" ,  savePath ) \n\n      data . saveAsNewAPIHadoopDataset ( job . getConfiguration ) \n\n      val bulkLoader  =   new   LoadIncrementalHFiles ( hbaseConf ) \n      bulkLoader . doBulkLoad ( new   Path ( savePath ) ,  connection . getAdmin ,  table ,  regionLocator ) \n\n     }   finally   { \n       //WARN LoadIncrementalHFiles: Skipping non-directory hdfs://linux-1:9000/hfile_save/_SUCCESS 不影响,直接把文件移到HBASE对应HDFS地址了 \n      table . close ( ) \n      connection . close ( ) \n     } \n\n    sparkSession . stop ( ) \n   } \n\n  def  creteHTable ( tableName :   TableName ,  connection :   Connection ) :   Unit   =   { \n    val admin  =  connection . getAdmin\n\n     if   ( ! admin . tableExists ( tableName ) )   { \n      val tableDescriptor  =   new   HTableDescriptor ( tableName ) \n      tableDescriptor . addFamily ( new   HColumnDescriptor ( Bytes . toBytes ( \"hfile-fy\" ) ) ) \n      admin . createTable ( tableDescriptor ) \n     } \n   } \n\n  def  delHdfsPath ( path :   String ,  sparkSession :   SparkSession )   { \n    val hdfs  =   FileSystem . get ( sparkSession . sessionState . newHadoopConf ( ) ) \n    val hdfsPath  =   new   Path ( path ) \n\n     if   ( hdfs . exists ( hdfsPath ) )   { \n       //val filePermission = new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.READ) \n      hdfs . delete ( hdfsPath ,   true ) \n     } \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 #  HBase和Hive的集成操作 \n hbase和hive的对比说明 \n \n hive:  就是一个数据仓库的工具 , 基于HADOOP, 数据存储在Datanode, 执行翻译为MR, 支持SQL, 支持join  主要是用于离线分析操作 与清洗操作, 延迟较高 \n hbase: 是一个nosql型数据库, 用于存储数据, 基于hadoop, 数据最终存储在datanode, 不支持SQL, 不支持join 主要是用于接入在线业务, 延迟较低 , 具有高效的随机读写能力 \n \n 说明: \n ​     hive 和 hbase都是基于hadoop的不同的工具, hive和hbase可以集成在一起,hive on hbase ，使用hql进行批量分析查询，若是要求随机读写需集成phoenix。 \n 1.2 hbase如何hive进行集成操作 \n 集成步骤: \n \n \n \n 将hive提供的一个和hbase整合的通信包, 导入到Hbase的lib目录下 \n \n \n \n    cd  /export/server/hive-2.1.0/lib/\n   cp  hive-hbase-handler-2.1.0.jar  /export/server/hbase-2.1.0/lib/\n \n 1 2 \n \n \n 将 这个通信包 发送给 node1 和 node2的hbase的lib目录下 \n \n \n \n    cd  /export/server/hbase-2.1.0/lib/\n   scp  hive-hbase-handler-2.1.0.jar node1:/export/server/hbase-2.1.0/lib/\n   scp  hive-hbase-handler-2.1.0.jar node2:/export/server/hbase-2.1.0/lib/\n \n 1 2 3 \n \n \n 修改 hive的 配置文件: \n \n \n hive-site.xml \n \n \n \n   cd /export/server/hive-2.1.0/conf/ \n  vim hive-site.xml \n  \n  添加以下内容:\n    < property > \n           < name > hive.zookeeper.quorum </ name > \n           < value > node1,node2,node3 </ value > \n    </ property > \n  \n  \n    < property > \n           < name > hbase.zookeeper.quorum </ name > \n           < value > node1,node2,node3 </ value > \n    </ property > \n  \n    < property > \n           < name > hive.server2.enable.doAs </ name > \n           < value > false </ value > \n    </ property > \n  \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \n hive-env.sh \n \n    cd  /export/server/hive-2.1.0/conf/ \n   vim  hive-env.sh \n  \n  添加以下内容:\n   export   HBASE_HOME = /export/server/hbase-2.1.0\n \n 1 2 3 4 5 \n \n 启动 \n \n   1)   先启动 zookeeper , 保证zookeeper启动良好 \n   2)   接着启动 hadoop集群, 保证hadoop是启动良好 \n   3)   然后启动 hbase集群, 保证hbase集群是启动良好的 \n   4)   最后启动 hive  , 保证hive是启动良好的 \n  \n   说明 :   3  和 4  可以调换 \n \n 1 2 3 4 5 6 \n 测试 查看是否可用: \n \n   第一步: 在hbase的shell客户端下, 创建一个表 并添加相关的数据\n  hbase ( main ) :007: 0 >  create  'hbase_hive_score' , 'cf' \n  \n  hbase ( main ) :007: 0 >  put  'hbase_hive_score'  , '1' , 'cf:name' , 'zhangsan' \n                                                                               \n  hbase ( main ) :007: 0 >  put  'hbase_hive_score'  , '1' , 'cf:age' , '25' \n                                                                                 \n  hbase ( main ) :008: 0 >  put  'hbase_hive_score'  , '2' , 'cf:name' , 'lisi' \n                                                                                   \n  hbase ( main ) :009: 0 >  put  'hbase_hive_score'  , '2' , 'cf:age' , '30' \n                                                                                \n  hbase ( main ) :010: 0 >  put  'hbase_hive_score'  , '3' , 'cf:name' , 'wangwu' \n                                                                                 \n  hbase ( main ) :011: 0 >  put  'hbase_hive_score'  , '3' , 'cf:age' , '18' \n  \n  hbase ( main ) :012: 0 >  scan  'hbase_hive_score' \n  ROW                        COLUMN+CELL                                                              \n    1                           column = cf:age,  timestamp = 1615427034130 ,  value = 25                          \n    1                           column = cf:name,  timestamp = 1615427024464 ,  value = zhangsan                  \n    2                           column = cf:age,  timestamp = 1615427052348 ,  value = 30                          \n    2                           column = cf:name,  timestamp = 1615427045923 ,  value = lisi                      \n    3                           column = cf:age,  timestamp = 1615427082291 ,  value = 18                          \n    3                           column = cf:name,  timestamp = 1615427073970 ,  value = wangwu      \n  \n  \n  \n  第二步: 在hive中对hbase的这个表进行映射匹配操作, 由于数据是被hbase所管理, 在hive中建表选择外部表\n  语法格式:\n      create  external table  表名  ( \n         \n         字段1 类型,\n         字段2 类型,\n         字段3 类型\n          .. .. \n       )  stored by  'org.apache.hadoop.hive.hbase.HBaseStorageHandler'  with serdeproperties ( 'hbase.columns.mapping' = ':key,列族1:列名1...' )  tblproperties ( 'hbase.table.name' = 'hbase的表名' ) ; \n      \n  注意: \n     表名  :  建议和hbase表名保持一致  ( 不一致也是OK的 ) \n     字段  :  建议和hbase列名保持一致 \n         字段的第一个, 建议放置的主键字段, 不需要加primary key\n  \n  create  external table  day03_hivehbase.hbase_hive_score  ( \n   id  int,\n  name string,\n  age int\n   )  stored by  'org.apache.hadoop.hive.hbase.HBaseStorageHandler'  with serdeproperties ( 'hbase.columns.mapping' = ':key,cf:name,cf:age' )  tblproperties ( 'hbase.table.name' = 'hbase_hive_score' ) ; \n  \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 \n hive集成存在的问题: \n  Apache Phoenix \n Apache   Phoenix  仅仅是一款可以通过 SQL的方式来操作（CRUD）hbase的工具 ，底层大量的利用hbase的协处理器。 \n  phoenix集成hbase与hive、impala集成的对比 \n HBase的协处理器 \n 3.1 协处理器的基本介绍 \n hbase提供的协处理器主要有二大类 \n \n \n \n observer: \n \n \n \n        可以将observer看做是   数据库的触发器 或者可以理解为监听器, 可以通过observer提供一些钩子(事件), 对某些事件进行监听操作, 一旦触发了这个事件, 理解通知绑定监听事件的人即可 \n       \n  这类协处理器还可以做什么事情呢?\n       1)   操作日志记录 \n       2)   权限的管理 \n \n 1 2 3 4 5  ![image-20210311153033576](hbase.assets/image-20210311153033576.png)\n \n \n \n \n endpoint: \n \n \n \n      这类协处理器   可以将其看做是 数据库的中存储过程, 也可以类似于在java中定义一个方法, 将相关的功能放置在这个方法中 即可 \n      一旦定义这样协处理器,   可以将这个协处理器提交到server(服务)端, 有各个服务端来执行这段操作, 将执行的结果返回给客户端, 客户端在根据返回结果做相应的处理即可  \n     \n   作用 :   做 聚合操作  sum  count  max ... \n \n 1 2 3 4 3.2 如何设置协处理器 \n \n \n 设置方式一: 静态设置  全局设置 \n \n \n \n   此配置需要配置到hbase-site.xml中    全局有效, 每个hbase表 都会有这个协处理器 \n \n 1 \n \n 设置方式二: 动态设置, 只针对某个表有效 \n \n \n 第一步: 禁用表 \n \n \n 第二步: 添加协处理器 \n \n \n \n 第三步: 启用表 \n \n \n \n \n 如何卸载动态设置协处理器: \n \n \n 第一步: 禁用表 \n \n \n 第二步: 删除协处理器 \n \n \n \n 第三步: 启用表 \n \n \n \n \n apache Phoenix的安装 \n 注意: \n 1) 在安装完成后, 如果hbase无法启动, 请检查 hbase的配置文件 以及lib目录,3个节点都需要检查 是否OK\n\n2) 在安装完成后, 如果Phoenix无法启动, 一启动就报错, 检查 Phoenix的bin目录下的hbase-site.xml 其内容是否是hbase的conf目录下的那个hbase-site.xml的内容\n \n 1 2 3 #  Apache Phoenix的基本入门操作 \n Grammar | Apache Phoenix \n \n \n \n 如何在Phoenix创建表 \n \n \n \n    --格式:  \n   create   table   [ if   not   exists ]  表名  ( \n     rowkey名称  数据类型  primary   key , \n     列族名 . 列名 1  数据类型  , \n     列族名 . 列名 2  数据类型  , \n     列族名 . 列名 3  数据类型  , \n     列族名 . 列名 4  数据类型 \n      . . . . . \n   ) ; \n \n 1 2 3 4 5 6 7 8 9 \n 案例: 创建一张订单表 \n \n    create   table   order_dtl  ( \n     id   varchar   primary   key , \n     c1 . status   varchar   , \n     c1 . money   integer   , \n     c1 . pay_way  integer   , \n     c1 . user_id  varchar , \n     c1 . operation  varchar , \n     c1 . category  varchar  \n   ) ; \n \n 1 2 3 4 5 6 7 8 9 \n 执行完成后, 可以在hbase的webui中查看到, 多出来一张表, 在此表中默认的region数量为1, 同时给表加入很多协处理器 \n \n ​     注意: Phoenix会自动将小写变更为大写: 表名 列族 列名 \n ​     需求: 字段必须为小写, 不使用大写 , 如何做 \n create table  \"order_dtl_01\" (\n   \"id\"  varchar primary key,\n   \"c1\".\"status\" varchar ,\n   \"c1\".money  integer ,\n   c1.\"pay_way\" integer ,\n   c1.user_id varchar,\n   c1.operation varchar,\n   c1.category varchar \n);\n \n 注意: \n如果想要使用小写, 只需要在需要小写的内容两端加上双引号(必须为双引号) \n 单引号 表示是普通字符串 \n 推荐: 建议使用大写, 如果为小写, 后续所有的操作, 只要用到这个小写的内容, 必须加双引号 \n \n \n \n 如何在Phoenix中查看所有的表 \n \n \n \n   格式:\n      !table\n \n 1 2 \n \n \n \n 查看某一个表的结构信息 \n \n \n \n   格式 : \n      !desc 表名 \n \n 1 2 \n 注意: Phoenix会自动将小写变更为大写: 表名 列族 列名 \n 需求: 字段必须为小写, 不使用大写 , 如何做 \n \n \n \n 如何向表中插入数据 \n \n \n \n   格式:\n     upsert into 表名(列族.列名1,列族.列名2,... ) values(值1,值2 ....)\n     \n  案例:\n     upsert into ORDER_DTL values('000002','未提交',4070,1,'4944191','2020/04/25 12:09:16','手机');\n \n 1 2 3 4 5 \n \n \n \n 查询操作: 与标准的SQL是一致的 \n \n \n 只不过 不支持 join 不支持多表关联 仅支持单表查询 \n \n \n \n \n 删除数据: 与标准SQL是一致的 \n \n \n \n \n \n 分页查询 \n \n \n \n   语法: \n      select * from 表  limit 每页显示n条 offset(m-1)\n      (从第m条开始显示n条)\n      \n  案例:\n  1) 首先先添加一坨数据\n  UPSERT INTO \"ORDER_DTL\" VALUES('000002','已提交',4070,1,'4944191','2020-04-25 12:09:16','手机;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000003','已完成',4350,1,'1625615','2020-04-25 12:09:37','家用电器;;电脑;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000004','已提交',6370,3,'3919700','2020-04-25 12:09:39','男装;男鞋;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000005','已付款',6370,3,'3919700','2020-04-25 12:09:44','男装;男鞋;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000006','已提交',9380,1,'2993700','2020-04-25 12:09:41','维修;手机;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000007','已付款',9380,1,'2993700','2020-04-25 12:09:46','维修;手机;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000008','已完成',6400,2,'5037058','2020-04-25 12:10:13','数码;女装;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000009','已付款',280,1,'3018827','2020-04-25 12:09:53','男鞋;汽车;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000010','已完成',5600,1,'6489579','2020-04-25 12:08:55','食品;家用电器;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000011','已付款',5600,1,'6489579','2020-04-25 12:09:00','食品;家用电器;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000012','已提交',8340,2,'2948003','2020-04-25 12:09:26','男装;男鞋;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000013','已付款',8340,2,'2948003','2020-04-25 12:09:30','男装;男鞋;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000014','已提交',7060,2,'2092774','2020-04-25 12:09:38','酒店;旅游;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000015','已提交',640,3,'7152356','2020-04-25 12:09:49','维修;手机;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000016','已付款',9410,3,'7152356','2020-04-25 12:10:01','维修;手机;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000017','已提交',9390,3,'8237476','2020-04-25 12:10:08','男鞋;汽车;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000018','已提交',7490,2,'7813118','2020-04-25 12:09:05','机票;文娱;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000019','已付款',7490,2,'7813118','2020-04-25 12:09:06','机票;文娱;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000020','已付款',5360,2,'5301038','2020-04-25 12:08:50','维修;手机;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000021','已提交',5360,2,'5301038','2020-04-25 12:08:53','维修;手机;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000022','已取消',5360,2,'5301038','2020-04-25 12:08:58','维修;手机;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000023','已付款',6490,0,'3141181','2020-04-25 12:09:22','食品;家用电器;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000024','已付款',3820,1,'9054826','2020-04-25 12:10:04','家用电器;;电脑;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000025','已提交',4650,2,'5837271','2020-04-25 12:08:52','机票;文娱;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000026','已付款',4650,2,'5837271','2020-04-25 12:08:57','机票;文娱;');\n  \n  2) 采用分页查询: 每页显示 5 条  , 显示第一页\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \n \n \n \n 删除表: 和标准SQL是一致 \n Apache Phoenix的预分区操作 \n ​\t通过Phoenix来构建表, 默认情况下, 只有一个region \n Phoenix预分区的方式: \n \n \n \n 手动预分区: \n \n \n \n   语法格式:\n   create   table   [ if   not   exists ]  表名  ( \n     rowkey名称  数据类型  primary   key , \n     列族名 . 列名 1  数据类型  , \n     列族名 . 列名 2  数据类型  , \n     列族名 . 列名 3  数据类型  , \n     列族名 . 列名 4  数据类型 \n      . . . . . \n   ) \n  compression = 'GZ'   -- 压缩方式 \n  split  on ( region分区方案 )    -- 定义手动预分区操作 \n   ; \n  \n  案例: \n   drop   table  order_dtl ; \n   create   table   order_dtl  ( \n     id   varchar   primary   key , \n     c1 . status   varchar   , \n     c1 . money   integer   , \n     c1 . pay_way  integer   , \n     c1 . user_id  varchar , \n     c1 . operation  varchar , \n     c1 . category  varchar  \n   ) \n  compression = 'GZ' \n  split  on ( '10' , '20' , '30' ) ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n \n \n hash预分区: \n \n \n \n   格式:\n   create   table   [ if   not   exists ]  表名  ( \n     rowkey名称  数据类型  primary   key , \n     列族名 . 列名 1  数据类型  , \n     列族名 . 列名 2  数据类型  , \n     列族名 . 列名 3  数据类型  , \n     列族名 . 列名 4  数据类型 \n      . . . . . \n   ) \n  compression = 'GZ' ,   -- 压缩方式 \n  salt_buckets = N   -- 加盐预分区 (hash + rowkey自动加盐) \n   ; \n  \n  案例\n   drop   table  order_dtl ; \n   create   table   order_dtl  ( \n     id   varchar   primary   key , \n     c1 . status   varchar   , \n     c1 . money   integer   , \n     c1 . pay_way  integer   , \n     c1 . user_id  varchar , \n     c1 . operation  varchar , \n     c1 . category  varchar  \n   ) \n  compression = 'GZ'   , \n  salt_buckets = 10 ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ​      总结: 如果使用Phoenix的加盐预分区方案, Phoenix在添加数据的时候, 会自动在rowkey的前面进行加盐处理, 但是对用户从操作Phoenix角度来说是无感操作,除非我们去hbase查看原始内容 \n apache  Phoenix的视图 \n ​\t   默认情况下, Phoenix中只展示由Phoenix自己创建表, 如果说hbase的表是通过hbase自己来构建的, 在Phoenix中无法查看到, 那么也就意味着, 无法通过Phoenix来操作hbase原有表 \n ​\t   如果想通过Phoenix对hbase原有表进行SQL的操作, 此时可以利用Phoenix提供的视图来解决 \n 如何实现视图呢? \n 格式:\n    create    view    \"名称空间\" . \"hbase对应的表的表名\"   ( \n        key   varchar    primary   key , \n        \"列族\" . \"列名\"  类型 , \n        . . . . . \n   \n    )   [ default_colunm_family = '列族名' ] ; \n\n注意事项:\n     视图的名称 一定要与 需要建立视图的hbase的表名是一致的\n      key 的名称是可以任意的 , 但是必须添加 primary   key \n     普通的列 ,  需要和 hbase的对应表保持一致\n     \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 案例: 针对 WATER_BILL 表 构建视图 \n \n   create    view    \"WATER_BILL\"   ( \n       ID  varchar    primary   key , \n       C1 . NAME  varchar , \n       C1 . ADDRESS  varchar , \n       C1 . LATEST_DATE  varchar , \n       C1 . NUM_CURRENT UNSIGNED_DOUBLE  , \n       C1 . NUM_PREVIOUS UNSIGNED_DOUBLE  , \n       C1 . NUM_USAGE UNSIGNED_DOUBLE  , \n       C1 . PAY_DATE  varchar , \n       C1 . RECORD_DATE  varchar , \n       C1 . SEX  varchar , \n       C1 . TOTAL_MONEY UNSIGNED_DOUBLE \n    ) ; \n   \n   UNSIGNED_DOUBLE: 无符号的 double 类型\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n 查询视图, 观察是否有数据: \n \n 查询 六月份的用户的用水量 \n select   name , num_usage ,  record_date  from  water_bill  where  record_date  between   '2020-06-01'   and   '2020-06-30' ; \n \n 1 \n Apache Phoenix的二级索引 \n ​    索引目的:  提高查询的效率 \n \n 2.1 Phoenix索引的分类 \n 在Phoenix中共计提供了四种索引: \n \n \n \n 全局索引 \n \n \n 特点: \n \n \n \n       在构建了全局索引之后,   会单独的形成一张索引表,单独的索引表与目标表拥有相同的region数量,  当查询数据时候, 可以先到索引表查询, 查询到之后, 再到目标中进行查询即可, 但是如果对数据进行修改, 此时索引表的数据也会随之进行修改操作 \n  \n   注意 :  \n       在修改索引的表,   对于全局索引而言, 需要做全局更新操作, 代价较大, 效率较低 \n       在查询的过程中,   如果SQL语句中出现了非索引的字段, 此时全局索引无法生效 \n  \n   全局索引一般和覆盖索引搭配使用，读的效率很高，但对写入的效率影响较大,   所以说 全局索引 比较适用于 读多 写少的场景 \n \n 1 2 3 4 5 6 7 \n 适用于: 读多 写少的场景 \n 如何来构建索引呢? \n \n   格式:\n       create   index   索引名称   on   表名 ( 列名 1 , 列名 2   . . . . ) \n \n 1 2 \n 如何删除索引呢? \n \n   格式:\n      drop   index   索引名称  on  表名 ; \n \n 1 2 \n \n \n \n 本地索引 \n \n \n 特点: \n \n \n \n       在构建了本地索引后,   不会单独创建一张索引表, 索引数据直接附带在目标表对应字段的后面, 这样在进行修改(增 删 改)数据操作的时候 , 直接在目标对索引数据一次性就处理掉了, 适用于写的多场景 \n       在执行查询的时候,   Phoenix会自动选择是否使用本地索引, 即使有非索引的字段, 依然可用 \n  \n   注意 : \n       如果表在构建的时候,   采用的加盐预分区的方案, 建议大家不要使用本地索引, 因为有部分操作是不支持的 \n \n 1 2 3 4 5 \n 适用于:   写多  读少的场景 \n 如何创建本地索引呢? \n \n   格式:\n       create   local   index   索引名称   on   表名 ( 列名 1 , 列名 2   . . . . ) \n \n 1 2 \n 如何删除索引 \n \n   格式:\n      drop   index   索引名称  on  表名 ; \n \n 1 2 \n \n \n 覆盖索引 \n \n \n 特点: \n \n \n \n        覆盖索引无法单独使用,   必须和全局索引或者本地索引配合使用, 在构建覆盖索引后, 将对应索引字段数据, 直接放置在本地索引或者全局索引字段的后面即可, 这样在查询的时候, 可以不去查询目标表, 直接在索引表中可以所有需要的字段数据, 这样可以提升读取数据时间 \n       \n       一般会和全局索引组合使用\n \n 1 2 3 \n 适用于: 将一些不参与条件的字段 但是会参与展示的字段 放置在覆盖索引上 \n 如何构建覆盖索引呢? \n \n   格式:\n      create   [ local ]   index   索引名称   on  表名 ( 列名 1 ,  列名 2. . . . )  include ( 列 1 , 列 2. . . ) \n \n 1 2 \n \n 如何删除覆盖索引: 随着全局或者本地索引的删除而删除 \n \n \n \n 函数索引 \n \n \n 特点 \n \n \n \n       无法单独使用,   在本地索引或者全局索引中使用,  主要用于针对某个函数的结果来构建索引, 这样以后用到这个对应结果数据的函数, 那么也可以执行索引优化 \n \n 1 \n 适用于: 多条SQL语句中, 需要频繁的使用某个函数的结果 \n 如何构建呢? \n \n   格式:\n      create   [ local ]   index   索引名称   on  表名 ( 列 1 , 列 2 , 函数名称 ( 字段 . . . ) . . . ) \n \n 1 2 2.2 案例一: 创建全局索引+覆盖索引 \n \n \n 需求: 查询  用户id为 8237476 的订单数据 \n \n \n SQL: \n \n \n    select   USER_ID , ID , MONEY  from  order_dtl  where  USER_ID = '8237476' ; \n  \n  使用  explain   检查其执行的效率: \n   explain   select   USER_ID , ID , MONEY  from  order_dtl  where  USER_ID = '8237476' ; \n  \n  结论:  通过扫描全表 获取的结果数据\n \n 1 2 3 4 5 6 \n \n 索引的方式来提升效率 \n \n 1 )  创建索引:\n       create    index   idx_index_order_dtl  on  order_dtl ( user_id )  include ( ID , MONEY ) ; \n \n 1 2 \n 2 )  重新检查刚刚 SQL ,  是否会执行操作\n    explain   select   USER_ID , ID , MONEY  from  order_dtl  where  USER_ID = '8237476' ; \n \n 1 2 \n 如果查询的中, 出现了非索引的字段, 会出现什么问题呢? \n \n 总结: 全局索引无法生效 \n 要求: 就想使用全局索引, 而且不想对这个非索引的字段构建索引 \n 解决方案: 通过强制使用全局索引方案 来生效 \n explain   select   /*+INDEX(order_dtl idx_index_order_dtl) */    *   from  order_dtl  where  USER_ID = '8237476' ; \n \n 1 \n 删除索引 \n drop   index  idx_index_order_dtl  on  order_dtl ; \n \n 1 2.3 案例二:  本地索引 \n \n 需求:   根据  订单ID  订单状态  支付金额  支付方式  用户ID 查询数据 \n \n 1) 构建索引:\n   create local index  IDX_LOCAL_order_dtl  on  order_dtl(ID,STATUS,MONEY,PAY_WAY,USER_ID);\n \n 1 2 \n 说明: 虽然可以通过!table看到索引表, 本质上在hbase上没有构建索引表 \n 2 )  测试 1 : 所有字段都是本地索引\n    explain   select   USER_ID , ID , MONEY  from  order_dtl  where  USER_ID = '8237476' ; \n \n 1 2 \n 3 )  测试 2 :  有部分字段没有索引\n    explain   select   USER_ID , ID , MONEY , CATEGORY  from  order_dtl  where  USER_ID = '8237476' ; \n \n 1 2 \n 3 )  测试 3 :  查询所有的字段\n     explain   select    *   from  order_dtl  where  USER_ID = '8237476' ;  \n    \n原因:\n   表是加盐预分区的操作\n \n 1 2 3 4 5 \n 注意: 本地索引 会直接对目标表数据产生影响, 所以一旦构建本地索引, 将不再支持通过原生API 查询数据 \n \n 2.4 案例三: 实现WATER_BILL查询操作 \n \n 原来的查询WATER_BILL的SQL语句: \n \n select  name,num_usage, record_date from water_bill where record_date between '2020-06-01' and '2020-06-30';\n\n花费时间为: 8s多\n \n 1 2 3 \n 接下来对其进行优化: \n \n 1 )  创建索引:  全局 | 本地  +  覆盖\n    create   index  IDX_INDEX_WATER_BILL  ON  WATER_BILL ( record_date )  include  ( name , num_usage ) ; \n\n 2 )  执行查询操作: \n    select   name , num_usage ,  record_date  from  water_bill  where  record_date  between   '2020-06-01'   and   '2020-06-30' ; \n  \n花费时间:   7 s左右\n\n \n 1 2 3 4 5 6 7 8 总结索引的使用 \n 1)   给那些经常查询使用字段添加索引 \n 2)   如果前期经常使用, 但是后期不使用, 请将索引信息删除 \n 3)   如果前期不经常使用,但是后期经常使用, 后期还需要添加索引 \n\n 索引好处 :   提升查询的效率 \n\n 弊端 :   数据会冗余存储 占用磁盘空间 \n\n 索引 :    以空间换时间操作 \n \n 1 2 3 4 5 6 7 8 9 #  hbase的表结构设计 \n hbase的名称空间(命名空间) \n ​\thbase的名称空间  类似于 在mysql 或者 hive中的数据库, 只不过在hbase将其称为叫做名称空间(命名空间) \n 思考: mysql或者hive 为什么会有数据库的概念呢? \n \n \n \n 利于管理,利于维护 \n \n \n \n \n 业务划分更加明确 \n \n \n \n \n 可以针对性设置权限 \n \n \n \n 同样的, hbase的名称空间也具有相似作用 \n 在hbase中, 默认情况下, 是有两个名称空间的, 一个 hbase  和 default \n \n 名称为 hbase的名称空间:  专门用于放置hbase内部的管理表的, 此名称空间, 一般不操作, hbase自己维护即可\n \n 内部有一张核心的元数据表:  meta表\n \n 此表存储了hbase中所有自定义表的相关的元数据信息 \n \n \n \n \n 名称为 default 的名称空间:   默认的名称空间, 在创建hbase表的时候, 如果没有指定名称空间, 默认就创建到此空间下\n \n 类似于 hive中也有一个default数据库 \n 在测试 学习的时候可以使用 \n \n \n \n \n 如何操作HBase的名称空间呢? \n如何创建名称空间: \ncreate_namespace   '名称空间' \n #查看当下有那些名称空间 \nlist_namespace\n #查看某一个名称空间 \ndescribe_namespace  '名称空间' \n #删除名称空间 \ndrop_namespace  '名称空间' \n   注意: 在删除这个空间的时候, 需要保证此空间下没有任何表\n #如何在某个名称空间下创建表 \ncreate  '名称空间名称:表名' , '列族'   .. .\n   注意: 一旦建立在default以外的名称空间下, 在后续操作这个表, 必须携带名称空间,否则hbase会到default空间下\n \n 1 2 3 4 5 6 7 8 9 10 11 12 #  hbase表的列族的设计 \n 结论:  在建表的时候, 如果能用一个列族来解决的, 坚决使用一个即可, 能少则少 \n 原因: \n ​     过多的列族会导致在region中出现过多的store模块, 而每个store模块中由一个memStore 和 多个storeFile构成的, 这样会导致数据在存储或者读取的时候, 需要跨域多个内存和多个文件才能将整行的数据都获取到, 此时增大了IO操作, 从而导致效率比较低 \n ​     而且这个列族多了之后, 会导致region可能会频繁的要进行刷新 和 compact合并操作 \n 多个列族在文件层次上存储在不同的HFile文件中。 \n 思考: 什么场景下 可能会构建多个列族呢? 一般为 2~5 \n \n 假设有一个表, 表中字段比较多, 但是某些字段不常使用, 此时可以将经常使用的字段放入某一个列族中, 另一个放置不常使用字段即可 \n 假设一个表要对接多个不同业务, 而多个不同业务可能会用到不同的字段, 可以根据业务 划分不同的列族中 \n hbase表的预分区 \n ​      在hbase中 表默认情况下 只有一个region,  而一个region只能被一个regionServer所管理 \n 思考: \n ​      假设通过默认的方式, 创建了一张hbase的表, 接着需要向这个表写入大量的数据, 同时也有大量的读请求在查询这个表中数据, 此时会发生什么问题呢? \n ​\t 出现问题:  对应管理region的regionServer的服务器可能出现延迟较大, 甚至出现宕机的问题 \n ​      原因: 只有一个region, 对应只有一个regionServer, 一个regionServer需要承载这个表所有并发读写请求 \n 如何解决呢? \n   如果可以在建表的时候,   一次性构建出多个region, 然后多个region能够均匀的分布在不同的regionServer上, 这样问题及迎刃而解了 \n \n 1 实现方式: 通过HBase的预分区 \n \n \n 预分区的目的:  让hbase在建表的时候, 就可以拥有多个region \n \n \n \n 如何实现预分区的操作: \n \n \n \n 手动预分区操作 \n \n create  '表名'  , '列族1' ,  SPLITS = > [ 分区范围即可 ]   \n\n例子: \ncreate  't1_split' , 'f1' ,SPLITS = > [ '10' , '20' , '30' , '40' ] \n \n 1 2 3 4 \n \n \n \n 采用 hash预分区方案 \n \n 格式 \n    create  '表名'  , '列族1' ,  { NUMREGIONS = > N,SPLITALGO = > 'HexStringSplit' } \n    \n案例: \n    create  't2_split' , 'f1' , { NUMREGIONS = > 10 ,SPLITALGO = > 'HexStringSplit' } \n \n 1 2 3 4 5 \n hbase.hregion.max.filesize不宜过大或过小，经过实战，生产高并发运行下，最佳大小5-10GB！关闭某些重要场景的HBase表的major_compact！在非高峰期的时候再去调用major_compact，这样可以减少split的同时，显著提供集群的性能，吞吐量、非常有用。 一般情况下，单个region的大小建议控制在5GB以内，可以通过参数hbase.hregion.max.filesize来设置，单个regionserver下的regions个数控制在200个以内。regions过多会导致集群不堪重负、regionserver 频繁FullGC的情况，进而影响集群的稳定性甚至上层的应用服务。 \n hbase的版本确界和TTL \n 什么是数据版本确界 \n 数据版本的确界 所描述的就是在hbase中 数据的历史版本的数量 \n \n 下界: hbase中对应单元格 至少需要保留多少版本, 即使数据已经过期了 ,\n \n 默认值为 0 \n \n \n 上界: hbase中对应单元格 最多可以保留多少个版本, 如果比设置多了, 最早期本部会被覆盖掉\n \n 默认值为 1 \n \n \n \n 2.6.2 什么是数据的TTL \n 在hbase中, 可以针对数据设置过期时间, 当时间过期后, hbase会自动将过期数据给清理掉 \n 2.6.3 代码演示数据版本确界和TTL \n \n 代码内容: \n \n public   class   HBaseTTLTest   { \n     //1. 创建一个Hbase的表 (设置 版本确界 以及 TTL) \n     //2. 向表添加数据, 添加后  进行多次修改操作(产生历史版本) \n     //3. 查询数据:  需要将其历史版本全部都查询到 \n\n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         //1. 根据连接工厂获取hbase的连接对象 \n         Configuration  conf  =   HBaseConfiguration . create ( ) ; \n        conf . set ( \"hbase.zookeeper.quorum\" , \"node1:2181,node2:2181,node3:2181\" ) ; \n         Connection  hbConn  =   ConnectionFactory . createConnection ( conf ) ; \n\n         //2. 获取相关的管理对象 \n         Admin  admin  =  hbConn . getAdmin ( ) ; \n\n         //3. 执行相关的操作: \n\n         //3.1: 判断表是否存在 \n         if (   ! admin . tableExists ( TableName . valueOf ( \"day03_hbaseTTL\" ) ) ) { \n             // 只要能进来 说明 表不存在 \n\n             TableDescriptorBuilder  descBuilder  =   TableDescriptorBuilder . newBuilder ( TableName . valueOf ( \"day03_hbaseTTL\" ) ) ; \n\n             ColumnFamilyDescriptorBuilder  familyDesc  =   ColumnFamilyDescriptorBuilder . newBuilder ( \"C1\" . getBytes ( ) ) ; \n\n             // 设置 版本 确界  和 TTL \n            familyDesc . setMinVersions ( 3 ) ;    // 设置 下界 \n            familyDesc . setMaxVersions ( 5 ) ;    // 设置 上界 \n\n            familyDesc . setTimeToLive ( 30 ) ;    // 设置 ttl  有效时间为30s \n\n             ColumnFamilyDescriptor  family  =  familyDesc . build ( ) ; \n            descBuilder . setColumnFamily ( family ) ; \n\n             TableDescriptor  desc  =  descBuilder . build ( ) ; \n\n             //3.2: 创建表 \n            admin . createTable ( desc ) ; \n         } \n\n\n         //3.3:  添加数据操作 \n         //3.3.1: 获取 table管理对象 \n         Table  table  =  hbConn . getTable ( TableName . valueOf ( \"day03_hbaseTTL\" ) ) ; \n\n         //3.3.2: 执行添加数据操作 \n\n         for ( int  i  =   1   ;  i <=   2   ;  i ++ ) { \n             Put  put  =   new   Put ( \"rk001\" . getBytes ( ) ) ; \n\n            put . addColumn ( \"C1\" . getBytes ( ) , \"NAME\" . getBytes ( ) , ( \"zhangsan\" + i ) . getBytes ( ) ) ; \n\n            table . put ( put ) ; \n         } \n\n         //3.4: 查询数据 \n         Get  get  =   new   Get ( \"rk001\" . getBytes ( ) ) ; \n        get . readAllVersions ( ) ;   // 获取所有的历史版本 \n         Result  result  =  table . get ( get ) ; \n\n         //Cell[] rawCells = result.rawCells(); \n         List < Cell >  cells  =  result . listCells ( ) ; \n\n         for   ( Cell  cell  :  rawCells )   { \n\n             System . out . println ( Bytes . toString ( CellUtil . cloneValue ( cell ) ) ) ; \n         } \n\n\n         //4. 释放资源 \n        table . close ( ) ; \n        admin . close ( ) ; \n        hbConn . close ( ) ; \n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 \n \n 总结: \n ​       即使所有的历史版本的数据都过期了, hbase也会至少保留 min_version个最新版本数据, 以保证我们在查询数据的时候. 可以有数据返回 \n hbase的中rowkey的设计原则 \n 说明: \n ​     单纯的通过预分区 是无法解决 向hbase存储数据的高并发的读写问题, 因为如果所有的数据都是某一个region内的数据, 此时依然相当于表只有一个region \n 解决方案: \n ​     需要在预分区的基础, 让rowkey的值 能够均匀的落在不同的region上, 才可以解决 \n 所以: rowkey设计良好, 直接关系到, 数据应该存储到那个region中, 如果设置不良好, 可能会导致数据倾斜问题 \n 设置rowkey有什么样原则呢? \n \n 官方rowkey设计原则 \n \n 1)   避免将 递增行键/时序数据 放置在rowkey最前面 \n 2)   避免rowkey和列的长度过大  \n    说明,   太长rowkey 和列 会导致 占用空间更大, 导致在内存中存储数据行数会更少, 从而最终导致提前进行flush操作, 影响效率 \n    rowkey支持大小 :   64kb \n    一般情况下 :   建议在 100字节以下 , 大部分长度为 10~20左右 \n   \n 3)   使用Long等类型 比String类型更省空间 \n 4)   保证rowkey的唯一性 \n \n 1 2 3 4 5 6 7 8 如何避免热点的问题呢? \n \n \n \n 反转操作 \n \n \n 弊端 : 如果是相关性比较强的数据, 此种打乱会导致读写效率降低 \n \n \n \n \n 加盐策略(随机在rowkey前面加一个随机字符串或者随机数) \n \n \n 弊端: 如果是相关性比较强的数据, 此种打乱会导致读写效率降低 \n \n \n \n \n 哈希策略 （MurmurHash3）:  保证相同内容 得到hash值是一致的,可以让相关性数据放置在一个region中 \n hbase的表的压缩方案的选择 \n \n 在生产中如何选择压缩方式呢? \n 1) 必须要压缩, 如果表的数据, 需要进行频繁的数据读写操作, 而且数据量比较大, 建议采用 snappy压缩方案\n2) 必须要压缩, 如果表的数据, 需要进行大量写入操作, 但是读取操作不是特别频繁, 建议采用 GZIP|GZ\n \n 1 2 如何在hbase设置表的压缩方案?   默认情况下HBASE是不压缩的, 压缩方案是针对列族来说的 \n \n 配置操作: \n 格式:\n   建表时: create '表名' , {NAME=>'列族',COMPRESSION=>'GZ|SNAPPY'}\n   对已经存在表, 通过修改表结构:  alter '表名', {NAME=>'列族',COMPRESSION=>'GZ|SNAPPY'}\n \n 1 2 3 \n 注意: \n ​       如果需要采用SNAPPY 压缩方案, 可能需要对HBase进行重新编译 , 在编译时类似于 Hadoop, 将支持压缩的C接口放入hbase编译包中 才可以 \n ​       如果要采用LZO的压缩方案, 需要放置LZO的压缩的jar包 \n HBase数据结构 \n 简介 \n 传统关系型数据库，一般都选择使用B+树作为索引结构，而在大数据场景下，HBase、Kudu这些存储引擎选择的是LSM树。LSM树，即日志结构合并树(Log-Structured Merge-Tree)。 \n \n \n B+树是建立索引的通用技术，但如果并发写入压力较大时，B+树需要大量的磁盘 随机IO ，而严重影响索引创建的速度，在一些写入操作非常频繁的应用场景中，就不太适合了 \n \n \n LSM树通过磁盘的 顺序写 ，来实现最好的写性能 \n \n \n LSM树设计思想 \n \n \n \n LSM 的主要思想是划分不同等级的结构，换句话来理解，就是LSM中不止一个数据结构，而是存在多种结构 \n \n \n 一个结构在内存、其他结构在磁盘（HBase存储结构中，有内存——MemStore、也有磁盘——StoreFile） \n \n \n 内存的结构可以是B树、红黑树、跳表等结构（HBase中是跳表），磁盘中的树就是一颗B+树 \n \n \n C0层保存了最近写入的数据，数据都是有序的，而且可以随机更新、随机查询 \n \n \n C1到CK层的数据都是存在磁盘中，每一层中key都是有序存储的 \n \n \n"},{title:"Docker",frontmatter:{title:"Docker",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["其他"],tags:["序列化"]},regularPath:"/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%84%9A%E6%9C%AC/protobuf3%E8%AF%AD%E6%B3%95.html",relativePath:"常用命令脚本/protobuf3语法.md",key:"v-e463dc58",path:"/2023/06/10/protobuf3%E8%AF%AD%E6%B3%95/",headers:[{level:2,title:"为什么要使用protobuf",slug:"为什么要使用protobuf"},{level:2,title:"定义ProtoBuf消息类型",slug:"定义protobuf消息类型"},{level:3,title:"指定字段类型",slug:"指定字段类型"},{level:3,title:"分配标识号",slug:"分配标识号"},{level:3,title:"指定字段规则",slug:"指定字段规则"},{level:3,title:"添加更多消息类型",slug:"添加更多消息类型"},{level:3,title:"添加注释",slug:"添加注释"},{level:3,title:"保留标识符（Reserved）",slug:"保留标识符-reserved"},{level:2,title:"从.proto文件生成了什么？",slug:"从-proto文件生成了什么"},{level:2,title:"标量数值类型",slug:"标量数值类型"},{level:2,title:"默认值",slug:"默认值"},{level:2,title:"枚举",slug:"枚举"},{level:2,title:"使用其他消息类型",slug:"使用其他消息类型"},{level:3,title:"嵌套类型",slug:"嵌套类型"},{level:3,title:"更新一个消息类型",slug:"更新一个消息类型"},{level:2,title:"Oneof",slug:"oneof"},{level:3,title:"使用Oneof",slug:"使用oneof"},{level:3,title:"Oneof 特性",slug:"oneof-特性"},{level:3,title:"向后兼容性问题",slug:"向后兼容性问题"},{level:2,title:"Map（映射）",slug:"map-映射"},{level:3,title:"向后兼容性问题",slug:"向后兼容性问题-2"},{level:2,title:"包",slug:"包"},{level:3,title:"包及名称的解析",slug:"包及名称的解析"},{level:2,title:"定义服务(Service)",slug:"定义服务-service"},{level:2,title:"JSON 映射",slug:"json-映射"},{level:2,title:"选项",slug:"选项"},{level:3,title:"自定义选项",slug:"自定义选项"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' Protobuf简介 \n 为什么要使用protobuf \n 使用protobuf的原因肯定是为了解决开发中的一些问题，那使用其他的序列化机制会出现什么问题呢? \n (1)java默认序列化机制：效率极低，而且还能不能跨语言之间共享数据。 \n (2)XML常用于与其他项目之间数据传输或者是共享数据，但是编码和解码会造成很大的性能损失。 \n (3)json格式也是常见的一种，但是在json在解析的时候非常耗时，而且json结构非常占内存。 \n 但是我们protobuf是一种灵活的、高效的、自动化的序列化机制，可以有效的解决上面的问题。 \n ProtoBuf 3语法 \n 定义ProtoBuf消息类型 \n 要定义一个“搜索请求”的消息格式，每一个请求含有一个查询字符串、感兴趣的查询结果所在的页数，以及每一页多少条查询结果。可以采用如下的方式来定义消息类型的.proto文件 \n syntax   =   "proto3" ; \n message   SearchRequest   { \n\t string  query  =   1 ; \n\t int32  page_number  =   2 ; \n\t int32  result_per_page  =   3 ; \n } 　\n \n 1 2 3 4 5 6 \n 文件的第一行指定了正在使用proto3语法。如果没有指定这个，编译器会使用proto2。这个指定语法行必须是文件的非空非注释的第一个行。 \n SearchRequest消息格式有3个字段，在消息中承载的数据分别对应于每一个字段。其中每个字段都有一个名字和一种类型。 \n 指定字段类型 \n 在上面的例子中，所有字段都是标量类型： \n \n 两个整型（page_number和result_per_page） \n 一个string类型（query） \n \n 当然，也可以为字段指定其他的合成类型，包括枚举（enumerations）或其他消息类型 \n 分配标识号 \n 在消息定义中，每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的，一旦开始使用就不能够再改变。 \n \n \n [1,15]之内的标识号在编码的时候会占用一个字节 \n [16,2047]之内的标识号则占用2个字节 \n 所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号 \n 切记：要为将来有可能添加的、频繁出现的标识号预留一些标识号 \n \n \n \n 最小的标识号可以从1开始，最大到2^29 - 1, or 536,870,911 \n 不可以使用其中的[19000－19999]的标识号， Protobuf协议实现中对这些进行了预留 \n 指定字段规则 \n 所指定的消息字段修饰符必须是如下之一： \n \n \n singular：一个格式良好的消息应该有0个或者1个这种字段（但是不能超过1个）。 \n \n \n repeated：在一个格式良好的消息中，这种字段可以重复任意多次（包括0次）。重复的值的顺序会被保留。 \n 在proto3中，repeated的标量域默认情况下使用packed。 \n 可以了解更多的pakced属性在 Protocol Buffer 编码 \n 添加更多消息类型 \n 在一个.proto文件中可以定义多个消息类型。在定义多个相关的消息的时候，这一点特别有用——例如，如果想定义与SearchResponse消息类型对应的回复消息格式的话，你可以将它添加到相同的.proto文件中，如： \n message   SearchRequest   { \n\t string  query  =   1 ; \n\t int32  page_number  =   2 ; \n\t int32  result_per_page  =   3 ; \n } \n message   SearchResponse   { \n\t . . . \n } \n \n 1 2 3 4 5 6 7 8 #  添加注释 \n 向.proto文件添加注释，可以使用C/C++/java风格的双斜杠（//） 语法格式，如： \n message   SearchRequest   { \n\t string  query  =   1 ; \n\t // Which page number do we want? \n\t int32  page_number  =   2 ;  \n\t // Number of results to return per page. \n\t int32  result_per_page  =   3 ;  \n } \n \n 1 2 3 4 5 6 7 #  保留标识符（Reserved） \n 如果通过删除或者注释所有域，以后的用户可以重用标识号。当重新更新类型的时候，如果使用旧版本加载相同的.proto文件这会导致严重的问题，包括数据损坏、隐私错误等等。现在有一种确保不会发生这种情况的方法就是指定保留标识符，protocol buffer的编译器会警告未来尝试使用这些域标识符的用户。 \n message   Foo   { \n\t reserved   2 ,   15 ,   9   to   11 ; \n\t reserved   "foo" ,   "bar" ; \n } \n \n 1 2 3 4 \n 不要在同一行reserved声明中同时声明域名字和标识号 \n 从.proto文件生成了什么？ \n 当用protocol buffer编译器来运行.proto文件时，编译器将生成所选择语言的代码，这些代码可以操作在.proto文件中定义的消息类型，包括获取、设置字段值，将消息序列化到一个输出流中，以及从一个输入流中解析消息。对Java来说，编译器为每一个消息类型生成了一个.java文件，以及一个特殊的Builder类（该类是用来创建消息类接口的）。 \n 标量数值类型 \n 一个标量消息字段可以含有一个如下的类型——该表格展示了定义于.proto文件中的类型，以及与之对应的、在自动生成的访问类中定义的类型： \n \n \n \n .proto Type \n Notes \n Java Type \n \n \n \n \n double \n 双精度浮点型 \n double \n \n \n float \n 单精度浮点型 \n float \n \n \n int32 \n 使用变长编码，对于负值的效率很低，如果域有可能有负值，请使用sint64替代 \n int \n \n \n uint32 \n 使用变长编码 \n int \n \n \n uint64 \n 使用变长编码 \n long \n \n \n sint32 \n 使用变长编码，这些编码在负值时比int32高效的多 \n int \n \n \n sint64 \n 使用变长编码，有符号的整型值。编码时比通常的int64高效。 \n long \n \n \n fixed32 \n 总是4个字节，如果数值总是比总是比228大的话，这个类型会比uint32高效。 \n int \n \n \n fixed64 \n 总是8个字节，如果数值总是比总是比256大的话，这个类型会比uint64高效。 \n long \n \n \n sfixed32 \n 总是4个字节 \n int \n \n \n sfixed64 \n 总是8个字节 \n long \n \n \n bool \n 布尔类型 \n boolean \n \n \n string \n 一个字符串必须是UTF-8编码或者7-bit ASCII编码的文本。 \n String \n \n \n bytes \n 可能包含任意顺序的字节数据。 \n ByteString \n \n \n \n \n 在java中，无符号32位和64位整型被表示成他们的整型对应形似，最高位被储存在标志位中。 \n 对于所有的情况，设定值会执行类型检查以确保此值是有效。 \n 64位或者无符号32位整型在解码时被表示成为ilong，但是在设置时可以使用int型值设定，在所有的情况下，值必须符合其设置其类型的要求。 \n Integer在64位的机器上使用，string在32位机器上使用 \n 默认值 \n 当一个消息被解析的时候，如果被编码的信息不包含一个特定的singular元素，被解析的对象锁对应的域被设置位一个默认值，对于不同类型指定如下： \n \n \n 对于strings，默认是一个空string \n \n \n 对于bytes，默认是一个空的bytes \n \n \n 对于bools，默认是false \n \n \n 对于数值类型，默认是0 \n \n \n 对于枚举，默认是第一个定义的枚举值，必须为0; \n \n \n 对于消息类型（message），域没有被设置，确切的消息是根据语言确定的 \n \n \n 对于可重复域的默认值是空（通常情况下是对应语言中空列表） \n \n \n \n 对于标量消息域，一旦消息被解析，就无法判断域释放被设置为默认值（例如，例如boolean值是否被设置为false）还是根本没有被设置。在定义消息类型时非常注意。例如，比如不应该定义boolean的默认值false作为任何行为的触发方式。也应该注意如果一个标量消息域被设置为标志位，这个值不应该被序列化传输 \n 枚举 \n 当需要定义一个消息类型的时候，可能想为一个字段指定某“预定义值序列”中的一个值。例如，假设要为每一个SearchRequest消息添加一个 corpus字段，而corpus的值可能是UNIVERSAL，WEB，IMAGES，LOCAL，NEWS，PRODUCTS或VIDEO中的一个。 其实可以很容易地实现这一点：通过向消息定义中添加一个枚举（enum）并且为每个可能的值定义一个常量就可以了。 \n 在下面的例子中，在消息格式中添加了一个叫做Corpus的枚举类型——它含有所有可能的值 ——以及一个类型为Corpus的字段： \n message   SearchRequest   { \n\t string  query  =   1 ; \n\t int32  page_number  =   2 ; \n\t int32  result_per_page  =   3 ; \n\t enum   Corpus   { \n\t\tUNIVERSAL  =   0 ; \n\t\tWEB  =   1 ; \n\t\tIMAGES  =   2 ; \n\t\tLOCAL  =   3 ; \n\t\tNEWS  =   4 ; \n\t\tPRODUCTS  =   5 ; \n\t\tVIDEO  =   6 ; \n      } \n      Corpus  corpus  =   4 ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 如你所见，Corpus枚举的第一个常量映射为0：每个枚举类型必须将其第一个类型映射为0，这是因为： \n \n \n 必须有有一个0值，我们可以用这个0值作为默认值。 \n \n \n 这个零值必须为第一个元素，为了兼容proto2语义，枚举类的第一个值总是默认值。 \n \n \n 可以通过将不同的枚举常量指定位相同的值。如果这样做需要将allow_alias设定位true，否则编译器会在别名的地方产生一个错误信息。 \n \n \n 枚举常量必须在32位整型值的范围内。因为enum值是使用可变编码方式的，对负数不够高效，因此不推荐在enum中使用负数。如上例所示，可以在 一个消息定义的内部或外部定义枚举——这些枚举可以在.proto文件中的任何消息定义里重用。当然也可以在一个消息中声明一个枚举类型，而在另一个不同 的消息中使用它——采用MessageType.EnumType的语法格式 \n \n \n 当对一个使用了枚举的.proto文件运行protocol buffer编译器的时候，生成的代码中将有一个对应的enum（对Java或C++来说），或者一个特殊的EnumDescriptor类（对 Python来说），它被用来在运行时生成的类中创建一系列的整型值符号常量（symbolic constants）。 \n \n \n 在反序列化的过程中，无法识别的枚举值会被保存在消息中，虽然这种表示方式需要依据所使用语言而定。在那些支持开放枚举类型超出指定范围之外的语言中（例如C++和Go），为识别的值会被表示成所支持的整型。在使用封闭枚举类型的语言中（Java），使用枚举中的一个类型来表示未识别的值，并且可以使用所支持整型来访问。在其他情况下，如果解析的消息被序列号，未识别的值将保持原样。 \n 使用其他消息类型 \n 可以将其他消息类型用作字段类型。例如，假设在每一个SearchResponse消息中包含Result消息，此时可以在相同的.proto文件中定义一个Result消息类型，然后在SearchResponse消息中指定一个Result类型的字段，如： \n 嵌套类型 \n 可以在其他消息类型中定义、使用消息类型，在下面的例子中，Result消息就定义在SearchResponse消息内，如： \n 如果想在它的父消息类型的外部重用这个消息类型，你需要以Parent.Type的形式使用它，如： \n message   SomeOtherMessage   { \n\t SearchResponse . Result  result  =   1 ; \n } \n \n 1 2 3 #  更新一个消息类型 \n 如果一个已有的消息格式已无法满足新的需求——如，要在消息中添加一个额外的字段——但是同时旧版本写的代码仍然可用。不用担心！更新消息而不破坏已有代码是非常简单的。在更新时只要记住以下的规则即可。 \n \n 不要更改任何已有的字段的数值标识 \n 如果增加新的字段，使用旧格式的字段仍然可以被新产生的代码所解析。应该记住这些元素的默认值这样新代码就可以以适当的方式和旧代码产生的数据交互。相似的，通过新代码产生的消息也可以被旧代码解析：只不过新的字段会被忽视掉。注意，未被识别的字段会在反序列化的过程中丢弃掉，所以如果消息再被传递给新的代码，新的字段依然是不可用的（这和proto2中的行为是不同的，在proto2中未定义的域依然会随着消息被序列化） \n 非required的字段可以移除——只要它们的标识号在新的消息类型中不再使用（更好的做法可能是重命名那个字段，例如在字段前添加“OBSOLETE_”前缀，那样的话，使用的.proto文件的用户将来就不会无意中重新使用了那些不该使用的标识号）。 \n int32, uint32, int64, uint64,和bool是全部兼容的，这意味着可以将这些类型中的一个转换为另外一个，而不会破坏向前、 向后的兼容性。如果解析出来的数字与对应的类型不相符，那么结果就像在C++中对它进行了强制类型转换一样（例如，如果把一个64位数字当作int32来 读取，那么它就会被截断为32位的数字）。 \n sint32和sint64是互相兼容的，但是它们与其他整数类型不兼容。 \n string和bytes是兼容的——只要bytes是有效的UTF-8编码。 \n 嵌套消息与bytes是兼容的——只要bytes包含该消息的一个编码过的版本。 \n fixed32与sfixed32是兼容的，fixed64与sfixed64是兼容的。 \n 枚举类型与int32，uint32，int64和uint64相兼容（注意如果值不相兼容则会被截断），然而在客户端反序列化之后他们可能会有不同的处理方式，例如，未识别的proto3枚举类型会被保留在消息中，但是他的表示方式会依照语言而定。int类型的字段总会保留他们的 \n Oneof \n 如果消息中有很多可选字段， 并且同时至多一个字段会被设置， 可以加强这个行为，使用oneof特性节省内存. \n Oneof字段就像可选字段， 除了它们会共享内存， 至多一个字段会被设置。 设置其中一个字段会清除其它字段。 你可以使用 case() 或者 WhichOneof()  方法检查哪个oneof字段被设置. \n 使用Oneof \n 为了在.proto定义Oneof字段， 需要在名字前面加上oneof关键字, 比如下面例子的test_oneof: \n message SampleMessage {\n\toneof test_oneof {\n\t\tstring name = 4;\n\t\tSubMessage sub_message = 9;\n\t}\n}\n \n 1 2 3 4 5 6 然后可以增加oneof字段到 oneof 定义中. 可以增加任意类型的字段, 但是不能使用repeated 关键字. \n 在产生的代码中, oneof字段拥有同样的 getters 和setters， 就像正常的可选字段一样. 也有一个特殊的方法来检查到底那个字段被设置. \n Oneof 特性 \n \n 设置oneof会自动清楚其它oneof字段的值. 所以设置多次后，只有最后一次设置的字段有值. \n \n SampleMessage  message ; \nmessage . set_name ( "name" ) ; \n CHECK ( message . has_name ( ) ) ; \nmessage . mutable_sub_message ( ) ;    // Will clear name field.``CHECK(!message.has_name());` \n \n 1 2 3 4 \n 如果解析器遇到同一个oneof中有多个成员，只有最会一个会被解析成消息。 \n oneof不支持 repeated . \n 反射API对oneof 字段有效. \n 向后兼容性问题 \n 当增加或者删除oneof字段时一定要小心. 如果检查oneof的值返回 None/NOT_SET , 它意味着oneof字段没有被赋值或者在一个不同的版本中赋值了。 不会知道是哪种情况，因为没有办法判断如果未识别的字段是一个oneof字段。 \n Tage 重用问题： \n \n 将字段移入或移除oneof：在消息被序列号或者解析后，你也许会失去一些信息（有些字段也许会被清除） \n 删除一个字段或者加入一个字段：在消息被序列号或者解析后，这也许会清除你现在设置的oneof字段 \n 分离或者融合oneof：行为与移动常规字段相似。 \n Map（映射） \n 如果希望创建一个关联映射，protocol buffer提供了一种快捷的语法： \n map<key_type, value_type> map_field = N;\n \n 1 其中 key_type 可以是任意Integer或者string类型（所以，除了floating和bytes的任意标量类型都是可以的） value_type 可以是任意类型。 \n 例如，如果希望创建一个project的映射，每个 Projecct 使用一个string作为key，你可以像下面这样定义： \n map<string, Project> projects = 3;\n \n 1 \n Map的字段可以是repeated。 \n 序列化后的顺序和map迭代器的顺序是不确定的，所以你不要期望以固定顺序处理Map \n 当为.proto文件产生生成文本格式的时候，map会按照key 的顺序排序，数值化的key会按照数值排序。 \n 从序列化中解析或者融合时，如果有重复的key则后一个key不会被使用，当从文本格式中解析map时，如果存在重复的key。 \n \n 生成map的API现在对于所有proto3支持的语言都可用了，你可以从 API指南 找到更多信息。 \n 向后兼容性问题 \n map语法序列化后等同于如下内容，因此即使是不支持map语法的protocol buffer实现也是可以处理你的数据的： \n message   MapFieldEntry   { \n   key_type  key  =   1 ; \n   value_type  value  =   2 ; \n } \n\n repeated   MapFieldEntry  map_field  =  N ; \n \n 1 2 3 4 5 6 #  包 \n 当然可以为.proto文件新增一个可选的package声明符，用来防止不同的消息类型有命名冲突。如： \n packag foo . bar ; \n message   Open   {   . . .   } \n \n 1 2 在其他的消息格式定义中可以使用包名+消息名的方式来定义域的类型，如： \n message   Foo   { \n\t . . . \n\t required   foo . bar . Open  open  =   1 ; \n\t . . . } \n \n 1 2 3 4 包的声明符会根据使用语言的不同影响生成的代码。 \n 包及名称的解析 \n Protocol buffer语言中类型名称的解析与C++是一致的：首先从最内部开始查找，依次向外进行，每个包会被看作是其父类包的内部类。当然对于 （ foo.bar.Baz ）这样以“.”分隔的意味着是从最外围开始的。 \n ProtocolBuffer编译器会解析.proto文件中定义的所有类型名。 对于不同语言的代码生成器会知道如何来指向每个具体的类型，即使它们使用了不同的规则。 \n 定义服务(Service) \n 如果想要将消息类型用在RPC(远程方法调用)系统中，可以在.proto文件中定义一个RPC服务接口，protocol buffer编译器将会根据所选择的不同语言生成服务接口代码及存根。如，想要定义一个RPC服务并具有一个方法，该方法能够接收 SearchRequest并返回一个SearchResponse，此时可以在.proto文件中进行如下定义： \n service   SearchService   { \n\t rpc   Search   ( SearchRequest )   returns   ( SearchResponse ) ; \n } \n \n 1 2 3 最直观的使用protocol buffer的RPC系统是 gRPC 一个由谷歌开发的语言和平台中的开源的PRC系统，gRPC在使用protocl buffer时非常有效，如果使用特殊的protocol buffer插件可以直接为您从.proto文件中产生相关的RPC代码。 \n 如果你不想使用gRPC，也可以使用protocol buffer用于自己的RPC实现，你可以从 proto2语言指南中找到更多信息 \n 还有一些第三方开发的PRC实现使用Protocol Buffer。参考 第三方插件wiki 查看这些实现的列表。 \n JSON 映射 \n Proto3 支持JSON的编码规范，使他更容易在不同系统之间共享数据，在下表中逐个描述类型。 \n 如果JSON编码的数据丢失或者其本身就是 null ，这个数据会在解析成protocol buffer的时候被表示成默认值。如果一个字段在protocol buffer中表示为默认值，体会在转化成JSON的时候编码的时候忽略掉以节省空间。具体实现可以提供在JSON编码中可选的默认值。 \n \n \n \n proto3 \n JSON \n JSON示例 \n 注意 \n \n \n \n \n message \n object \n {“fBar”: v, “g”: null, …} \n 产生JSON对象，消息字段名可以被映射成lowerCamelCase形式，并且成为JSON对象键，null被接受并成为对应字段的默认值 \n \n \n enum \n string \n “FOO_BAR” \n 枚举值的名字在proto文件中被指定 \n \n \n map \n object \n {“k”: v, …} \n 所有的键都被转换成string \n \n \n repeated V \n array \n [v, …] \n null被视为空列表 \n \n \n bool \n true, false \n true, false \n \n \n \n string \n string \n “Hello World!” \n \n \n \n bytes \n base64 string \n “YWJjMTIzIT8kKiYoKSctPUB+” \n \n \n \n int32, fixed32, uint32 \n number \n 1, -10, 0 \n JSON值会是一个十进制数，数值型或者string类型都会接受 \n \n \n int64, fixed64, uint64 \n string \n “1”, “-10” \n JSON值会是一个十进制数，数值型或者string类型都会接受 \n \n \n float, double \n number \n 1.1, -10.0, 0, “NaN”, “Infinity” \n JSON值会是一个数字或者一个指定的字符串如”NaN”,”infinity”或者”-Infinity”，数值型或者字符串都是可接受的，指数符号也可以接受 \n \n \n Any \n object \n {“@type”: “url”, “f”: v, … } \n 如果一个Any保留一个特上述的JSON映射，则它会转换成一个如下形式： {"@type": xxx, "value": yyy} 否则，该值会被转换成一个JSON对象， @type 字段会被插入所指定的确定的值 \n \n \n Timestamp \n string \n “1972-01-01T10:00:20.021Z” \n 使用RFC 339，其中生成的输出将始终是Z-归一化啊的，并且使用0，3，6或者9位小数 \n \n \n Duration \n string \n “1.000340012s”, “1s” \n 生成的输出总是0，3，6或者9位小数，具体依赖于所需要的精度，接受所有可以转换为纳秒级的精度 \n \n \n Struct \n object \n { … } \n 任意的JSON对象，见struct.proto \n \n \n Wrapper types \n various types \n 2, “2”, “foo”, true, “true”, null, 0, … \n 包装器在JSON中的表示方式类似于基本类型，但是允许nulll，并且在转换的过程中保留null \n \n \n FieldMask \n string \n “f.fooBar,h” \n 见fieldmask.proto \n \n \n ListValue \n array \n [foo, bar, …] \n \n \n \n Value \n value \n \n 任意JSON值 \n \n \n NullValue \n null \n \n JSON null \n 选项 \n 在定义.proto文件时能够标注一系列的options。Options并不改变整个文件声明的含义，但却能够影响特定环境下处理方式。完整的可用选项可以在google/protobuf/descriptor.proto找到。 \n 一些选项是文件级别的，意味着它可以作用于最外范围，不包含在任何消息内部、enum或服务定义中。一些选项是消息级别的，意味着它可以用在消息定义的内部。当然有些选项可以作用在域、enum类型、enum值、服务类型及服务方法中。到目前为止，并没有一种有效的选项能作用于所有的类型。 \n 如下就是一些常用的选择： \n 1、 java_package  (文件选项) :这个选项表明生成java类所在的包。如果在.proto文件中没有明确的声明java_package，就采用默认的包名。当然了，默认方式产生的 java包名并不是最好的方式，按照应用名称倒序方式进行排序的。如果不需要产生java代码，则该选项将不起任何作用。如： \n option java_package  =   "com.example.foo" ; \n \n 1 2、 java_outer_classname  (文件选项): 该选项表明想要生成Java类的名称。如果在.proto文件中没有明确的java_outer_classname定义，生成的class名称将会根据.proto文件的名称采用驼峰式的命名方式进行生成。如（foo_bar.proto生成的java类名为FooBar.java）,如果不生成java代码，则该选项不起任何作用。如： \n option  java_outer_classname  =   "Ponycopter" ; \n \n 1 3、 optimize_for (文件选项): 可以被设置为 SPEED, CODE_SIZE,或者LITE_RUNTIME。这些值将通过如下的方式影响C++及java代码的生成： \n \n SPEED (default) : protocol buffer编译器将通过在消息类型上执行序列化、语法分析及其他通用的操作。这种代码是最优的。 \n CODE_SIZE : protocol buffer编译器将会产生最少量的类，通过共享或基于反射的代码来实现序列化、语法分析及各种其它操作。采用该方式产生的代码将比SPEED要少得多， 但是操作要相对慢些。当然实现的类及其对外的API与SPEED模式都是一样的。这种方式经常用在一些包含大量的.proto文件而且并不盲目追求速度的 应用中。 \n LITE_RUNTIME : protocol buffer编译器依赖于运行时核心类库来生成代码（即采用libprotobuf-lite 替代libprotobuf）。这种核心类库由于忽略了一 些描述符及反射，要比全类库小得多。这种模式经常在移动手机平台应用多一些。编译器采用该模式产生的方法实现与SPEED模式不相上下，产生的类通过实现 MessageLite接口，但它仅仅是Messager接口的一个子集。 \n \n option  optimize_for  =  CODE_SIZE ; \n \n 1 \n deprecated (字段选项):如果设置为 true 则表示该字段已经被废弃，并且不应该在新的代码中使用。在大多数语言中没有实际的意义。在java中，这回变成 @Deprecated 注释，在未来，其他语言的代码生成器也许会在字标识符中产生废弃注释，废弃注释会在编译器尝试使用该字段时发出警告。如果字段没有被使用你也不希望有新用户使用它，尝试使用保留语句替换字段声明。 \n \n int32 old_field = 6 [deprecated=true];\n \n 1 #  自定义选项 \n ProtocolBuffers允许自定义并使用选项。该功能应该属于一个高级特性，对于大部分人是用不到的。如果你的确希望创建自己的选项，请参看  Proto2 Language Guide 。注意创建自定义选项使用了拓展，拓展只在proto3中可用。 \n'},{title:"ElasticSearch",frontmatter:{title:"ElasticSearch",date:"2019-10-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["存储引擎"],tags:["搜索","倒排索引"]},regularPath:"/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/ElasticSearch.html",relativePath:"存储引擎/ElasticSearch.md",key:"v-007b24d3",path:"/2019/10/08/elasticsearch/",headers:[{level:2,title:"前言",slug:"前言"},{level:3,title:"引入搜索领域相关知识",slug:"引入搜索领域相关知识"},{level:2,title:"ES介绍",slug:"es介绍"},{level:2,title:"ES特点",slug:"es特点"},{level:2,title:"入门案例",slug:"入门案例"},{level:3,title:"全文检索构建和查询步骤",slug:"全文检索构建和查询步骤"},{level:2,title:"ES分布式架构的角色划分",slug:"es分布式架构的角色划分"},{level:2,title:"ES原理",slug:"es原理"},{level:3,title:"写入原理",slug:"写入原理"},{level:3,title:"检索原理!image-20210621212250395",slug:"检索原理"},{level:2,title:"Elasticsearch准实时索引实现",slug:"elasticsearch准实时索引实现"},{level:2,title:"ES架构图(分布式搜索)",slug:"es架构图-分布式搜索"},{level:2,title:"ES安装",slug:"es安装"},{level:2,title:"9.ES的RestFulAPI介绍",slug:"_9-es的restfulapi介绍"},{level:3,title:"",slug:""}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:' 前言 \n 引入搜索领域相关知识 \n \n \n 技术栈 \n \n elasticSearch : 主要是用于 做全文检索  功能: 数据的存储 和 数据的查询 --- 重要的 \n logstash: 主要是用于进行 数据的传递采集工作 : 将数据从一个地方，搬运到另一个地方去-- 了解 \n Kibana : 主要是用于 图标报表展示 以及 数据探索  --- 了解，使用较少 \n Beats : 主要是用于进行 数据的写入工作  -- 了解 \n 大数据分析过程：数据采集-数据存储-数据ETL(清洗)-数据分析-数据展示 \n \n \n \n \n 场景： \n \n 百度搜索 \n 谷歌搜索 \n 360搜索 \n UC搜索 \n JD搜索-淘宝商品搜索 \n OA查找关键人 \n \n \n \n 大数据搜索 \n \n 搜索：任意场景下，寻找你想要的信息或数据\n \n 需要键入一些感兴趣的关键字查询出信息 \n \n \n 传统的搜索\n \n 关系型数据库的弊端 \n ES介绍 \n \n \n ES提供分布式搜索和查询的引擎 \n \n \n \n \n \n ES发展经历阶段 \n \n Lucene(单机版本的Jar包搜索服务)---------Solr(3.0以后就具备分布式功能)---------ES(分布式搜索和查询) \n ES是使用Java开发的 \n ES特点 \n 1.分布式的实时文件存储，每个字段都被索引并可被搜索 \n 2.分布式的实时分析搜索引擎 \n 3.可以扩展到上百台服务器，处理PB级结构化或非结构化数据 \n \n \n ES那些公司在使用：\n \n \n \n \n \n \n \n Spark比Hadoop块100倍\n \n Spark拿的最强项和Hadoop最弱项对比 \n Hadoop3.x是Spark的3倍 \n \n \n ES和Solr对比\n \n ES是使用 Json文件格式 同时 自己管理自己的分布式状态 的搜索工具 \n 后面学习的Zookeeper就是负责状态的管理，但是ES自己管理状态(集群状态) \n 入门案例 \n \n \n 什么是Lucene？ \n \n 单机搜索框架，提供搜索功能 \n \n \n \n 如何实现全文检索 \n \n 1-引入结构化数据和非结构化数据搜索\n \n 针对结构化数据，使用MySQL的技术 \n 针对非结构化和半结构化，使用ES-Solr-lucecne完成 \n \n \n 2-全文检索，构建倒排索引\n \n \n \n \n \n \n \n \n \n \n \n 如何执行搜索\n \n \n \n \n \n \n \n 引入美文案例 \n \n \n 实现搜索的步骤 \n \n \n 1-分词 \n \n \n 如何实现分词？ \n \n \n 我爱北京天安门 \n \n \n 精确模式：我 爱 北京 天安门 \n \n \n 详细模式：我 爱 我爱 北京 我爱北京 天安门 北京天安门 \n \n \n IK分词-----分词的方式 \n \n \n \n \n \n \n package   cn . itcast . lucene ; \n\n import   org . wltea . analyzer . core . IKSegmenter ; \n import   org . wltea . analyzer . core . Lexeme ; \n\n import   java . io . IOException ; \n import   java . io . StringReader ; \n\n /**\n * IK分词器在是一款 基于词典和规则 的中文分词器。\n * IK提供两种分词模式：智能模式和细粒度模式（智能：对应es的IK插件的ik_smart，细粒度：对应es的IK插件的ik_max_word）。\n * 先看两种分词模式的demo和效果\n * 词典：黑马程序员，默认的情况不可能组织在一起，在词典中增加-黑马程序员的词条\n * 停用词：去掉语气词，停顿词，自己指定词\n */ \n public   class   IKSegmenterTest   { \n     static   String  text  =   "我是中国人，我爱黑马程序员" ; \n\n     public   static   void   main ( String [ ]  args )   throws   IOException   { \n         IKSegmenter  segmenter  =   new   IKSegmenter ( new   StringReader ( text ) ,   false ) ; \n         Lexeme  next ; \n         System . out . print ( "非智能分词结果：" ) ; \n         while   ( ( next  =  segmenter . next ( ) )   !=   null )   { \n             System . out . print ( next . getLexemeText ( )   +   " " ) ; \n         } \n         System . out . println ( ) ; \n         System . out . println ( "-------------------分割线------------------------------" ) ; \n         IKSegmenter  smartSegmenter  =   new   IKSegmenter ( new   StringReader ( text ) ,   true ) ; \n         System . out . print ( "智能分词结果：" ) ; \n         while   ( ( next  =  smartSegmenter . next ( ) )   !=   null )   { \n             System . out . print ( next . getLexemeText ( )   +   " " ) ; \n             //我 中国人 我 爱 黑马 程序员  \n         } \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \n \n 2-根据分词的结果查询目标文章 \n \n \n 需求：客户输入一个关键词，需要将对应的相关的文章返回到客户部分 \n \n \n 步骤： \n \n 1-首先需要构建索引\n \n 1-首先选择合适的分词工具，这里选择是Standard分词和IK分词 \n 2-构建文档索引 \n \n \n 2-基于索引查询关联文章\n \n 相似度匹配 \n \n \n \n \n \n \n \n \n 如果需要检索，必然需要分词 \n \n \n \n \n \n 利用标准分词查询指定词语，比如“心” \n \n \n \n \n \n 引入IK分词就是解决中文的分词的问题 \n \n 除了IK分词还有Python的Jieba分词(中文中最火的Python分词器) \n Java的IK分词器 \n \n \n \n \n \n \n 3-返回客户 \n \n \n 倒排索引 \n \n \n Lucene不怎么使用： \n \n 原因：支持单机搜索 \n \n \n \n 注意：数据采集，数据存储，数据分析(查询搜索计算)，数据展示 \n \n \n 总结： \n \n 大数据分布式搜索，lucene原理类似 \n 在分词的情况下，对于给定的搜索词给出合适的匹配的文章等信息 \n 全文检索构建和查询步骤 \n \n 1-首先需要构建索引\n \n 1-首先选择合适的分词工具，这里选择是Standard分词和IK分词（除了java IK分词还有Python的Jieba分词(中文中最火的Python分词器)） \n 2-构建文档索引 \n \n \n 2-基于索引查询关联文章\n \n 相似度算法得分排序 \n 返回topn结果 \n ES分布式架构的角色划分 \n \n \n 补充JPS： \n \n jps是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。 \n 搭建起来hadoop \n \n \n \n \n 目标：在分词的情况下，对于给定的搜索词给出合适的匹配的文章等信息 \n \n \n 首先观察WebUI端口：启动ES \n \n 简单丑陋的Web界面--无法忍受 \n \n \n \n \n \n 简单美化版本的Web界面--可以接受，基于nodejs开发的插件 \n ES-Head插件 \n \n \n \n \n \n \n \n \n \n \n 分析角色 \n \n Master主节点 \n Worker从节点 \n Primary Shard----分布式读写，并发读写 \n Repication Shard \n index索引 \n type类型 \n id唯一标识 \n document：一行数据就是一个文档 \n \n field字段 \n ES原理 \n  写入原理 \n \n \n \n 选择任意一个DataNode发送请求，例如：node2.itcast.cn。此时，node2.itcast.cn就成为一个   coordinating node（协调节点） \n \n \n 计算得到文档要写入的分片 \n \n \n l 根据文档ID确定要将数据写入到那个分片中 \n l  shard = hash(routing) % number_of_primary_shards \n l routing 是一个可变值， 默认是文档的 _id \n \n \n coordinating node会进行路由，将请求转发给对应的primary shard所在的DataNode（假设primary shard在node1.itcast.cn、replica shard在node2.itcast.cn） \n \n \n node1.itcast.cn节点上的Primary Shard处理请求，写入数据到索引库中，并将数据同步到         Replica shard \n \n \n Primary Shard和Replica Shard都保存好了文档，返回client \n  检索原理 \n \n \n client发起查询请求，某个DataNode接收到请求，该DataNode就会成为协调节点（Coordinating Node） \n 2.协调节点（Coordinating Node）将查询请求广播到 每一个数据节点（查询是基于所有节点查询得到结果汇总） ， 这些数据节点的分片会处理该查询请求 。协调节点会 轮询 所有的分片来自动进行负载均衡 \n \n \n 每个分片进行数据查询，将符合条件的数据放在一个 优先队列 中，并将这些数据的文档ID、节点信息、 分片信息返回给协调节点 \n \n \n 协调节点将所有的结果进行汇总，并进行全局排序 \n \n \n 协调节点向包含 这些文档ID的分片发送get请求，对应的分片将文档数据返回给 协调节点， 最后协调节点将数据返回给客户端 \n  Elasticsearch准实时索引实现 \n 溢写到文件系统缓存 \n 当数据写入到ES分片时，会首先写入到内存中，然后通过内存的buffer生成一个segment，并刷到 文件系统缓存 中，数据可以被检索（注意不是直接刷到磁盘） \n ES中默认1秒，refresh一次 \n 写translog保障容错 \n 在写入到内存中的同时，也会记录translog日志，在refresh期间出现异常，会根据translog来进行数据恢复 \n 等到文件系统缓存中的segment数据都刷到磁盘中，清空translog文件 \n flush到磁盘 \n ES默认 每隔30分钟 会将文件系统缓存的数据刷入到磁盘 \n segment 合并 \n Segment太多时，ES定期会将多个segment合并成为大的segment，减少索引查询时IO开销，此阶段ES会真正的物理删除（之前执行过的delete的数据,Hbase也是这样的） \n ES架构图(分布式搜索) \n \n \n 和关系型数据库对比 \n \n \n mapping 的信息 \n \n 理解primary shard和replica \n \n \n 注意：\n \n Double等类型 \n 作为全文搜索引擎，ES将String域的字段可以分为准确数据类型，和全文文本类型。\n \n String \n 我爱你--keyword \n 我爱你一生一世-----text \n ES安装 \n \n \n 大数据组件安装步骤 \n \n ES的安装需要使用普通用户，不能使用Root用户 \n \n \n 1-上传压缩包----如果没有编译需要重新编译 \n 2-进入Config或Conf目录下更改配置文件 \n 3-ES架构应该如何配置？ \n \n \n \n 4-进入bin目录下启动 \n 5-查看WebUi \n \n \n \n 安装过程 \n \n \n 安装脚本：\n\n1-创建用户\nuseradd itcast \npasswd itcast\n2-加sudo权限\nvisudo\n第100行\nitcast      ALL=(ALL)       ALL\n3-上传压缩包\nmkdir -p /export/server/es\nchown -R itcast:itcast /export/server/es\n4-使用root用户执行\n在node1.itcast.cn、node2.itcast.cn、node3.itcast.cn创建es文件夹，并修改owner为itcast用户\nmkdir -p /export/server/es\nchown -R itcast:itcast /export/server/es\n5-使用itcast用户来执行以下操作\n解压Elasticsearch\ncd /export/software/ \ntar -zvxf elasticsearch-7.6.1-linux-x86_64.tar.gz -C /export/server/es/\n6-node1上修改\ncd /export/server/es/elasticsearch-7.6.1/config\nmkdir -p /export/server/es/elasticsearch-7.6.1/log\nmkdir -p /export/server/es/elasticsearch-7.6.1/data\nrm -rf elasticsearch.yml\n\nvim elasticsearch.yml\ncluster.name: itcast-es\nnode.name: node1.itcast.cn\npath.data: /export/server/es/elasticsearch-7.6.1/data\npath.logs: /export/server/es/elasticsearch-7.6.1/log\nnetwork.host: node1.itcast.cn\nhttp.port: 9200\ndiscovery.seed_hosts: ["node1.itcast.cn", "node2.itcast.cn", "node3.itcast.cn"]\ncluster.initial_master_nodes: ["node1.itcast.cn", "node2.itcast.cn"]\nbootstrap.system_call_filter: false\nbootstrap.memory_lock: false\nhttp.cors.enabled: true\nhttp.cors.allow-origin: "*"\n\ncd /export/server/es/elasticsearch-7.6.1/config\nvim jvm.options\n-Xms2g\n-Xmx2g\n\n7-上传到其他节点上\ncd /export/server/es/\nscp -r elasticsearch-7.6.1/ node2.itcast.cn:$PWD\nscp -r elasticsearch-7.6.1/ node3.itcast.cn:$PWD\n\n8-node2和node3更改\nnode.name: node2.itcast.cn\nnetwork.host: node2.itcast.cn\n\nnode.name: node3.itcast.cn\nnetwork.host: node3.itcast.cn\n8-更改文件数的限制\nsudo vi /etc/security/limits.conf \n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 2048\n* hard nproc 4096\n9-普通用户启动线程数限制\nCentos7\nsudo vi /etc/security/limits.d/20-nproc.conf\n* soft nproc 4096\n\n10-原因：最大虚拟内存太小\n每次启动机器都手动执行下。\n三台机器执行以下命令 \n第一种调整: 临时调整, 退出会话 重新登录 就会失效的 (测试环境下配置)，建议使用第一种因为测试情况下只有es占用大内存\nsudo  sysctl -w vm.max_map_count=262144    \n\n注意：如果下次退出登录，重启机器后需要重新执行上述调整虚拟内存最大值\n\n11-启动\n/export/server/es/elasticsearch-7.6.1/bin/elasticsearch 前台启动---为什么，通过前台启动如果没有报错，说明没问题\nnohup /export/server/es/elasticsearch-7.6.1/bin/elasticsearch  & 后台启动\nnohup /export/server/es/elasticsearch-7.6.1/bin/elasticsearch 2>&1 &\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 \n \n 界面 \n \n \n \n \n \n ES的插件的安装--ES-Head \n \n 安装 \n \n \n \n ES安装IK分词 \n 9.ES的RestFulAPI介绍 \n \n \n Rest:用URI表示资源，用HTTP方法(GET, POST, PUT, DELETE) \n \n \n RESTful API 要求前端以一种预定义的语法格式发送请求，那么服务端就只需要定义一个统一的响应接口，不必像之前那样解析各色各式的请求。 \n \n \n \n \n \n 下面的代码无需执行，只需看懂，如需要执行在后面执行\nPUT /my-index\n{\n    "mapping": {\n        "properties": {\n            "employee-id": {\n                "type": "keyword",\n                "index": false\n            }\n        }\n    }\n}\n\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n \n GET   / _sql ? format = json\n { \n     "query" :   "SELECT * FROM tbl_waybill limit 1" \n } \n\n \n 1 2 3 4 5 \n \n \n \n \n \n \n \n \n \n \n \n 完毕 \n \n \n VSCode结合ES编程 \n \n \n \n \n \n \n \n \n \n \n \n 总结 \n \n 引入搜索领域相关知识\n \n 什么是搜索？根据输入的关键词得到想要的信息 \n 搜索基础：\n \n 传统搜索：MySQL \n Lucene：单机版本的引擎 \n Solr：3.0以后出现分布式，格式多，但是Solr分布式读写请求效果不好 \n ES：分布式搜索 \n \n \n \n \n ES的引入\n \n 百度 \n 京东 \n 天猫 \n \n \n Lucene的引入及案例\n \n IK分词 \n 构建lucene的索引 \n 通过索引查询想要得到的结果 \n \n \n ES分布式架构的角色划分 \n \n Master \n Worker(DataNode) \n PrimaryShard \n Replica Shard \n Index \n Type \n documnet \n filed \n mapping \n \n \n ES架构图(分布式搜索)\n \n 两套主从：\n \n master和worker主从 \n primary shard和replica shard \n \n \n \n \n ES安装----因为有架构才更改配置文件\n \n elasticserach.yml文件 \n 增加集群名字，节点名字，节点端口号等 \n \n \n ES的插件的安装--ES-Head\n \n 本质上就是web套件 \n \n \n ES的RestFulAPI介绍\n \n 类似于Json格式的数据 \n \n \n VSCode结合ES编程\n \n 安装ElasticSearch-Vscode插件 \n 执行查询结果 \n  \n'},{frontmatter:{},regularPath:"/%E5%85%B6%E4%BB%96/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95.html",relativePath:"其他/常见的限流算法.md",key:"v-305ac8e0",path:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/",lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:""},{title:"redis操作命令锦集",frontmatter:{},regularPath:"/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%84%9A%E6%9C%AC/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86.html",relativePath:"常用命令脚本/redis操作命令锦集.md",key:"v-4fd15c58",path:"/1970/01/01/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86/",headers:[{level:2,title:"redis中五种数据类型",slug:"redis中五种数据类型"},{level:2,title:"redis中String类型相关命令",slug:"redis中string类型相关命令"},{level:2,title:"redis中hash类型的相关命令",slug:"redis中hash类型的相关命令"},{level:2,title:"redis中list集合类型的相关命令",slug:"redis中list集合类型的相关命令"},{level:2,title:"redis中的set集合的相关命令操作",slug:"redis中的set集合的相关命令操作"},{level:2,title:"redis中的sortedset（zset）集合的相关操作:",slug:"redis中的sortedset-zset-集合的相关操作"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:" redis操作命令锦集 \n 本章节给大家规整一下redis中常用的操作命令 \n redis中五种数据类型 \n \n \n \n 字符串 String    ---- 重点 \n \n \n 特点: 存储所有的字符和字符串 \n 应用场景:  做缓存使用 \n \n \n \n \n 哈希  hash \n \n \n 特点: 相当于java中hashMap集合 \n 应用场景: 可以存储javaBean对象, 此种使用场景不多,可被String替代 \n \n \n \n \n list集合 \n \n \n 特点: 相当于java中linkList, 是一个链表的结构 \n 应用场景: 做任务队列,\n \n 在java中客户端提供了线程安全获取集合数据的方式 \n \n \n \n \n \n \n set 集合 \n \n \n 特点: 唯一, 无序 \n 应用场景: 集合运算\n \n 例如去重复的操作 \n \n \n \n \n \n \n 有序set集合: sorted set \n \n \n 特点:唯一, 有序 \n 应用场景: 一般用来做排行榜 \n redis中String类型相关命令 \n \n 赋值:  set key value\n \n 设定key持有指定的字符串value，如果该key存在则进行覆盖操作。总是返回”OK” \n \n \n 取值: get key\n \n 获取key的value。如果与该key关联的value不是String类型，redis将返回错误信息，因为get命令只能用于获取String value；如果该key不存在，返回(nil) \n \n \n 删除: del key\n \n 删除指定的key \n \n \n 数值增减:\n \n 增减值: incr key\n \n 将指定的key的value原子性的递增1.如果该key不存在，其初始值为0，在incr之后其值为1。如果value的值不能转成整型，如hello，该操作将执行失败并返回相应的错误信息。 \n \n \n 减少值: decr key\n \n 将指定的key的value原子性的递减1.如果该key不存在，其初始值为0，在incr之后其值为-1。如果value的值不能转成整型，如hello，该操作将执行失败并返回相应的错误信息 \n \n \n 增加固定的值: incrby key increment\n \n 将指定的key的value原子性增加increment，如果该key不存在，器初始值为0，在incrby之后，该值为increment。如果该值不能转成整型，如hello则失败并返回错误信息 \n \n \n 减少固定的值: decrby key decrement\n \n 将指定的key的value原子性减少decrement，如果该key不存在，器初始值为0，在decrby之后，该值为decrement。如果该值不能转成整型，如hello则失败并返回错误信息 \n \n \n \n \n 拼接value值:  append key  value\n \n 拼凑字符串。如果该key存在，则在原有的value后追加该值；如果该key不存在，则重新创建一个key|value \n \n \n 为key中内容设置有效时长:\n \n 为新创建的key设置时长\n \n setex key seconds value \n \n \n 为已有的key设置有效时长\n \n expire key seconds \n \n \n \n \n 判断key是否存在: exists key\n \n 返回1 表示存在, 返回0 表示不存在 \n \n \n 获取key还剩余有效时长:  ttl  key\n \n 特殊: 返回-1 表示永久有效 返回-2 表示不存在 \n redis中hash类型的相关命令 \n \n 存值: hset key field value\n \n key为一个字符串, value类似于map,同样有一个field和value \n \n \n 取值:\n \n 获取指定key的field的值:  hget key field \n 获取指定key的多个field值: hmget key fields \n 获取指定key中的所有的field与value的值:  hgetall key \n 获取指定key中map的所有的field: hkeys key \n 获取指定key中map的所有的value: hvals key \n \n \n 删除:\n \n hdel key field [field … ] ：可以删除一个或多个字段，返回值是被删除的字段个数 \n del key ：删除整个内容 \n \n \n 增加数字:\n \n hincrby key field number：为某个key的某个属性增加值 \n \n \n 判断某个key中的filed是否存在: hexists key field\n \n 返回 0表示没有,  返回1 表示有 \n \n \n 获取key中所包含的field的数量: hlen key \n redis中list集合类型的相关命令 \n redis的中的list集合类似于java中的linkedlist集合,此集合也是队列的一种, 支持向两端操作 \n \n 添加:\n \n 从左侧添加: lpush key values[value1 value2…]\n \n 在指定的key所关联的list的头部插入所有的values，如果该key不存在，该命令在插入的之前创建一个与该key关联的空链表，之后再向该链表的头部插入数据。插入成功，返回元素的个数。 \n \n \n 从右侧添加: rpush key values[value1、value2…]\n \n 在该list的尾部添加元素 \n \n \n \n \n 查看列表 : lrange key start end\n \n 获取链表中从start到end的元素的值，start、end从0开始计数；end可为负数，若为-1则表示链表尾部的元素，-2则表示倒数第二个，依次类推… \n \n \n 删除(弹出):\n \n 从左侧弹出:lpop key\n \n 返回并弹出指定的key关联的链表中的第一个元素，即头部元素。如果该key不存在，返回nil；若key存在，则返回链表的头部元素 \n \n \n 从右侧弹出: rpop key\n \n 从尾部弹出元素 \n \n \n \n \n 获取列表中元素的个数: llen key\n \n 返回指定的key关联的链表中的元素的数量 \n \n \n 向指定的key插入数据, 仅在key存在时插入, 不存在不插入\n \n 从左侧:lpushx key value \n 从右侧: rpushx key value \n \n \n lrem key count value:\n \n 删除count个值为value的元素，如果count大于0，从头向尾遍历并删除count个值为value的元素，如果count小于0，则从尾向头遍历并删除。如果count等于0，则删除链表中所有等于value的元素 \n \n \n lset key index value:\n \n 设置链表中的index的脚标的元素值，0代表链表的头元素，-1代表链表的尾元素。操作链表的脚标不存在则抛异常。 \n \n \n linsert key before|after pivot value\n \n 在pivot元素前或者后插入value这个元素。 \n \n \n rpoplpush resource destination\n \n 将链表中的尾部元素弹出并添加到头部。[循环操作] \n rpoplpush  list1 list2 （把list1尾部的元素，加入到list2头部的元素里） \n \n \n 注意:我们index是从0开始， 最后一位的index是用-1表示。 \n redis中的set集合的相关命令操作 \n \n 添加: sadd key values[value1、value2…]\n \n 向set中添加数据，如果该key的值已有则不会重复添加 \n \n \n 删除: srem key members[member1、member2…]\n \n 删除set中指定的成员 \n \n \n 获取所有的元素: smembers key\n \n 获取set中所有的成员 \n \n \n 判断元素是否存在: sismember key member\n \n 判断参数中指定的成员是否在该set中，1表示存在，0表示不存在或者该key本身就不存在。（无论集合中有多少元素都可以极速的返回结果） \n \n \n 集合的差集运算: sdiff key1 key2…\n \n 返回key1与key2中相差的成员，而且与key的顺序有关。那个在前, 返回那个key对应的差集 \n \n \n 集合的交集运算:sinter key1 key2 key3…\n \n 返回交集, 两个key都有的 \n \n \n 集合的并集运算:sunion key1 key2 key3…\n \n 返回并集 \n \n \n 获取set中成员的数量:\n \n scard key \n \n \n 随机返回set中的一个成员:\n \n srandmember key \n \n \n 将key1、key2相差的成员存储在destination上:\n \n sdiffstore destination key1 key2… \n \n \n 将返回的交集存储在destination上:\n \n sinterstore destination key[key…] \n \n \n 将返回的并集存储在destination上:\n \n sunionstore destination key[key…] \n redis中的sortedset（zset）集合的相关操作: \n \n \n 添加数据: zadd key score member \n \n 将所有成员以及该成员的分数存放到sorted-set中。如果该元素已经存在则会用新的分数替换原有的分数。返回值是新加入到集合中的元素个数，不包含之前已经存在的元素 \n \n \n \n 获得元素: \n \n zscore key member: 返回指定元素的值 \n zcard key: 获取集合中的成员数量 \n \n \n \n 删除元素:zrem key member[member…] \n \n 移除集合中指定的成员，可以指定多个成员。 \n \n \n \n zrank key member: \n \n 返回成员在集合中的排名。（从小到大） \n \n \n \n zrevrank key member \n \n 返回成员在集合中的排名。（从大到小） \n \n \n \n zincrby key 增加分数 member: \n \n 设置指定成员的增加的分数。返回值是更改后的分数 ... \n \n \n \n 范围查询: \n \n zrange key start end [withscores]: 获取集合中脚标为start-end的成员，[withscores]参数表明返回的成员包含其分数 \n zrevrange key start stop [withscores]: 按照元素分数从大到小的顺序返回索引从start到stop之间的所有元素（包含两端的元素） \n \n \n \n zremrangebyrank key start stop: 按照排名范围删除元素 \n \n \n zremrangebyscore key min max: 按照分数范围删除元素 \n \n \n zrangebyscore key min max [withscores][limit offset count]: \n \n 返回分数在[min,max]的成员并按照分数从低到高排序。[withscores]：显示分数；[limit offset count]：offset，表明从脚标为offset的元素开始并返回count个成员a \n \n \n \n zcount key min max: \n \n 获取分数在[min,max]之间的成员 \n \n \n \n"},{title:"Apache beam",frontmatter:{title:"Apache beam",date:"2023-04-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["计算引擎"],tags:["流批一体编程框架","pipeline"]},regularPath:"/%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/Apache-beam.html",relativePath:"计算引擎/Apache-beam.md",key:"v-36184f82",path:"/2023/04/08/apache-beam/",headers:[{level:2,title:"背景",slug:"背景"},{level:2,title:"轻松入门",slug:"轻松入门"},{level:2,title:"特性",slug:"特性"},{level:3,title:"核心概念",slug:"核心概念"},{level:3,title:"编程深入",slug:"编程深入"}],lastUpdated:"2023-7-19 7:18:59 ├F10: PM┤",lastUpdatedTimestamp:1689765539e3,content:' 背景 \n 为什么选择apache beam? \n Apache Beam 是一种用于批处理和流式数据处理管道的开源统一编程模型. \n 业务场景繁多，数据处理不是单一的处理模式，批处理和流式处理在开发过程中同时都存在，为了统一一套开发框架，提升开发效率，Apache beam便成为了选择之一。 \n 官网：https://beam.apache.org \n 轻松入门 \n 1.创建apache-beam项目，引入依赖 \n < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n         < apache.beam.version > 2.47.0 </ apache.beam.version > \n         < commons.io.version > 2.8.0 </ commons.io.version > \n         < flink.version > 1.12.7 </ flink.version > \n         < spark.version > 3.1.2 </ spark.version > \n         < jackson.version > 2.14.1 </ jackson.version > \n     </ properties > \n     \x3c!--第一版本管理--\x3e \n     < dependencyManagement > \n         < dependencies > \n             < dependency > \n                 < groupId > commons-io </ groupId > \n                 < artifactId > commons-io </ artifactId > \n                 < version > ${commons.io.version} </ version > \n             </ dependency > \n             < dependency > \n                 < groupId > com.fasterxml.jackson.core </ groupId > \n                 < artifactId > jackson-databind </ artifactId > \n                 < version > ${jackson.version} </ version > \n             </ dependency > \n             < dependency > \n                 < groupId > com.fasterxml.jackson.core </ groupId > \n                 < artifactId > jackson-core </ artifactId > \n                 < version > ${jackson.version} </ version > \n             </ dependency > \n         </ dependencies > \n     </ dependencyManagement > \n\n     < dependencies > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-core </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-direct-java </ artifactId > \n             < version > ${apache.beam.version} </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-jdbc </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > mysql </ groupId > \n             < artifactId > mysql-connector-java </ artifactId > \n             < version > 5.1.48 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > com.mchange </ groupId > \n             < artifactId > c3p0 </ artifactId > \n             < version > 0.9.5.4 </ version > \n         </ dependency > \n\n         \x3c!-- https://mvnrepository.com/artifact/org.apache.beam/beam-sdks-java-io-hcatalog --\x3e \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-hcatalog </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n\n         \x3c!-- https://mvnrepository.com/artifact/org.apache.hive.hcatalog/hive-hcatalog-core --\x3e \n         < dependency > \n             < groupId > org.apache.hive.hcatalog </ groupId > \n             < artifactId > hive-hcatalog-core </ artifactId > \n             < version > 2.1.0 </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > org.apache.calcite </ groupId > \n                     < artifactId > calcite-avatica </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-hadoop-format </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-kafka </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.kafka </ groupId > \n             < artifactId > kafka-clients </ artifactId > \n             < version > 2.4.1 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-flink-1.12 </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n\n         \x3c!--<dependency>\n            <groupId>org.apache.beam</groupId>\n            <artifactId>beam-examples-java</artifactId>\n            <version>${apache.beam.version}</version>\n        </dependency>--\x3e \n         \x3c!-- https://mvnrepository.com/artifact/commons-cli/commons-cli --\x3e \n         < dependency > \n             < groupId > commons-cli </ groupId > \n             < artifactId > commons-cli </ artifactId > \n             < version > 1.4 </ version > \n         </ dependency > \n\n         \x3c!-- https://mvnrepository.com/artifact/commons-io/commons-io --\x3e \n         < dependency > \n             < groupId > commons-io </ groupId > \n             < artifactId > commons-io </ artifactId > \n         </ dependency > \n\n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-clients_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-runtime_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-java_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-spark-3 </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-core_2.12 </ artifactId > \n             < version > ${spark.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.module </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-streaming_2.12 </ artifactId > \n             < version > ${spark.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > com.fasterxml.jackson.core </ groupId > \n             < artifactId > jackson-core </ artifactId > \n         </ dependency > \n\n         < dependency > \n             < groupId > com.fasterxml.jackson.core </ groupId > \n             < artifactId > jackson-databind </ artifactId > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-extensions-sql </ artifactId > \n             < version > ${apache.beam.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > jackson-databind </ artifactId > \n                 </ exclusion > \n                 < exclusion > \n                     < artifactId > jackson-annotations </ artifactId > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n\n     </ dependencies > \n\n     < build > \n         < plugins > \n\n             \x3c!-- 编译插件 --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.5.1 </ version > \n                 < configuration > \n                     < source > 1.8 </ source > \n                     < target > 1.8 </ target > \n                     \x3c!--<encoding>${project.build.sourceEncoding}</encoding>--\x3e \n                 </ configuration > \n             </ plugin > \n\n             \x3c!-- 打包插件(会包含所有依赖) --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-shade-plugin </ artifactId > \n                 < version > 2.3 </ version > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < goals > \n                             < goal > shade </ goal > \n                         </ goals > \n                         < configuration > \n                             < filters > \n                                 < filter > \n                                     < artifact > *:* </ artifact > \n                                     < excludes > \n                                         \x3c!--\n                                        zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --\x3e \n                                         < exclude > META-INF/*.SF </ exclude > \n                                         < exclude > META-INF/*.DSA </ exclude > \n                                         < exclude > META-INF/*.RSA </ exclude > \n                                     </ excludes > \n                                 </ filter > \n                             </ filters > \n                             < transformers > \n                                 < transformer   implementation = " org.apache.maven.plugins.shade.resource.ManifestResourceTransformer " > \n                                     \x3c!-- 设置jar包的入口类(可选) 不指定类时默认该主入口--\x3e \n                                     < mainClass > com.gordon.quickstart.HelloWorld </ mainClass > \n                                 </ transformer > \n                             </ transformers > \n                         </ configuration > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n         </ plugins > \n     </ build > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 2.编写入门HelloWorld \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . options . * ; \n import   org . apache . beam . sdk . transforms . Create ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n\n import   java . util . Arrays ; \n\n /***\n * 1.创建一个Pipline\n * 2.初始化一个PCollection\n * 3。处理PCollection\n * 4.运行\n */ \n public   class   HelloWorld   { \n\n     public   static   void   main ( String [ ]  args )   { \n         //todo 1.创建一个Pipline \n         PipelineOptions  options  =   PipelineOptionsFactory . create ( ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n         //todo 2.初始化一个PCollection \n        pipeline\n                 . apply ( "Create elements" ,   Create . of ( Arrays . asList ( "Hello" ,   "World!" ) ) ) \n                 //todo 3.处理PCollection \n                 . apply ( "Print elements" , \n                         MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                             System . out . println ( x ) ; \n                             return  x ; \n                         } ) ) ; \n         //todo 4.运行 \n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 java   -cp  apache-beam-1.0-SNAPSHOT.jar com.gordon.quickstart.HelloWorld  --runner = DirectRunner\n \n 1 \n 自定义参数以及使用FlinkRunner \n import   org . apache . beam . runners . flink . FlinkPipelineOptions ; \n import   org . apache . beam . runners . spark . SparkPipelineOptions ; \n import   org . apache . beam . sdk . options . Default ; \n import   org . apache . beam . sdk . options . Description ; \n import   org . apache . beam . sdk . options . PipelineOptions ; \n\n public   interface   MyOptions   extends   SparkPipelineOptions , FlinkPipelineOptions , PipelineOptions   { \n     //You can also specify a description, which appears when a user passes as a command-line argument, and a default value.--help \n     //You set the description and default value using annotations \n     @Description ( "Input for the pipeline" ) \n     @Default.String ( "/" ) \n     String   getInput ( ) ; \n     void   setInput ( String  input ) ; \n\n     @Description ( "Output for the pipeline" ) \n     @Default.String ( "/test1" ) \n     String   getOutput ( ) ; \n     void   setOutput ( String  output ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 修改后的HelloWorld \n \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . Create ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . slf4j . Logger ; \n import   org . slf4j . LoggerFactory ; \n\n import   java . util . Arrays ; \n\n /***\n * 1.创建一个Pipline\n * 2.初始化一个PCollection\n * 3。处理PCollection\n * 4.运行\n */ \n public   class   HelloWorld   { \n\n     private   static   final   Logger   LOGGER   =   LoggerFactory . getLogger ( HelloWorld . class ) ; \n\n     public   static   void   main ( String [ ]  args )   { \n\n         //todo 1.创建一个Pipline \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         //PipelineOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().create(); \n         //commindline中设置 \n         //options.setRunner(FlinkRunner.class); \n         System . out . println ( "options.getRunner() = "   +  options . getRunner ( ) ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n         //todo 2.初始化一个PCollection \n        pipeline\n                 . apply ( "Create elements" ,   Create . of ( Arrays . asList ( "Hello" ,   "World!" ) ) ) \n                 //todo 3。处理PCollection \n                 . apply ( "Print elements" , \n                         MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                             System . out . println ( x ) ; \n                             try   { \n                                 Thread . sleep ( 60 * 1000 ) ; \n                             }   catch   ( InterruptedException  e )   { \n                                e . printStackTrace ( ) ; \n                             } \n                             return  x ; \n                         } ) ) ; \n\n         //todo 4.运行 \n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 测试DirectRunner \n \n 可通过命令行传参指定runner， \n //options.setRunner(FlinkRunner.class)\n/export/server/flink/bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 /export/server/flink/examples/batch/apache-beam-1.0-SNAPSHOT.jar --runner=org.apache.beam.runners.flink.FlinkRunner\n \n 1 2 \n FlinkUI可以看到有输出HelloWorld \n \n SparkRunner \n bin/spark-submit    --master   yarn    --deploy-mode client   /export/server/spark3/examples/jars/apache-beam-1.0-SNAPSHOT.jar  --runner = org.apache.beam.runners.spark.SparkRunner\n \n 1 \n 特性 \n 核心概念 \n 学习以下四个概念Beam model, Beam SDKs and Runners, and Beam’s native I/O connectors. \n Beam model \n 涉及的基础概念： \n \n Pipeline  - 管道是用户构建的图形 定义所需数据处理操作的转换。（封装了处理过程） \n PCollection  - A 是数据集或数据 流。管道处理的数据是 PCollection 的一部分。（不同操作间传递的数据格式） \n PTransform  - A（或 变换 ）表示 管道中的数据处理操作或步骤。转换是 应用于零个或多个对象，并生成零个或多个对象。（处理操作） \n Aggregation  - 聚合正在计算来自 多个（1 个或多个）输入元素。 \n User-defined function (UDF) 一些波束 操作允许您运行用户定义的代码作为配置 变换。 \n Schema   - 架构是与语言无关的类型定义 一个的架构将其元素定义为命名字段的有序列表。 \n SDK  - 一个特定于语言的库，允许 管道作者生成转换、构造管道并提交 他们到跑步者。 \n Runner - 流道使用 您选择的数据处理引擎。（计算引擎） \n [ Window ](https://beam.apache.org/documentation/basics/#window) - A 可以根据 各个元素的时间戳。窗口启用分组操作 通过将集合划分到窗口中来覆盖随时间增长的集合 的有限集合。 \n Watermark  - 水印是对何时所有数据在 预计某些窗口已经到来。这是必需的，因为数据不是 始终保证按时间顺序到达管道，或始终到达 以可预测的时间间隔。 \n Trigger  - 触发器确定何时聚合 每个窗口。 \n State and timers  - 每个键的状态和计时器回调 是较低级别的基元，可让您完全控制聚合输入 随着时间的推移而增长的集合。 \n State and timers  可拆分 DoFns 可让您处理 元素以非整体方式。您可以对 元素，并且运行器可以拆分剩余的工作以产生额外的 排比。 \n \n Pipline \n pipline具体来说是一个有向无环图，体现的数据处理流转过程。 \n 基础类型：单输入单输出 \n \n 多输出过程： \n \n \n \n 多输入： \n \n PCollection \n PCollection是一个 无序 的元素集合。可以是有界的，也可以是无界的。有界的数据按批处理pipline，无界数据根据指定间隔时间处理，以流模式处理。PCollection可以同时存有界和无界，但是如果runner只支持处理批模式，则需要剔除流数据，如何只支持流模式，可以将有界数据转换为流数据。 \n PCollection有如下特点： \n 1.元素类型可以是任何一种，但是由于分布式处理，需要传输，需要序列化和反序列化，比如自定义的实体类实现Serializable，beam还可以支持扩展其他序列化。（如何扩展？） \n public   class   MyCustomCoder   extends   CustomCoder < KV < String ,   Long > >   { \n     private   final   String  key ; \n\n     public   MyCustomCoder ( String  key )   { \n       this . key  =  key ; \n     } \n\n     @Override \n     public   void   encode ( KV < String ,   Long >  kv ,   OutputStream  out )   throws   IOException   { \n       new   DataOutputStream ( out ) . writeLong ( kv . getValue ( ) ) ; \n     } \n\n     @Override \n     public   KV < String ,   Long >   decode ( InputStream  inStream )   throws   IOException   { \n       return   KV . of ( key ,   new   DataInputStream ( inStream ) . readLong ( ) ) ; \n     } \n\n     @Override \n     public   boolean   equals ( @Nullable   Object  other )   { \n       return  other  instanceof   MyCustomCoder   &&  key . equals ( ( ( MyCustomCoder )  other ) . key ) ; \n     } \n\n     @Override \n     public   int   hashCode ( )   { \n       return  key . hashCode ( ) ; \n     } \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 2.部分具有schema，比如JSON, Protocol Buffer, Avro, and database records \n 3.不可变，一旦创建，就不能再增加，删除，更改，但是可以Ptranform一个新的Pcollection \n 4.不支持随机访问单个元素。 \n 5.PCollection大小不受限制，但是受到内存的约束。PCollection是无界的还是有界的，取决于读取的数据源。读取的是file或数据库是有界的，读取的是streaming或持续更新的数据源就是无界的。无界数据源可以用窗口进行聚合操作。(如何区分批、流？) \n 6.PCollection可以携带时间戳，一般数据源中有事件时间，日志上报时间，没有，也可以自己增加 \n        PCollection < LogEntry >  unstampedLogs  =   . . . ; \n       PCollection < LogEntry >  stampedLogs  = \n          unstampedLogs . apply ( ParDo . of ( new   DoFn < LogEntry ,   LogEntry > ( )   { \n             public   void   processElement ( @Element   LogEntry  element ,   OutputReceiver < LogEntry >  out )   { \n               // Extract the timestamp from log entry we\'re currently processing. \n               Instant  logTimeStamp  =   extractTimeStampFromLogEntry ( element ) ; \n               // Use OutputReceiver.outputWithTimestamp (rather than \n               // OutputReceiver.output) to emit the entry with timestamp attached. \n              out . outputWithTimestamp ( element ,  logTimeStamp ) ; \n             } \n           } ) ) ; \n \n 1 2 3 4 5 6 7 8 9 10 11 PTtransform \n transfrom 类型有： \n \n Source transforms  such as  TextIO.Read  and  Create . A source transform conceptually has no input. \n Processing and conversion operations  such as  ParDo ,  GroupByKey ,  CoGroupByKey ,  Combine , and  Count . \n Outputting transforms  such as  TextIO.Write . \n User-defined , application-specific composite transforms. \n Beam SDKs \n 提供了不同的SDK语言包，构建统一编程模型pipeline， \n Runners \n 支持在不同的平台上运行，目前支持的语言有java，python，go \n Beam’s native I/O connectors. \n 官网已支持的IO connector  I/O Connectors (apache.org) \n 编程深入 \n 针对helloworld的基础编程步骤深入每个步骤。 \n 1.创建一个Pipline \n \n option默认参数 \n \n // Start by defining the options for the pipeline. \n PipelineOptions  options  =   PipelineOptionsFactory . create ( ) ; \n\n // Then create the pipeline. \n Pipeline  p  =   Pipeline . create ( options ) ; \n \n 1 2 3 4 5 \n option默认有35个 \n \n 命令行也可通过--help来查询 \n \n 比如PipelineOptions \n \n option设置：\n \n 从命令行中直接配置已有的 \n 扩展自定义Myoptions接口继承PipelineOptions \n \n \n \n 自定义Myoptions接口 \n import   org . apache . beam . sdk . options . Default ; \n import   org . apache . beam . sdk . options . Description ; \n import   org . apache . beam . sdk . options . PipelineOptions ; \n\n public   interface   MyOptions   extends   PipelineOptions   { \n     //You can also specify a description, which appears when a user passes as a command-line argument, and a default value.--help \n     //You set the description and default value using annotations \n     @Description ( "Input for the pipeline" ) \n     @Default.String ( "gs://my-bucket/input" ) \n     String   getInput ( ) ; \n     void   setInput ( String  input ) ; \n\n     @Description ( "Output for the pipeline" ) \n     @Default.String ( "gs://my-bucket/output" ) \n     String   getOutput ( ) ; \n     void   setOutput ( String  output ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . options . PipelineOptions ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . Create ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n\n import   java . util . Arrays ; \n\n public   class   CreatePipeline   { \n     public   static   void   main ( String [ ]  args )   { \n\n         //模拟从命令行获取参数 \n         String  command_line = "--jobName=CreatePipeline" ; \n\n         //todo 1.创建一个Pipline \n\n         //todo 1.1 从命令行中获取默认参数配置 \n         /*PipelineOptions options = PipelineOptionsFactory.fromArgs(command_line).withValidation().create();\n        System.out.println("options.getJobName() = " + options.getJobName());*/ \n\n         /**\n         * fromArgs，确定strictParsing 为false，isCli 为true，withValidation 赋值validation 为true，验证参数是否有效\n         * Builder(String[] args, boolean validation, boolean strictParsing, boolean isCli)\n         * public PipelineOptionsFactory.Builder fromArgs(String... args) {\n         *             Preconditions.checkNotNull(args, "Arguments should not be null.");\n         *             return new PipelineOptionsFactory.Builder(args, this.validation, this.strictParsing, true);\n         *         }\n         */ \n\n         //todo 1.2 自定义Myoptions \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( command_line ) \n                 . withValidation ( ) \n                 . as ( MyOptions . class ) ; \n         System . out . println ( "options.getInput() = "   +  options . getInput ( ) ) ; \n         System . out . println ( "options.getOutput() = "   +  options . getOutput ( ) ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n         //todo 2.初始化一个PCollection \n        pipeline\n                 . apply ( "Create elements" ,   Create . of ( Arrays . asList ( "Hello" ,   "World!" ) ) ) \n                 //todo 3。处理PCollection \n                 . apply ( "Print elements" , \n                         MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                             System . out . println ( x ) ; \n                             return  x ; \n                         } ) ) ; \n         //todo 4.运行 \n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #   2.数据的输入和数据 \n 官网已支持的IO connector  I/O Connectors (apache.org) \n \n 针对text file,使用TextIO connector,详见 \n \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . TextIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . joda . time . Duration ; \n\n import   java . util . Arrays ; \n import   java . util . List ; \n\n import   static   org . apache . beam . sdk . transforms . Watch . Growth . afterTimeSinceNewOutput ; \n\n public   class   AboutText   { \n\n     // Create a Java Collection, in this case a List of Strings. \n     static   final   List < String >   LINES   =   Arrays . asList ( \n             "To be, or not to be: that is the question: " , \n             "Whether \'tis nobler in the mind to suffer " , \n             "The slings and arrows of outrageous fortune, " , \n             "Or to take arms against a sea of troubles, " ) ; \n\n     public   static   void   main ( String [ ]  args )   { \n\n\n         //todo 1.创建pipline \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         //todo 2.读取数据源 \n         //todo  一般测试使用，由List生成一个PCollection \n         /*PCollection<String> from_list = pipeline.apply("from List", Create.of(LINES));\n        from_list.apply("Print elements",\n                MapElements.into(TypeDescriptors.strings()).via(x -> {\n                    System.out.println(x);\n                    return x;\n                }));*/ \n\n         //todo  读取text file \n         /*PCollection<String> from_text = pipeline.apply(TextIO.read().from("input/input.txt"));\n        from_text\n                .apply("Print elements",\n                        MapElements.into(TypeDescriptors.strings()).via(x -> {\n                            System.out.println(x);\n                            return x;\n                        }));\n\n        //如何控制生成的文件数？\n        from_text.apply("write_text", TextIO.write().to("output/from_text"));*/ \n\n         //以streaming的方式监控文件下有无新文件，有则读取 \n         PCollection < String >  from_text  =  pipeline . apply ( TextIO . read ( ) \n                 . from ( "input/*.txt" ) \n                 . watchForNewFiles ( \n                         // Check for new files every ten seconds \n                         Duration . standardSeconds ( 10 ) , \n                         // Stop watching the filepattern if no new files appear within an minute \n                         afterTimeSinceNewOutput ( Duration . standardMinutes ( 1 ) ) ) ) ; \n\n        from_text . apply ( "Print elements" , \n                 MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                     System . out . println ( x ) ; \n                     return  x ; \n                 } ) ) ; \n\n         //如果读取大量文件，建议使用TextIO.Read.withHintMatchesManyFiles()提升性能 \n         // todo from_mysql \n\n         // todo hadoop \n\n         // todo from_hive \n\n         // todo kafka \n\n\n\n         //todo 4.运行 \n        pipeline . run ( ) . waitUntilFinish ( ) ; \n\n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 \n 针对mysql ，使用JdbcIO \n \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n\n import   org . apache . beam . sdk . io . jdbc . JdbcIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . ProcessFunction ; \n import   org . apache . beam . sdk . values . * ; \n import   org . checkerframework . checker . initialization . qual . Initialized ; \n import   org . checkerframework . checker . nullness . qual . NonNull ; \n import   org . checkerframework . checker . nullness . qual . UnknownKeyFor ; \n\n import   java . sql . PreparedStatement ; \n import   java . sql . ResultSet ; \n import   java . sql . SQLException ; \n\n public   class   AboutMysql   { \n     public   static   void   main ( String [ ]  args )   { \n         // todo 创建Pipeline \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         // todo 读取数据源 \n         /*PCollection<KV<Integer, String>> from_mysql = pipeline.apply(JdbcIO.<KV<Integer, String>>read()\n                .withDataSourceConfiguration(JdbcIO.DataSourceConfiguration.create(\n                        "com.mysql.jdbc.Driver", "jdbc:mysql://node1:3306/shop")\n                        .withUsername("root")\n                        .withPassword("123456"))\n                .withQuery("select pid,pname from shop_product")\n                .withRowMapper(new JdbcIO.RowMapper<KV<Integer, String>>() {\n                    public KV<Integer, String> mapRow(ResultSet resultSet) throws Exception {\n                        System.out.print("pid = " + resultSet.getInt(1)+"\\t");\n                        System.out.println("pname = " + resultSet.getString(2));\n                        return KV.of(resultSet.getInt(1), resultSet.getString(2));\n                    }\n                })\n        );*/ \n         //todo 使用占位符，对查询条件进行配置查询 \n         /*PCollection<KV<Integer, String>> from_mysql = pipeline.apply(JdbcIO.<KV<Integer, String>>read()\n                .withDataSourceConfiguration(JdbcIO.DataSourceConfiguration.create(\n                        "com.mysql.jdbc.Driver", "jdbc:mysql://node1:3306/shop")\n                        .withUsername("root")\n                        .withPassword("123456"))\n                .withQuery("select pid,pname from shop_product where 1 = ?")\n                .withStatementPreparator(new JdbcIO.StatementPreparator() {\n                    @Override\n                    public void setParameters(PreparedStatement preparedStatement) throws Exception {\n                        preparedStatement.setInt(1, 1);\n                    }\n                })\n                .withRowMapper(new JdbcIO.RowMapper<KV<Integer, String>>() {\n                    public KV<Integer, String> mapRow(ResultSet resultSet) throws Exception {\n                        System.out.print("pid = " + resultSet.getInt(1) + "\\t");\n                        System.out.println("pname = " + resultSet.getString(2));\n\n                        return KV.of(resultSet.getInt(1), resultSet.getString(2));\n                    }\n                })\n        );\n        from_mysql.apply(JdbcIO.<KV<Integer, String>>write()\n                .withDataSourceConfiguration(JdbcIO.DataSourceConfiguration.create(\n                        "com.mysql.jdbc.Driver", "jdbc:mysql://node1:3306/shop")\n                        .withUsername("root")\n                        .withPassword("123456"))\n                .withStatement("insert into shop_product values(?, ?,1,1)")\n                .withPreparedStatementSetter(new JdbcIO.PreparedStatementSetter<KV<Integer, String>>() {\n                    public void setParameters(KV<Integer, String> element, PreparedStatement query)\n                            throws SQLException {\n                        query.setInt(1, element.getKey());\n                        query.setString(2, element.getValue());\n                    }\n                })\n        );*/ \n\n         // todo 为避免建立太多链接，考虑使用自定义数据源接入线程池方式 \n\n         PCollection < ProductEntity >  from_mysql  =  pipeline . apply ( JdbcIO . < ProductEntity > read ( ) \n                 . withDataSourceProviderFn ( new   MyDataSourceProviderFn ( ) ) \n                 . withQuery ( "select pid,pname,pprice,stock from shop_product where 1 = ?" ) \n                 . withStatementPreparator ( new   JdbcIO . StatementPreparator ( )   { \n                     @Override \n                     public   void   setParameters ( PreparedStatement  preparedStatement )   throws   Exception   { \n                        preparedStatement . setInt ( 1 ,   1 ) ; \n                     } \n                 } ) \n                 . withRowMapper ( new   JdbcIO . RowMapper < ProductEntity > ( )   { \n                     //ProductEntity productEntity=new ProductEntity(); 这种写法报错，要求元素是不可变的 \n                     public   ProductEntity   mapRow ( ResultSet  resultSet )   throws   Exception   { \n                         ProductEntity  productEntity = new   ProductEntity ( ) ; \n                         System . out . println ( "resultSet.getInt(0) = "   +  resultSet . getInt ( 1 ) ) ; \n                        productEntity . setPid ( resultSet . getInt ( 1 ) ) ; \n                        productEntity . setPname ( resultSet . getString ( 2 ) ) ; \n                        productEntity . setPrice ( resultSet . getDouble ( 3 ) ) ; \n                        productEntity . setStock ( resultSet . getInt ( 4 ) ) ; \n                         return  productEntity ; \n                     } \n                 } ) \n         ) ; \n\n\n        from_mysql . apply ( JdbcIO . < ProductEntity > write ( ) \n                 . withDataSourceProviderFn ( new   MyDataSourceProviderFn ( ) ) \n                 . withStatement ( "insert into shop.shop_product(pid, pname, pprice, stock) values (?,?,?,?)"   + \n                         " on duplicate key update pname=values(pname),pprice=values(pprice),stock=values(stock)" ) \n                 . withPreparedStatementSetter ( new   JdbcIO . PreparedStatementSetter < ProductEntity > ( )   { \n                     public   void   setParameters ( ProductEntity  productEntity ,   PreparedStatement  query ) \n                             throws   SQLException   { \n\n                        query . setInt ( 1 ,  productEntity . getPid ( ) + 10 ) ; \n                        query . setString ( 2 ,  productEntity . getPname ( ) ) ; \n                        query . setDouble ( 3 ,  productEntity . getPrice ( ) ) ; \n                        query . setInt ( 4 ,  productEntity . getStock ( ) ) ; \n                     } \n                 } ) \n         ) ; \n\n\n         //todo 运行 \n        pipeline . run ( ) . waitUntilFinish ( ) ; \n\n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 自定义的MyDataSourceProviderFn \n import   org . apache . beam . sdk . transforms . SerializableFunction ; \n\n import   javax . sql . DataSource ; \n\n public    class   MyDataSourceProviderFn   implements   SerializableFunction < Void ,   DataSource > { \n     private   static   transient   DataSource  dataSource ; \n\n     @Override \n     public   DataSource   apply ( Void  input )   { \n         if   ( dataSource  ==   null )   { \n            dataSource  =   C3P0Utils . getDataSource ( "otherc3p0" ) ; \n         } \n         return  dataSource ; \n     } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ProductEntity \n import   java . io . Serializable ; \n\n public   class   ProductEntity   implements   Serializable   { \n     private   Integer  pid ; \n     private   String  pname ; \n     private   double  price ; \n     private   Integer  stock ; \n\n     public   static    ProductEntity  of  ( Integer  pid ,   String  pname ,   double  price ,   Integer  stock )   { \n         return   new   ProductEntity (  pid ,   pname ,   price ,   stock ) ; \n     } \n\n     public   ProductEntity ( )   { \n     } \n\n     public   void   setPid ( Integer  pid )   { \n         this . pid  =  pid ; \n     } \n\n     public   void   setPname ( String  pname )   { \n         this . pname  =  pname ; \n     } \n\n     public   void   setPrice ( double  price )   { \n         this . price  =  price ; \n     } \n\n     public   void   setStock ( Integer  stock )   { \n         this . stock  =  stock ; \n     } \n\n     public   ProductEntity ( Integer  pid ,   String  pname ,   double  price ,   Integer  stock )   { \n         this . pid  =  pid ; \n         this . pname  =  pname ; \n         this . price  =  price ; \n         this . stock  =  stock ; \n     } \n\n     public   Integer   getPid ( )   { \n         return  pid ; \n     } \n\n     public   String   getPname ( )   { \n         return  pname ; \n     } \n\n     public   double   getPrice ( )   { \n         return  price ; \n     } \n\n     public   Integer   getStock ( )   { \n         return  stock ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "ProductEntity{"   + \n                 "pid="   +  pid  + \n                 ", pname=\'"   +  pname  +   \'\\\'\'   + \n                 ", price="   +  price  + \n                 ", stock="   +  stock  + \n                 \'}\' ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 \n import   com . mchange . v2 . c3p0 . ComboPooledDataSource ; \n\n import   javax . sql . DataSource ; \n import   java . sql . Connection ; \n import   java . sql . ResultSet ; \n import   java . sql . SQLException ; \n import   java . sql . Statement ; \n\n //DBCP: 属于Apache软件基金组织的, 不能自动回收空闲连接. \n //3P0: 属于Apache软件基金组织的, 能自动回收空闲连接, Java框架的底层源码中但凡涉及到数据库连接池内容了, 用的都是它. \n\n //自定义的针对于C3P0数据库连接池的工具类. \n public   class   C3P0Utils   { \n     //成员都是静态的. \n     //1. 定义一个私有的静态的常量, 表示数据库连接池对象. \n     private   static    ComboPooledDataSource  cpds = new   ComboPooledDataSource ( ) ; \n\n     //2. 构造方法私有化. \n     private   C3P0Utils ( )   { \n     } \n\n\n     //3. 对外提供一个方法, 用来获取 数据库连接池对象. \n     //DataSource是JDBC中针对于数据库连接池的规范, 即: 所有的数据库连接池对象都是它的子类. \n     public   static   DataSource   getDataSource ( String  s )   { \n        cpds = new   ComboPooledDataSource ( s ) ; \n         return  cpds ; \n     } \n     /*public static DataSource getDataSource() {\n        cpds=new ComboPooledDataSource();\n        return cpds;\n    }*/ \n\n     //4. 对外提供一个方法, 用来获取 连接对象, 这个连接对象是从 数据库连接池中获取的. \n     public   static   Connection   getConnection ( )   { \n         try   { \n             return  cpds . getConnection ( ) ; \n         }   catch   ( SQLException  throwables )   { \n            throwables . printStackTrace ( ) ; \n         } \n         return   null ; \n     } \n\n     //5. 对外提供一个方法, 用来 释放资源. \n     public   static   void   release ( Connection  conn ,   Statement  stat ,   ResultSet  rs )   { \n         try   { \n             if   ( rs  !=   null )   { \n                rs . close ( ) ; \n                rs  =   null ;    //GC会优先回收null对象. \n             } \n         }   catch   ( SQLException  throwables )   { \n            throwables . printStackTrace ( ) ; \n         }   finally   { \n             try   { \n                 if   ( stat  !=   null )   { \n                    stat . close ( ) ; \n                    stat  =   null ;    //GC会优先回收null对象. \n                 } \n             }   catch   ( SQLException  throwables )   { \n                throwables . printStackTrace ( ) ; \n             }   finally   { \n                 try   { \n                     if   ( conn  !=   null )   { \n                        conn . close ( ) ; \n                        conn  =   null ;    //GC会优先回收null对象. \n                     } \n                 }   catch   ( SQLException  throwables )   { \n                    throwables . printStackTrace ( ) ; \n                 } \n             } \n         } \n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 针对hive，使用HCatalogIO \n import   com . google . common . collect . ImmutableMap ; \n import   com . gordon . create_pipeline . MyOptions ; \n import   com . gordon . read_write . entity . StudentEntity ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . hcatalog . HCatalogIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . ProcessFunction ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . apache . hive . hcatalog . data . HCatRecord ; \n\n import   java . util . Map ; \n\n public   class   AboutHive   { \n     static   final   Map < String ,   String >  configProperties  =   ImmutableMap . of ( "hive.metastore.uris" ,   "thrift://node3:9083" ) ; \n\n\n     public   static   void   main ( String [ ]  args )   { \n         //todo 创建Pipeline \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         //todo hive io sql限制有点大，只能全部导入数据再处理，耗内存 \n         PCollection < HCatRecord >  from_hive  =  pipeline\n                 . apply ( HCatalogIO . read ( ) \n                         . withConfigProperties ( configProperties ) \n                         . withDatabase ( "default" )   //optional, assumes default if none specified \n                         . withTable ( "student" ) ) ; \n                         //.withFilter(filterString)); //optional, may be specified if the table is partitioned ,有分区才生效 \n         //from_hive.apply(ParDo.of(new CreateHCatFn())); \n        from_hive . apply ( "from_hive" , MapElements . into ( TypeDescriptors . nulls ( ) ) . via ( new   ProcessFunction < HCatRecord ,   Void > ( )   { \n             @Override \n             public   Void   apply ( HCatRecord  input )   throws   Exception   { \n                 StudentEntity  studentEntity  =   new   StudentEntity ( ( String )  input . get ( 0 ) ,   ( String )  input . get ( 1 ) ,   ( String )  input . get ( 2 ) ,   ( String )  input . get ( 3 ) ) ; \n\n                 System . out . println ( "studentEntity = "   +  studentEntity ) ; \n                 return   null ; \n             } \n         } ) ) ; \n         //直接新插入，相同不覆盖更新 \n        from_hive . apply ( HCatalogIO . write ( ) \n                 . withConfigProperties ( configProperties ) \n                 . withDatabase ( "default" )   //optional, assumes default if none specified \n                 . withTable ( "student" ) \n                 //.withPartition(partitionValues) //optional, may be specified if the table is partitioned \n                 . withBatchSize ( 1024L ) ) ;   //optional, assumes a default batch size of 1024 if none specified \n\n         //todo run \n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import   org . apache . beam . sdk . transforms . DoFn ; \n import   org . apache . hive . hcatalog . data . HCatRecord ; \n\n public   class   CreateHCatFn   extends   DoFn < HCatRecord , Void >   { \n\n     @ProcessElement \n     public   void   processElement ( ProcessContext  c )   { \n         System . out . println ( "c.element().get(0) = "   +  c . element ( ) . get ( 0 ) ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 import   java . io . Serializable ; \n\n public   class   StudentEntity   implements   Serializable   { \n     private   String  s_id ; \n     private   String  s_name ; \n     private   String  s_birth ; \n     private   String  s_sex ; \n\n     public   StudentEntity ( String  s_id ,   String  s_name ,   String  s_birth ,   String  s_sex )   { \n         this . s_id  =  s_id ; \n         this . s_name  =  s_name ; \n         this . s_birth  =  s_birth ; \n         this . s_sex  =  s_sex ; \n     } \n\n     public   StudentEntity ( )   { \n     } \n\n     public   String   getS_id ( )   { \n         return  s_id ; \n     } \n\n     public   void   setS_id ( String  s_id )   { \n         this . s_id  =  s_id ; \n     } \n\n     public   String   getS_name ( )   { \n         return  s_name ; \n     } \n\n     public   void   setS_name ( String  s_name )   { \n         this . s_name  =  s_name ; \n     } \n\n     public   String   getS_birth ( )   { \n         return  s_birth ; \n     } \n\n     public   void   setS_birth ( String  s_birth )   { \n         this . s_birth  =  s_birth ; \n     } \n\n     public   String   getS_sex ( )   { \n         return  s_sex ; \n     } \n\n     public   void   setS_sex ( String  s_sex )   { \n         this . s_sex  =  s_sex ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "StudentEntity{"   + \n                 "s_id=\'"   +  s_id  +   \'\\\'\'   + \n                 ", s_name=\'"   +  s_name  +   \'\\\'\'   + \n                 ", s_birth=\'"   +  s_birth  +   \'\\\'\'   + \n                 ", s_sex=\'"   +  s_sex  +   \'\\\'\'   + \n                 \'}\' ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 针对hadoop，使用HadoopFormatIO \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . hadoop . format . HDFSSynchronization ; \n import   org . apache . beam . sdk . io . hadoop . format . HadoopFormatIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . values . KV ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptor ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . apache . hadoop . conf . Configuration ; \n import   org . apache . hadoop . io . LongWritable ; \n import   org . apache . hadoop . io . Text ; \n import   org . apache . hadoop . mapreduce . InputFormat ; \n import   org . apache . hadoop . mapreduce . OutputFormat ; \n import   org . apache . hadoop . mapreduce . lib . input . TextInputFormat ; \n import   org . apache . hadoop . mapreduce . lib . output . TextOutputFormat ; \n\n import   java . io . IOException ; \n\n public   class   AboutHadoop   { \n     public   static   void   main ( String [ ]  args )   throws   IOException   { \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         // todo HadoopFormatIO \n\n\n         Configuration  conf  =   new   Configuration ( ) ; \n // Set Hadoop InputFormat, key and value class in configuration \n         //mapreduce 中map input 的key是偏移量，value才是一行的值 \n        conf . set ( "fs.defaultFS" , "hdfs://node1:8020" ) ; \n        conf . setClass ( "key.class" ,   LongWritable . class ,   Object . class ) ; \n        conf . setClass ( "value.class" ,   Text . class ,   Object . class ) ; \n        conf . setClass ( "mapreduce.job.inputformat.class" ,   TextInputFormat . class ,   InputFormat . class ) ; \n        conf . set ( "mapreduce.input.fileinputformat.inputdir" , "/data/words.txt" ) ; \n\n         PCollection < KV < LongWritable ,   Text > >  from_hdfs  =  pipeline . apply ( "from_hdfs" ,   HadoopFormatIO . < LongWritable ,   Text > read ( ) . withConfiguration ( conf ) ) ; \n         PCollection < KV < Text ,   Text > >  res  =  from_hdfs\n                 . apply ( "from_hadoop" ,   MapElements . into ( TypeDescriptors . kvs ( new   TypeDescriptor < Text > ( )   { \n                 } ,   new   TypeDescriptor < Text > ( )   { \n                 } ) ) . via ( x  ->   { \n                     System . out . println ( "line = "   +  x . getValue ( ) ) ; \n                     return   KV . of ( new   Text ( String . valueOf ( System . currentTimeMillis ( ) ) ) , x . getValue ( ) ) ; \n                 } ) ) ; \n        conf . set ( HadoopFormatIO . JOB_ID , "AboutHadoop" ) ; \n        conf . setClass ( HadoopFormatIO . OUTPUT_FORMAT_CLASS_ATTR ,   TextOutputFormat . class ,   OutputFormat . class ) ; \n        conf . setClass ( HadoopFormatIO . OUTPUT_KEY_CLASS , Text . class , Object . class ) ; \n        conf . setClass ( HadoopFormatIO . OUTPUT_VALUE_CLASS , Text . class , Object . class ) ; \n        conf . setInt ( HadoopFormatIO . NUM_REDUCES , 2 ) ; \n        conf . set ( HadoopFormatIO . OUTPUT_DIR , "hdfs://node1:8020/data/words3.txt" ) ; \n\n\n\n        res . apply ( \n                 "writeBatch" , \n                 HadoopFormatIO . < Text ,   Text > write ( ) \n                         . withConfiguration ( conf ) \n                         . withPartitioning ( ) \n                         . withExternalSynchronization ( new   HDFSSynchronization ( conf . get ( HadoopFormatIO . OUTPUT_DIR ) ) ) ) ; \n\n         /*unboundedWordsCount.apply(\n                "writeStream",\n                HadoopFormatIO.<Text, LongWritable>write()\n                        .withConfigurationTransform(configTransform)\n                        .withExternalSynchronization(new HDFSSynchronization(locksDirPath)));*/ \n\n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 针对Kafka，使用KafkaIO \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . runners . flink . FlinkRunner ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . kafka . KafkaIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . Values ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . apache . beam . vendor . guava . v26_0_jre . com . google . common . collect . ImmutableMap ; \n import   org . apache . kafka . common . serialization . StringDeserializer ; \n import   org . apache . kafka . common . serialization . StringSerializer ; \n\n public   class   AboutKafka   { \n\n     public   static   void   main ( String [ ]  args )   { \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n        options . setRunner ( FlinkRunner . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         //todo KafkaIO \n         PCollection < String >  from_kafka  =  pipeline\n                 . apply ( KafkaIO . < String ,   String > read ( ) \n                         . withBootstrapServers ( "node1:9092,node2:9092,node3:9092" ) \n                         . withTopic ( "KafkaWordCount" )    // use withTopics(List<String>) to read from multiple topics. \n                         . withKeyDeserializer ( StringDeserializer . class ) \n                         . withValueDeserializer ( StringDeserializer . class ) \n                         // Rest of the settings are optional : \n\n                         // you can further customize KafkaConsumer used to read the records by adding more \n                         // settings for ConsumerConfig. e.g : \n                         . withConsumerConfigUpdates ( ImmutableMap . of ( \n                                 "group.id" ,   "flink_wordcount" , \n                                 "auto.offset.reset" ,   "latest" , \n                                 "enable.auto.commit" ,   "true" \n                         ) ) \n                         . withoutMetadata ( ) \n\n                 ) . apply ( Values . create ( ) ) \n                 . apply ( "from_kafka" ,   MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                     System . out . println ( x ) ; \n                     return  x ; \n                 } ) ) ; \n\n\n        from_kafka . apply ( KafkaIO . < Void ,   String > write ( ) \n                         . withBootstrapServers ( "node1:9092,node2:9092,node3:9092" ) \n                         . withTopic ( "test01" )    // use withTopics(List<String>) to read from multiple topics. \n                         . withValueSerializer ( StringSerializer . class )   // just need serializer for value \n                         . values ( ) ) ; \n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 运行在FlinkRunner on Yarn \n /export/server/flink/bin/flink run  -m  yarn-cluster  -yjm   1024   -ytm   1024   -c  com.gordon.source_sink.AboutKafka /export/server/flink/examples/batch/apache-beam-1.0-SNAPSHOT.jar  --runner = org.apache.beam.runners.flink.FlinkRunner\n \n 1 \n SparkRunner \n bin/spark-submit  --master   yarn  --deploy-mode client  --class = com.gordon.source_sink.AboutKafka /export/server/spark3/examples/jars/apache-beam-1.0-SNAPSHOT.jar  --runner = org.apache.beam.runners.spark.SparkRunner\n \n 1 \n 3.处理和转换 \n transform的过程是每个worker并行执行代码逻辑，最后聚合合并成一个。（不像rdd一样有分区？合并成一个，在哪合并？合并之后资源受限？） \n 要进行transform操作，需调用apply，包含以下两类操作： \n 1.基础型的，通过ParDo或combin \n 2.自定义transform \n 基础型： \n ParDo：类似Map \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . TextIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . DoFn ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . ParDo ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n\n\n public   class   ParDoDemo   { \n     public   static   void   main ( String [ ]  args )   { \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n\n         PCollection < String >  words  =  pipeline . apply ( TextIO . read ( ) . from ( "/input/input.txt" ) ) ; \n         //匿名内部类或者另外自定义一个类继承DoFn \n        words . apply ( "ParDo" , ParDo . of ( new   DoFn < String ,   Integer > ( )   { \n             @ProcessElement \n             public   void   processElement ( ProcessContext  c ) { \n                 System . out . println ( "ParDo.line.length() = "   +  c . element ( ) . length ( ) ) ; \n               c . output ( c . element ( ) . length ( ) ) ; \n             } \n         } ) ) ; \n\n         //高级用法 MapElements \n        words . apply ( "MapElements" ,   MapElements . into ( TypeDescriptors . integers ( ) ) . via ( ( String  line ) -> { \n             System . out . println ( "MapElements.line.length() = "   +  line . length ( ) ) ; \n             return  line . length ( ) ; \n         } ) ) ; \n\n\n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 GroupByKey/CoGroupByKey/Combine \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . TextIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . * ; \n import   org . apache . beam . sdk . transforms . join . CoGbkResult ; \n import   org . apache . beam . sdk . transforms . join . CoGroupByKey ; \n import   org . apache . beam . sdk . transforms . join . KeyedPCollectionTuple ; \n import   org . apache . beam . sdk . values . * ; \n\n import   java . util . Arrays ; \n import   java . util . List ; \n\n\n public   class   GroupByKey_Join_CombineDemo   { \n     public   static   void   main ( String [ ]  args )   { \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n\n         PCollection < KV < String ,   Integer > >  wordNum  =  pipeline . apply ( TextIO . read ( ) . from ( "/input/input.txt" ) ) \n                 . apply ( ParDo . of ( new   DoFn < String ,  KV < String ,   Integer > > ( )   { \n                     @ProcessElement \n                     public   void   processElement ( ProcessContext  c )   { \n                         String [ ]  split  =  c . element ( ) . split ( "\\\\s+" ) ; \n                         for   ( String  s  :  split )   { \n                            c . output ( KV . of ( s ,   1 ) ) ; \n                         } \n                     } \n                 } ) ) \n                 // Combine(map side join) \n                 . apply ( Sum . < String > integersPerKey ( ) ) ; \n\n        wordNum\n                 //GroupByKey \n                 . apply ( GroupByKey . create ( ) ) \n                 . apply ( MapElements . into ( TypeDescriptors . nulls ( ) ) . via ( x  ->   { \n                     System . out . println ( String . format ( "%s, %s" ,  x . getKey ( ) ,  x . getValue ( ) ) ) ; \n                     return   null ; \n                 } ) ) ; \n\n         //CoGroupByKey join \n         PCollection < KV < String ,   Integer > >  wordNum2  =  pipeline . apply ( TextIO . read ( ) . from ( "/input/input2.txt" ) ) \n                 . apply ( ParDo . of ( new   DoFn < String ,  KV < String ,   Integer > > ( )   { \n                     @ProcessElement \n                     public   void   processElement ( ProcessContext  c )   { \n                         String [ ]  split  =  c . element ( ) . split ( "\\\\s+" ) ; \n                         for   ( String  s  :  split )   { \n                            c . output ( KV . of ( s ,   1 ) ) ; \n                         } \n                     } \n                 } ) ) \n                 . apply ( Sum . < String > integersPerKey ( ) ) ; \n         final   TupleTag < Integer >  num1Tag  =   new   TupleTag < > ( ) ; \n         final   TupleTag < Integer >  num2Tag  =   new   TupleTag < > ( ) ; \n         PCollection < KV < String ,   CoGbkResult > >  join  =   KeyedPCollectionTuple . of ( num1Tag ,  wordNum ) . and ( num2Tag ,  wordNum2 ) \n                 . apply ( CoGroupByKey . create ( ) ) ; \n\n         PCollection < KV < String ,   Integer > >  res  =  join . apply ( ParDo . of ( new   DoFn < KV < String ,   CoGbkResult > ,  KV < String ,   Integer > > ( )   { \n             @ProcessElement \n             public   void   processElement ( ProcessContext  c )   { \n                 KV < String ,   CoGbkResult >  element  =  c . element ( ) ; \n                 Integer  first  =  element . getValue ( ) . getAll ( num1Tag ) . iterator ( ) . next ( ) ; \n                 Integer  second  =  element . getValue ( ) . getOnly ( num2Tag ) ; \n                 //sum \n                 //System.out.println(String.format("%s,%s", element.getKey(), first + second)); \n                c . output ( KV . of ( element . getKey ( ) ,  first  +  second ) ) ; \n             } \n         } ) ) ; \n         //引入字典,sideinput是每台机器，还是每个task线程？ \n         PCollectionView < List < String > >  black_list  =  pipeline\n                 . apply ( Create . of ( Arrays . asList ( "c++" ,   "go" ,   "python" ,   "scala" ) ) ) \n                 . apply ( View . asList ( ) ) ; \n        res\n                 . apply ( ParDo \n                         . of ( new   DoFn < KV < String ,   Integer > ,  KV < String ,   Integer > > ( )   { \n                             @ProcessElement \n                             public   void   processElement ( ProcessContext  c )   { \n                                 KV < String ,   Integer >  element  =  c . element ( ) ; \n                                 List < String >  list  =  c . sideInput ( black_list ) ; \n                                 if ( ! list . contains ( element . getKey ( ) ) ) { \n                                     System . out . println ( String . format ( "%s,%s" ,  element . getKey ( ) ,  element . getValue ( ) ) ) ; \n                                    c . output ( element ) ; \n                                 } \n                             } \n                         } ) . withSideInputs ( black_list ) \n                 ) \n                 . apply ( Combine . globally ( new   CustomerCombinFn ( ) ) ) \n                 . apply ( MapElements . into ( TypeDescriptors . nulls ( ) ) . via ( x  ->   { \n                     System . out . println ( String . format ( "%s, %s" ,  x . getKey ( ) ,  x . getValue ( ) ) ) ; \n                     return   null ; \n                 } ) ) ; \n\n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 自定义Combine \n import   org . apache . beam . sdk . transforms . Combine ; \n import   org . apache . beam . sdk . values . KV ; \n\n import   java . io . Serializable ; \n //自定义CombinFn \n class   CustomerCombinFn   extends   Combine . CombineFn < KV < String , Integer > ,   CustomerCombinFn . Max ,  KV < String , Integer > >    {   //继承的 CombineFn已经实现了序列化 \n   public   static   class   Max   implements   Serializable   { // 自定义Max需要实现序列化 \n     String  key ; \n     int  max_num  ; \n\n   } \n   //todo 1.创建累加器 \n   @Override \n   public   Max   createAccumulator ( )   {   return   new   Max ( ) ;   } \n\n   //todo 新元素的聚合 \n   @Override \n   public   Max   addInput ( Max  max ,   KV < String , Integer >  input )   { \n     if ( input . getValue ( ) > max . max_num ) { \n      max . key = input . getKey ( ) ; \n      max . max_num = input . getValue ( ) ; \n     } \n      return  max ; \n   } \n\n   //todo 每个work的结果合并起来 \n   @Override \n   public   Max   mergeAccumulators ( Iterable < Max >  maxs )   { \n     Max  merged  =   createAccumulator ( ) ; \n     for   ( Max  max  :  maxs )   { \n      merged = max . max_num > merged . max_num ? max : merged ; \n     } \n     return  merged ; \n   } \n   //todo 抽出最终结果 \n   @Override \n   public   KV < String , Integer >   extractOutput ( Max  merged )   { \n     return   KV . of ( merged . key , merged . max_num ) ; \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 FlatMapElements/Partition \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . TextIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . FlatMapElements ; \n import   org . apache . beam . sdk . transforms . InferableFunction ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . Partition ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . PCollectionList ; \n\n import   java . util . Arrays ; \n import   java . util . List ; \n\n public   class   FlatMapDemo   { \n     public   static   void   main ( String [ ]  args )   { \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         PCollection < String >  lines  =  pipeline . apply ( TextIO . read ( ) . from ( "/input/input.txt" ) ) ; \n         /*PCollection<List<String>> map_list = lines.apply(MapElements.via(\n                new InferableFunction<String, List<String>>() {\n                    public List<String> apply(String line) throws Exception {\n\n                        return Arrays.asList(line.split("\\\\s+"));\n                    }\n                }));\n        map_list.apply(Flatten.iterables())\n                .apply(MapElements.via(new InferableFunction<String, Void>() {\n                    @Override\n                    public Void apply(String input) throws Exception {\n                        System.out.println(" map Flatten word = " + input);\n                        return null;\n                    }\n                }));*/ \n\n         PCollection < String >  words  =  lines . apply ( FlatMapElements . via ( \n                 new   InferableFunction < String ,   List < String > > ( )   { \n                     public   List < String >   apply ( String  line )   throws   Exception   { \n                         return   Arrays . asList ( line . split ( "\\\\s+" ) ) ; \n                     } \n                 } ) ) ; \n         //由此可见flatmap 是map和flatten两个的组合的操作,能在一个transform中处理完的，别用两个。 \n        words . apply ( MapElements . via ( new   InferableFunction < String ,   Void > ( )   { \n             @Override \n             public   Void   apply ( String  input )   throws   Exception   { \n                 System . out . println ( "flatmap word = "   +  input ) ; \n                 return   null ; \n             } \n         } ) ) ; \n\n\n         PCollectionList < String >  partition  =  words\n                 . apply ( Partition . of ( 5 ,   new   Partition . PartitionFn < String > ( )   { \n                     int  count  =   0 ; \n\n                     @Override \n                     public   int   partitionFor ( String  elem ,   int  numPartitions )   { \n                         int  i  =  count  /  numPartitions ; \n                        count ++ ; \n                         return  i ; \n                     } \n                 } ) ) ; \n\n        partition . get ( 1 ) . apply ( MapElements . via ( new   InferableFunction < String ,   Void > ( )   { \n             @Override \n             public   Void   apply ( String  input )   throws   Exception   { \n                 System . out . println ( "patition = "   +  input ) ; \n                 return   null ; \n             } \n         } ) ) ; \n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 4.引入sql \n \n \n sqlDemo \n \n \n import   com . gordon . create_pipeline . MyOptions ; \n import   com . gordon . source_sink . entity . ProductEntity ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . coders . RowCoder ; \n import   org . apache . beam . sdk . extensions . sql . SqlTransform ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . schemas . Schema ; \n import   org . apache . beam . sdk . transforms . Create ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . values . PBegin ; \n import   org . apache . beam . sdk . values . Row ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n\n public   class   SqlDemo   { \n     public   static   void   main ( String [ ]  args )   { \n         //初始化pipline环境 \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         //通过Schema创建Row \n         //todo 定义Schema \n         Schema  appSchema  =   Schema . builder ( ) \n                 . addInt32Field ( "pid" ) \n                 . addStringField ( "pname" ) \n                 . addDoubleField ( "pprice" ) \n                 . addInt32Field ( "stock" ) \n                 . build ( ) ; \n         //todo 创建对应Row类型，PCollection可操作对象 \n         Row  row  =   Row . withSchema ( appSchema ) \n                 . addValues ( 25 ,   "怒米亚" ,   21999d ,   123 ) \n                 . build ( ) ; \n\n         PBegin . in ( pipeline ) . apply ( Create . of ( row ) . withCoder ( RowCoder . of ( appSchema ) ) ) \n                 . apply ( SqlTransform . query ( "select * from PCOLLECTION" ) ) \n         . apply ( MapElements . into ( TypeDescriptors . nulls ( ) ) . via ( ( Row  r ) -> { \n             System . out . println ( "r.getValues() = "   +  r . getValues ( ) ) ; \n             return   null ; \n         } ) ) ; \n\n         //todo 通过pojo创建 ,pojo类加上注解@DefaultSchema(JavaBeanSchema.class) \n        pipeline\n                 . apply ( Create . of ( \n                         ProductEntity . of ( 25 ,   "怒米亚" ,   21999d ,   123 ) , \n                         ProductEntity . of ( 26 ,   "怒亚" ,   21999d ,   123 ) ) \n                 ) . apply ( SqlTransform . query ( "select * from PCOLLECTION" ) ) //PCOLLECTION需要大写 \n                 . apply ( MapElements . into ( TypeDescriptors . nulls ( ) ) . via ( ( Row  r ) -> { \n                     System . out . println ( "r.getValues() = "   +  r . getValues ( ) ) ; \n                     return   null ; \n                 } ) ) ; \n\n\n\n\n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n\n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \n \n @DefaultSchema ( JavaBeanSchema . class ) \n public   class   ProductEntity   implements   Serializable   { \n     private   Integer  pid ; \n     private   String  pname ; \n     private   double  price ; \n     private   Integer  stock ; \n\n     public   static    ProductEntity  of  ( Integer  pid ,   String  pname ,   double  price ,   Integer  stock )   { \n         return   new   ProductEntity (  pid ,   pname ,   price ,   stock ) ; \n     } \n\n     public   ProductEntity ( )   { \n     } \n\n     public   void   setPid ( Integer  pid )   { \n         this . pid  =  pid ; \n     } \n\n     public   void   setPname ( String  pname )   { \n         this . pname  =  pname ; \n     } \n\n     public   void   setPrice ( double  price )   { \n         this . price  =  price ; \n     } \n\n     public   void   setStock ( Integer  stock )   { \n         this . stock  =  stock ; \n     } \n\n     public   ProductEntity ( Integer  pid ,   String  pname ,   double  price ,   Integer  stock )   { \n         this . pid  =  pid ; \n         this . pname  =  pname ; \n         this . price  =  price ; \n         this . stock  =  stock ; \n     } \n\n     public   Integer   getPid ( )   { \n         return  pid ; \n     } \n\n     public   String   getPname ( )   { \n         return  pname ; \n     } \n\n     public   double   getPrice ( )   { \n         return  price ; \n     } \n\n     public   Integer   getStock ( )   { \n         return  stock ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "ProductEntity{"   + \n                 "pid="   +  pid  + \n                 ", pname=\'"   +  pname  +   \'\\\'\'   + \n                 ", price="   +  price  + \n                 ", stock="   +  stock  + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 \n \n SqlOnHive \n \n \n / export / server / spark3 / bin / spark - submit  -- master yarn  -- deploy - mode client  -- class = com . gordon . sql . SqlOnHive   / export / server / spark3 / examples / jars / apache - beam - 1.0 - SNAPSHOT . jar  -- runner = org . apache . beam . runners . spark . SparkRunner \n \n 1 \n \n \n \n \n Flink 1.2之后强调流批一体，与之比较有什么不同？ \n Apache beam 如何做到流批一体？ \n 源码解读--如何将beam code 转化成对应runner的pipeline。 \n'},{title:"flink",frontmatter:{title:"flink",date:"2019-08-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["计算引擎"],tags:["实时计算","流批一体"]},regularPath:"/%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/flink.html",relativePath:"计算引擎/flink.md",key:"v-49f14a1b",path:"/2019/08/08/flink/",headers:[{level:2,title:"前言",slug:"前言"},{level:3,title:"发展历史",slug:"发展历史"},{level:3,title:"官方介绍",slug:"官方介绍"},{level:3,title:"组件栈",slug:"组件栈"},{level:3,title:"应用场景",slug:"应用场景"},{level:2,title:"Flink安装部署",slug:"flink安装部署"},{level:3,title:"local本地模式-了解",slug:"local本地模式-了解"},{level:3,title:"Standalone独立集群模式-了解",slug:"standalone独立集群模式-了解"},{level:3,title:"Standalone-HA高可用集群模式-了解",slug:"standalone-ha高可用集群模式-了解"},{level:3,title:"Flink-On-Yarn-开发使用",slug:"flink-on-yarn-开发使用"},{level:2,title:"Flink入门案例",slug:"flink入门案例"},{level:2,title:"Flink核心概念",slug:"flink核心概念"},{level:3,title:"角色分工",slug:"角色分工"},{level:3,title:"DataFlow、Operator、Partition、Parallelism、SubTask",slug:"dataflow、operator、partition、parallelism、subtask"},{level:3,title:"OperatorChain和Task",slug:"operatorchain和task"},{level:3,title:"TaskSlot和TaskSlotSharing",slug:"taskslot和taskslotsharing"},{level:3,title:"执行流程图生成",slug:"执行流程图生成"},{level:3,title:"流处理说明",slug:"流处理说明"},{level:2,title:"Operator分类",slug:"operator分类"},{level:3,title:"Source",slug:"source"},{level:3,title:"Transformation",slug:"transformation"},{level:3,title:"Sink",slug:"sink"},{level:2,title:"Flink四大基石",slug:"flink四大基石"},{level:3,title:"Window",slug:"window"},{level:3,title:"Time/Watermarker",slug:"time-watermarker"},{level:3,title:"State",slug:"state"},{level:3,title:"Checkpoint",slug:"checkpoint"},{level:2,title:"BroadcastState-动态更新规则配置（存内存）",slug:"broadcaststate-动态更新规则配置-存内存"},{level:2,title:"Flink-双流Join",slug:"flink-双流join"},{level:2,title:"Flink-End-to-End Exactly-Once",slug:"flink-end-to-end-exactly-once"},{level:2,title:"Flink-异步IO-了解",slug:"flink-异步io-了解"},{level:2,title:"Flink-Streaming Flie Sink（新版本弃用，整合到File sink）",slug:"flink-streaming-flie-sink-新版本弃用-整合到file-sink"},{level:2,title:"Flink监控",slug:"flink监控"},{level:2,title:"Flink内存管理",slug:"flink内存管理"},{level:2,title:"Flink性能优化",slug:"flink性能优化"},{level:3,title:"问题定位口诀",slug:"问题定位口诀"},{level:3,title:"常见性能问题",slug:"常见性能问题"},{level:3,title:"经典场景调优",slug:"经典场景调优"},{level:3,title:"内存调优",slug:"内存调优"},{level:2,title:"Flink Table&SQL",slug:"flink-table-sql"},{level:3,title:"两种Table planners",slug:"两种table-planners"},{level:3,title:"核心概念",slug:"核心概念"},{level:3,title:"Flink SQL空闲状态保留时间（idle state retention time）",slug:"flink-sql空闲状态保留时间-idle-state-retention-time"},{level:3,title:"Flink-高级特性-新特性-FlinkSQL整合Hive",slug:"flink-高级特性-新特性-flinksql整合hive"},{level:3,title:"整合kafka和hive",slug:"整合kafka和hive"},{level:3,title:"Flink-练习-双十一实时交易大屏-掌握",slug:"flink-练习-双十一实时交易大屏-掌握"},{level:3,title:"Flink-练习-订单自动好评-掌握",slug:"flink-练习-订单自动好评-掌握"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:' 前言 \n 发展历史 \n 官方介绍 \n 组件栈 \n 应用场景 \n 所有的流式计算 \n Flink安装部署 \n local本地模式-了解 \n 原理 \n \n 操作 \n 1.下载安装包 \n https://archive.apache.org/dist/flink/ \n 2.上传flink-1.12.0-bin-scala_2.12.tgz到node1的指定目录 \n 3.解压 \n tar -zxvf flink-1.12.0-bin-scala_2.12.tgz \n 4.如果出现权限问题，需要修改权限 \n chown -R root:root /export/server/flink-1.12.0 \n 5.改名或创建软链接 \n mv flink-1.12.0 flink \n ln -s /export/server/flink-1.12.0 /export/server/flink \n 测试 \n 1.准备文件/root/words.txt \n vim /root/words.txt \n hello me you her\nhello me you\nhello me\nhello\n \n 1 2 3 4 2.启动Flink本地“集群” \n /export/server/flink/bin/start-cluster.sh \n 3.使用jps可以查看到下面两个进程 \n - TaskManagerRunner \n - StandaloneSessionClusterEntrypoint \n 4.访问Flink的Web UI \n http://node1:8081/#/overview \n \n slot在Flink里面可以认为是资源组，Flink是通过将任务分成子任务并且将这些子任务分配到slot来并行执行程序。 \n 5.执行官方示例 \n /export/server/flink/bin/flink run /export/server/flink/examples/batch/WordCount.jar  --input  /root/words.txt  --output  /root/out\n \n 1 6.停止Flink \n /export/server/flink/bin/stop-cluster.sh \n 启动shell交互式窗口(目前所有Scala 2.12版本的安装包暂时都不支持 Scala Shell) \n /export/server/flink/bin/start-scala-shell.sh local \n 执行如下命令 \n benv.readTextFile("/root/words.txt").flatMap(_.split(" ")).map((_,1)).groupBy(0).sum(1).print()\n \n 1 退出shell \n :quit \n Standalone独立集群模式-了解 \n 原理 \n \n 操作 \n 1.集群规划: \n - 服务器: node1(Master + Slave): JobManager + TaskManager \n - 服务器: node2(Slave): TaskManager \n - 服务器: node3(Slave): TaskManager \n 2.修改flink-conf.yaml \n vim /export/server/flink/conf/flink-conf.yaml \n jobmanager.rpc.address: node1\ntaskmanager.numberOfTaskSlots: 2\nweb.submit.enable: true\n\n#历史服务器\njobmanager.archive.fs.dir: hdfs://node1:8020/flink/completed-jobs/\nhistoryserver.web.address: node1\nhistoryserver.web.port: 8082\nhistoryserver.archive.fs.dir: hdfs://node1:8020/flink/completed-jobs/\n \n 1 2 3 4 5 6 7 8 9 2.修改masters \n vim /export/server/flink/conf/masters \n node1:8081\n \n 1 3.修改slaves \n vim /export/server/flink/conf/workers \n node1\nnode2\nnode3\n \n 1 2 3 4.添加HADOOP_CONF_DIR环境变量 \n vim /etc/profile \n export HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop\n \n 1 5.分发 \n scp -r /export/server/flink node2:/export/server/flink \n scp -r /export/server/flink node3:/export/server/flink \n scp  /etc/profile node2:/etc/profile \n scp  /etc/profile node3:/etc/profile \n 或 \n  for i in {2..3}; do scp -r flink node$i:$PWD; done\n \n 1 6.source \n source /etc/profile \n 测试 \n 1.启动集群，在node1上执行如下命令 \n /export/server/flink/bin/start-cluster.sh \n 或者单独启动 \n /export/server/flink/bin/jobmanager.sh ((start|start-foreground) cluster)|stop|stop-all \n /export/server/flink/bin/taskmanager.sh start|start-foreground|stop|stop-all \n 2.启动历史服务器 \n ​\t/export/server/flink/bin/historyserver.sh start \n 3.访问Flink UI界面或使用jps查看 \n http://node1:8081/#/overview \n http://node1:8082/#/overview \n 4.执行官方测试案例 \n /export/server/flink/bin/flink run /export/server/flink/examples/batch/WordCount.jar\n \n 1 6.停止Flink集群 \n /export/server/flink/bin/stop-cluster.sh \n Standalone-HA高可用集群模式-了解 \n 原理 \n \n 操作 \n 1.集群规划 \n - 服务器: node1(Master + Slave): JobManager + TaskManager \n - 服务器: node2(Master + Slave): JobManager + TaskManager \n - 服务器: node3(Slave): TaskManager \n 2.启动ZooKeeper \n zkServer.sh status \n zkServer.sh stop \n zkServer.sh start \n 3.启动HDFS \n /export/serves/hadoop/sbin/start-dfs.sh \n 4.停止Flink集群 \n /export/server/flink/bin/stop-cluster.sh \n 5.修改flink-conf.yaml \n vim /export/server/flink/conf/flink-conf.yaml \n 增加如下内容G \n state.backend: filesystem\nstate.backend.fs.checkpointdir: hdfs://node1:8020/flink-checkpoints\nhigh-availability: zookeeper\nhigh-availability.storageDir: hdfs://node1:8020/flink/ha/\nhigh-availability.zookeeper.quorum: node1:2181,node2:2181,node3:2181\n \n 1 2 3 4 5 6.修改masters \n vim /export/server/flink/conf/masters \n 7.同步 \n scp -r /export/server/flink/conf/flink-conf.yaml node2:/export/server/flink/conf/\nscp -r /export/server/flink/conf/flink-conf.yaml node3:/export/server/flink/conf/\nscp -r /export/server/flink/conf/masters node2:/export/server/flink/conf/\nscp -r /export/server/flink/conf/masters node3:/export/server/flink/conf/\n \n 1 2 3 4 8.修改node2上的flink-conf.yaml \n vim /export/server/flink/conf/flink-conf.yaml \n jobmanager.rpc.address: node2\n \n 1 9.重新启动Flink集群,node1上执行 \n /export/server/flink/bin/stop-cluster.sh \n /export/server/flink/bin/start-cluster.sh \n \n 10.使用jps命令查看 \n 发现没有Flink相关进程被启动 \n 11.查看日志 \n cat /export/server/flink/log/flink-root-standalonesession-0-node1.log \n 发现如下错误 \n \n 因为在Flink1.8版本后,Flink官方提供的安装包里没有整合HDFS的jar \n 12.下载jar包并在Flink的lib目录下放入该jar包并分发使Flink能够支持对Hadoop的操作 \n 下载地址 \n https://flink.apache.org/downloads.html \n 13.放入lib目录 \n cd /export/server/flink/lib \n \n 14.分发 \n for i in {2..3}; do scp -r flink-shaded-hadoop-2-uber-2.7.5-10.0.jar node$i:$PWD; done \n 15.重新启动Flink集群,node1上执行 \n /export/server/flink/bin/stop-cluster.sh \n /export/server/flink/bin/start-cluster.sh \n 16.使用jps命令查看,发现三台机器已经ok \n 测试 \n 1.访问WebUI \n http://node1:8081/#/job-manager/config \n http://node2:8081/#/job-manager/config \n 2.执行wc \n /export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar \n 3.kill掉其中一个master \n 4.重新执行wc,还是可以正常执行 \n /export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar \n 3.停止集群 \n /export/server/flink/bin/stop-cluster.sh \n Flink-On-Yarn-开发使用 \n 原理 \n 两种模式 \n Session会话模式 \n \n Job分离模式 \n \n 操作 \n 1.关闭yarn的内存检查 \n vim /export/server/hadoop/etc/hadoop/yarn-site.xml \n  \x3c!-- 关闭yarn内存检查 --\x3e\n    <property>\n        <name>yarn.nodemanager.pmem-check-enabled</name>\n        <value>false</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.vmem-check-enabled</name>\n        <value>false</value>\n    </property>\n \n 1 2 3 4 5 6 7 8 9 2.分发 \n scp -r /export/server/hadoop/etc/hadoop/yarn-site.xml node2:/export/server/hadoop/etc/hadoop/yarn-site.xml\nscp -r /export/server/hadoop/etc/hadoop/yarn-site.xml node3:/export/server/hadoop/etc/hadoop/yarn-site.xml\n \n 1 2 3.重启yarn \n /export/server/hadoop/sbin/stop-yarn.sh \n /export/server/hadoop/sbin/start-yarn.sh \n 测试 \n Session会话模式 \n 在Yarn上启动一个Flink集群,并重复使用该集群,后续提交的任务都是给该集群,资源会被一直占用,除非手动关闭该集群----适用于大量的小任务 \n 1.在yarn上启动一个Flink集群/会话，node1上执行以下命令 \n \n /export/server/flink/bin/yarn-session.sh -n 2 -tm 800 -s 1 -d \n \n 说明: \n 申请2个CPU、1600M内存 \n-n 表示申请2个容器，这里指的就是多少个taskmanager \n-tm 表示每个TaskManager的内存大小 \n-s 表示每个TaskManager的slots数量 \n-d 表示以后台程序方式运行 \n 注意: \n 该警告不用管 \n WARN  org.apache.hadoop.hdfs.DFSClient  - Caught exception \n java.lang.InterruptedException \n 2.查看UI界面 \n http://node1:8088/cluster \n \n 3.使用flink run提交任务： \n /export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar \n 运行完之后可以继续运行其他的小任务 \n /export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar \n 4.通过上方的ApplicationMaster可以进入Flink的管理界面 \n \n \n ==5.关闭yarn-session：== \n yarn application -kill application_1609508087977_0005 \n \n Job分离模式--用的更多 \n 针对每个Flink任务在Yarn上启动一个独立的Flink集群并运行,结束后自动关闭并释放资源,----适用于大任务 \n 1.直接提交job \n /export/server/flink/bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 /export/server/flink/examples/batch/WordCount.jar \n-m  jobmanager的地址 \n-yjm 1024 指定jobmanager的内存信息 \n-ytm 1024 指定taskmanager的内存信息 \n 2.查看UI界面 \n http://node1:8088/cluster \n \n \n 参数说明 \n /export/server/flink/bin/flink run  -m  yarn-cluster  -yjm   1024   -ytm   1024  /export/server/flink/examples/batch/WordCount.jar/export/server/flink/bin/flink  --help \nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding  in   [ jar:file:/export/server/flink/lib/log4j-slf4j-impl-2.12.1.jar ! /org/slf4j/impl/StaticLoggerBinder.class ] \nSLF4J: Found binding  in   [ jar:file:/export/server/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar ! /org/slf4j/impl/StaticLoggerBinder.class ] \nSLF4J: See http://www.slf4j.org/codes.html #multiple_bindings for an explanation. \nSLF4J: Actual binding is of  type   [ org.apache.logging.slf4j.Log4jLoggerFactory ] \n./flink  < ACTION >   [ OPTIONS ]   [ ARGUMENTS ] \n\nThe following actions are available:\n\nAction  "run"  compiles and runs a program.\n\n  Syntax: run  [ OPTIONS ]   < jar-file >   < arguments > \n   "run"  action options:\n     -c,--class  < classname >                Class with the program entry point\n                                           ( "main()"  method ) . Only needed  if  the\n                                          JAR  file  does not specify the class  in \n                                          its manifest.\n     -C,--classpath  < url >                  Adds a URL to each user code\n                                          classloader  on all nodes  in  the\n                                          cluster. The paths must specify a\n                                          protocol  ( e.g. file:// )  and be\n                                          accessible on all nodes  ( e.g. by means\n                                          of a NFS share ) . You can use this\n                                          option multiple  times   for  specifying\n                                           more  than one URL. The protocol must\n                                          be supported by the  { @link\n                                          java.net.URLClassLoader } .\n     -d,--detached                        If present, runs the job  in  detached\n                                          mode\n     -n,--allowNonRestoredState           Allow to skip savepoint state that\n                                          cannot be restored. You need to allow\n                                          this  if  you removed an operator from\n                                          your program that was part of the\n                                          program when the savepoint was\n                                          triggered.\n     -p,--parallelism  < parallelism >        The parallelism with  which  to run the\n                                          program. Optional flag to override the\n                                          default value specified  in  the\n                                          configuration.\n     -py,--python  < pythonFile >             Python script with the program entry\n                                          point. The dependent resources can be\n                                          configured with the  ` --pyFiles ` \n                                          option.\n     -pyarch,--pyArchives  < arg >            Add python archive files  for  job. The\n                                          archive files will be extracted to the\n                                          working directory of python UDF\n                                          worker. Currently only zip-format is\n                                          supported. For each archive file, a\n                                          target directory be specified. If the\n                                          target directory name is specified,\n                                          the archive  file  will be extracted to\n                                          a name can directory with the\n                                          specified name. Otherwise, the archive\n                                           file  will be extracted to a directory\n                                          with the same name of the archive\n                                          file. The files uploaded via this\n                                          option are accessible via relative\n                                          path.  \'#\'  could be used as the\n                                          separator of the archive  file  path and\n                                          the target directory name. Comma  ( \',\' ) \n                                          could be used as the separator to\n                                          specify multiple archive files. This\n                                          option can be used to upload the\n                                          virtual environment, the data files\n                                          used  in  Python UDF  ( e.g.:  --pyArchives \n                                          file:///tmp/py37.zip,file:///tmp/data.\n                                           zip #data --pyExecutable \n                                          py37.zip/py37/bin/python ) . The data\n                                          files could be accessed  in  Python UDF,\n                                          e.g.: f  =  open ( \'data/data.txt\' ,  \'r\' ) .\n     -pyexec,--pyExecutable  < arg >          Specify the path of the python\n                                          interpreter used to execute the python\n                                          UDF worker  ( e.g.:  --pyExecutable \n                                          /usr/local/bin/python3 ) . The python\n                                          UDF worker depends on Python  3.5 +,\n                                          Apache Beam  ( version  ==   2.23 .0 ) , Pip\n                                           ( version  >=   7.1 .0 )  and SetupTools\n                                           ( version  >=   37.0 .0 ) . Please ensure\n                                          that the specified environment meets\n                                          the above requirements.\n     -pyfs,--pyFiles  < pythonFiles >         Attach custom python files  for  job.\n                                          These files will be added to the\n                                          PYTHONPATH of both the  local  client\n                                          and the remote python UDF worker. The\n                                          standard python resource  file  suffixes\n                                          such as .py/.egg/.zip or directory are\n                                          all supported. Comma  ( \',\' )  could be\n                                          used as the separator to specify\n                                          multiple files  ( e.g.:  --pyFiles \n                                          file:///tmp/myresource.zip,hdfs:/// $na \n                                          menode_address/myresource2.zip ) .\n     -pym,--pyModule  < pythonModule >        Python module with the program entry\n                                          point. This option must be used  in \n                                          conjunction with  ` --pyFiles ` . \n     -pyreq,--pyRequirements  < arg >         Specify a requirements.txt  file   which \n                                          defines the third-party dependencies.\n                                          These dependencies will be installed\n                                          and added to the PYTHONPATH of the\n                                          python UDF worker. A directory  which \n                                          contains the installation packages of\n                                          these dependencies could be specified\n                                          optionally. Use  \'#\'  as the separator\n                                           if  the optional parameter exists\n                                           ( e.g.:  --pyRequirements \n                                          file:///tmp/requirements.txt #file:///t \n                                          mp/cached_dir ) .\n     -s,--fromSavepoint  < savepointPath >    Path to a savepoint to restore the job\n                                          from  ( for example\n                                          hdfs:///flink/savepoint-1537 ) .\n     -sae,--shutdownOnAttachedExit        If the job is submitted  in  attached\n                                          mode, perform a best-effort cluster\n                                           shutdown  when the CLI is terminated\n                                          abruptly, e.g.,  in  response to a user\n                                          interrupt, such as typing Ctrl + C.\n  Options  for  Generic CLI mode:\n      -D   < property = value >    Allows specifying multiple generic configuration\n                           options. The available options can be found at\n                           https://ci.apache.org/projects/flink/flink-docs-stabl\n                           e/ops/config.html\n     -e,--executor  < arg >    DEPRECATED: Please use the  -t  option instead  which  is\n                           also available with the  "Application Mode" . \n                           The name of the executor to be used  for  executing the\n                           given job,  which  is equivalent to the\n                            "execution.target"  config option. The currently\n                           available executors are:  "remote" ,  "local" ,\n                            "kubernetes-session" ,  "yarn-per-job" ,  "yarn-session" . \n     -t,--target  < arg >      The deployment target  for  the given application,\n                            which  is equivalent to the  "execution.target"  config\n                           option. For the  "run"  action the currently available\n                           targets are:  "remote" ,  "local" ,  "kubernetes-session" ,\n                            "yarn-per-job" ,  "yarn-session" .  For the\n                            "run-application"  action the currently available\n                           targets are:  "kubernetes-application" ,\n                            "yarn-application" . \n\n  Options  for  yarn-cluster mode:\n     -d,--detached                        If present, runs the job  in  detached\n                                          mode\n     -m,--jobmanager  < arg >                 Set to yarn-cluster to use YARN\n                                          execution mode.\n     -yat,--yarnapplicationType  < arg >      Set a custom application  type   for  the\n                                          application on YARN\n      -yD   < property = value >                  use value  for  given property\n     -yd,--yarndetached                   If present, runs the job  in  detached\n                                          mode  ( deprecated ;  use non-YARN\n                                          specific option instead ) \n     -yh,--yarnhelp                       Help  for  the Yarn session CLI.\n     -yid,--yarnapplicationId  < arg >        Attach to running YARN session\n     -yj,--yarnjar  < arg >                   Path to Flink jar  file \n     -yjm,--yarnjobManagerMemory  < arg >     Memory  for  JobManager Container with\n                                          optional unit  ( default: MB ) \n     -ynl,--yarnnodeLabel  < arg >            Specify YARN  node  label  for  the YARN\n                                          application\n     -ynm,--yarnname  < arg >                 Set a custom name  for  the application\n                                          on YARN\n     -yq,--yarnquery                      Display available YARN resources\n                                           ( memory, cores ) \n     -yqu,--yarnqueue  < arg >                Specify YARN queue.\n     -ys,--yarnslots  < arg >                 Number of slots per TaskManager\n     -yt,--yarnship  < arg >                  Ship files  in  the specified directory\n                                           ( t  for  transfer ) \n     -ytm,--yarntaskManagerMemory  < arg >    Memory per TaskManager Container with\n                                          optional unit  ( default: MB ) \n     -yz,--yarnzookeeperNamespace  < arg >    Namespace to create the Zookeeper\n                                          sub-paths  for  high availability mode\n     -z,--zookeeperNamespace  < arg >         Namespace to create the Zookeeper\n                                          sub-paths  for  high availability mode\n\n  Options  for  default mode:\n      -D   < property = value >              Allows specifying multiple generic\n                                     configuration options. The available\n                                     options can be found at\n                                     https://ci.apache.org/projects/flink/flink-\n                                     docs-stable/ops/config.html\n     -m,--jobmanager  < arg >            Address of the JobManager to  which  to\n                                     connect. Use this flag to connect to a\n                                     different JobManager than the one specified\n                                      in  the configuration. Attention: This\n                                     option is respected only  if  the\n                                     high-availability configuration is NONE.\n     -z,--zookeeperNamespace  < arg >    Namespace to create the Zookeeper sub-paths\n                                      for  high availability mode\n\n\n\nAction  "run-application"  runs an application  in  Application Mode.\n\n  Syntax: run-application  [ OPTIONS ]   < jar-file >   < arguments > \n  Options  for  Generic CLI mode:\n      -D   < property = value >    Allows specifying multiple generic configuration\n                           options. The available options can be found at\n                           https://ci.apache.org/projects/flink/flink-docs-stabl\n                           e/ops/config.html\n     -e,--executor  < arg >    DEPRECATED: Please use the  -t  option instead  which  is\n                           also available with the  "Application Mode" . \n                           The name of the executor to be used  for  executing the\n                           given job,  which  is equivalent to the\n                            "execution.target"  config option. The currently\n                           available executors are:  "remote" ,  "local" ,\n                            "kubernetes-session" ,  "yarn-per-job" ,  "yarn-session" . \n     -t,--target  < arg >      The deployment target  for  the given application,\n                            which  is equivalent to the  "execution.target"  config\n                           option. For the  "run"  action the currently available\n                           targets are:  "remote" ,  "local" ,  "kubernetes-session" ,\n                            "yarn-per-job" ,  "yarn-session" .  For the\n                            "run-application"  action the currently available\n                           targets are:  "kubernetes-application" ,\n                            "yarn-application" . \n\n\n\nAction  "info"  shows the optimized execution plan of the program  ( JSON ) .\n\n  Syntax: info  [ OPTIONS ]   < jar-file >   < arguments > \n   "info"  action options:\n     -c,--class  < classname >            Class with the program entry point\n                                       ( "main()"  method ) . Only needed  if  the JAR\n                                       file  does not specify the class  in  its\n                                      manifest.\n     -p,--parallelism  < parallelism >    The parallelism with  which  to run the\n                                      program. Optional flag to override the\n                                      default value specified  in  the\n                                      configuration.\n\n\nAction  "list"  lists running and scheduled programs.\n\n  Syntax: list  [ OPTIONS ] \n   "list"  action options:\n     -a,--all         Show all programs and their JobIDs\n     -r,--running     Show only running programs and their JobIDs\n     -s,--scheduled   Show only scheduled programs and their JobIDs\n  Options  for  Generic CLI mode:\n      -D   < property = value >    Allows specifying multiple generic configuration\n                           options. The available options can be found at\n                           https://ci.apache.org/projects/flink/flink-docs-stabl\n                           e/ops/config.html\n     -e,--executor  < arg >    DEPRECATED: Please use the  -t  option instead  which  is\n                           also available with the  "Application Mode" . \n                           The name of the executor to be used  for  executing the\n                           given job,  which  is equivalent to the\n                            "execution.target"  config option. The currently\n                           available executors are:  "remote" ,  "local" ,\n                            "kubernetes-session" ,  "yarn-per-job" ,  "yarn-session" . \n     -t,--target  < arg >      The deployment target  for  the given application,\n                            which  is equivalent to the  "execution.target"  config\n                           option. For the  "run"  action the currently available\n                           targets are:  "remote" ,  "local" ,  "kubernetes-session" ,\n                            "yarn-per-job" ,  "yarn-session" .  For the\n                            "run-application"  action the currently available\n                           targets are:  "kubernetes-application" ,\n                            "yarn-application" . \n\n  Options  for  yarn-cluster mode:\n     -m,--jobmanager  < arg >             Set to yarn-cluster to use YARN execution\n                                      mode.\n     -yid,--yarnapplicationId  < arg >    Attach to running YARN session\n     -z,--zookeeperNamespace  < arg >     Namespace to create the Zookeeper\n                                      sub-paths  for  high availability mode\n\n  Options  for  default mode:\n      -D   < property = value >              Allows specifying multiple generic\n                                     configuration options. The available\n                                     options can be found at\n                                     https://ci.apache.org/projects/flink/flink-\n                                     docs-stable/ops/config.html\n     -m,--jobmanager  < arg >            Address of the JobManager to  which  to\n                                     connect. Use this flag to connect to a\n                                     different JobManager than the one specified\n                                      in  the configuration. Attention: This\n                                     option is respected only  if  the\n                                     high-availability configuration is NONE.\n     -z,--zookeeperNamespace  < arg >    Namespace to create the Zookeeper sub-paths\n                                      for  high availability mode\n\n\n\nAction  "stop"  stops a running program with a savepoint  ( streaming  jobs  only ) .\n\n  Syntax: stop  [ OPTIONS ]   < Job ID > \n   "stop"  action options:\n     -d,--drain                           Send MAX_WATERMARK before taking the\n                                          savepoint and stopping the pipelne.\n     -p,--savepointPath  < savepointPath >    Path to the savepoint  ( for example\n                                          hdfs:///flink/savepoint-1537 ) . If no\n                                          directory is specified, the configured\n                                          default will be used\n                                           ( "state.savepoints.dir" ) .\n  Options  for  Generic CLI mode:\n      -D   < property = value >    Allows specifying multiple generic configuration\n                           options. The available options can be found at\n                           https://ci.apache.org/projects/flink/flink-docs-stabl\n                           e/ops/config.html\n     -e,--executor  < arg >    DEPRECATED: Please use the  -t  option instead  which  is\n                           also available with the  "Application Mode" . \n                           The name of the executor to be used  for  executing the\n                           given job,  which  is equivalent to the\n                            "execution.target"  config option. The currently\n                           available executors are:  "remote" ,  "local" ,\n                            "kubernetes-session" ,  "yarn-per-job" ,  "yarn-session" . \n     -t,--target  < arg >      The deployment target  for  the given application,\n                            which  is equivalent to the  "execution.target"  config\n                           option. For the  "run"  action the currently available\n                           targets are:  "remote" ,  "local" ,  "kubernetes-session" ,\n                            "yarn-per-job" ,  "yarn-session" .  For the\n                            "run-application"  action the currently available\n                           targets are:  "kubernetes-application" ,\n                            "yarn-application" . \n\n  Options  for  yarn-cluster mode:\n     -m,--jobmanager  < arg >             Set to yarn-cluster to use YARN execution\n                                      mode.\n     -yid,--yarnapplicationId  < arg >    Attach to running YARN session\n     -z,--zookeeperNamespace  < arg >     Namespace to create the Zookeeper\n                                      sub-paths  for  high availability mode\n\n  Options  for  default mode:\n      -D   < property = value >              Allows specifying multiple generic\n                                     configuration options. The available\n                                     options can be found at\n                                     https://ci.apache.org/projects/flink/flink-\n                                     docs-stable/ops/config.html\n     -m,--jobmanager  < arg >            Address of the JobManager to  which  to\n                                     connect. Use this flag to connect to a\n                                     different JobManager than the one specified\n                                      in  the configuration. Attention: This\n                                     option is respected only  if  the\n                                     high-availability configuration is NONE.\n     -z,--zookeeperNamespace  < arg >    Namespace to create the Zookeeper sub-paths\n                                      for  high availability mode\n\n\n\nAction  "cancel"  cancels a running program.\n\n  Syntax: cancel  [ OPTIONS ]   < Job ID > \n   "cancel"  action options:\n     -s,--withSavepoint  < targetDirectory >    **DEPRECATION WARNING**: Cancelling\n                                            a job with savepoint is deprecated.\n                                            Use  "stop"  instead.\n                                            Trigger savepoint and cancel job.\n                                            The target directory is optional. If\n                                            no directory is specified, the\n                                            configured default directory\n                                             ( state.savepoints.dir )  is used.\n  Options  for  Generic CLI mode:\n      -D   < property = value >    Allows specifying multiple generic configuration\n                           options. The available options can be found at\n                           https://ci.apache.org/projects/flink/flink-docs-stabl\n                           e/ops/config.html\n     -e,--executor  < arg >    DEPRECATED: Please use the  -t  option instead  which  is\n                           also available with the  "Application Mode" . \n                           The name of the executor to be used  for  executing the\n                           given job,  which  is equivalent to the\n                            "execution.target"  config option. The currently\n                           available executors are:  "remote" ,  "local" ,\n                            "kubernetes-session" ,  "yarn-per-job" ,  "yarn-session" . \n     -t,--target  < arg >      The deployment target  for  the given application,\n                            which  is equivalent to the  "execution.target"  config\n                           option. For the  "run"  action the currently available\n                           targets are:  "remote" ,  "local" ,  "kubernetes-session" ,\n                            "yarn-per-job" ,  "yarn-session" .  For the\n                            "run-application"  action the currently available\n                           targets are:  "kubernetes-application" ,\n                            "yarn-application" . \n\n  Options  for  yarn-cluster mode:\n     -m,--jobmanager  < arg >             Set to yarn-cluster to use YARN execution\n                                      mode.\n     -yid,--yarnapplicationId  < arg >    Attach to running YARN session\n     -z,--zookeeperNamespace  < arg >     Namespace to create the Zookeeper\n                                      sub-paths  for  high availability mode\n\n  Options  for  default mode:\n      -D   < property = value >              Allows specifying multiple generic\n                                     configuration options. The available\n                                     options can be found at\n                                     https://ci.apache.org/projects/flink/flink-\n                                     docs-stable/ops/config.html\n     -m,--jobmanager  < arg >            Address of the JobManager to  which  to\n                                     connect. Use this flag to connect to a\n                                     different JobManager than the one specified\n                                      in  the configuration. Attention: This\n                                     option is respected only  if  the\n                                     high-availability configuration is NONE.\n     -z,--zookeeperNamespace  < arg >    Namespace to create the Zookeeper sub-paths\n                                      for  high availability mode\n\n\n\nAction  "savepoint"  triggers savepoints  for  a running job or disposes existing ones.\n\n  Syntax: savepoint  [ OPTIONS ]   < Job ID >   [ < target directory > ] \n   "savepoint"  action options:\n     -d,--dispose  < arg >        Path of savepoint to dispose.\n     -j,--jarfile  < jarfile >    Flink program JAR file.\n  Options  for  Generic CLI mode:\n      -D   < property = value >    Allows specifying multiple generic configuration\n                           options. The available options can be found at\n                           https://ci.apache.org/projects/flink/flink-docs-stabl\n                           e/ops/config.html\n     -e,--executor  < arg >    DEPRECATED: Please use the  -t  option instead  which  is\n                           also available with the  "Application Mode" . \n                           The name of the executor to be used  for  executing the\n                           given job,  which  is equivalent to the\n                            "execution.target"  config option. The currently\n                           available executors are:  "remote" ,  "local" ,\n                            "kubernetes-session" ,  "yarn-per-job" ,  "yarn-session" . \n     -t,--target  < arg >      The deployment target  for  the given application,\n                            which  is equivalent to the  "execution.target"  config\n                           option. For the  "run"  action the currently available\n                           targets are:  "remote" ,  "local" ,  "kubernetes-session" ,\n                            "yarn-per-job" ,  "yarn-session" .  For the\n                            "run-application"  action the currently available\n                           targets are:  "kubernetes-application" ,\n                            "yarn-application" . \n\n  Options  for  yarn-cluster mode:\n     -m,--jobmanager  < arg >             Set to yarn-cluster to use YARN execution\n                                      mode.\n     -yid,--yarnapplicationId  < arg >    Attach to running YARN session\n     -z,--zookeeperNamespace  < arg >     Namespace to create the Zookeeper\n                                      sub-paths  for  high availability mode\n\n  Options  for  default mode:\n      -D   < property = value >              Allows specifying multiple generic\n                                     configuration options. The available\n                                     options can be found at\n                                     https://ci.apache.org/projects/flink/flink-\n                                     docs-stable/ops/config.html\n     -m,--jobmanager  < arg >            Address of the JobManager to  which  to\n                                     connect. Use this flag to connect to a\n                                     different JobManager than the one specified\n                                      in  the configuration. Attention: This\n                                     option is respected only  if  the\n                                     high-availability configuration is NONE.\n     -z,--zookeeperNamespace  < arg >    Namespace to create the Zookeeper sub-paths\n                                      for  high availability mode\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 #  Flink入门案例 \n 前置说明 \n \n 注意:入门案例使用DataSet后续就不再使用了,而是使用流批一体的DataStream \n \n \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/ \n \n 准备环境 \n \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 " \n          xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     < groupId > cn.itcast </ groupId > \n     < artifactId > flink_study_47 </ artifactId > \n     < version > 1.0-SNAPSHOT </ version > \n     \x3c!-- 指定仓库位置，依次为aliyun、apache和cloudera仓库 --\x3e \n     < repositories > \n         < repository > \n             < id > aliyun </ id > \n             < url > http://maven.aliyun.com/nexus/content/groups/public/ </ url > \n         </ repository > \n         < repository > \n             < id > apache </ id > \n             < url > https://repository.apache.org/content/repositories/snapshots/ </ url > \n         </ repository > \n         < repository > \n             < id > cloudera </ id > \n             < url > https://repository.cloudera.com/artifactory/cloudera-repos/ </ url > \n         </ repository > \n     </ repositories > \n\n     < properties > \n         < encoding > UTF-8 </ encoding > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < maven.compiler.source > 1.8 </ maven.compiler.source > \n         < maven.compiler.target > 1.8 </ maven.compiler.target > \n         < java.version > 1.8 </ java.version > \n         < scala.version > 2.12 </ scala.version > \n         < flink.version > 1.12.0 </ flink.version > \n     </ properties > \n     < dependencies > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-clients_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-scala_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-java </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-scala_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-java_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-table-api-scala-bridge_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-table-api-java-bridge_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         \x3c!-- flink执行计划,这是1.9版本之前的--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-table-planner_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         \x3c!-- blink执行计划,1.11+默认的--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-table-planner-blink_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-table-common </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n\n         \x3c!--<dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-cep_2.12</artifactId>\n            <version>${flink.version}</version>\n        </dependency>--\x3e \n\n         \x3c!-- flink连接器--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-connector-kafka_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-sql-connector-kafka_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-connector-jdbc_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-csv </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-json </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n\n         \x3c!-- <dependency>\n           <groupId>org.apache.flink</groupId>\n           <artifactId>flink-connector-filesystem_2.12</artifactId>\n           <version>${flink.version}</version>\n       </dependency>--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-jdbc_2.12</artifactId>\n            <version>${flink.version}</version>\n        </dependency>--\x3e \n         \x3c!--<dependency>\n              <groupId>org.apache.flink</groupId>\n              <artifactId>flink-parquet_2.12</artifactId>\n              <version>${flink.version}</version>\n         </dependency>--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.avro</groupId>\n            <artifactId>avro</artifactId>\n            <version>1.9.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.parquet</groupId>\n            <artifactId>parquet-avro</artifactId>\n            <version>1.10.0</version>\n        </dependency>--\x3e \n\n\n         < dependency > \n             < groupId > org.apache.bahir </ groupId > \n             < artifactId > flink-connector-redis_2.11 </ artifactId > \n             < version > 1.0 </ version > \n             < exclusions > \n                 < exclusion > \n                     < artifactId > flink-streaming-java_2.11 </ artifactId > \n                     < groupId > org.apache.flink </ groupId > \n                 </ exclusion > \n                 < exclusion > \n                     < artifactId > flink-runtime_2.11 </ artifactId > \n                     < groupId > org.apache.flink </ groupId > \n                 </ exclusion > \n                 < exclusion > \n                     < artifactId > flink-core </ artifactId > \n                     < groupId > org.apache.flink </ groupId > \n                 </ exclusion > \n                 < exclusion > \n                     < artifactId > flink-java </ artifactId > \n                     < groupId > org.apache.flink </ groupId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-connector-hive_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.hive </ groupId > \n             < artifactId > hive-metastore </ artifactId > \n             < version > 2.1.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.hive </ groupId > \n             < artifactId > hive-exec </ artifactId > \n             < version > 2.1.0 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-shaded-hadoop-2-uber </ artifactId > \n             < version > 2.7.5-10.0 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.hbase </ groupId > \n             < artifactId > hbase-client </ artifactId > \n             < version > 2.1.0 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > mysql </ groupId > \n             < artifactId > mysql-connector-java </ artifactId > \n             < version > 5.1.38 </ version > \n             \x3c!--<version>8.0.20</version>--\x3e \n         </ dependency > \n\n         \x3c!-- 高性能异步组件：Vertx--\x3e \n         < dependency > \n             < groupId > io.vertx </ groupId > \n             < artifactId > vertx-core </ artifactId > \n             < version > 3.9.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > io.vertx </ groupId > \n             < artifactId > vertx-jdbc-client </ artifactId > \n             < version > 3.9.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > io.vertx </ groupId > \n             < artifactId > vertx-redis-client </ artifactId > \n             < version > 3.9.0 </ version > \n         </ dependency > \n\n         \x3c!-- 日志 --\x3e \n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-log4j12 </ artifactId > \n             < version > 1.7.7 </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > log4j </ groupId > \n             < artifactId > log4j </ artifactId > \n             < version > 1.2.17 </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n\n         < dependency > \n             < groupId > com.alibaba </ groupId > \n             < artifactId > fastjson </ artifactId > \n             < version > 1.2.44 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.projectlombok </ groupId > \n             < artifactId > lombok </ artifactId > \n             < version > 1.18.2 </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n         \x3c!-- 参考：https://blog.csdn.net/f641385712/article/details/84109098--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.commons</groupId>\n            <artifactId>commons-collections4</artifactId>\n            <version>4.4</version>\n        </dependency>--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.thrift</groupId>\n            <artifactId>libfb303</artifactId>\n            <version>0.9.3</version>\n            <type>pom</type>\n            <scope>provided</scope>\n         </dependency>--\x3e \n         \x3c!--<dependency>\n           <groupId>com.google.guava</groupId>\n           <artifactId>guava</artifactId>\n           <version>28.2-jre</version>\n       </dependency>--\x3e \n\n     </ dependencies > \n\n     < build > \n         < sourceDirectory > src/main/java </ sourceDirectory > \n         < plugins > \n             \x3c!-- 编译插件 --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.5.1 </ version > \n                 < configuration > \n                     < source > 1.8 </ source > \n                     < target > 1.8 </ target > \n                     \x3c!--<encoding>${project.build.sourceEncoding}</encoding>--\x3e \n                 </ configuration > \n             </ plugin > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-surefire-plugin </ artifactId > \n                 < version > 2.18.1 </ version > \n                 < configuration > \n                     < useFile > false </ useFile > \n                     < disableXmlReport > true </ disableXmlReport > \n                     < includes > \n                         < include > **/*Test.* </ include > \n                         < include > **/*Suite.* </ include > \n                     </ includes > \n                 </ configuration > \n             </ plugin > \n             \x3c!-- 打包插件(会包含所有依赖) --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-shade-plugin </ artifactId > \n                 < version > 2.3 </ version > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < goals > \n                             < goal > shade </ goal > \n                         </ goals > \n                         < configuration > \n                             < filters > \n                                 < filter > \n                                     < artifact > *:* </ artifact > \n                                     < excludes > \n                                         \x3c!--\n                                        zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --\x3e \n                                         < exclude > META-INF/*.SF </ exclude > \n                                         < exclude > META-INF/*.DSA </ exclude > \n                                         < exclude > META-INF/*.RSA </ exclude > \n                                     </ excludes > \n                                 </ filter > \n                             </ filters > \n                             < transformers > \n                                 < transformer   implementation = " org.apache.maven.plugins.shade.resource.ManifestResourceTransformer " > \n                                     \x3c!-- 设置jar包的入口类(可选) --\x3e \n                                     < mainClass > </ mainClass > \n                                 </ transformer > \n                             </ transformers > \n                         </ configuration > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n         </ plugins > \n     </ build > \n </ project > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 \n \n 代码实现-DataSet-了解 \n \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . java . DataSet ; \n import   org . apache . flink . api . java . ExecutionEnvironment ; \n import   org . apache . flink . api . java . operators . AggregateOperator ; \n import   org . apache . flink . api . java . operators . UnsortedGrouping ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc 演示Flink-DataSet-API-实现WordCount\n */ \n public   class   WordCount   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         ExecutionEnvironment  env  =   ExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //TODO 1.source \n         DataSet < String >  lines  =  env . fromElements ( "itcast hadoop spark" ,   "itcast hadoop spark" ,   "itcast hadoop" ,   "itcast" ) ; \n\n         //TODO 2.transformation \n         //切割 \n         /*\n        @FunctionalInterface\n        public interface FlatMapFunction<T, O> extends Function, Serializable {\n            void flatMap(T value, Collector<O> out) throws Exception;\n        }\n         */ \n         DataSet < String >  words  =  lines . flatMap ( new   FlatMapFunction < String ,   String > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < String >  out )   throws   Exception   { \n                 //value表示每一行数据 \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( word ) ; \n                 } \n             } \n         } ) ; \n\n         //记为1 \n         /*\n        @FunctionalInterface\n        public interface MapFunction<T, O> extends Function, Serializable {\n            O map(T value) throws Exception;\n        }\n         */ \n         DataSet < Tuple2 < String ,   Integer > >  wordAndOne  =  words . map ( new   MapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   Tuple2 < String ,   Integer >   map ( String  value )   throws   Exception   { \n                 //value就是每一个单词 \n                 return   Tuple2 . of ( value ,   1 ) ; \n             } \n         } ) ; \n\n         //分组 \n         UnsortedGrouping < Tuple2 < String ,   Integer > >  grouped  =  wordAndOne . groupBy ( 0 ) ; \n\n         //聚合 \n         AggregateOperator < Tuple2 < String ,   Integer > >  result  =  grouped . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 代码实现-DataStream-匿名内部类-处理批 \n \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author\n * Desc 演示Flink-DataStream-API-实现WordCount\n * 注意:在Flink1.12中DataStream既支持流处理也支持批处理,如何区分?\n */ \n public   class   WordCount2   {  \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         //ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //env.setRuntimeMode(RuntimeExecutionMode.BATCH);//注意:使用DataStream实现批处理 \n         //env.setRuntimeMode(RuntimeExecutionMode.STREAMING);//注意:使用DataStream实现流处理 \n         //env.setRuntimeMode(RuntimeExecutionMode.AUTOMATIC);//注意:使用DataStream根据数据源自动选择使用流还是批 \n\n         //TODO 1.source \n         //DataSet<String> lines = env.fromElements("itcast hadoop spark", "itcast hadoop spark", "itcast hadoop", "itcast"); \n         DataStream < String >  lines  =  env . fromElements ( "itcast hadoop spark" ,   "itcast hadoop spark" ,   "itcast hadoop" ,   "itcast" ) ; \n\n         //TODO 2.transformation \n         //切割 \n         /*\n        @FunctionalInterface\n        public interface FlatMapFunction<T, O> extends Function, Serializable {\n            void flatMap(T value, Collector<O> out) throws Exception;\n        }\n         */ \n         DataStream < String >  words  =  lines . flatMap ( new   FlatMapFunction < String ,   String > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < String >  out )   throws   Exception   { \n                 //value就是每一行数据 \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( word ) ; \n                 } \n             } \n         } ) ; \n\n         //记为1 \n         /*\n        @FunctionalInterface\n        public interface MapFunction<T, O> extends Function, Serializable {\n            O map(T value) throws Exception;\n        }\n         */ \n         DataStream < Tuple2 < String ,   Integer > >  wordAndOne  =  words . map ( new   MapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   Tuple2 < String ,   Integer >   map ( String  value )   throws   Exception   { \n                 //value就是一个个单词 \n                 return   Tuple2 . of ( value ,   1 ) ; \n             } \n         } ) ; \n\n         //分组:注意DataSet中分组是groupBy,DataStream分组是keyBy \n         //wordAndOne.keyBy(0); \n         /*\n        @FunctionalInterface\n        public interface KeySelector<IN, KEY> extends Function, Serializable {\n            KEY getKey(IN value) throws Exception;\n        }\n         */ \n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  grouped  =  wordAndOne . keyBy ( t  ->  t . f0 ) ; \n\n         //聚合 \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  grouped . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute/启动并等待程序结束 \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 代码实现-DataStream-匿名内部类-处理流 \n package   cn . itcast . hello ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * \n * Desc 演示Flink-DataStream-API-实现WordCount\n * 注意:在Flink1.12中DataStream既支持流处理也支持批处理,如何区分?\n */ \n public   class   WordCount3   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         //ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //env.setRuntimeMode(RuntimeExecutionMode.BATCH);//注意:使用DataStream实现批处理 \n         //env.setRuntimeMode(RuntimeExecutionMode.STREAMING);//注意:使用DataStream实现流处理 \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; //注意:使用DataStream根据数据源自动选择使用流还是批 \n\n         //TODO 1.source \n         //DataSet<String> lines = env.fromElements("itcast hadoop spark", "itcast hadoop spark", "itcast hadoop", "itcast"); \n         //DataStream<String> lines = env.fromElements("itcast hadoop spark", "itcast hadoop spark", "itcast hadoop", "itcast"); \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n         //TODO 2.transformation \n         //切割 \n         /*\n        @FunctionalInterface\n        public interface FlatMapFunction<T, O> extends Function, Serializable {\n            void flatMap(T value, Collector<O> out) throws Exception;\n        }\n         */ \n         DataStream < String >  words  =  lines . flatMap ( new   FlatMapFunction < String ,   String > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < String >  out )   throws   Exception   { \n                 //value就是每一行数据 \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( word ) ; \n                 } \n             } \n         } ) ; \n\n         //记为1 \n         /*\n        @FunctionalInterface\n        public interface MapFunction<T, O> extends Function, Serializable {\n            O map(T value) throws Exception;\n        }\n         */ \n         DataStream < Tuple2 < String ,   Integer > >  wordAndOne  =  words . map ( new   MapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   Tuple2 < String ,   Integer >   map ( String  value )   throws   Exception   { \n                 //value就是一个个单词 \n                 return   Tuple2 . of ( value ,   1 ) ; \n             } \n         } ) ; \n\n         //分组:注意DataSet中分组是groupBy,DataStream分组是keyBy \n         //wordAndOne.keyBy(0); \n         /*\n        @FunctionalInterface\n        public interface KeySelector<IN, KEY> extends Function, Serializable {\n            KEY getKey(IN value) throws Exception;\n        }\n         */ \n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  grouped  =  wordAndOne . keyBy ( t  ->  t . f0 ) ; \n\n         //聚合 \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  grouped . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute/启动并等待程序结束 \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 代码实现-DataStream-Lambda \n package   cn . itcast . hello ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . typeinfo . Types ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . Arrays ; \n\n /**\n * Author itcast\n * Desc 演示Flink-DataStream-API-实现WordCount\n * 注意:在Flink1.12中DataStream既支持流处理也支持批处理,如何区分?\n */ \n public   class   WordCount4   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         //ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //env.setRuntimeMode(RuntimeExecutionMode.BATCH);//注意:使用DataStream实现批处理 \n         //env.setRuntimeMode(RuntimeExecutionMode.STREAMING);//注意:使用DataStream实现流处理 \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; //注意:使用DataStream根据数据源自动选择使用流还是批 \n\n         //TODO 1.source \n         //DataSet<String> lines = env.fromElements("itcast hadoop spark", "itcast hadoop spark", "itcast hadoop", "itcast"); \n         DataStream < String >  lines  =  env . fromElements ( "itcast hadoop spark" ,   "itcast hadoop spark" ,   "itcast hadoop" ,   "itcast" ) ; \n\n         //TODO 2.transformation \n         //切割 \n         /*\n        @FunctionalInterface\n        public interface FlatMapFunction<T, O> extends Function, Serializable {\n            void flatMap(T value, Collector<O> out) throws Exception;\n        }\n         */ \n         /*DataStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public void flatMap(String value, Collector<String> out) throws Exception {\n                //value就是每一行数据\n                String[] arr = value.split(" ");\n                for (String word : arr) {\n                    out.collect(word);\n                }\n            }\n        });*/ \n         SingleOutputStreamOperator < String >  words  =  lines . flatMap ( \n                 ( String  value ,   Collector < String >  out )   ->   Arrays . stream ( value . split ( " " ) ) . forEach ( out :: collect ) \n         ) . returns ( Types . STRING ) ; \n\n\n         //记为1 \n         /*\n        @FunctionalInterface\n        public interface MapFunction<T, O> extends Function, Serializable {\n            O map(T value) throws Exception;\n        }\n         */ \n         /*DataStream<Tuple2<String, Integer>> wordAndOne = words.map(new MapFunction<String, Tuple2<String, Integer>>() {\n            @Override\n            public Tuple2<String, Integer> map(String value) throws Exception {\n                //value就是一个个单词\n                return Tuple2.of(value, 1);\n            }\n        });*/ \n         DataStream < Tuple2 < String ,   Integer > >  wordAndOne  =  words . map ( \n                 ( String  value )   ->   Tuple2 . of ( value ,   1 ) \n         ) . returns ( Types . TUPLE ( Types . STRING , Types . INT ) ) ; \n\n         //分组:注意DataSet中分组是groupBy,DataStream分组是keyBy \n         //wordAndOne.keyBy(0); \n         /*\n        @FunctionalInterface\n        public interface KeySelector<IN, KEY> extends Function, Serializable {\n            KEY getKey(IN value) throws Exception;\n        }\n         */ \n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  grouped  =  wordAndOne . keyBy ( t  ->  t . f0 ) ; \n\n         //聚合 \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  grouped . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute/启动并等待程序结束 \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 代码实现-On-Yarn-掌握 \n import   org . apache . flink . api . common . typeinfo . Types ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . api . java . utils . ParameterTool ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . Arrays ; \n\n /**\n * \n * Desc 演示Flink-DataStream-API-实现WordCount\n * 注意:在Flink1.12中DataStream既支持流处理也支持批处理,如何区分?\n */ \n public   class   WordCount5_Yarn   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         ParameterTool  parameterTool  =   ParameterTool . fromArgs ( args ) ; \n         String  output  =   "" ; \n         if   ( parameterTool . has ( "output" ) )   { \n            output  =  parameterTool . get ( "output" ) ; \n             System . out . println ( "指定了输出路径使用:"   +  output ) ; \n         }   else   { \n            output  =   "hdfs://node1:8020/wordcount/output47_" ; \n             System . out . println ( "可以指定输出路径使用 --output ,没有指定使用默认的:"   +  output ) ; \n         } \n\n         //TODO 0.env \n         //ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //env.setRuntimeMode(RuntimeExecutionMode.BATCH);//注意:使用DataStream实现批处理 \n         //env.setRuntimeMode(RuntimeExecutionMode.STREAMING);//注意:使用DataStream实现流处理 \n         //env.setRuntimeMode(RuntimeExecutionMode.AUTOMATIC);//注意:使用DataStream根据数据源自动选择使用流还是批 \n\n         //TODO 1.source \n         //DataSet<String> lines = env.fromElements("itcast hadoop spark", "itcast hadoop spark", "itcast hadoop", "itcast"); \n         DataStream < String >  lines  =  env . fromElements ( "itcast hadoop spark" ,   "itcast hadoop spark" ,   "itcast hadoop" ,   "itcast" ) ; \n\n         //TODO 2.transformation \n         //切割 \n         /*\n        @FunctionalInterface\n        public interface FlatMapFunction<T, O> extends Function, Serializable {\n            void flatMap(T value, Collector<O> out) throws Exception;\n        }\n         */ \n         /*DataStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public void flatMap(String value, Collector<String> out) throws Exception {\n                //value就是每一行数据\n                String[] arr = value.split(" ");\n                for (String word : arr) {\n                    out.collect(word);\n                }\n            }\n        });*/ \n         SingleOutputStreamOperator < String >  words  =  lines . flatMap ( \n                 ( String  value ,   Collector < String >  out )   ->   Arrays . stream ( value . split ( " " ) ) . forEach ( out :: collect ) \n         ) . returns ( Types . STRING ) ; \n\n\n         //记为1 \n         /*\n        @FunctionalInterface\n        public interface MapFunction<T, O> extends Function, Serializable {\n            O map(T value) throws Exception;\n        }\n         */ \n         /*DataStream<Tuple2<String, Integer>> wordAndOne = words.map(new MapFunction<String, Tuple2<String, Integer>>() {\n            @Override\n            public Tuple2<String, Integer> map(String value) throws Exception {\n                //value就是一个个单词\n                return Tuple2.of(value, 1);\n            }\n        });*/ \n         DataStream < Tuple2 < String ,   Integer > >  wordAndOne  =  words . map ( \n                 ( String  value )   ->   Tuple2 . of ( value ,   1 ) \n         ) . returns ( Types . TUPLE ( Types . STRING ,   Types . INT ) ) ; \n\n         //分组:注意DataSet中分组是groupBy,DataStream分组是keyBy \n         //wordAndOne.keyBy(0); \n         /*\n        @FunctionalInterface\n        public interface KeySelector<IN, KEY> extends Function, Serializable {\n            KEY getKey(IN value) throws Exception;\n        }\n         */ \n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  grouped  =  wordAndOne . keyBy ( t  ->  t . f0 ) ; \n\n         //聚合 \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  grouped . sum ( 1 ) ; \n\n         //TODO 3.sink \n         //如果执行报hdfs权限相关错误,可以执行 hadoop fs -chmod -R 777  / \n         System . setProperty ( "HADOOP_USER_NAME" ,   "root" ) ; //设置用户名 \n         //result.print(); \n         //result.writeAsText("hdfs://node1:8020/wordcount/output47_"+System.currentTimeMillis()).setParallelism(1); \n        result . writeAsText ( output  +   System . currentTimeMillis ( ) ) . setParallelism ( 1 ) ; \n\n         //TODO 4.execute/启动并等待程序结束 \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 打包改名上传 \n \n 提交 \n /export/server/flink/bin/flink run -Dexecution.runtime-mode = BATCH  -m  yarn-cluster  -yjm   1024   -ytm   1024   -c  cn.itcast.hello.WordCount5_Yarn /root/wc.jar  --output  hdfs://node1:8020/wordcount/output_xx\n \n 1 注意 \n RuntimeExecutionMode.BATCH//使用DataStream实现批处理\nRuntimeExecutionMode.STREAMING//使用DataStream实现流处理\nRuntimeExecutionMode.AUTOMATIC//使用DataStream根据数据源自动选择使用流还是批\n//如果不指定,默认是流\n \n 1 2 3 4 在后续的Flink开发中,把一切数据源看做流即可或者使用AUTOMATIC就行了 \n Flink1.12使用StreamingFileSink写hdfs会有truncate file fail失败问题。 \n 解决方案： \n 1.使用cdh版本的hadoop \n 2.手动释放租约（spark读取的时候进行修复） \n Flink核心概念 \n 角色分工 \n \n 执行流程 \n \n DataFlow \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/glossary.html \n DataFlow、Operator、Partition、Parallelism、SubTask \n OperatorChain和Task \n TaskSlot和TaskSlotSharing \n 执行流程图生成 \n 流处理说明 \n 无边界的流unbounded stream:真正的流数据 \n Operator分类 \n Source \n 基于集合 \n \n package   cn . itcast . source ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n import   java . util . Arrays ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Source-基于集合\n */ \n public   class   SourceDemo01_Collection   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  ds1  =  env . fromElements ( "hadoop spark flink" ,   "hadoop spark flink" ) ; \n         DataStream < String >  ds2  =  env . fromCollection ( Arrays . asList ( "hadoop spark flink" ,   "hadoop spark flink" ) ) ; \n         DataStream < Long >  ds3  =  env . generateSequence ( 1 ,   100 ) ; \n         DataStream < Long >  ds4  =  env . fromSequence ( 1 ,   100 ) ; \n\n         //TODO 2.transformation \n\n         //TODO 3.sink \n        ds1 . print ( ) ; \n        ds2 . print ( ) ; \n        ds3 . print ( ) ; \n        ds4 . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #  基于文件 \n \n package   cn . itcast . source ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Source-基于本地/HDFS的文件/文件夹/压缩文件\n */ \n public   class   SourceDemo02_File   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  ds1  =  env . readTextFile ( "data/input/words.txt" ) ; \n         DataStream < String >  ds2  =  env . readTextFile ( "data/input/dir" ) ; \n         DataStream < String >  ds3  =  env . readTextFile ( "data/input/wordcount.txt.gz" ) ; \n\n\n         //TODO 2.transformation \n\n         //TODO 3.sink \n        ds1 . print ( ) ; \n        ds2 . print ( ) ; \n        ds3 . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #  基于Socket \n \n package   cn . itcast . source ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Source-基于Socket\n */ \n public   class   SourceDemo03_Socket   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n\n         //TODO 2.transformation \n         /*SingleOutputStreamOperator<String> words = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public void flatMap(String value, Collector<String> out) throws Exception {\n                String[] arr = value.split(" ");\n                for (String word : arr) {\n                    out.collect(word);\n                }\n            }\n        });\n\n        words.map(new MapFunction<String, Tuple2<String,Integer>>() {\n            @Override\n            public Tuple2<String, Integer> map(String value) throws Exception {\n                return Tuple2.of(value,1);\n            }\n        });*/ \n\n         //注意:下面的操作将上面的2步合成了1步,直接切割单词并记为1返回 \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  wordAndOne  =  lines . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n\n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  wordAndOne . keyBy ( t  ->  t . f0 ) . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 #  自定义Source-随机订单数据 \n 注意: lombok的使用 \n \n \n \n package   cn . itcast . source ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichParallelSourceFunction ; \n\n import   java . util . Random ; \n import   java . util . UUID ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Source-自定义数据源\n * 需求:\n */ \n public   class   SourceDemo04_Customer   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < Order >  orderDS  =  env . addSource ( new   MyOrderSource ( ) ) . setParallelism ( 2 ) ; \n\n         //TODO 2.transformation \n\n         //TODO 3.sink \n        orderDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   Order { \n         private   String  id ; \n         private   Integer  userId ; \n         private   Integer  money ; \n         private   Long  createTime ; \n     } \n     public   static   class   MyOrderSource   extends   RichParallelSourceFunction < Order > { \n\n         private   Boolean  flag  =   true ; \n         //执行并生成数据 \n         @Override \n         public   void   run ( SourceContext < Order >  ctx )   throws   Exception   { \n             Random  random  =   new   Random ( ) ; \n             while   ( flag )   { \n                 String  oid  =   UUID . randomUUID ( ) . toString ( ) ; \n                 int  userId  =  random . nextInt ( 3 ) ; \n                 int  money  =  random . nextInt ( 101 ) ; \n                 long  createTime  =   System . currentTimeMillis ( ) ; \n                ctx . collect ( new   Order ( oid , userId , money , createTime ) ) ; \n                 Thread . sleep ( 1000 ) ; \n             } \n         } \n\n         //执行cancel命令的时候执行 \n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 #  自定义Source-MySQL \n \n package   cn . itcast . source ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichParallelSourceFunction ; \n\n import   java . sql . Connection ; \n import   java . sql . DriverManager ; \n import   java . sql . PreparedStatement ; \n import   java . sql . ResultSet ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Source-自定义数据源-MySQL\n * 需求:\n */ \n public   class   SourceDemo05_Customer_MySQL   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < Student >  studentDS  =  env . addSource ( new   MySQLSource ( ) ) . setParallelism ( 1 ) ; \n\n         //TODO 2.transformation \n\n         //TODO 3.sink \n        studentDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n\n    /*\n   CREATE TABLE `t_student` (\n    `id` int(11) NOT NULL AUTO_INCREMENT,\n    `name` varchar(255) DEFAULT NULL,\n    `age` int(11) DEFAULT NULL,\n    PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8;\n\nINSERT INTO `t_student` VALUES (\'1\', \'jack\', \'18\');\nINSERT INTO `t_student` VALUES (\'2\', \'tom\', \'19\');\nINSERT INTO `t_student` VALUES (\'3\', \'rose\', \'20\');\nINSERT INTO `t_student` VALUES (\'4\', \'tom\', \'19\');\nINSERT INTO `t_student` VALUES (\'5\', \'jack\', \'18\');\nINSERT INTO `t_student` VALUES (\'6\', \'rose\', \'20\');\n    */ \n\n     @Data \n     @NoArgsConstructor \n     @AllArgsConstructor \n     public   static   class   Student   { \n         private   Integer  id ; \n         private   String  name ; \n         private   Integer  age ; \n     } \n\n     public   static   class   MySQLSource   extends   RichParallelSourceFunction < Student >   { \n         private   boolean  flag  =   true ; \n         private   Connection  conn  =   null ; \n         private   PreparedStatement  ps  = null ; \n         private   ResultSet  rs   =   null ; \n         //open只执行一次,适合开启资源 \n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            conn  =   DriverManager . getConnection ( "jdbc:mysql://localhost:3306/bigdata" ,   "root" ,   "root" ) ; \n             String  sql  =   "select id,name,age from t_student" ; \n            ps  =  conn . prepareStatement ( sql ) ; \n         } \n\n         @Override \n         public   void   run ( SourceContext < Student >  ctx )   throws   Exception   { \n             while   ( flag )   { \n                rs  =  ps . executeQuery ( ) ; \n                 while   ( rs . next ( ) )   { \n                     int  id  =  rs . getInt ( "id" ) ; \n                     String  name  =  rs . getString ( "name" ) ; \n                     int  age   =  rs . getInt ( "age" ) ; \n                    ctx . collect ( new   Student ( id , name , age ) ) ; \n                 } \n                 Thread . sleep ( 5000 ) ; \n             } \n         } \n\n         //接收到cancel命令时取消数据生成 \n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n\n         //close里面关闭资源 \n         @Override \n         public   void   close ( )   throws   Exception   { \n             if ( conn  !=   null )  conn . close ( ) ; \n             if ( ps  !=   null )  ps . close ( ) ; \n             if ( rs  !=   null )  rs . close ( ) ; \n\n         } \n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 #  Transformation \n 基本操作 \n map/flatMap/filter/keyBy/sum/reduce... \n 和之前学习的Scala/Spark里面的一样的意思 \n 需求 \n 对流数据中的单词进行统计，排除敏感词TMD(Theater Missile Defense 战区导弹防御) \n package   cn . itcast . transformation ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FilterFunction ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . common . functions . ReduceFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Transformation-基本操作\n */ \n public   class   TransformationDemo01   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n\n         //TODO 2.transformation \n         DataStream < String >  words  =  lines . flatMap ( new   FlatMapFunction < String ,   String > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < String >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( word ) ; \n                 } \n             } \n         } ) ; \n\n         DataStream < String >  filted  =  words . filter ( new   FilterFunction < String > ( )   { \n             @Override \n             public   boolean   filter ( String  value )   throws   Exception   { \n                 return   ! value . equals ( "TMD" ) ; //如果是TMD则返回false表示过滤掉 \n             } \n         } ) ; \n\n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  wordAndOne  =  filted . map ( new   MapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   Tuple2 < String ,   Integer >   map ( String  value )   throws   Exception   { \n                 return   Tuple2 . of ( value ,   1 ) ; \n             } \n         } ) ; \n\n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  grouped  =  wordAndOne . keyBy ( t  ->  t . f0 ) ; \n\n         //SingleOutputStreamOperator<Tuple2<String, Integer>> result = grouped.sum(1); \n\n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  grouped . reduce ( new   ReduceFunction < Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   Tuple2 < String ,   Integer >   reduce ( Tuple2 < String ,   Integer >  value1 ,   Tuple2 < String ,   Integer >  value2 )   throws   Exception   { \n                 //Tuple2<String, Integer> value1 :进来的(单词,历史值) \n                 //Tuple2<String, Integer> value2 :进来的(单词,1) \n                 //需要返回(单词,数量) \n                 return   Tuple2 . of ( value1 . f0 ,  value1 . f1  +  value2 . f1 ) ;   //_+_ \n             } \n         } ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 #  合并和连接 \n \n package   cn . itcast . transformation ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . streaming . api . datastream . ConnectedStreams ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . co . CoMapFunction ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Transformation-合并和连接操作\n */ \n public   class   TransformationDemo02   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  ds1  =  env . fromElements ( "hadoop" ,   "spark" ,   "flink" ) ; \n         DataStream < String >  ds2  =  env . fromElements ( "hadoop" ,   "spark" ,   "flink" ) ; \n         DataStream < Long >  ds3  =  env . fromElements ( 1L ,   2L ,   3L ) ; \n\n         //TODO 2.transformation \n         DataStream < String >  result1  =  ds1 . union ( ds2 ) ; //注意union能合并同类型 \n         //ds1.union(ds3);//注意union不可以合并不同类型 \n         ConnectedStreams < String ,   String >  result2  =  ds1 . connect ( ds2 ) ; //注意:connect可以合并同类型 \n         ConnectedStreams < String ,   Long >  result3  =  ds1 . connect ( ds3 ) ; //注意conncet可以合并不同类型 \n\n         /*\n        public interface CoMapFunction<IN1, IN2, OUT> extends Function, Serializable {\n            OUT map1(IN1 value) throws Exception;\n            OUT map2(IN2 value) throws Exception;\n        }\n         */ \n         SingleOutputStreamOperator < String >  result  =  result3 . map ( new   CoMapFunction < String ,   Long ,   String > ( )   { \n             @Override \n             public   String   map1 ( String  value )   throws   Exception   { \n                 return   "String:"   +  value ; \n             } \n\n             @Override \n             public   String   map2 ( Long  value )   throws   Exception   { \n                 return   "Long:"   +  value ; \n             } \n         } ) ; \n\n\n         //TODO 3.sink \n        result1 . print ( ) ; \n         //result2.print();//注意:connect之后需要做其他的处理,不能直接输出 \n         //result3.print();//注意:connect之后需要做其他的处理,不能直接输出 \n        result . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 #  拆分和选择 \n \n package   cn . itcast . transformation ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . typeinfo . TypeInformation ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . ProcessFunction ; \n import   org . apache . flink . util . Collector ; \n import   org . apache . flink . util . OutputTag ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Transformation-拆分(split)和选择(select)操作\n * 注意split和select在flink1.12中已经过期并移除了\n * 所以得使用outPutTag和process来实现\n * 需求:对流中的数据按照奇数和偶数拆分并选择\n */ \n public   class   TransformationDemo03   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStreamSource < Integer >  ds  =  env . fromElements ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ) ; \n\n         //TODO 2.transformation \n         //需求:对流中的数据按照奇数和偶数拆分并选择 \n         OutputTag < Integer >  oddTag  =   new   OutputTag < > ( "奇数" ,   TypeInformation . of ( Integer . class ) ) ; \n         OutputTag < Integer >  evenTag  =   new   OutputTag < > ( "偶数" , TypeInformation . of ( Integer . class ) ) ; \n\n         /*\n        public abstract class ProcessFunction<I, O> extends AbstractRichFunction {\n            public abstract void processElement(I value, ProcessFunction.Context ctx, Collector<O> out) throws Exception;\n        }\n         */ \n         SingleOutputStreamOperator < Integer >  result  =  ds . process ( new   ProcessFunction < Integer ,   Integer > ( )   { \n             @Override \n             public   void   processElement ( Integer  value ,   Context  ctx ,   Collector < Integer >  out )   throws   Exception   { \n                 //out收集完的还是放在一起的,ctx可以将数据放到不同的OutputTag \n                 if   ( value  %   2   ==   0 )   { \n                    ctx . output ( evenTag ,  value ) ; \n                 }   else   { \n                    ctx . output ( oddTag ,  value ) ; \n                 } \n             } \n         } ) ; \n\n         DataStream < Integer >  oddResult  =  result . getSideOutput ( oddTag ) ; \n         DataStream < Integer >  evenResult  =  result . getSideOutput ( evenTag ) ; \n\n         //TODO 3.sink \n         System . out . println ( oddTag ) ; //OutputTag(Integer, 奇数) \n         System . out . println ( evenTag ) ; //OutputTag(Integer, 偶数) \n        oddResult . print ( "奇数:" ) ; \n        evenResult . print ( "偶数:" ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 #  rebalance重平衡分区 \n 解决数据倾斜的问题 \n \n \n package   cn . itcast . transformation ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FilterFunction ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Transformation-rebalance-重平衡分区\n */ \n public   class   TransformationDemo04   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < Long >  longDS  =  env . fromSequence ( 0 ,   100 ) ; \n         //下面的操作相当于将数据随机分配一下,有可能出现数据倾斜 \n         DataStream < Long >  filterDS  =  longDS . filter ( new   FilterFunction < Long > ( )   { \n             @Override \n             public   boolean   filter ( Long  num )   throws   Exception   { \n                 return  num  >   10 ; \n             } \n         } ) ; \n\n         //TODO 2.transformation \n         //没有经过rebalance有可能出现数据倾斜 \n         SingleOutputStreamOperator < Tuple2 < Integer ,   Integer > >  result1  =  filterDS\n                 . map ( new   RichMapFunction < Long ,   Tuple2 < Integer ,   Integer > > ( )   { \n                     @Override \n                     public   Tuple2 < Integer ,   Integer >   map ( Long  value )   throws   Exception   { \n                         int  subTaskId  =   getRuntimeContext ( ) . getIndexOfThisSubtask ( ) ; //子任务id/分区编号 \n                         return   Tuple2 . of ( subTaskId ,   1 ) ; \n                     } \n                     //按照子任务id/分区编号分组,并统计每个子任务/分区中有几个元素 \n                 } ) . keyBy ( t  ->  t . f0 ) . sum ( 1 ) ; \n\n         //调用了rebalance解决了数据倾斜 \n         SingleOutputStreamOperator < Tuple2 < Integer ,   Integer > >  result2  =  filterDS . rebalance ( ) \n                 . map ( new   RichMapFunction < Long ,   Tuple2 < Integer ,   Integer > > ( )   { \n                     @Override \n                     public   Tuple2 < Integer ,   Integer >   map ( Long  value )   throws   Exception   { \n                         int  subTaskId  =   getRuntimeContext ( ) . getIndexOfThisSubtask ( ) ; //子任务id/分区编号 \n                         return   Tuple2 . of ( subTaskId ,   1 ) ; \n                     } \n                     //按照子任务id/分区编号分组,并统计每个子任务/分区中有几个元素 \n                 } ) . keyBy ( t  ->  t . f0 ) . sum ( 1 ) ; \n\n\n         //TODO 3.sink \n        result1 . print ( "result1" ) ; \n        result2 . print ( "result2" ) ; \n\n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 其他分区操作 \n \n package   cn . itcast . transformation ; \n\n         import   org . apache . flink . api . common . RuntimeExecutionMode ; \n         import   org . apache . flink . api . common . functions . FlatMapFunction ; \n         import   org . apache . flink . api . common . functions . Partitioner ; \n         import   org . apache . flink . api . java . tuple . Tuple2 ; \n         import   org . apache . flink . streaming . api . datastream . DataStream ; \n         import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n         import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n         import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Transformation-各种分区\n */ \n public   class   TransformationDemo05   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  linesDS  =  env . readTextFile ( "data/input/words.txt" ) ; \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  tupleDS  =  linesDS . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  words  =  value . split ( " " ) ; \n                 for   ( String  word  :  words )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n\n         //TODO 2.transformation \n         DataStream < Tuple2 < String ,   Integer > >  result1  =  tupleDS . global ( ) ; \n         DataStream < Tuple2 < String ,   Integer > >  result2  =  tupleDS . broadcast ( ) ; \n         DataStream < Tuple2 < String ,   Integer > >  result3  =  tupleDS . forward ( ) ; \n         DataStream < Tuple2 < String ,   Integer > >  result4  =  tupleDS . shuffle ( ) ; \n         DataStream < Tuple2 < String ,   Integer > >  result5  =  tupleDS . rebalance ( ) ; \n         DataStream < Tuple2 < String ,   Integer > >  result6  =  tupleDS . rescale ( ) ; \n         DataStream < Tuple2 < String ,   Integer > >  result7  =  tupleDS . partitionCustom ( new   MyPartitioner ( ) ,  t  ->  t . f0 ) ; \n\n\n         //TODO 3.sink \n        result1 . print ( "result1" ) ; \n        result2 . print ( "result2" ) ; \n        result3 . print ( "result3" ) ; \n        result4 . print ( "result4" ) ; \n        result5 . print ( "result5" ) ; \n        result6 . print ( "result6" ) ; \n        result7 . print ( "result7" ) ; \n\n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     public   static   class   MyPartitioner   implements   Partitioner < String > { \n         @Override \n         public   int   partition ( String  key ,   int  numPartitions )   { \n             //if(key.equals("北京")) return 0;  这里写自己的分区逻辑即可 \n             return   0 ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 \n Sink \n 基于控制台和文件 \n \n package   cn . itcast . sink ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Sink-基于控制台和文件\n */ \n public   class   SinkDemo01   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  ds  =  env . readTextFile ( "data/input/words.txt" ) ; \n\n         //TODO 2.transformation \n         //TODO 3.sink \n        ds . print ( ) ; \n        ds . print ( "输出标识" ) ; \n        ds . printToErr ( ) ; //会在控制台上以红色输出 \n        ds . printToErr ( "输出标识" ) ; //会在控制台上以红色输出 \n        ds . writeAsText ( "data/output/result1" ) . setParallelism ( 1 ) ; \n        ds . writeAsText ( "data/output/result2" ) . setParallelism ( 2 ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #  自定义Sink \n \n package   cn . itcast . sink ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . sink . RichSinkFunction ; \n\n import   java . sql . Connection ; \n import   java . sql . DriverManager ; \n import   java . sql . PreparedStatement ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Sink-自定义Sink\n */ \n public   class   SinkDemo02   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < Student >  studentDS  =  env . fromElements ( new   Student ( null ,   "tony" ,   18 ) ) ; \n         //TODO 2.transformation \n         //TODO 3.sink \n        studentDS . addSink ( new   MySQLSink ( ) ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @NoArgsConstructor \n     @AllArgsConstructor \n     public   static   class   Student   { \n         private   Integer  id ; \n         private   String  name ; \n         private   Integer  age ; \n     } \n\n     public   static   class   MySQLSink   extends   RichSinkFunction < Student >   { \n         private   Connection  conn  =   null ; \n         private   PreparedStatement  ps  = null ; \n\n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            conn  =   DriverManager . getConnection ( "jdbc:mysql://localhost:3306/bigdata" ,   "root" ,   "root" ) ; \n             String  sql  =   "INSERT INTO `t_student` (`id`, `name`, `age`) VALUES (null, ?, ?);" ; \n            ps  =  conn . prepareStatement ( sql ) ; \n         } \n\n         @Override \n         public   void   invoke ( Student  value ,   Context  context )   throws   Exception   { \n             //设置?占位符参数值 \n            ps . setString ( 1 , value . getName ( ) ) ; \n            ps . setInt ( 2 , value . getAge ( ) ) ; \n             //执行sql \n            ps . executeUpdate ( ) ; \n         } \n         @Override \n         public   void   close ( )   throws   Exception   { \n             if ( conn  !=   null )  conn . close ( ) ; \n             if ( ps  !=   null )  ps . close ( ) ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Connectors \n JDBC \n package   cn . itcast . connectors ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . connector . jdbc . JdbcConnectionOptions ; \n import   org . apache . flink . connector . jdbc . JdbcSink ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n /**\n * Author itcast\n * Desc 演示Flink官方提供的JdbcSink\n */ \n public   class   JDBCDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < Student >  studentDS  =  env . fromElements ( new   Student ( null ,   "tony2" ,   18 ) ) ; \n         //TODO 2.transformation \n         //TODO 3.sink \n        studentDS . addSink ( JdbcSink . sink ( \n                 "INSERT INTO `t_student` (`id`, `name`, `age`) VALUES (null, ?, ?)" , \n                 ( ps ,  value )   ->   { \n                    ps . setString ( 1 ,  value . getName ( ) ) ; \n                    ps . setInt ( 2 ,  value . getAge ( ) ) ; \n                 } ,   new   JdbcConnectionOptions . JdbcConnectionOptionsBuilder ( ) \n                         . withUrl ( "jdbc:mysql://localhost:3306/bigdata" ) \n                         . withUsername ( "root" ) \n                         . withPassword ( "root" ) \n                         . withDriverName ( "com.mysql.jdbc.Driver" ) \n                         . build ( ) ) ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n\n     @Data \n     @NoArgsConstructor \n     @AllArgsConstructor \n     public   static   class   Student   { \n         private   Integer  id ; \n         private   String  name ; \n         private   Integer  age ; \n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 Kafka Consumer/Source \n 参数 \n env.addSource(new Kafka Consumer/Source(参数)) \n \n package   cn . itcast . connectors ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaConsumer ; \n\n import   java . util . Properties ; \n\n /**\n * Author itcast\n * Desc 演示Flink-Connectors-KafkaComsumer/Source\n */ \n public   class   KafkaComsumerDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         //准备kafka连接参数 \n         Properties  props   =   new   Properties ( ) ; \n        props . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; //集群地址 \n        props . setProperty ( "group.id" ,   "flink" ) ; //消费者组id \n        props . setProperty ( "auto.offset.reset" , "latest" ) ; //latest有offset记录从记录位置开始消费,没有记录从最新的/最后的消息开始消费 /earliest有offset记录从记录位置开始消费,没有记录从最早的/最开始的消息开始消费 \n        props . setProperty ( "flink.partition-discovery.interval-millis" , "5000" ) ; //会开启一个后台线程每隔5s检测一下Kafka的分区情况,实现动态分区检测,要使用字符串格式，否则返回值是null，设置不生效，比如 \n  //      prop.put("flink.partition-discovery.interval-millis", 60000); \n        props . setProperty ( "enable.auto.commit" ,   "true" ) ; //自动提交(提交到默认主题,后续学习了Checkpoint后随着Checkpoint存储在Checkpoint和默认主题中) \n        props . setProperty ( "auto.commit.interval.ms" ,   "2000" ) ; //自动提交的时间间隔 \n         //使用连接参数创建FlinkKafkaConsumer/kafkaSource \n         FlinkKafkaConsumer < String >  kafkaSource  =   new   FlinkKafkaConsumer < String > ( "flink_kafka" ,   new   SimpleStringSchema ( ) ,  props ) ; \n         //使用kafkaSource \n         DataStream < String >  kafkaDS  =  env . addSource ( kafkaSource ) ; \n\n         //TODO 2.transformation \n\n         //TODO 3.sink \n        kafkaDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n //准备主题 /export/server/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181 --replication-factor 2 --partitions 3 --topic flink_kafka \n //启动控制台生产者发送数据 /export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092 --topic flink_kafka \n //启动程序FlinkKafkaConsumer \n //观察控制台输出结果 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #  Kafka Producer/Sink \n 控制台生成者 ---\x3e flink_kafka主题 --\x3e Flink --\x3eetl ---\x3e flink_kafka2主题---\x3e控制台消费者 \n package   cn . itcast . connectors ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FilterFunction ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaConsumer ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaProducer ; \n\n import   java . util . Properties ; \n\n /**\n * Author itcast\n * Desc 演示Flink-Connectors-KafkaComsumer/Source + KafkaProducer/Sink\n */ \n public   class   KafkaSinkDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         //准备kafka连接参数 \n         Properties  props   =   new   Properties ( ) ; \n        props . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; //集群地址 \n        props . setProperty ( "group.id" ,   "flink" ) ; //消费者组id \n        props . setProperty ( "auto.offset.reset" , "latest" ) ; //latest有offset记录从记录位置开始消费,没有记录从最新的/最后的消息开始消费 /earliest有offset记录从记录位置开始消费,没有记录从最早的/最开始的消息开始消费 \n        props . setProperty ( "flink.partition-discovery.interval-millis" , "5000" ) ; //会开启一个后台线程每隔5s检测一下Kafka的分区情况,实现动态分区检测 \n        props . setProperty ( "enable.auto.commit" ,   "true" ) ; //自动提交(提交到默认主题,后续学习了Checkpoint后随着Checkpoint存储在Checkpoint和默认主题中) \n        props . setProperty ( "auto.commit.interval.ms" ,   "2000" ) ; //自动提交的时间间隔 \n         //使用连接参数创建FlinkKafkaConsumer/kafkaSource \n         FlinkKafkaConsumer < String >  kafkaSource  =   new   FlinkKafkaConsumer < String > ( "flink_kafka" ,   new   SimpleStringSchema ( ) ,  props ) ; \n         //使用kafkaSource \n         DataStream < String >  kafkaDS  =  env . addSource ( kafkaSource ) ; \n\n         //TODO 2.transformation \n         SingleOutputStreamOperator < String >  etlDS  =  kafkaDS . filter ( new   FilterFunction < String > ( )   { \n             @Override \n             public   boolean   filter ( String  value )   throws   Exception   { \n                 return  value . contains ( "success" ) ; \n             } \n         } ) ; \n\n         //TODO 3.sink \n        etlDS . print ( ) ; \n\n         Properties  props2  =   new   Properties ( ) ; \n        props2 . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; \n         FlinkKafkaProducer < String >  kafkaSink  =   new   FlinkKafkaProducer < > ( "flink_kafka2" ,   new   SimpleStringSchema ( ) ,  props2 ) ; \n        etlDS . addSink ( kafkaSink ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n //控制台生成者 ---\x3e flink_kafka主题 --\x3e Flink --\x3eetl ---\x3e flink_kafka2主题---\x3e控制台消费者 \n //准备主题 /export/server/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181 --replication-factor 2 --partitions 3 --topic flink_kafka \n //准备主题 /export/server/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181 --replication-factor 2 --partitions 3 --topic flink_kafka2 \n //启动控制台生产者发送数据 /export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092 --topic flink_kafka \n //log:2020-10-10 success xxx \n //log:2020-10-10 success xxx \n //log:2020-10-10 success xxx \n //log:2020-10-10 fail xxx \n //启动控制台消费者消费数据 /export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic flink_kafka2 --from-beginning \n //启动程序FlinkKafkaConsumer \n //观察控制台输出结果 \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 #  Redis \n https://bahir.apache.org/docs/flink/current/flink-streaming-redis/ \n \n 需求: \n 从Socket接收实时流数据,做WordCount,并将结果写入到Redis \n 数据结构使用: \n 单词:数量 (key-String, value-String) \n wcresult: 单词:数量 (key-String, value-Hash) \n 注意: Redis的Key始终是String, value可以是:String/Hash/List/Set/有序Set \n package   cn . itcast . connectors ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . connectors . redis . RedisSink ; \n import   org . apache . flink . streaming . connectors . redis . common . config . FlinkJedisPoolConfig ; \n import   org . apache . flink . streaming . connectors . redis . common . mapper . RedisCommand ; \n import   org . apache . flink . streaming . connectors . redis . common . mapper . RedisCommandDescription ; \n import   org . apache . flink . streaming . connectors . redis . common . mapper . RedisMapper ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc 演示Flink-Connectors-三方提供的RedisSink\n */ \n public   class   RedisDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n         //TODO 2.transformation \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  lines . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) . keyBy ( t  ->  t . f0 ) . sum ( 1 ) ; \n\n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         FlinkJedisPoolConfig  conf  =   new   FlinkJedisPoolConfig . Builder ( ) . setHost ( "127.0.0.1" ) . build ( ) ; \n         RedisSink < Tuple2 < String ,   Integer > >  redisSink  =   new   RedisSink < Tuple2 < String ,   Integer > > ( conf , new   MyRedisMapper ( ) ) ; \n        result . addSink ( redisSink ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     public   static   class   MyRedisMapper   implements   RedisMapper < Tuple2 < String ,   Integer > > { \n         @Override \n         public   RedisCommandDescription   getCommandDescription ( )   { \n             //我们选择的数据结构对应的是 key:String("wcresult"),value:Hash(单词,数量),命令为HSET \n             return   new   RedisCommandDescription ( RedisCommand . HSET , "wcresult" ) ; \n         } \n\n         @Override \n         public   String   getKeyFromData ( Tuple2 < String ,   Integer >  t )   { \n             return   t . f0 ; \n         } \n\n         @Override \n         public   String   getValueFromData ( Tuple2 < String ,   Integer >  t )   { \n             return  t . f1 . toString ( ) ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 #  Flink四大基石 \n Window \n 窗口的分类 \n \n \n 基于时间的滑动窗口(掌握) \n 基于时间的滚动窗口(掌握) \n 基于数量的滑动窗口(了解) \n 基于数量的滚动窗口(了解) \n \n API \n 基于时间的滚动和滑动-掌握 \n \n package   cn . itcast . window ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . windowing . assigners . SlidingProcessingTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . assigners . TumblingProcessingTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n\n /**\n * Author itcast\n * Desc 演示基于时间的滚动和滑动窗口\n */ \n public   class   WindowDemo_1_2   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n         //TODO 2.transformation \n         SingleOutputStreamOperator < CartInfo >  carDS  =  lines . map ( new   MapFunction < String ,   CartInfo > ( )   { \n             @Override \n             public   CartInfo   map ( String  value )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( "," ) ; \n                 return   new   CartInfo ( arr [ 0 ] ,   Integer . parseInt ( arr [ 1 ] ) ) ; \n             } \n         } ) ; \n\n         //注意: 需求中要求的是各个路口/红绿灯的结果,所以需要先分组 \n         //carDS.keyBy(car->car.getSensorId()) \n         KeyedStream < CartInfo ,   String >  keyedDS  =  carDS . keyBy ( CartInfo :: getSensorId ) ; \n         // * 需求1:每5秒钟统计一次，最近5秒钟内，各个路口通过红绿灯汽车的数量--基于时间的滚动窗口 \n         //keyedDS.timeWindow(Time.seconds(5)) \n         SingleOutputStreamOperator < CartInfo >  result1  =  keyedDS\n                 . window ( TumblingProcessingTimeWindows . of ( Time . seconds ( 5 ) ) ) \n                 . sum ( "count" ) ; \n         // * 需求2:每5秒钟统计一次，最近10秒钟内，各个路口通过红绿灯汽车的数量--基于时间的滑动窗口 \n         SingleOutputStreamOperator < CartInfo >  result2  =  keyedDS\n                 //of(Time size, Time slide) \n                 . window ( SlidingProcessingTimeWindows . of ( Time . seconds ( 10 ) , Time . seconds ( 5 ) ) ) \n                 . sum ( "count" ) ; \n\n         //TODO 3.sink \n         //result1.print(); \n        result2 . print ( ) ; \n /*\n1,5\n2,5\n3,5\n4,5\n*/ \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   CartInfo   { \n         private   String  sensorId ; //信号灯id \n         private   Integer  count ; //通过该信号灯的车的数量 \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 #  基于数量的滚动和滑动 \n \n package   cn . itcast . window ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n /**\n * Author itcast\n * Desc 演示基于数量的滚动和滑动窗口\n */ \n public   class   WindowDemo_3_4   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n         //TODO 2.transformation \n         SingleOutputStreamOperator < CartInfo >  carDS  =  lines . map ( new   MapFunction < String ,   CartInfo > ( )   { \n             @Override \n             public   CartInfo   map ( String  value )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( "," ) ; \n                 return   new   CartInfo ( arr [ 0 ] ,   Integer . parseInt ( arr [ 1 ] ) ) ; \n             } \n         } ) ; \n\n         //注意: 需求中要求的是各个路口/红绿灯的结果,所以需要先分组 \n         //carDS.keyBy(car->car.getSensorId()) \n         KeyedStream < CartInfo ,   String >  keyedDS  =  carDS . keyBy ( CartInfo :: getSensorId ) ; \n\n         // * 需求1:统计在最近5条消息中,各自路口通过的汽车数量,相同的key每出现5次进行统计--基于数量的滚动窗口 \n         SingleOutputStreamOperator < CartInfo >  result1  =  keyedDS\n                 . countWindow ( 5 ) \n                 . sum ( "count" ) ; \n         // * 需求2:统计在最近5条消息中,各自路口通过的汽车数量,相同的key每出现3次进行统计--基于数量的滑动窗口 \n         SingleOutputStreamOperator < CartInfo >  result2  =  keyedDS\n                 . countWindow ( 5 , 3 ) \n                 . sum ( "count" ) ; \n\n         //TODO 3.sink \n         //result1.print(); \n         /*\n1,1\n1,1\n1,1\n1,1\n2,1\n1,1\n         */ \n        result2 . print ( ) ; \n         /*\n1,1\n1,1\n2,1\n1,1\n2,1\n3,1\n4,1\n         */ \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   CartInfo   { \n         private   String  sensorId ; //信号灯id \n         private   Integer  count ; //通过该信号灯的车的数量 \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 #  Session会话窗口 \n (开始会话，就建立窗口，不会话超时一定时间关闭，还是针对所有数据) \n \n package   cn . itcast . window ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . windowing . assigners . ProcessingTimeSessionWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n\n /**\n * Author itcast\n * Desc 演示会话窗口\n */ \n public   class   WindowDemo_5   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n         //TODO 2.transformation \n         SingleOutputStreamOperator < CartInfo >  carDS  =  lines . map ( new   MapFunction < String ,   CartInfo > ( )   { \n             @Override \n             public   CartInfo   map ( String  value )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( "," ) ; \n                 return   new   CartInfo ( arr [ 0 ] ,   Integer . parseInt ( arr [ 1 ] ) ) ; \n             } \n         } ) ; \n\n         //注意: 需求中要求的是各个路口/红绿灯的结果,所以需要先分组 \n         //carDS.keyBy(car->car.getSensorId()) \n         KeyedStream < CartInfo ,   String >  keyedDS  =  carDS . keyBy ( CartInfo :: getSensorId ) ; \n\n         //需求:设置会话超时时间为10s,10s内没有数据到来,则触发上个窗口的计算(前提是上一个窗口得有数据!) \n         SingleOutputStreamOperator < CartInfo >  result  =  keyedDS . window ( ProcessingTimeSessionWindows . withGap ( Time . seconds ( 10 ) ) ) \n                 . sum ( "count" ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n         /*\n1,1\n1,1\n2,1\n2,1\n         */ \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   CartInfo   { \n         private   String  sensorId ; //信号灯id \n         private   Integer  count ; //通过该信号灯的车的数量 \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 session window(会话窗口)：一段持续时间内的元素为一个窗口，超过时间间隙的归到另一个窗口。比如我们在浏览器上登录访问某网站，会分配一个session，session在有效期内可以持续访问，当超过有效期需要重新登录，session就相当于一个窗口。 \n Time/Watermarker \n 时间分类 \n \n EventTime的重要性和Watermarker的引入 \n 使用watermark要注意的问题： \n 1.某一个分区一直没收到数据，导致没更新低水位，不触发窗口计算--\x3e官方解决方案：1.11版本加入了idle机制。 \n （structed streaming 是固定周期触发器触发，所以flink也可以通过自定义triger来实现） \n 2.某个分区生成的水印太快，数据也会滞后输出--\x3e官方解决方案：1.5版本支持了水印对齐。 \n 自定义trigger \n import   groovy . lang . Tuple ; \n import   org . apache . commons . lang3 . time . FastDateFormat ; \n import   org . apache . flink . api . common . eventtime . WatermarkStrategy ; \n import   org . apache . flink . api . common . functions . RichFlatMapFunction ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . api . java . tuple . Tuple3 ; \n import   org . apache . flink . api . java . utils . ParameterTool ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . SourceFunction ; \n import   org . apache . flink . streaming . api . windowing . assigners . SlidingEventTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . assigners . TumblingEventTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n import   org . apache . flink . streaming . api . windowing . triggers . * ; \n import   org . apache . flink . streaming . api . windowing . windows . TimeWindow ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaConsumer ; \n import   org . apache . flink . util . Collector ; \n\n import   java . sql . Timestamp ; \n import   java . time . Duration ; \n import   java . util . Properties ; \n import   java . util . Random ; \n import   java . util . UUID ; \n\n /**\n * 2019-10-10 12:00:03,dog\n * 2019-10-10 12:00:04,cat\n * 2019-10-10 12:00:04,dog\n * 2019-10-10 12:00:20,dog\n * 2019-10-10 12:00:30,dog\n * 2019-10-10 12:00:40,dog\n * 2019-10-10 12:00:03,dog\n * 2019-10-10 12:00:04,cat\n * 2019-10-10 12:00:20,dog\n * 2019-10-10 12:00:30,dog\n */ \n public   class   WaterMaker   { \n\n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //env.setStreamTimeCharacteristic();默认事件时间 \n        env . setParallelism ( 3 ) ; \n         /*FastDateFormat df = FastDateFormat.getInstance("HH:mm:ss");\n        DataStreamSource<Order> orderDS = env.addSource(new SourceFunction<Order>() {\n            boolean flag = true;\n\n            @Override\n            public void run(SourceContext<Order> ctx) throws Exception {\n\n                while (flag) {\n                    Random random = new Random();\n                    String orderId = UUID.randomUUID().toString();\n                    Integer userId = random.nextInt(3);\n                    Integer money = random.nextInt(100);\n                    Long evenTime = System.currentTimeMillis() - random.nextInt(6) * 1000;\n                    System.out.println("发送1条数据: " + userId + "  evenTime: " + df.format(evenTime));\n                    ctx.collect(new Order(orderId, userId, money, evenTime));\n                    Thread.sleep(2000);\n                }\n            }\n\n            @Override\n            public void cancel() {\n                flag = false;\n            }\n        });*/ \n         //DataStreamSource<String> source = env.socketTextStream("node1", 9999); \n         ParameterTool  param  =   ParameterTool . fromArgs ( args ) ; \n         String  topic  =  param . get ( "topic" ,   "KafkaWordCount" ) ; \n         String  group_id  =  param . get ( "group_id" ,   "flink_wordcount" ) ; \n         boolean  isWriteKafka  =  param . getBoolean ( "isWriteKafka" ,   false ) ; \n         boolean  isWriteHdfs  =  param . getBoolean ( "isWriteHdfs" ,   false ) ; \n         boolean  isWriteMysql  =  param . getBoolean ( "isWriteMysql" ,   false ) ; \n\n         Properties  prop  =   new   Properties ( ) ; \n        prop . setProperty ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        prop . setProperty ( "group.id" ,  group_id ) ; \n        prop . setProperty ( "auto.offset.reset" ,   "latest" ) ; \n        prop . setProperty ( "enable.auto.commit" ,   "true" ) ; \n        prop . setProperty ( "key.deserializer" ,   "StringDeserializer" ) ; \n        prop . setProperty ( "value.deserializer" ,   "StringDeserializer" ) ; \n         FlinkKafkaConsumer < String >  kafka  =   new   FlinkKafkaConsumer < > ( topic ,   new   SimpleStringSchema ( ) ,  prop ) ; \n         DataStreamSource < String >  source  =  env . addSource ( kafka ) ; \n\n         SingleOutputStreamOperator < Order >  result  =  source . rebalance ( ) . map ( new   RichMapFunction < String ,   Order > ( )   { \n             @Override \n             public   Order   map ( String  value )   throws   Exception   { \n                 String [ ]  timeAndWord  =  value . split ( "," ) ; \n                 Timestamp  timestamp  =   Timestamp . valueOf ( timeAndWord [ 0 ] ) ; \n                 Order  order  =   new   Order ( ) ; \n                order . setOrderId ( timeAndWord [ 1 ] ) ; \n                order . setMoney ( 1 ) ; \n                order . setEvenTime ( timestamp . getTime ( ) ) ; \n                 return  order ; \n             } \n         } ) \n                 . assignTimestampsAndWatermarks ( WatermarkStrategy \n                         . < Order > forBoundedOutOfOrderness ( Duration . ofSeconds ( 10 ) ) \n                         . withTimestampAssigner ( ( element ,  recordTimestamp )   ->  element . getEvenTime ( ) ) ) \n                 . keyBy ( t  ->  t . getOrderId ( ) ) \n                 //包左不包右 \n                 . window ( TumblingEventTimeWindows . of ( Time . seconds ( 10 ) ) ) \n                 //没有trigger，某个分区没数据，不触发计算，或者都没数据，最后一个窗口不触发计算，末尾窗口数据丢失 \n                 /**\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:03}\n                 * 结果是:> Order{orderId=\'cat\', userId=null, money=1, evenTime=12:00:04}\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:20}\n                 *\n                 */ \n                 //ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(10)结合EventTimeTrigger窗口不触发计算，就等待超时计算,且之前触发过的计算不会再计算，只更新有更新的累计值，对有更新的key覆盖更新，但是过期数据都没销毁 \n                 //.trigger(ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(10))) \n                 //定期触发 spark定期触发一样 \n                 . trigger ( ProcessingTimeoutTrigger . of ( EventTimeTrigger . create ( ) ,   Duration . ofSeconds ( 5 ) , false , true ) ) \n                 /**\n                 * 单并行度验证：\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:03}\n                 * 结果是:> Order{orderId=\'cat\', userId=null, money=1, evenTime=12:00:04}\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:20}\n                 * --以上是水印触发计算\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:30}\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:40}\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:30}\n                 * --超时触发计算\n                 */ \n                 //ProcessingTimeTrigger 立马触发，水印生效，过期数据销毁，跟spark一样trigger不设置或设置为0效果一样 \n                 //.trigger(ProcessingTimeTrigger.create()) \n                 //但实际要实现的是事件时间窗口根据水印触发计算销毁，数据一直不来或推进水印太慢设置一个定期触发计算并销毁 \n                 //模拟实现.trigger(ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(5),false,true)) \n                 //.trigger(EventAndProcessingTimeOutTrigger.of(Duration.ofSeconds(5),false)) \n\n         /**\n         * 思考1：多并行度时某个分区没有数据，官方1.1 idle机制可以解决，但是都没数据了，无数据推进低水位，最后一个窗口数据不触发计算，丢失。\n         *       但定义基于eventtime trigger没数据超时触发可以解决\n         * 思考2：多并行度时某个分区低水位推进很快，其他很慢，快的那个就会堆积，等待慢的，最后一个窗口数据一样也会不触发计算，丢失。\n         *       针对水印推进速度不一致的问题，官方1.5版本是给一个水印对齐的策略。但定义基于eventtime trigger定期触发可以解决。\n *       * 综合：定义基于eventtime trigger定期触发可以解决，1.没数据不触发；2.数据部分分区推进水位太慢不触发；3.最后一个窗口水印不推进不触发\n         */ \n\n\n                 . sum ( "money" ) ; \n\n        result . print ( "结果是:" ) ; \n\n        env . execute ( ) ; \n\n\n     } \n     public   static   class   Order { \n         private   String  orderId ; \n         private   Integer  userId ; \n         private   Integer  money ; \n         private   Long  evenTime ; \n\n         public   Order ( String  orderId ,   Integer  userId ,   Integer  money ,   Long  evenTime )   { \n             this . orderId  =  orderId ; \n             this . userId  =  userId ; \n             this . money  =  money ; \n             this . evenTime  =  evenTime ; \n         } \n\n         public   Order ( )   { \n         } \n\n         public   String   getOrderId ( )   { \n             return  orderId ; \n         } \n\n         public   void   setOrderId ( String  orderId )   { \n             this . orderId  =  orderId ; \n         } \n\n         public   Integer   getUserId ( )   { \n             return  userId ; \n         } \n\n         public   void   setUserId ( Integer  userId )   { \n             this . userId  =  userId ; \n         } \n\n         public   Integer   getMoney ( )   { \n             return  money ; \n         } \n\n         public   void   setMoney ( Integer  money )   { \n             this . money  =  money ; \n         } \n\n         public   Long   getEvenTime ( )   { \n             return  evenTime ; \n         } \n\n         public   void   setEvenTime ( Long  evenTime )   { \n             this . evenTime  =  evenTime ; \n         } \n\n         @Override \n         public   String   toString ( )   { \n             FastDateFormat  df  =   FastDateFormat . getInstance ( "HH:mm:ss" ) ; \n             return   "Order{"   + \n                     "orderId=\'"   +  orderId  +   \'\\\'\'   + \n                     ", userId="   +  userId  + \n                     ", money="   +  money  + \n                     ", evenTime="   +  df . format ( evenTime )   + \n                     \'}\' ; \n         } \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 模拟实现打印定期触发trigger \n import   org . apache . flink . api . common . state . ValueState ; \n import   org . apache . flink . api . common . state . ValueStateDescriptor ; \n import   org . apache . flink . api . common . typeutils . base . LongSerializer ; \n import   org . apache . flink . runtime . operators . TaskContext ; \n import   org . apache . flink . streaming . api . windowing . triggers . Trigger ; \n import   org . apache . flink . streaming . api . windowing . triggers . TriggerResult ; \n import   org . apache . flink . streaming . api . windowing . windows . Window ; \n import   org . slf4j . Logger ; \n import   org . slf4j . LoggerFactory ; \n\n\n import   java . sql . Timestamp ; \n import   java . time . Duration ; \n\n\n\n public   class   EventAndProcessingTimeOutTrigger < T ,   W   extends   Window >   extends   Trigger < T ,   W >   { \n     private   static   final   Logger   LOG =   LoggerFactory . getLogger ( EventAndProcessingTimeOutTrigger . class ) ; \n     private   static   final   long  serialVersionUID  =   1L ; \n     private   final   long  interval ; \n     private   final   boolean  resetTimerOnNewRecord ; \n\n     private   final   ValueStateDescriptor < Long >  timeoutStateDesc  =   new   ValueStateDescriptor < > ( "timeout" ,   LongSerializer . INSTANCE ) ; \n\n\n     private   EventAndProcessingTimeOutTrigger ( long  interval , boolean  resetTimerOnNewRecord )   { \n         this . interval = interval ; \n         this . resetTimerOnNewRecord = resetTimerOnNewRecord ; \n     } \n\n     @Override \n     public   TriggerResult   onElement ( T  element ,   long  timestamp ,   W  window ,   TriggerContext  ctx )   throws   Exception   { \n         if   ( window . maxTimestamp ( )   <=  ctx . getCurrentWatermark ( ) )   { \n             // if the watermark is already past the window fire immediately \n             return   TriggerResult . FIRE ; \n         }   else   { \n             ValueState < Long >  timeoutState  =  ctx . getPartitionedState ( this . timeoutStateDesc ) ; \n             long  nextFireTimestamp  =  ctx . getCurrentProcessingTime ( )   +   this . interval ; \n             Long  timeoutTimestamp  =  timeoutState . value ( ) ; \n             if   ( timeoutTimestamp  !=   null   &&  resetTimerOnNewRecord )   { \n                ctx . deleteProcessingTimeTimer ( timeoutTimestamp ) ; \n                timeoutState . clear ( ) ; \n                timeoutTimestamp  =   null ; \n             } \n\n             if   ( timeoutTimestamp  ==   null )   { \n                timeoutState . update ( nextFireTimestamp ) ; \n                ctx . registerProcessingTimeTimer ( nextFireTimestamp ) ; \n             } \n\n            ctx . registerEventTimeTimer ( window . maxTimestamp ( ) ) ; \n             return   TriggerResult . CONTINUE ; \n         } \n\n     } \n\n     @Override \n     public   TriggerResult   onProcessingTime ( long  time ,   W  window ,   TriggerContext  ctx )   throws   Exception   { \n         //超时了触发 窗口计算 和推进水位线 \n         long  maxTimestamp  =  window . maxTimestamp ( ) ; \n         this . clear ( window ,  ctx ) ; \n         System . out . println ( "maxTimestamp = "   +   new   Timestamp ( maxTimestamp ) + " CurrentWatermark = " + new   Timestamp ( ctx . getCurrentWatermark ( ) ) ) ; \n         LOG . warn ( "LOG: maxTimestamp = "   +   new   Timestamp ( maxTimestamp ) + " CurrentWatermark = " + new   Timestamp ( ctx . getCurrentWatermark ( ) ) ) ; \n        ctx . registerEventTimeTimer ( maxTimestamp  + this . interval ) ; \n         return   TriggerResult . FIRE ; \n         /**\n         * 单并行度验证\n         * 结果是:> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:03}\n         * 结果是:> Order{orderId=\'cat\', userId=null, money=1, evenTime=12:00:04}\n         * 结果是:> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:20}\n         * maxTimestamp = 2019-10-10 12:00:39.999 CurrentWatermark = 2019-10-10 12:00:29.999\n         * 结果是:> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:30}\n         * maxTimestamp = 2019-10-10 12:00:49.999 CurrentWatermark = 2019-10-10 12:00:29.999\n         * 结果是:> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:40}\n         * maxTimestamp = 2019-10-10 12:00:39.999 CurrentWatermark = 2019-10-10 12:00:29.999\n         * 结果是:> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:30}\n         */ \n     } \n\n     @Override \n     public   TriggerResult   onEventTime ( long  time ,   W  window ,   TriggerContext  ctx )   throws   Exception   { \n         return  time  ==  window . maxTimestamp ( )   ? \n                 TriggerResult . FIRE   : \n                 TriggerResult . CONTINUE ; \n     } \n\n     @Override \n     public   boolean   canMerge ( )   { \n         return   true ; \n     } \n\n     @Override \n     public   void   onMerge ( W  window ,   OnMergeContext  ctx )   throws   Exception   { \n         // only register a timer if the watermark is not yet past the end of the merged window \n         // this is in line with the logic in onElement(). If the watermark is past the end of \n         // the window onElement() will fire and setting a timer here would fire the window twice. \n         long  windowMaxTimestamp  =  window . maxTimestamp ( ) ; \n         if   ( windowMaxTimestamp  >  ctx . getCurrentWatermark ( ) )   { \n            ctx . registerEventTimeTimer ( windowMaxTimestamp ) ; \n         } \n     } \n\n     @Override \n     public   void   clear ( W  window ,   TriggerContext  ctx )   throws   Exception   { \n         ValueState < Long >  timeoutTimestampState  =  ctx . getPartitionedState ( this . timeoutStateDesc ) ; \n         Long  timeoutTimestamp  =  timeoutTimestampState . value ( ) ; \n         if   ( timeoutTimestamp  !=   null )   { \n            ctx . deleteProcessingTimeTimer ( timeoutTimestamp ) ; \n            timeoutTimestampState . clear ( ) ; \n         } \n        ctx . deleteEventTimeTimer ( window . maxTimestamp ( ) ) ; \n     } \n\n     public   static   < T ,   W   extends   Window >   EventAndProcessingTimeOutTrigger < T ,   W >   of ( Duration  timeout )   { \n         return   new   EventAndProcessingTimeOutTrigger < > ( timeout . toMillis ( ) , true ) ; \n     } \n     public   static   < T ,   W   extends   Window >   EventAndProcessingTimeOutTrigger < T ,   W >   of ( Duration  timeout , boolean  resetTimerOnNewRecord )   { \n         return   new   EventAndProcessingTimeOutTrigger < > ( timeout . toMillis ( ) , resetTimerOnNewRecord ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 #  Watermarker详解 \n ==总结:== \n 1.Watermarker本质是时间戳 \n 2.Watermarker = 当前进来的数据最大的事件时间 - ==最大允许的数据延迟时间或乱序时间== \n 3.Watermarker 可以通过改变窗口触发计算时机来解决一定程度上的数据乱序或延迟达到的问题 \n ==4.Watermarker  >= 窗口结束时间 时触发窗口计算== \n 5.当前的最大的事件时间 - 最大允许的数据延迟时间或乱序时间>= 窗口结束时间时触发窗口计算 \n 6.当前的最大的事件时间 >= 窗口结束时间 +最大允许的数据延迟时间或乱序时间时触发窗口计算 \n 7.在 [window_start_time,window_end_time) 中有数据存在，这个窗口是左闭右开的。 \n \n \n \n \n \n D不在10:00:00-10:10:00窗口进行计算 \n 代码演示-验证版-了解 \n package   cn . itcast . watermaker ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . commons . lang3 . time . FastDateFormat ; \n import   org . apache . flink . api . common . eventtime . * ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . SourceFunction ; \n import   org . apache . flink . streaming . api . functions . windowing . WindowFunction ; \n import   org . apache . flink . streaming . api . windowing . assigners . TumblingEventTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n import   org . apache . flink . streaming . api . windowing . windows . TimeWindow ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . ArrayList ; \n import   java . util . List ; \n import   java . util . Random ; \n import   java . util . UUID ; \n\n /**\n * Author itcast\n * Desc\n * 模拟实时订单数据,格式为: (订单ID，用户ID，订单金额，时间戳/事件时间)\n * 要求每隔5s,计算5秒内(基于时间的滚动窗口)，每个用户的订单总金额\n * 并添加Watermaker来解决一定程度上的数据延迟和数据乱序问题。\n */ \n public   class   WatermakerDemo02_Check   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         FastDateFormat  df  =   FastDateFormat . getInstance ( "HH:mm:ss" ) ; \n\n         //TODO 1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n\n         //TODO 2.Source \n         //模拟实时订单数据(数据有延迟和乱序) \n         DataStreamSource < Order >  orderDS  =  env . addSource ( new   SourceFunction < Order > ( )   { \n             private   boolean  flag  =   true ; \n             @Override \n             public   void   run ( SourceContext < Order >  ctx )   throws   Exception   { \n                 Random  random  =   new   Random ( ) ; \n                 while   ( flag )   { \n                     String  orderId  =   UUID . randomUUID ( ) . toString ( ) ; \n                     int  userId  =  random . nextInt ( 3 ) ; \n                     int  money  =  random . nextInt ( 100 ) ; \n                     //模拟数据延迟和乱序! \n                     long  eventTime  =   System . currentTimeMillis ( )   -  random . nextInt ( 5 )   *   1000 ; \n                     System . out . println ( "发送的数据为: " + userId  +   " : "   +  df . format ( eventTime ) ) ; \n                    ctx . collect ( new   Order ( orderId ,  userId ,  money ,  eventTime ) ) ; \n                     //TimeUnit.SECONDS.sleep(1); \n                     Thread . sleep ( 1000 ) ; \n                 } \n             } \n\n             @Override \n             public   void   cancel ( )   { \n                flag  =   false ; \n             } \n         } ) ; \n\n         //TODO 3.Transformation \n         /*DataStream<Order> watermakerDS = orderDS\n                .assignTimestampsAndWatermarks(\n                        WatermarkStrategy.<Order>forBoundedOutOfOrderness(Duration.ofSeconds(3))\n                                .withTimestampAssigner((event, timestamp) -> event.getEventTime())\n                );*/ \n\n         //开发中直接使用上面的即可 \n         //学习测试时可以自己实现 \n         DataStream < Order >  watermakerDS  =  orderDS\n                 . assignTimestampsAndWatermarks ( \n                         new   WatermarkStrategy < Order > ( )   { \n                             @Override \n                             public   WatermarkGenerator < Order >   createWatermarkGenerator ( WatermarkGeneratorSupplier . Context  context )   { \n                                 return   new   WatermarkGenerator < Order > ( )   { \n                                     private   int  userId  =   0 ; \n                                     private   long  eventTime  =   0L ; \n                                     private   final   long  outOfOrdernessMillis  =   3000 ; \n                                     private   long  maxTimestamp  =   Long . MIN_VALUE   +  outOfOrdernessMillis  +   1 ; \n\n                                     @Override \n                                     public   void   onEvent ( Order  event ,   long  eventTimestamp ,   WatermarkOutput  output )   { \n                                        userId  =  event . userId ; \n                                        eventTime  =  event . eventTime ; \n                                        maxTimestamp  =   Math . max ( maxTimestamp ,  eventTimestamp ) ; \n                                     } \n\n                                     @Override \n                                     public   void   onPeriodicEmit ( WatermarkOutput  output )   { \n                                         //Watermaker = 当前最大事件时间 - 最大允许的延迟时间或乱序时间 \n                                         Watermark  watermark  =   new   Watermark ( maxTimestamp  -  outOfOrdernessMillis  -   1 ) ; \n                                         System . out . println ( "key:"   +  userId  +   ",系统时间:"   +  df . format ( System . currentTimeMillis ( ) )   +   ",事件时间:"   +  df . format ( eventTime )   +   ",水印时间:"   +  df . format ( watermark . getTimestamp ( ) ) ) ; \n                                        output . emitWatermark ( watermark ) ; \n                                     } \n                                 } ; \n                             } \n                         } . withTimestampAssigner ( ( order ,  timestamp )   ->  order . getEventTime ( ) ) \n                 ) ; \n\n\n         //代码走到这里,就已经被添加上Watermaker了!接下来就可以进行窗口计算了 \n         //要求每隔5s,计算5秒内(基于时间的滚动窗口)，每个用户的订单总金额 \n        /* DataStream<Order> result = watermakerDS\n                 .keyBy(Order::getUserId)\n                //.timeWindow(Time.seconds(5), Time.seconds(5))\n                .window(TumblingEventTimeWindows.of(Time.seconds(5)))\n                .sum("money");*/ \n\n         //开发中使用上面的代码进行业务计算即可 \n         //学习测试时可以使用下面的代码对数据进行更详细的输出,如输出窗口触发时各个窗口中的数据的事件时间,Watermaker时间 \n         SingleOutputStreamOperator < String >  result  =  watermakerDS\n                 . keyBy ( Order :: getUserId ) \n                 . window ( TumblingEventTimeWindows . of ( Time . seconds ( 5 ) ) ) \n                 //把apply中的函数应用在窗口中的数据上 \n                 //WindowFunction<IN, OUT, KEY, W extends Window> \n                 . apply ( new   WindowFunction < Order ,   String ,   Integer ,   TimeWindow > ( )   { \n                     @Override \n                     public   void   apply ( Integer  key ,   TimeWindow  window ,   Iterable < Order >  orders ,   Collector < String >  out )   throws   Exception   { \n                         //用来存放当前窗口的数据的格式化后的事件时间 \n                         List < String >  list  =   new   ArrayList < > ( ) ; \n                         for   ( Order  order  :  orders )   { \n                             Long  eventTime  =  order . eventTime ; \n                             String  formatEventTime  =  df . format ( eventTime ) ; \n                            list . add ( formatEventTime ) ; \n                         } \n                         String  start  =  df . format ( window . getStart ( ) ) ; \n                         String  end  =  df . format ( window . getEnd ( ) ) ; \n                         //现在就已经获取到了当前窗口的开始和结束时间,以及属于该窗口的所有数据的事件时间,把这些拼接并返回 \n                         String  outStr  =   String . format ( "key:%s,窗口开始结束:[%s~%s),属于该窗口的事件时间:%s" ,  key . toString ( ) ,  start ,  end ,  list . toString ( ) ) ; \n                        out . collect ( outStr ) ; \n                     } \n                 } ) ; \n\n         //4.Sink \n        result . print ( ) ; \n\n         //5.execute \n        env . execute ( ) ; \n     } \n\n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   Order   { \n         private   String  orderId ; \n         private   Integer  userId ; \n         private   Integer  money ; \n         private   Long  eventTime ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 \n 侧道输出解决数据丢失-掌握 \n \n public   class   WaterMaker   { \n\n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //env.setStreamTimeCharacteristic();默认事件时间 \n        env . setParallelism ( 1 ) ; \n         /*FastDateFormat df = FastDateFormat.getInstance("HH:mm:ss");\n        DataStreamSource<Order> orderDS = env.addSource(new SourceFunction<Order>() {\n            boolean flag = true;\n\n            @Override\n            public void run(SourceContext<Order> ctx) throws Exception {\n\n                while (flag) {\n                    Random random = new Random();\n                    String orderId = UUID.randomUUID().toString();\n                    Integer userId = random.nextInt(3);\n                    Integer money = random.nextInt(100);\n                    Long evenTime = System.currentTimeMillis() - random.nextInt(6) * 1000;\n                    System.out.println("发送1条数据: " + userId + "  evenTime: " + df.format(evenTime));\n                    ctx.collect(new Order(orderId, userId, money, evenTime));\n                    Thread.sleep(2000);\n                }\n            }\n\n            @Override\n            public void cancel() {\n                flag = false;\n            }\n        });*/ \n         DataStreamSource < String >  source  =  env . socketTextStream ( "node1" ,   9999 ) ; \n         /*ParameterTool param = ParameterTool.fromArgs(args);\n        String topic = param.get("topic", "KafkaWordCount");\n        String group_id = param.get("group_id", "flink_wordcount");\n        boolean isWriteKafka = param.getBoolean("isWriteKafka", false);\n        boolean isWriteHdfs = param.getBoolean("isWriteHdfs", false);\n        boolean isWriteMysql = param.getBoolean("isWriteMysql", false);\n\n        Properties prop = new Properties();\n        prop.setProperty("bootstrap.servers", "node1:9092,node2:9092,node3:9092");\n        prop.setProperty("group.id", group_id);\n        prop.setProperty("auto.offset.reset", "latest");\n        prop.setProperty("enable.auto.commit", "true");\n        prop.setProperty("key.deserializer", "StringDeserializer");\n        prop.setProperty("value.deserializer", "StringDeserializer");\n        FlinkKafkaConsumer<String> kafka = new FlinkKafkaConsumer<>(topic, new SimpleStringSchema(), prop);\n        DataStreamSource<String> source = env.addSource(kafka);*/ \n\n         OutputTag < Order >  seriousLateOutputTag  =   new   OutputTag < > ( "seriousLateOutputTag" ,   TypeInformation . of ( Order . class ) ) ; \n         SingleOutputStreamOperator < Order >  result  =  source . rebalance ( ) . map ( new   RichMapFunction < String ,   Order > ( )   { \n             @Override \n             public   Order   map ( String  value )   throws   Exception   { \n                 String [ ]  timeAndWord  =  value . split ( "," ) ; \n                 Timestamp  timestamp  =   Timestamp . valueOf ( timeAndWord [ 0 ] ) ; \n                 Order  order  =   new   Order ( ) ; \n                order . setOrderId ( timeAndWord [ 1 ] ) ; \n                order . setMoney ( 1 ) ; \n                order . setEvenTime ( timestamp . getTime ( ) ) ; \n                 return  order ; \n             } \n         } ) \n                 . assignTimestampsAndWatermarks ( WatermarkStrategy \n                         . < Order > forBoundedOutOfOrderness ( Duration . ofSeconds ( 10 ) ) \n                         . withTimestampAssigner ( ( element ,  recordTimestamp )   ->  element . getEvenTime ( ) ) ) \n                 . keyBy ( t  ->  t . getOrderId ( ) ) \n                 //包左不包右 \n                 . window ( TumblingEventTimeWindows . of ( Time . seconds ( 10 ) ) ) \n                 //没有trigger，某个分区没数据，不触发计算，或者都没数据，最后一个窗口不触发计算，末尾窗口数据丢失 \n                 /**\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:03}\n                 * 结果是:> Order{orderId=\'cat\', userId=null, money=1, evenTime=12:00:04}\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:20}\n                 *\n                 */ \n                 //ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(10)结合EventTimeTrigger窗口不触发计算，就等待超时计算,且之前触发过的计算不会再计算，只更新有更新的累计值，对有更新的key覆盖更新，但是过期数据都没销毁 \n                 //.trigger(ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(10))) \n                 //定期触发 spark定期触发一样 \n                 //.trigger(ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(5),false,true)) \n                 /**\n                 * 单并行度验证：\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:03}\n                 * 结果是:> Order{orderId=\'cat\', userId=null, money=1, evenTime=12:00:04}\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:20}\n                 * --以上是水印触发计算\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:30}\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:40}\n                 * 结果是:> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:30}\n                 * --超时触发计算\n                 */ \n                 //ProcessingTimeTrigger 立马触发，水印生效，过期数据销毁，跟spark一样trigger不设置或设置为0效果一样 \n                 //.trigger(ProcessingTimeTrigger.create()) \n                 //但实际要实现的是事件时间窗口根据水印触发计算销毁，数据一直不来或推进水印太慢设置一个定期触发计算并销毁 \n                 //模拟实现.trigger(ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(5),false,true)) \n                 . trigger ( EventAndProcessingTimeOutTrigger . of ( Duration . ofSeconds ( 5 ) , false ) ) \n\n         /**\n         * 思考1：多并行度时某个分区没有数据，官方1.1 idle机制可以解决，但是都没数据了，无数据推进低水位，最后一个窗口数据不触发计算，丢失。\n         *       但定义基于eventtime trigger没数据超时触发可以解决\n         * 思考2：多并行度时某个分区低水位推进很快，其他很慢，快的那个就会堆积，等待慢的，最后一个窗口数据一样也会不触发计算，丢失。\n         *       针对水印推进速度不一致的问题，官方1.5版本是给一个水印对齐的策略。但定义基于eventtime trigger定期触发可以解决。\n *       * 综合：定义基于eventtime trigger定期触发可以解决，1.没数据不触发；2.数据部分分区推进水位太慢不触发；3.最后一个窗口水印不推进不触发\n         *\n         */ \n                 . allowedLateness ( Time . seconds ( 15 ) ) \n                 . sideOutputLateData ( seriousLateOutputTag ) \n                 . sum ( "money" ) ; \n\n        result . print ( "结果是:" ) ; \n        result . getSideOutput ( seriousLateOutputTag ) . print ( "严重迟到丢失的数据:" ) ; \n\n        env . execute ( ) ; \n\n\n     } \n     public   static   class   Order { \n         private   String  orderId ; \n         private   Integer  userId ; \n         private   Integer  money ; \n         private   Long  evenTime ; \n\n         public   Order ( String  orderId ,   Integer  userId ,   Integer  money ,   Long  evenTime )   { \n             this . orderId  =  orderId ; \n             this . userId  =  userId ; \n             this . money  =  money ; \n             this . evenTime  =  evenTime ; \n         } \n\n         public   Order ( )   { \n         } \n\n         public   String   getOrderId ( )   { \n             return  orderId ; \n         } \n\n         public   void   setOrderId ( String  orderId )   { \n             this . orderId  =  orderId ; \n         } \n\n         public   Integer   getUserId ( )   { \n             return  userId ; \n         } \n\n         public   void   setUserId ( Integer  userId )   { \n             this . userId  =  userId ; \n         } \n\n         public   Integer   getMoney ( )   { \n             return  money ; \n         } \n\n         public   void   setMoney ( Integer  money )   { \n             this . money  =  money ; \n         } \n\n         public   Long   getEvenTime ( )   { \n             return  evenTime ; \n         } \n\n         public   void   setEvenTime ( Long  evenTime )   { \n             this . evenTime  =  evenTime ; \n         } \n\n         @Override \n         public   String   toString ( )   { \n             FastDateFormat  df  =   FastDateFormat . getInstance ( "HH:mm:ss" ) ; \n             return   "Order{"   + \n                     "orderId=\'"   +  orderId  +   \'\\\'\'   + \n                     ", userId="   +  userId  + \n                     ", money="   +  money  + \n                     ", evenTime="   +  df . format ( evenTime )   + \n                     \'}\' ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 #  State \n Flink中状态的自动管理 \n 之前写的Flink代码中其实已经做好了状态自动管理,如 \n 发送hello ,得出(hello,1) \n 再发送hello ,得出(hello,2) \n 说明Flink已经自动的将当前数据和历史状态/历史结果进行了聚合,做到了状态的自动管理 \n 在实际开发中绝大多数情况下,我们直接使用自动管理即可 \n 一些特殊情况才会使用手动的状态管理!---后面项目中会使用! \n 所以这里得先学习state状态如何手动管理! \n package   cn . itcast . source ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc 演示DataStream-Source-基于Socket\n */ \n public   class   SourceDemo03_Socket   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n\n         //TODO 2.transformation \n         /*SingleOutputStreamOperator<String> words = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public void flatMap(String value, Collector<String> out) throws Exception {\n                String[] arr = value.split(" ");\n                for (String word : arr) {\n                    out.collect(word);\n                }\n            }\n        });\n\n        words.map(new MapFunction<String, Tuple2<String,Integer>>() {\n            @Override\n            public Tuple2<String, Integer> map(String value) throws Exception {\n                return Tuple2.of(value,1);\n            }\n        });*/ \n\n         //注意:下面的操作将上面的2步合成了1步,直接切割单词并记为1返回 \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  wordAndOne  =  lines . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n\n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  wordAndOne . keyBy ( t  ->  t . f0 ) . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 #  无状态计算和有状态计算 \n \n 无状态计算,不需要考虑历史值, 如map \n \n hello --\x3e (hello,1) \n hello --\x3e (hello,1) \n \n \n 有状态计算,需要考虑历史值,如:sum \n \n hello , (hello,1) \n hello , (hello,2) \n 状态分类 \n \n State\n \n ManagerState--开发中推荐使用 : Fink自动管理/优化,支持多种数据结构\n \n KeyState--只能在keyedStream上使用,支持多种数据结构 \n- OperatorState--一般用在Source上,支持ListState \n \n \n RawState--完全由用户自己管理,只支持byte[],只能在自定义Operator上使用\n \n OperatorState \n \n \n \n \n \n 分类详细图解: \n ManagerState-keyState \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/state.html \n package   cn . itcast . state ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . common . state . ValueState ; \n import   org . apache . flink . api . common . state . ValueStateDescriptor ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . api . java . tuple . Tuple3 ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n /**\n * Author itcast\n * Desc 使用KeyState中的ValueState获取流数据中的最大值/实际中可以使用maxBy即可\n */ \n public   class   StateDemo01_KeyState   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n //如果RuntimeExecutionMode.AUTOMATIC根据源数据是流还是批来采用对应的处理方式,该Demo是批处理,keyBy的时候会排序 \n         /*\n        (上海,2,2)\n        (上海,8,8)\n        (上海,4,8)\n        (北京,1,1)\n        (北京,6,6)\n        (北京,3,6)\n         */ \n         //如果不设置,默认流处理方式,一个个处理,没排序 \n         /*\n        (北京,1,1)\n        (上海,2,2)\n        (北京,6,6)\n        (上海,8,8)\n        (北京,3,6)\n        (上海,4,8)\n         */ \n         //TODO 1.source \n         DataStream < Tuple2 < String ,   Long > >  tupleDS  =  env . fromElements ( \n                 Tuple2 . of ( "北京" ,   1L ) , \n                 Tuple2 . of ( "上海" ,   2L ) , \n                 Tuple2 . of ( "北京" ,   6L ) , \n                 Tuple2 . of ( "上海" ,   8L ) , \n                 Tuple2 . of ( "北京" ,   3L ) , \n                 Tuple2 . of ( "上海" ,   4L ) \n         ) ; \n\n         //TODO 2.transformation \n         //需求:求各个城市的value最大值 \n         //实际中使用maxBy即可 \n         DataStream < Tuple2 < String ,   Long > >  result1  =  tupleDS . keyBy ( t  ->  t . f0 ) . maxBy ( 1 ) ; \n\n         //学习时可以使用KeyState中的ValueState来实现maxBy的底层 \n         DataStream < Tuple3 < String ,   Long ,   Long > >  result2  =  tupleDS . keyBy ( t  ->  t . f0 ) . map ( new   RichMapFunction < Tuple2 < String ,   Long > ,   Tuple3 < String ,   Long ,   Long > > ( )   { \n             //-1.定义一个状态用来存放最大值 \n             private   ValueState < Long >  maxValueState ; \n\n             //-2.状态初始化 \n               /*\n                    实现open方法,意味着只要创建一次连接,如果不实现,每来一条数据,就要创建一次连接,占用资源.\n                     */ \n             @Override \n             public   void   open ( Configuration  parameters )   throws   Exception   { \n                 //创建状态描述器 \n                 ValueStateDescriptor  stateDescriptor  =   new   ValueStateDescriptor ( "maxValueState" ,   Long . class ) ; \n                 //根据状态描述器获取/初始化状态 \n                maxValueState  =   getRuntimeContext ( ) . getState ( stateDescriptor ) ; \n             } \n\n             //-3.使用状态 \n             @Override \n             public   Tuple3 < String ,   Long ,   Long >   map ( Tuple2 < String ,   Long >  value )   throws   Exception   { \n                 Long  currentValue  =  value . f1 ; \n                 //获取状态 \n                 Long  historyValue  =  maxValueState . value ( ) ; \n                 //判断状态 \n                 if   ( historyValue  ==   null   ||  currentValue  >  historyValue )   { \n                    historyValue  =  currentValue ; \n                     //更新状态 \n                    maxValueState . update ( historyValue ) ; \n                     return   Tuple3 . of ( value . f0 ,  currentValue ,  historyValue ) ; \n                 }   else   { \n                     return   Tuple3 . of ( value . f0 ,  currentValue ,  historyValue ) ; \n                 } \n             } \n         } ) ; \n\n\n         //TODO 3.sink \n         //result1.print(); \n         //4> (北京,6) \n         //1> (上海,8) \n        result2 . print ( ) ; \n         //1> (上海,xxx,8) \n         //4> (北京,xxx,6) \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 #  ManagerState-OperatorState \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/state.html \n package   cn . itcast . state ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . restartstrategy . RestartStrategies ; \n import   org . apache . flink . api . common . state . ListState ; \n import   org . apache . flink . api . common . state . ListStateDescriptor ; \n import   org . apache . flink . runtime . state . FunctionInitializationContext ; \n import   org . apache . flink . runtime . state . FunctionSnapshotContext ; \n import   org . apache . flink . runtime . state . filesystem . FsStateBackend ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . checkpoint . CheckpointedFunction ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . environment . CheckpointConfig ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichParallelSourceFunction ; \n\n import   java . util . Iterator ; \n\n /**\n * Author itcast\n * Desc 使用OperatorState中的ListState模拟KafkaSource进行offset维护\n */ \n public   class   StateDemo02_OperatorState   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n        env . setParallelism ( 1 ) ; //并行度设置为1方便观察 \n         //下面的Checkpoint和重启策略配置先直接使用,下次课学 \n        env . enableCheckpointing ( 1000 ) ; //每隔1s执行一次Checkpoint \n        env . setStateBackend ( new   FsStateBackend ( "file:///D:/ckp" ) ) ; \n        env . getCheckpointConfig ( ) . enableExternalizedCheckpoints ( CheckpointConfig . ExternalizedCheckpointCleanup . RETAIN_ON_CANCELLATION ) ; \n        env . getCheckpointConfig ( ) . setCheckpointingMode ( CheckpointingMode . EXACTLY_ONCE ) ; \n         //固定延迟重启策略: 程序出现异常的时候，重启2次，每次延迟3秒钟重启，超过2次，程序退出 \n        env . setRestartStrategy ( RestartStrategies . fixedDelayRestart ( 2 ,   3000 ) ) ; \n\n         //TODO 1.source \n         DataStreamSource < String >  ds  =  env . addSource ( new   MyKafkaSource ( ) ) . setParallelism ( 1 ) ; \n\n         //TODO 2.transformation \n\n         //TODO 3.sink \n        ds . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     //使用OperatorState中的ListState模拟KafkaSource进行offset维护 \n     public   static   class   MyKafkaSource   extends   RichParallelSourceFunction < String >   implements   CheckpointedFunction   { \n         private   boolean  flag  =   true ; \n         //-1.声明ListState \n         private   ListState < Long >  offsetState  =   null ;   //用来存放offset \n         private   Long  offset  =   0L ; //用来存放offset的值 \n\n         //-2.初始化/创建ListState \n         @Override \n         public   void   initializeState ( FunctionInitializationContext  context )   throws   Exception   { \n             ListStateDescriptor < Long >  stateDescriptor  =   new   ListStateDescriptor < > ( "offsetState" ,   Long . class ) ; \n            offsetState  =  context . getOperatorStateStore ( ) . getListState ( stateDescriptor ) ; \n         } \n         //-3.使用state \n         @Override \n         public   void   run ( SourceContext < String >  ctx )   throws   Exception   { \n          \n                 Iterator < Long >  iterator  =  offsetState . get ( ) . iterator ( ) ; \n                 if ( iterator . hasNext ( ) ) { \n                    offset  =  iterator . next ( ) ; \n                 } \n               while   ( flag ) { \n                offset  +=   1 ; \n                 int  subTaskId  =   getRuntimeContext ( ) . getIndexOfThisSubtask ( ) ; \n                ctx . collect ( "subTaskId:" +  subTaskId  +   ",当前的offset值为:" + offset ) ; \n                 Thread . sleep ( 1000 ) ; \n\n                 //模拟异常 \n                 if ( offset  %   5   ==   0 ) { \n                     throw   new   Exception ( "bug出现了....." ) ; \n                 } \n             } \n         } \n         //-4.state持久化 \n         //该方法会定时执行将state状态从内存存入Checkpoint磁盘目录中 \n         @Override \n         public   void   snapshotState ( FunctionSnapshotContext  context )   throws   Exception   { \n            offsetState . clear ( ) ; //清理内容数据并存入Checkpoint磁盘目录中 \n            offsetState . add ( offset ) ; \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 #  state的定时器 \n ctx.timerService().registerProcessingTimeTimer(long time）注册，到点时触发 onTimer操作。 \n State TTL （联想redis） \n 使用 flink 进行实时计算中，会遇到一些状态数不断累积，导致状态量越来越大的情形。 \n 例如，作业中定义了超长的时间窗口，或者在动态表上应用了无限范围的 GROUP BY 语句，以及执行了没有时间窗口限制的双流 JOIN 等等操作。 \n 对于这些情况，经常导致堆内存出现 OOM，或者堆外内存（RocksDB）用量持续增长导致超出容器的配额上限，造成作业的频繁崩溃。 从 Flink 1.6 版本开始引入了State TTL 特性 ，该特性可以允许对作业中定义的 Keyed 状态进行超时自动清理，对于Table API 和 SQL 模块引入了空闲状态保留时间（Idle State Retention Time）进行状态管理，下面我们具体介绍一下。 \n 在 Flink 的官方文档 中给我们展示了State TTL的基本用法，用法示例如下： \n import   org . apache . flink . api . common . state . StateTtlConfig ; \n import   org . apache . flink . api . common . state . ValueStateDescriptor ; \n import   org . apache . flink . api . common . time . Time ; \n\n StateTtlConfig  ttlConfig  =   StateTtlConfig \n    . newBuilder ( Time . seconds ( 1 ) ) \n    . setUpdateType ( StateTtlConfig . UpdateType . OnCreateAndWrite ) \n    . setStateVisibility ( StateTtlConfig . StateVisibility . NeverReturnExpired ) \n    . build ( ) ; \n   \n ValueStateDescriptor < String >  stateDescriptor  =   new   ValueStateDescriptor < > ( "text state" ,   String . class ) ; \nstateDescriptor . enableTimeToLive ( ttlConfig ) ; \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 可以看到，要使用 State TTL 功能，首先要定义一个  StateTtlConfig  对象。 \n 1.通过构造器模式（Builder Pattern）来创建，传入一个 Time 对象作为 TTL 时间; \n 2.然后设置更新类型（Update Type） \n 3.状态可见性（State Visibility）， \n 4.在后续声明的状态描述符（State Descriptor）中启用 State TTL 功能了。 \n StateTtl \n Config 的参数说明 \n l  TTL ：表示状态的过期时间，是一个 org.apache.flink.api.common.time.Time 对象。一旦设置了 TTL，那么如果上次访问的时间戳 + TTL 超过了当前时间，则表明状态过期了（这是一个简化的说法，严谨的定义请参考org.apache.flink.runtime.state.ttl.TtlUtils 类中关于 expired 的实现） \n l  UpdateType ：表示状态时间戳的更新的时机，是一个 Enum 对象。如果设置为 Disabled，则表明不更新时间戳；如果设置为 OnCreateAndWrite，则表明当状态创建或每次写入时都会更新时间戳；如果设置为 OnReadAndWrite，则除了在状态创建和写入时更新时间戳外，读取也会更新状态的时间戳 \n l  StateVisibility ：表示对已过期但还未被清理掉的状态如何处理，也是 Enum 对象。如果设置为 ReturnExpiredIfNotCleanedUp，那么即使这个状态的时间戳表明它已经过期了，但是只要还未被真正清理掉，就会被返回给调用方； 如果设置为 NeverReturnExpired，那么一旦这个状态过期了，那么永远不会被返回给调用方，只会返回空状态，避免了过期状态带来的干扰 \n TimeCharacteristic   以及 TtlTimeCharacteristic ：表示 State TTL 功能所适用的时间模式，仍然是 Enum 对象。前者已经被标记为 Deprecated（废弃），推荐新代码采用新的 TtlTimeCharacteristic 参数。截止到 Flink 1.8，只支持 ProcessingTime 一种时间模式，对 EventTime 模式的 State TTL 支持还在开发中 \n CleanupStrategies ： \n 1.默认被动清理，再读这个key时，进行清除；（内存有压力考虑换rocksdb） \n 2.手动清理：如下 \n 表示过期对象的清理策略，目前来说有三种 Enum 值。当设置为 FULL_STATE_SCAN_SNAPSHOT 时，对应的是 EmptyCleanupStrategy 类，表示对过期状态不做主动清理，当执行完整快照（Snapshot / Checkpoint）时，会生成一个较小的状态文件，但本地状态并不会减小 \n 唯有当作业重启并从上一个快照点恢复后，本地状态才会实际减小，因此可能仍然不能解决内存压力的问题。为了应对这个问题，Flink 还提供了增量清理的枚举值，分别是针对 Heap StateBackend 的 INCREMENTAL_CLEANUP（对应 IncrementalCleanupStrategy 类），以及对 RocksDB StateBackend 有效的 ROCKSDB_COMPACTION_FILTER（对应 RocksdbCompactFilterCleanupStrategy 类） \n 对于增量清理功能，Flink 可以被配置为每读取若干条记录就执行一次清理操作，而且可以指定每次要清理多少条失效记录；对于 RocksDB 的状态清理，则是通过 JNI 来调用 C++ 语言编写的 FlinkCompactionFilter 来实现，底层是通过 RocksDB 提供的后台 Compaction 操作来实现对失效状态过滤的 \n 配置中有下面几个配置项可以选择：StateTtlConfig中的newBuilder这个方法是必须的，它是设置生存周期的值。 \n TTL 刷新策略（默认OnCreateAndWrite） \n \n \n \n 策略类型 \n 描述 \n \n \n \n \n StateTtlConfig.UpdateType.Disabled \n 禁用TTL，永不过期 \n \n \n StateTtlConfig.UpdateType.OnCreateAndWrite \n 每次写操作都会更新State的最后访问时间 \n \n \n StateTtlConfig.UpdateType.OnReadAndWrite \n 每次读写操作都会跟新State的最后访问时间 \n \n \n \n 状态可见性（默认NeverReturnExpired） \n \n \n \n 策略类型 \n 描述 \n \n \n \n \n StateTtlConfig.StateVisibility.NeverReturnExpired \n 永不返回过期状态 \n \n \n StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp \n 可以返回过期但尚未被清理的状态值 \n Checkpoint \n Checkpoint和State的区别 \n Checkpoint执行流程 \n \n 0.Flink的JobManager创建CheckpointCoordinator \n 1.Coordinator向所有的SourceOperator发送Barrier栅栏(理解为执行Checkpoint的信号) \n 2.SourceOperator接收到Barrier之后,暂停当前的操作(暂停的时间很短,因为后续的写快照是异步的),并制作State快照, 然后将自己的快照保存到指定的介质中(如HDFS), 一切 ok之后向Coordinator汇报并将Barrier发送给下游的其他Operator \n 3.其他的如TransformationOperator接收到Barrier,重复第2步,最后将Barrier发送给Sink \n 4.Sink接收到Barrier之后重复第2步 \n 5.Coordinator接收到所有的Operator的执行ok的汇报结果,认为本次快照执行成功 \n Chandy-Lamport algorithm 分布式快照算法 \n Flink中的Checkpoint底层使用了Chandy-Lamport algorithm 分布式快照算法 可以保证数据的在分布式环境下的 一致性 ! \n https://zhuanlan.zhihu.com/p/53482103 \n Chandy-Lamport algorithm算法的作者也是ZK中Paxos 一致性算法的作者 \n https://www.cnblogs.com/shenguanpu/p/4048660.html \n Flink中使用Chandy-Lamport algorithm分布式快照算法取得了成功,后续Spark的StructuredStreaming也借鉴了该算法 \n 状态后端/存储介质 \n \n \n \n \n   < dependency > \n        < groupId > org.apache.flink </ groupId > \n        < artifactId > flink-statebackend-rocksdb_2.12 </ artifactId > \n        < version > 1.12.0 </ version > \n     </ dependency > \n \n 1 2 3 4 5 Checkpoint代码演示 \n \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/checkpointing.html \n package   cn . itcast . checkpoint ; \n\n import   org . apache . commons . lang3 . SystemUtils ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . runtime . state . filesystem . FsStateBackend ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . CheckpointConfig ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaProducer ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . Properties ; \n\n /**\n * Author itcast\n * Desc 演示Flink-Checkpoint相关配置\n */ \n public   class   CheckpointDemo01   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO ===========Checkpoint参数设置==== \n         //===========类型1:必须参数============= \n         //设置Checkpoint的时间间隔为1000ms做一次Checkpoint/其实就是每隔1000ms发一次Barrier! \n        env . enableCheckpointing ( 1000 ) ; \n         //设置State状态存储介质/状态后端 \n         //Memory:State存内存,Checkpoint存内存--开发不用! \n         //Fs:State存内存,Checkpoint存FS(本地/HDFS)--一般情况下使用 \n         //RocksDB:State存RocksDB(内存+磁盘),Checkpoint存FS(本地/HDFS)--超大状态使用,但是对于状态的读写效率要低一点 \n         /*if(args.length > 0){\n            env.setStateBackend(new FsStateBackend(args[0]));\n        }else {\n            env.setStateBackend(new FsStateBackend("file:///D:\\\\data\\\\ckp"));\n        }*/ \n         if   ( SystemUtils . IS_OS_WINDOWS )   { \n            env . setStateBackend ( new   FsStateBackend ( "file:///D:/ckp" ) ) ; \n         }   else   { \n            env . setStateBackend ( new   FsStateBackend ( "hdfs://node1:8020/flink-checkpoint/checkpoint" ) ) ; \n         } \n         //===========类型2:建议参数=========== \n         //设置两个Checkpoint 之间最少等待时间,如设置Checkpoint之间最少是要等 500ms(为了避免每隔1000ms做一次Checkpoint的时候,前一次太慢和后一次重叠到一起去了) \n         //如:高速公路上,每隔1s关口放行一辆车,但是规定了两车之前的最小车距为500m \n        env . getCheckpointConfig ( ) . setMinPauseBetweenCheckpoints ( 500 ) ; //默认是0 \n         //设置如果在做Checkpoint过程中出现错误，是否让整体任务失败：true是  false不是 \n         //env.getCheckpointConfig().setFailOnCheckpointingErrors(false);//默认是true \n        env . getCheckpointConfig ( ) . setTolerableCheckpointFailureNumber ( 10 ) ; //默认值为0，表示不容忍任何检查点失败 \n         //设置是否清理检查点,表示 Cancel 时是否需要保留当前的 Checkpoint，默认 Checkpoint会在作业被Cancel时被删除 \n         //ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION：true,当作业被取消时，删除外部的checkpoint(默认值) \n         //ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION：false,当作业被取消时，保留外部的checkpoint \n        env . getCheckpointConfig ( ) . enableExternalizedCheckpoints ( CheckpointConfig . ExternalizedCheckpointCleanup . RETAIN_ON_CANCELLATION ) ; \n\n         //===========类型3:直接使用默认的即可=============== \n         //设置checkpoint的执行模式为EXACTLY_ONCE(默认) \n        env . getCheckpointConfig ( ) . setCheckpointingMode ( CheckpointingMode . EXACTLY_ONCE ) ; \n         //设置checkpoint的超时时间,如果 Checkpoint在 60s内尚未完成说明该次Checkpoint失败,则丢弃。 \n        env . getCheckpointConfig ( ) . setCheckpointTimeout ( 60000 ) ; //默认10分钟 \n         //设置同一时间有多少个checkpoint可以同时执行 \n         //env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);//默认为1 \n\n         //2.Source \n         DataStream < String >  linesDS  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n         //3.Transformation \n         //3.1切割出每个单词并直接记为1 \n         DataStream < Tuple2 < String ,   Integer > >  wordAndOneDS  =  linesDS . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 //value就是每一行 \n                 String [ ]  words  =  value . split ( " " ) ; \n                 for   ( String  word  :  words )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n         //3.2分组 \n         //注意:批处理的分组是groupBy,流处理的分组是keyBy \n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  groupedDS  =  wordAndOneDS . keyBy ( t  ->  t . f0 ) ; \n         //3.3聚合 \n         DataStream < Tuple2 < String ,   Integer > >  aggResult  =  groupedDS . sum ( 1 ) ; \n\n         DataStream < String >  result  =   ( SingleOutputStreamOperator < String > )  aggResult . map ( new   RichMapFunction < Tuple2 < String ,   Integer > ,   String > ( )   { \n             @Override \n             public   String   map ( Tuple2 < String ,   Integer >  value )   throws   Exception   { \n                 return  value . f0  +   ":::"   +  value . f1 ; \n             } \n         } ) ; \n\n         //4.sink \n        result . print ( ) ; \n\n         Properties  props  =   new   Properties ( ) ; \n        props . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; \n         FlinkKafkaProducer < String >  kafkaSink  =   new   FlinkKafkaProducer < > ( "flink_kafka" ,   new   SimpleStringSchema ( ) ,  props ) ; \n        result . addSink ( kafkaSink ) ; \n\n         //5.execute \n        env . execute ( ) ; \n\n         // /export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic flink_kafka \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 #  状态恢复-自动重启-全自动 \n \n package   cn . itcast . checkpoint ; \n\n import   org . apache . commons . lang3 . SystemUtils ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . common . restartstrategy . RestartStrategies ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . api . common . time . Time ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . runtime . state . filesystem . FsStateBackend ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . CheckpointConfig ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaProducer ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . Properties ; \n import   java . util . concurrent . TimeUnit ; \n\n /**\n * Author itcast\n * Desc 演示Flink-Checkpoint+重启策略实现状态恢复\n */ \n public   class   CheckpointDemo02_Restart   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO ===========Checkpoint参数设置==== \n         //===========类型1:必须参数============= \n         //设置Checkpoint的时间间隔为1000ms做一次Checkpoint/其实就是每隔1000ms发一次Barrier! \n        env . enableCheckpointing ( 1000 ) ; \n         //设置State状态存储介质/状态后端 \n         //Memory:State存内存,Checkpoint存内存--开发不用! \n         //Fs:State存内存,Checkpoint存FS(本地/HDFS)--一般情况下使用 \n         //RocksDB:State存RocksDB(内存+磁盘),Checkpoint存FS(本地/HDFS)--超大状态使用,但是对于状态的读写效率要低一点 \n         /*if(args.length > 0){\n            env.setStateBackend(new FsStateBackend(args[0]));\n        }else {\n            env.setStateBackend(new FsStateBackend("file:///D:\\\\data\\\\ckp"));\n        }*/ \n         if   ( SystemUtils . IS_OS_WINDOWS )   { \n            env . setStateBackend ( new   FsStateBackend ( "file:///D:/ckp" ) ) ; \n         }   else   { \n            env . setStateBackend ( new   FsStateBackend ( "hdfs://node1:8020/flink-checkpoint/checkpoint" ) ) ; \n         } \n         //===========类型2:建议参数=========== \n         //设置两个Checkpoint 之间最少等待时间,如设置Checkpoint之间最少是要等 500ms(为了避免每隔1000ms做一次Checkpoint的时候,前一次太慢和后一次重叠到一起去了) \n         //如:高速公路上,每隔1s关口放行一辆车,但是规定了两车之前的最小车距为500m \n        env . getCheckpointConfig ( ) . setMinPauseBetweenCheckpoints ( 500 ) ; //默认是0 \n         //设置如果在做Checkpoint过程中出现错误，是否让整体任务失败：true是  false不是 \n         //env.getCheckpointConfig().setFailOnCheckpointingErrors(false);//默认是true \n        env . getCheckpointConfig ( ) . setTolerableCheckpointFailureNumber ( 10 ) ; //默认值为0，表示不容忍任何检查点失败 \n         //设置是否清理检查点,表示 Cancel 时是否需要保留当前的 Checkpoint，默认 Checkpoint会在作业被Cancel时被删除 \n         //ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION：true,当作业被取消时，删除外部的checkpoint(默认值) \n         //ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION：false,当作业被取消时，保留外部的checkpoint \n        env . getCheckpointConfig ( ) . enableExternalizedCheckpoints ( CheckpointConfig . ExternalizedCheckpointCleanup . RETAIN_ON_CANCELLATION ) ; \n\n         //===========类型3:直接使用默认的即可=============== \n         //设置checkpoint的执行模式为EXACTLY_ONCE(默认) \n        env . getCheckpointConfig ( ) . setCheckpointingMode ( CheckpointingMode . EXACTLY_ONCE ) ; \n         //设置checkpoint的超时时间,如果 Checkpoint在 60s内尚未完成说明该次Checkpoint失败,则丢弃。 \n        env . getCheckpointConfig ( ) . setCheckpointTimeout ( 60000 ) ; //默认10分钟 \n         //设置同一时间有多少个checkpoint可以同时执行 \n        env . getCheckpointConfig ( ) . setMaxConcurrentCheckpoints ( 1 ) ; //默认为1 \n\n         //TODO ===配置重启策略: \n         //1.配置了Checkpoint的情况下不做任务配置:默认是无限重启并自动恢复,可以解决小问题,但是可能会隐藏真正的bug \n         //2.单独配置无重启策略 \n         //env.setRestartStrategy(RestartStrategies.noRestart()); \n         //3.固定延迟重启--开发中常用 \n        env . setRestartStrategy ( RestartStrategies . fixedDelayRestart ( \n                 3 ,   // 最多重启3次数 \n                 Time . of ( 5 ,   TimeUnit . SECONDS )   // 重启时间间隔 \n         ) ) ; \n         //上面的设置表示:如果job失败,重启3次, 每次间隔5s \n         //4.失败率重启--开发中偶尔使用 \n         /*env.setRestartStrategy(RestartStrategies.failureRateRestart(\n                3, // 每个测量阶段内最大失败次数\n                Time.of(1, TimeUnit.MINUTES), //失败率测量的时间间隔\n                Time.of(3, TimeUnit.SECONDS) // 两次连续重启的时间间隔\n        ));*/ \n         //上面的设置表示:如果1分钟内job失败不超过三次,自动重启,每次重启间隔3s (如果1分钟内程序失败达到3次,则程序退出) \n\n\n         //2.Source \n         DataStream < String >  linesDS  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n         //3.Transformation \n         //3.1切割出每个单词并直接记为1 \n         DataStream < Tuple2 < String ,   Integer > >  wordAndOneDS  =  linesDS . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 //value就是每一行 \n                 String [ ]  words  =  value . split ( " " ) ; \n                 for   ( String  word  :  words )   { \n                     if   ( word . equals ( "bug" ) )   { \n                         System . out . println ( "bug....." ) ; \n                         throw   new   Exception ( "bug....." ) ; \n                     } \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n         //3.2分组 \n         //注意:批处理的分组是groupBy,流处理的分组是keyBy \n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  groupedDS  =  wordAndOneDS . keyBy ( t  ->  t . f0 ) ; \n         //3.3聚合 \n         DataStream < Tuple2 < String ,   Integer > >  aggResult  =  groupedDS . sum ( 1 ) ; \n\n         DataStream < String >  result  =   ( SingleOutputStreamOperator < String > )  aggResult . map ( new   RichMapFunction < Tuple2 < String ,   Integer > ,   String > ( )   { \n             @Override \n             public   String   map ( Tuple2 < String ,   Integer >  value )   throws   Exception   { \n                 return  value . f0  +   ":::"   +  value . f1 ; \n             } \n         } ) ; \n\n         //4.sink \n        result . print ( ) ; \n\n         Properties  props  =   new   Properties ( ) ; \n        props . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; \n         FlinkKafkaProducer < String >  kafkaSink  =   new   FlinkKafkaProducer < > ( "flink_kafka" ,   new   SimpleStringSchema ( ) ,  props ) ; \n        result . addSink ( kafkaSink ) ; \n\n         //5.execute \n        env . execute ( ) ; \n\n         // /export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic flink_kafka \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 状态恢复-手动重启-半自动 \n 1.打包-用到了kafka \n 2.启动Flink集群 \n 3.上传jar包配置并提交 \n http://node1:8081/#/submit \n \n 4.发送单词并观察hdfs目录 \n 5.取消任务 \n \n 6.重新提交任务并指定从指定的ckp目录恢复状态接着计算 \n hdfs://node1:8020/flink-checkpoint/checkpoint/acb9071752276e86552a30fda41e021c/chk-100 \n \n 7.继续发送数据发现可以恢复从之前的状态继续计算 \n Savepoint-全手动 \n \n \n 演示 \n启动yarn session\n/export/server/flink/bin/yarn-session.sh -n 2 -tm 800 -s 2 -d\n运行job-会自动执行Checkpoint\n/export/server/flink/bin/flink run --class cn.itcast.checkpoint.CheckpointDemo01 /root/ckp.jar\n手动创建savepoint--相当于手动做了一次Checkpoint\n/export/server/flink/bin/flink savepoint 0e921a10eb31bb0983b637929ec87a8a hdfs://node1:8020/flink-checkpoint/savepoint/\n停止job\n/export/server/flink/bin/flink cancel 0e921a10eb31bb0983b637929ec87a8a\n重新启动job,手动加载savepoint数据\n/export/server/flink/bin/flink run -s hdfs://node1:8020/flink-checkpoint/savepoint/savepoint-0e921a-1cac737bff7a --class cn.itcast.checkpoint.CheckpointDemo01 /root/ckp.jar \n停止yarn session\nyarn application -kill application_1607782486484_0014\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #  BroadcastState-动态更新规则配置（存内存） \n flink广播变量有两种方式： \n 1.静态广播： \n //在分布式缓存中将本地的文件进行注册 \n        env . registerCachedFile ( "D:/wxgz-local/resources_ceshi/too.properties" ,   "too" ) \n         //使用时，在open中从上下文中加载 \n            val file :   File   =  getRuntimeContext . getDistributedCache . getFile ( "too" ) \n        val prop  =   new   Properties \n\n        prop . load ( new   FileInputStream ( file ) ) \n\n        val value  =  prop . getProperty ( "cycle" ) \n                \n //如果需要配置更新，可以增加定时器，定时去拉取更新，存在延迟。==>启动预加载（Guava Cache 可以过期，避免数据一直累积）+（单独线程reload）定时更新/实时lookup \n //这种方式与在open中初始化的有何不同？ \n区别 : \n 1. 广播变量是基于内存的 , 是将变量分发到各个worker节点的内存上（避免多次复制，节省内存）\n 2. 分布式缓存是基于磁盘的 , 将文件copy到各个节点上 , 当函数运行时可以在本地文件系统检索该文件（避免多次复制，提高执行效率）\n分布式缓存：\n Flink 提供了一个分布式缓存，类似于hadoop，可以使用户在并行函数中很方便的读取本地文件，并把它放在taskmanager节点中，防止task重复拉取。\n此缓存的工作机制如下：程序注册一个文件或者目录 ( 本地或者远程文件系统，例如hdfs或者s3 ) ，通过 ExecutionEnvironment 注册缓存文件并为它起一个名称。\n当程序执行， Flink 自动将文件或者目录复制到所有taskmanager节点的本地文件系统，仅会执行一次。用户可以通过这个指定的名称查找文件或者目录，然后从taskmanager节点的本地文件系统访问它。\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 2.动态广播更新 \n //如果失败了，且不从checkpoint恢复，共享变量就丢失，需要处理的问题：2.刚开始事件流会存在匹配不上的，就比如任务刚启动的时候；3.广播流是存内存不可能全量加载，也会出现join不上； \n //需要解决的问题： \n 1. 需要重新加载：\n    重新广播还是预加载？定时覆盖保存一份到本地，恢复时如果不从checkpoint或savepoint恢复时可以自己从之前的历史的加载。\n 2. 任务刚启动的时候，数据join不上，广播流先到，也有可能是未来的属性，事件流先到，就会有join不上的问题\n     SQL  temporary join 可以先保存两流状态，等到join上输出，可是保存在内存中，数据量大，版本太多占用内存，设置空闲状态过期\n \n 1 2 3 4 5 6 注意：广播流数据源是必须running状态，否则checkpoint会失败。所以要么mysql每次加载时间比ck小，要么mysql数据要打到kafka。或采用cdc 监测changelog方式 \n \n 需求 \n \n \n l 注意事项 \n \n \n Broadcast State 是Map 类型，即K-V 类型。 \n \n \n Broadcast State 只有在广播的一侧, 即在BroadcastProcessFunction 或KeyedBroadcastProcessFunction 的processBroadcastElement 方法中可以修改。在非广播的一侧， 即在BroadcastProcessFunction 或KeyedBroadcastProcessFunction 的processElement 方法中只读。 \n \n \n Broadcast State 中元素的顺序，在各Task 中可能不同。基于顺序的处理，需要注意。 \n \n \n Broadcast State 在Checkpoint 时，每个Task 都会Checkpoint 广播状态。 \n \n \n Broadcast State 在运行时 保存在内存中 ，目前还不能保存在RocksDB State Backend 中。 \n \n \n \n 有一个事件流--用户的行为日志,里面有用户id,但是没有用户的详细信息 \n 有一个配置流/规则流--用户信息流--里面有用户的详细的信息 \n 现在要将事件流和配置流进行关联, 得出日志中用户的详细信息,如 (用户id,详细信息, 操作) \n 那么我们可以将配置流/规则流--用户信息流 作为状态进行广播 (因为配置流/规则流--用户信息流较小) \n 思考： \n 1.配置流不应该是只上报有更新的配置，然后更新broadcast state的值，事件流再读取？ \n 数据 \n /**\n     * 随机事件流--数据量较大\n     * 用户id,时间,类型,产品id\n     * <userID, eventTime, eventType, productID>\n     */ \n     public   static   class   MySource   implements   SourceFunction < Tuple4 < String ,   String ,   String ,   Integer > >   { \n         private   boolean  isRunning  =   true ; \n         @Override \n         public   void   run ( SourceContext < Tuple4 < String ,   String ,   String ,   Integer > >  ctx )   throws   Exception   { \n             Random  random  =   new   Random ( ) ; \n             SimpleDateFormat  df  =   new   SimpleDateFormat ( "yyyy-MM-dd HH:mm:ss" ) ; \n             while   ( isRunning ) { \n                 int  id  =  random . nextInt ( 4 )   +   1 ; \n                 String  user_id  =   "user_"   +  id ; \n                 String  eventTime  =  df . format ( new   Date ( ) ) ; \n                 String  eventType  =   "type_"   +  random . nextInt ( 3 ) ; \n                 int  productId  =  random . nextInt ( 4 ) ; \n                ctx . collect ( Tuple4 . of ( user_id , eventTime , eventType , productId ) ) ; \n                 Thread . sleep ( 500 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            isRunning  =   false ; \n         } \n     } \n     /**\n     * 配置流/规则流/用户信息流--数量较小\n     * <用户id,<姓名,年龄>>\n     */ \n     /*\nCREATE TABLE `user_info` (\n  `userID` varchar(20) NOT NULL,\n  `userName` varchar(10) DEFAULT NULL,\n  `userAge` int(11) DEFAULT NULL,\n  PRIMARY KEY (`userID`) USING BTREE\n) ENGINE=MyISAM DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;\n\nINSERT INTO `user_info` VALUES (\'user_1\', \'张三\', 10);\nINSERT INTO `user_info` VALUES (\'user_2\', \'李四\', 20);\nINSERT INTO `user_info` VALUES (\'user_3\', \'王五\', 30);\nINSERT INTO `user_info` VALUES (\'user_4\', \'赵六\', 40);\n     */ \n     public   static   class   MySQLSource   extends   RichSourceFunction < Map < String ,   Tuple2 < String ,   Integer > > >   { \n         private   boolean  flag  =   true ; \n         private   Connection  conn  =   null ; \n         private   PreparedStatement  ps  =   null ; \n         private   ResultSet  rs  =   null ; \n\n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            conn  =   DriverManager . getConnection ( "jdbc:mysql://localhost:3306/bigdata" ,   "root" ,   "root" ) ; \n             String  sql  =   "select `userID`, `userName`, `userAge` from `user_info`" ; \n            ps  =  conn . prepareStatement ( sql ) ; \n         } \n         @Override \n         public   void   run ( SourceContext < Map < String ,   Tuple2 < String ,   Integer > > >  ctx )   throws   Exception   { \n             while   ( flag ) { \n                 Map < String ,   Tuple2 < String ,   Integer > >  map  =   new   HashMap < > ( ) ; \n                 ResultSet  rs  =  ps . executeQuery ( ) ; \n                 while   ( rs . next ( ) ) { \n                     String  userID  =  rs . getString ( "userID" ) ; \n                     String  userName  =  rs . getString ( "userName" ) ; \n                     int  userAge  =  rs . getInt ( "userAge" ) ; \n                     //Map<String, Tuple2<String, Integer>> \n                    map . put ( userID ,   Tuple2 . of ( userName , userAge ) ) ; \n                 } \n                ctx . collect ( map ) ; \n                 Thread . sleep ( 5000 ) ; //每隔5s更新一下用户的配置信息! \n             } \n         } \n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n         @Override \n         public   void   close ( )   throws   Exception   { \n             if   ( conn  !=   null )  conn . close ( ) ; \n             if   ( ps  !=   null )  ps . close ( ) ; \n             if   ( rs  !=   null )  rs . close ( ) ; \n         } \n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 代码步骤 \n 1.env\n2.source\n-1.构建实时数据事件流-自定义随机\n<userID, eventTime, eventType, productID>\n-2.构建配置流-从MySQL\n<用户id,<姓名,年龄>>\n3.transformation\n-1.定义状态描述器\nMapStateDescriptor<Void, Map<String, Tuple2<String, Integer>>> descriptor =\nnew MapStateDescriptor<>("config",Types.VOID, Types.MAP(Types.STRING, Types.TUPLE(Types.STRING, Types.INT)));\n\n-2.广播配置流\nBroadcastStream<Map<String, Tuple2<String, Integer>>> broadcastDS = configDS.broadcast(descriptor);\n-3.将事件流和广播流进行连接\nBroadcastConnectedStream<Tuple4<String, String, String, Integer>, Map<String, Tuple2<String, Integer>>> connectDS =eventDS.connect(broadcastDS);\n-4.处理连接后的流-根据配置流补全事件流中的用户的信息\n\n4.sink\n5.execute\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 代码实现 \n package   cn . itcast . feature ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . state . BroadcastState ; \n import   org . apache . flink . api . common . state . MapStateDescriptor ; \n import   org . apache . flink . api . common . state . ReadOnlyBroadcastState ; \n import   org . apache . flink . api . common . typeinfo . Types ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . api . java . tuple . Tuple4 ; \n import   org . apache . flink . api . java . tuple . Tuple6 ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . BroadcastConnectedStream ; \n import   org . apache . flink . streaming . api . datastream . BroadcastStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . co . BroadcastProcessFunction ; \n import   org . apache . flink . streaming . api . functions . source . RichSourceFunction ; \n import   org . apache . flink . streaming . api . functions . source . SourceFunction ; \n import   org . apache . flink . util . Collector ; \n\n import   java . sql . Connection ; \n import   java . sql . DriverManager ; \n import   java . sql . PreparedStatement ; \n import   java . sql . ResultSet ; \n import   java . text . SimpleDateFormat ; \n import   java . util . Date ; \n import   java . util . HashMap ; \n import   java . util . Map ; \n import   java . util . Random ; \n\n /**\n * Author itcast\n * Desc\n */ \n public   class   BroadcastStateDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n        env . setParallelism ( 1 ) ; \n\n         //TODO 2.source \n         //-1.构建实时数据事件流--数据量较大 \n         //<userID, eventTime, eventType, productID> \n         DataStreamSource < Tuple4 < String ,   String ,   String ,   Integer > >  eventDS  =  env . addSource ( new   MySource ( ) ) ; \n\n         //-2.配置流/规则流/用户信息流--数据量较小-从MySQL \n         //<用户id,<姓名,年龄>> \n         DataStreamSource < Map < String ,   Tuple2 < String ,   Integer > > >  userDS  =  env . addSource ( new   MySQLSource ( ) ) ; \n\n         //TODO 3.transformation \n         //-1.定义状态描述器 \n         //key为什么用null？ \n         MapStateDescriptor < Void ,   Map < String ,   Tuple2 < String ,   Integer > > >  descriptor  = \n                 new   MapStateDescriptor < > ( "info" ,   Types . VOID ,   Types . MAP ( Types . STRING ,   Types . TUPLE ( Types . STRING ,   Types . INT ) ) ) ; \n\n         //-2.广播配置流 \n         BroadcastStream < Map < String ,   Tuple2 < String ,   Integer > > >  broadcastDS  =  userDS . broadcast ( descriptor ) ; \n\n         //-3.将事件流和广播流进行连接 \n         BroadcastConnectedStream < Tuple4 < String ,   String ,   String ,   Integer > ,   Map < String ,   Tuple2 < String ,   Integer > > >  connectDS  =  eventDS . connect ( broadcastDS ) ; \n\n         //-4.处理连接后的流-根据配置流补全事件流中的用户的信息 \n         //BroadcastProcessFunction<IN1, IN2, OUT> \n         SingleOutputStreamOperator < Tuple6 < String ,   String ,   String ,   Integer ,   String ,   Integer > >  result  = \n                connectDS . process ( new   BroadcastProcessFunction < \n                         //<userID, eventTime, eventType, productID> //事件流 \n                         Tuple4 < String ,   String ,   String ,   Integer > , \n                         //<用户id,<姓名,年龄>> //广播流 \n                         Map < String ,   Tuple2 < String ,   Integer > > , \n                         //<用户id，eventTime，eventType，productID，姓名，年龄> //结果流 需要收集的数据 \n                         Tuple6 < String ,   String ,   String ,   Integer ,   String ,   Integer > \n                         > ( )   { \n                     //处理事件流中的每一个元素 \n                     @Override \n                     public   void   processElement ( Tuple4 < String ,   String ,   String ,   Integer >  value ,   ReadOnlyContext  ctx ,   Collector < Tuple6 < String ,   String ,   String ,   Integer ,   String ,   Integer > >  out )   throws   Exception   { \n                         //value就是事件流中的数据 \n                         //<userID, eventTime, eventType, productID> //事件流--已经有了 \n                         //Tuple4<String, String, String, Integer>, \n                         //目标是将value和广播流中的数据进行关联,返回结果流 \n                         //<用户id,<姓名,年龄>> //广播流--需要获取 \n                         //Map<String, Tuple2<String, Integer>> \n                         //<用户id，eventTime，eventType，productID，姓名，年龄> //结果流 需要收集的数据 \n                         // Tuple6<String, String, String, Integer, String, Integer> \n\n                         //获取广播流 \n                         ReadOnlyBroadcastState < Void ,   Map < String ,   Tuple2 < String ,   Integer > > >  broadcastState  =  ctx . getBroadcastState ( descriptor ) ; \n                         //用户id,<姓名,年龄> \n                         Map < String ,   Tuple2 < String ,   Integer > >  map  =  broadcastState . get ( null ) ; //广播流中的数据 \n                         if   ( map  !=   null )   { \n                             //根据value中的用户id去map中获取用户信息 \n                             String  userId  =  value . f0 ; \n                             Tuple2 < String ,   Integer >  tuple2  =  map . get ( userId ) ; \n                             String  username  =  tuple2 . f0 ; \n                             Integer  age  =  tuple2 . f1 ; \n\n                             //收集数据 \n                            out . collect ( Tuple6 . of ( userId ,  value . f1 ,  value . f2 ,  value . f3 ,  username ,  age ) ) ; \n                         } \n                     } \n\n                     //更新处理广播流中的数据 \n                     @Override \n                     public   void   processBroadcastElement ( Map < String ,   Tuple2 < String ,   Integer > >  value ,   Context  ctx ,   Collector < Tuple6 < String ,   String ,   String ,   Integer ,   String ,   Integer > >  out )   throws   Exception   { \n                         //value就是从MySQL中每隔5是查询出来并广播到状态中的最新数据! \n                         //要把最新的数据放到state中 \n                         BroadcastState < Void ,   Map < String ,   Tuple2 < String ,   Integer > > >  broadcastState  =  ctx . getBroadcastState ( descriptor ) ; \n                        broadcastState . clear ( ) ; //清空旧数据 \n                        broadcastState . put ( null ,  value ) ; //放入新数据 \n                     } \n                 } ) ; \n\n         //TODO 4.sink \n        result . print ( ) ; \n\n         //TODO 5.execute \n        env . execute ( ) ; \n     } \n\n\n     /**\n     * 随机事件流--数据量较大\n     * 用户id,时间,类型,产品id\n     * <userID, eventTime, eventType, productID>\n     */ \n     public   static   class   MySource   implements   SourceFunction < Tuple4 < String ,   String ,   String ,   Integer > >   { \n         private   boolean  isRunning  =   true ; \n\n         @Override \n         public   void   run ( SourceContext < Tuple4 < String ,   String ,   String ,   Integer > >  ctx )   throws   Exception   { \n             Random  random  =   new   Random ( ) ; \n             SimpleDateFormat  df  =   new   SimpleDateFormat ( "yyyy-MM-dd HH:mm:ss" ) ; \n             while   ( isRunning )   { \n                 int  id  =  random . nextInt ( 4 )   +   1 ; \n                 String  user_id  =   "user_"   +  id ; \n                 String  eventTime  =  df . format ( new   Date ( ) ) ; \n                 String  eventType  =   "type_"   +  random . nextInt ( 3 ) ; \n                 int  productId  =  random . nextInt ( 4 ) ; \n                ctx . collect ( Tuple4 . of ( user_id ,  eventTime ,  eventType ,  productId ) ) ; \n                 Thread . sleep ( 500 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            isRunning  =   false ; \n         } \n     } \n\n     /**\n     * 配置流/规则流/用户信息流--数据量较小\n     * <用户id,<姓名,年龄>>\n     */ \n     /*\nCREATE TABLE `user_info` (\n  `userID` varchar(20) NOT NULL,\n  `userName` varchar(10) DEFAULT NULL,\n  `userAge` int(11) DEFAULT NULL,\n  PRIMARY KEY (`userID`) USING BTREE\n) ENGINE=MyISAM DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;\n\nINSERT INTO `user_info` VALUES (\'user_1\', \'张三\', 10);\nINSERT INTO `user_info` VALUES (\'user_2\', \'李四\', 20);\nINSERT INTO `user_info` VALUES (\'user_3\', \'王五\', 30);\nINSERT INTO `user_info` VALUES (\'user_4\', \'赵六\', 40);\n     */ \n     public   static   class   MySQLSource   extends   RichSourceFunction < Map < String ,   Tuple2 < String ,   Integer > > >   { \n         private   boolean  flag  =   true ; \n         private   Connection  conn  =   null ; \n         private   PreparedStatement  ps  =   null ; \n         private   ResultSet  rs  =   null ; \n\n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            conn  =   DriverManager . getConnection ( "jdbc:mysql://localhost:3306/bigdata" ,   "root" ,   "root" ) ; \n             String  sql  =   "select `userID`, `userName`, `userAge` from `user_info`" ; \n            ps  =  conn . prepareStatement ( sql ) ; \n         } \n\n         @Override \n         public   void   run ( SourceContext < Map < String ,   Tuple2 < String ,   Integer > > >  ctx )   throws   Exception   { \n             while   ( flag )   { \n                 Map < String ,   Tuple2 < String ,   Integer > >  map  =   new   HashMap < > ( ) ; \n                 ResultSet  rs  =  ps . executeQuery ( ) ; \n                 while   ( rs . next ( ) )   { \n                     String  userID  =  rs . getString ( "userID" ) ; \n                     String  userName  =  rs . getString ( "userName" ) ; \n                     int  userAge  =  rs . getInt ( "userAge" ) ; \n                     //Map<String, Tuple2<String, Integer>> \n                    map . put ( userID ,   Tuple2 . of ( userName ,  userAge ) ) ; \n                 } \n                ctx . collect ( map ) ; \n                 Thread . sleep ( 5000 ) ; //每隔5s更新一下用户的配置信息! \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n\n         @Override \n         public   void   close ( )   throws   Exception   { \n             if   ( conn  !=   null )  conn . close ( ) ; \n             if   ( ps  !=   null )  ps . close ( ) ; \n             if   ( rs  !=   null )  rs . close ( ) ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 个人认为： \n 1.维表数据量小，且可控，使用broadcast+预加载 \n （单独线程从本地reload或从分布式缓存加载，从本地reload每个task都要拉取一份，分布式缓存是每个节点拉取一份，从网络io考虑，分布式缓存。） \n 2.维表数据量大，或者两条事件流join，把行为串联，可以用热存储，把大概率先到的一条流存外部系统，然后另一条流interval join对应版本(异步io和Guava Cache减轻网络延迟和压力)。 \n 3.上面的方式维表的更新存在延迟，如果对实时要求高，且两条流互相等待的时间不大，可以考虑flink sql temporary join，实现起来代码简单。但是state的管理是个问题，设置不合理，内存会溢出。 \n 涉及到低水位的就要注意一个问题，如果某个分区没数据，低水位就不会推进。简单的方法就是加空闲源检测机制idle，比较好的方法是定义一个定期触发的trigger，推进水位。 \n Flink-双流Join \n join的分类 \n \n join() 算子提供的语义为"Window join"，即按照指定字段和（滚动/滑动/会话）窗口进行 inner join，支持处理时间和事件时间两种时间特征。 \n coGroup() \n只有 inner join 肯定还不够，如何实现 left/right outer join 呢？答案就是利用 coGroup() 算子。 \n 它的调用方式类似于 join() 算子，也需要开窗，但是 CoGroupFunction 比 JoinFunction 更加灵活，可以按照用户指定的逻辑匹配左流和/或右流的数据并输出。 \n dataStream . coGroup ( otherStream ) \n     . where ( 0 ) . equalTo ( 1 ) \n     . window ( TumblingEventTimeWindows . of ( Time . seconds ( 3 ) ) ) \n     . apply  ( new   CoGroupFunction   ( )   { . . . } ) ; \n \n 1 2 3 4 \n \n \n join() 和 coGroup() 都是基于窗口做关联的。但是在某些情况下，两条流的数据步调未必一致。例如，订单流的数据有可能在点击流的购买动作发生之后很久才被写入，如果用窗口来圈定，很容易 join 不上。 \n 所以 Flink 又提供了"Interval join"的语义，按照指定字段以及右流相对左流偏移的时间区间进行关联，即：right.timestamp ∈ [left.timestamp + lowerBound; left.timestamp + upperBound] \n interval join 也是 inner join，虽然不需要开窗，但是需要用户指定偏移区间的上下界，并且只支持事件时间。注意在运行之前，需要分别 在两个流上应用 assignTimestampsAndWatermarks() 方法获取事件时间戳和水印。 \n //调用between进行检查 \n if   ( timeBehaviour  !=   TimeBehaviour . EventTime )   { \n\t\t\t\t throw   new   UnsupportedTimeCharacteristicException ( "Time-bounded stream joins are only supported in event time" ) ; \n\t\t\t } \n \n 1 2 3 4 为什么说window join和interval join的区别在于interval state有清理机制？ \n 双流join的本质就是把双流利用state把数据储存起来，然后计算时嵌套循环判断join。window state 保存一个窗口的 state的数据，interval 如果不清除保留的是历史这个key的所有state，状态太大，join前把不需要的清理掉再循环join。 \n \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/joining.html \n 代码演示-WindowJoin \n \n package   cn . itcast . feature ; \n\n import   com . alibaba . fastjson . JSON ; \n import   lombok . Data ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . eventtime . * ; \n import   org . apache . flink . api . common . functions . JoinFunction ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichSourceFunction ; \n import   org . apache . flink . streaming . api . windowing . assigners . TumblingEventTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n\n import   java . math . BigDecimal ; \n import   java . util . ArrayList ; \n import   java . util . List ; \n import   java . util . Random ; \n import   java . util . UUID ; \n import   java . util . concurrent . TimeUnit ; \n\n /**\n * Author itcast\n * Desc 演示Flink双流Join-windowJoin\n */ \n public   class   JoinDemo01_WindowJoin   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         //商品数据流 \n         DataStreamSource < Goods >  goodsDS  =  env . addSource ( new   GoodsSource ( ) ) ; \n         //订单数据流 \n         DataStreamSource < OrderItem >   OrderItemDS   =  env . addSource ( new   OrderItemSource ( ) ) ; \n         //给数据添加水印(这里简单一点直接使用系统时间作为事件时间) \n         /*\n         SingleOutputStreamOperator<Order> orderDSWithWatermark = orderDS.assignTimestampsAndWatermarks(\n                WatermarkStrategy.<Order>forBoundedOutOfOrderness(Duration.ofSeconds(3))//指定maxOutOfOrderness最大无序度/最大允许的延迟时间/乱序时间\n                        .withTimestampAssigner((order, timestamp) -> order.getEventTime())//指定事件时间列\n        );\n         */ \n         SingleOutputStreamOperator < Goods >  goodsDSWithWatermark  =  goodsDS . assignTimestampsAndWatermarks ( new   GoodsWatermark ( ) ) ; \n         SingleOutputStreamOperator < OrderItem >   OrderItemDSWithWatermark   =   OrderItemDS . assignTimestampsAndWatermarks ( new   OrderItemWatermark ( ) ) ; \n\n\n         //TODO 2.transformation---这里是重点 \n         //商品类(商品id,商品名称,商品价格) \n         //订单明细类(订单id,商品id,商品数量) \n         //关联结果(商品id,商品名称,商品数量,商品价格*商品数量) \n         DataStream < FactOrderItem >  resultDS  =  goodsDSWithWatermark . join ( OrderItemDSWithWatermark ) \n                 . where ( Goods :: getGoodsId ) \n                 . equalTo ( OrderItem :: getGoodsId ) \n                 . window ( TumblingEventTimeWindows . of ( Time . seconds ( 5 ) ) ) \n                 //<IN1, IN2, OUT> \n                 . apply ( new   JoinFunction < Goods ,   OrderItem ,   FactOrderItem > ( )   { \n                     @Override \n                     public   FactOrderItem   join ( Goods  first ,   OrderItem  second )   throws   Exception   { \n                         FactOrderItem  result  =   new   FactOrderItem ( ) ; \n                        result . setGoodsId ( first . getGoodsId ( ) ) ; \n                        result . setGoodsName ( first . getGoodsName ( ) ) ; \n                        result . setCount ( new   BigDecimal ( second . getCount ( ) ) ) ; \n                        result . setTotalMoney ( new   BigDecimal ( second . getCount ( ) ) . multiply ( first . getGoodsPrice ( ) ) ) ; \n                         return  result ; \n                     } \n                 } ) ; \n\n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     //商品类(商品id,商品名称,商品价格) \n     @Data \n     public   static   class   Goods   { \n         private   String  goodsId ; \n         private   String  goodsName ; \n         private   BigDecimal  goodsPrice ; \n         public   static   List < Goods >   GOODS_LIST ; \n         public   static   Random  r ; \n\n         static    { \n            r  =   new   Random ( ) ; \n             GOODS_LIST   =   new   ArrayList < > ( ) ; \n             GOODS_LIST . add ( new   Goods ( "1" ,   "小米12" ,   new   BigDecimal ( 4890 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "2" ,   "iphone12" ,   new   BigDecimal ( 12000 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "3" ,   "MacBookPro" ,   new   BigDecimal ( 15000 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "4" ,   "Thinkpad X1" ,   new   BigDecimal ( 9800 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "5" ,   "MeiZu One" ,   new   BigDecimal ( 3200 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "6" ,   "Mate 40" ,   new   BigDecimal ( 6500 ) ) ) ; \n         } \n         public   static   Goods   randomGoods ( )   { \n             int  rIndex  =  r . nextInt ( GOODS_LIST . size ( ) ) ; \n             return   GOODS_LIST . get ( rIndex ) ; \n         } \n         public   Goods ( )   { \n         } \n         public   Goods ( String  goodsId ,   String  goodsName ,   BigDecimal  goodsPrice )   { \n             this . goodsId  =  goodsId ; \n             this . goodsName  =  goodsName ; \n             this . goodsPrice  =  goodsPrice ; \n         } \n         @Override \n         public   String   toString ( )   { \n             return   JSON . toJSONString ( this ) ; \n         } \n     } \n\n     //订单明细类(订单id,商品id,商品数量) \n     @Data \n     public   static   class   OrderItem   { \n         private   String  itemId ; \n         private   String  goodsId ; \n         private   Integer  count ; \n         @Override \n         public   String   toString ( )   { \n             return   JSON . toJSONString ( this ) ; \n         } \n     } \n\n     //商品类(商品id,商品名称,商品价格) \n     //订单明细类(订单id,商品id,商品数量) \n     //关联结果(商品id,商品名称,商品数量,商品价格*商品数量) \n     @Data \n     public   static   class   FactOrderItem   { \n         private   String  goodsId ; \n         private   String  goodsName ; \n         private   BigDecimal  count ; \n         private   BigDecimal  totalMoney ; \n         @Override \n         public   String   toString ( )   { \n             return   JSON . toJSONString ( this ) ; \n         } \n     } \n\n     //实时生成商品数据流 \n     //构建一个商品Stream源（这个好比就是维表） \n     public   static   class   GoodsSource   extends   RichSourceFunction < Goods >   { \n         private   Boolean  isCancel ; \n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            isCancel  =   false ; \n         } \n         @Override \n         public   void   run ( SourceContext  sourceContext )   throws   Exception   { \n             while ( ! isCancel )   { \n                 Goods . GOODS_LIST . stream ( ) . forEach ( goods  ->  sourceContext . collect ( goods ) ) ; \n                 TimeUnit . SECONDS . sleep ( 1 ) ; \n             } \n         } \n         @Override \n         public   void   cancel ( )   { \n            isCancel  =   true ; \n         } \n     } \n     //实时生成订单数据流 \n     //构建订单明细Stream源 \n     public   static   class   OrderItemSource   extends   RichSourceFunction < OrderItem >   { \n         private   Boolean  isCancel ; \n         private   Random  r ; \n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            isCancel  =   false ; \n            r  =   new   Random ( ) ; \n         } \n         @Override \n         public   void   run ( SourceContext  sourceContext )   throws   Exception   { \n             while ( ! isCancel )   { \n                 Goods  goods  =   Goods . randomGoods ( ) ; \n                 OrderItem  orderItem  =   new   OrderItem ( ) ; \n                orderItem . setGoodsId ( goods . getGoodsId ( ) ) ; \n                orderItem . setCount ( r . nextInt ( 10 )   +   1 ) ; \n                orderItem . setItemId ( UUID . randomUUID ( ) . toString ( ) ) ; \n                sourceContext . collect ( orderItem ) ; \n                orderItem . setGoodsId ( "111" ) ; \n                sourceContext . collect ( orderItem ) ; \n                 TimeUnit . SECONDS . sleep ( 1 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            isCancel  =   true ; \n         } \n     } \n     //构建水印分配器，学习测试直接使用系统时间了 \n     public   static   class   GoodsWatermark   implements   WatermarkStrategy < Goods >   { \n         @Override \n         public   TimestampAssigner < Goods >   createTimestampAssigner ( TimestampAssignerSupplier . Context  context )   { \n             return   ( element ,  recordTimestamp )   ->   System . currentTimeMillis ( ) ; \n         } \n         @Override \n         public   WatermarkGenerator < Goods >   createWatermarkGenerator ( WatermarkGeneratorSupplier . Context  context )   { \n             return   new   WatermarkGenerator < Goods > ( )   { \n                 @Override \n                 public   void   onEvent ( Goods  event ,   long  eventTimestamp ,   WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n\n                 @Override \n                 public   void   onPeriodicEmit ( WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n             } ; \n         } \n     } \n     //构建水印分配器，学习测试直接使用系统时间了 \n     public   static   class   OrderItemWatermark   implements   WatermarkStrategy < OrderItem >   { \n         @Override \n         public   TimestampAssigner < OrderItem >   createTimestampAssigner ( TimestampAssignerSupplier . Context  context )   { \n             return   ( element ,  recordTimestamp )   ->   System . currentTimeMillis ( ) ; \n         } \n         @Override \n         public   WatermarkGenerator < OrderItem >   createWatermarkGenerator ( WatermarkGeneratorSupplier . Context  context )   { \n             return   new   WatermarkGenerator < OrderItem > ( )   { \n                 @Override \n                 public   void   onEvent ( OrderItem  event ,   long  eventTimestamp ,   WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n                 @Override \n                 public   void   onPeriodicEmit ( WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n             } ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 #  代码演示-IntervalJoin \n \n package   cn . itcast . feature ; \n\n import   com . alibaba . fastjson . JSON ; \n import   lombok . Data ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . eventtime . * ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . co . ProcessJoinFunction ; \n import   org . apache . flink . streaming . api . functions . source . RichSourceFunction ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n import   org . apache . flink . util . Collector ; \n\n import   java . math . BigDecimal ; \n import   java . util . ArrayList ; \n import   java . util . List ; \n import   java . util . Random ; \n import   java . util . UUID ; \n import   java . util . concurrent . TimeUnit ; \n\n /**\n * Author itcast\n * Desc 演示Flink双流Join-IntervalJoin\n */ \n public   class   JoinDemo02_IntervalJoin   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         //商品数据流 \n         DataStreamSource < Goods >  goodsDS  =  env . addSource ( new   GoodsSource ( ) ) ; \n         //订单数据流 \n         DataStreamSource < OrderItem >   OrderItemDS   =  env . addSource ( new   OrderItemSource ( ) ) ; \n         //给数据添加水印(这里简单一点直接使用系统时间作为事件时间) \n         /*\n         SingleOutputStreamOperator<Order> orderDSWithWatermark = orderDS.assignTimestampsAndWatermarks(\n                WatermarkStrategy.<Order>forBoundedOutOfOrderness(Duration.ofSeconds(3))//指定maxOutOfOrderness最大无序度/最大允许的延迟时间/乱序时间\n                        .withTimestampAssigner((order, timestamp) -> order.getEventTime())//指定事件时间列\n        );\n         */ \n         SingleOutputStreamOperator < Goods >  goodsDSWithWatermark  =  goodsDS . assignTimestampsAndWatermarks ( new   GoodsWatermark ( ) ) ; \n         SingleOutputStreamOperator < OrderItem >   OrderItemDSWithWatermark   =   OrderItemDS . assignTimestampsAndWatermarks ( new   OrderItemWatermark ( ) ) ; \n\n\n         //TODO 2.transformation---这里是重点 \n         //商品类(商品id,商品名称,商品价格) \n         //订单明细类(订单id,商品id,商品数量) \n         //关联结果(商品id,商品名称,商品数量,商品价格*商品数量) \n         SingleOutputStreamOperator < FactOrderItem >  resultDS  =  goodsDSWithWatermark . keyBy ( Goods :: getGoodsId ) \n                 . intervalJoin ( OrderItemDSWithWatermark . keyBy ( OrderItem :: getGoodsId ) ) \n                 //join的条件: \n                 // 条件1.id要相等 \n                 // 条件2. OrderItem的时间戳 - 2 <=Goods的时间戳 <= OrderItem的时间戳 + 1 \n                 . between ( Time . seconds ( - 2 ) ,   Time . seconds ( 1 ) ) \n                 //ProcessJoinFunction<IN1, IN2, OUT> \n                 . process ( new   ProcessJoinFunction < Goods ,   OrderItem ,   FactOrderItem > ( )   { \n                     @Override \n                     public   void   processElement ( Goods  left ,   OrderItem  right ,   Context  ctx ,   Collector < FactOrderItem >  out )   throws   Exception   { \n                         FactOrderItem  result  =   new   FactOrderItem ( ) ; \n                        result . setGoodsId ( left . getGoodsId ( ) ) ; \n                        result . setGoodsName ( left . getGoodsName ( ) ) ; \n                        result . setCount ( new   BigDecimal ( right . getCount ( ) ) ) ; \n                        result . setTotalMoney ( new   BigDecimal ( right . getCount ( ) ) . multiply ( left . getGoodsPrice ( ) ) ) ; \n                        out . collect ( result ) ; \n                     } \n                 } ) ; \n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     //商品类(商品id,商品名称,商品价格) \n     @Data \n     public   static   class   Goods   { \n         private   String  goodsId ; \n         private   String  goodsName ; \n         private   BigDecimal  goodsPrice ; \n         public   static   List < Goods >   GOODS_LIST ; \n         public   static   Random  r ; \n\n         static    { \n            r  =   new   Random ( ) ; \n             GOODS_LIST   =   new   ArrayList < > ( ) ; \n             GOODS_LIST . add ( new   Goods ( "1" ,   "小米12" ,   new   BigDecimal ( 4890 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "2" ,   "iphone12" ,   new   BigDecimal ( 12000 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "3" ,   "MacBookPro" ,   new   BigDecimal ( 15000 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "4" ,   "Thinkpad X1" ,   new   BigDecimal ( 9800 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "5" ,   "MeiZu One" ,   new   BigDecimal ( 3200 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "6" ,   "Mate 40" ,   new   BigDecimal ( 6500 ) ) ) ; \n         } \n         public   static   Goods   randomGoods ( )   { \n             int  rIndex  =  r . nextInt ( GOODS_LIST . size ( ) ) ; \n             return   GOODS_LIST . get ( rIndex ) ; \n         } \n         public   Goods ( )   { \n         } \n         public   Goods ( String  goodsId ,   String  goodsName ,   BigDecimal  goodsPrice )   { \n             this . goodsId  =  goodsId ; \n             this . goodsName  =  goodsName ; \n             this . goodsPrice  =  goodsPrice ; \n         } \n         @Override \n         public   String   toString ( )   { \n             return   JSON . toJSONString ( this ) ; \n         } \n     } \n\n     //订单明细类(订单id,商品id,商品数量) \n     @Data \n     public   static   class   OrderItem   { \n         private   String  itemId ; \n         private   String  goodsId ; \n         private   Integer  count ; \n         @Override \n         public   String   toString ( )   { \n             return   JSON . toJSONString ( this ) ; \n         } \n     } \n\n     //商品类(商品id,商品名称,商品价格) \n     //订单明细类(订单id,商品id,商品数量) \n     //关联结果(商品id,商品名称,商品数量,商品价格*商品数量) \n     @Data \n     public   static   class   FactOrderItem   { \n         private   String  goodsId ; \n         private   String  goodsName ; \n         private   BigDecimal  count ; \n         private   BigDecimal  totalMoney ; \n         @Override \n         public   String   toString ( )   { \n             return   JSON . toJSONString ( this ) ; \n         } \n     } \n\n     //实时生成商品数据流 \n     //构建一个商品Stream源（这个好比就是维表） \n     public   static   class   GoodsSource   extends   RichSourceFunction < Goods >   { \n         private   Boolean  isCancel ; \n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            isCancel  =   false ; \n         } \n         @Override \n         public   void   run ( SourceContext  sourceContext )   throws   Exception   { \n             while ( ! isCancel )   { \n                 Goods . GOODS_LIST . stream ( ) . forEach ( goods  ->  sourceContext . collect ( goods ) ) ; \n                 TimeUnit . SECONDS . sleep ( 1 ) ; \n             } \n         } \n         @Override \n         public   void   cancel ( )   { \n            isCancel  =   true ; \n         } \n     } \n     //实时生成订单数据流 \n     //构建订单明细Stream源 \n     public   static   class   OrderItemSource   extends   RichSourceFunction < OrderItem >   { \n         private   Boolean  isCancel ; \n         private   Random  r ; \n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            isCancel  =   false ; \n            r  =   new   Random ( ) ; \n         } \n         @Override \n         public   void   run ( SourceContext  sourceContext )   throws   Exception   { \n             while ( ! isCancel )   { \n                 Goods  goods  =   Goods . randomGoods ( ) ; \n                 OrderItem  orderItem  =   new   OrderItem ( ) ; \n                orderItem . setGoodsId ( goods . getGoodsId ( ) ) ; \n                orderItem . setCount ( r . nextInt ( 10 )   +   1 ) ; \n                orderItem . setItemId ( UUID . randomUUID ( ) . toString ( ) ) ; \n                sourceContext . collect ( orderItem ) ; \n                orderItem . setGoodsId ( "111" ) ; \n                sourceContext . collect ( orderItem ) ; \n                 TimeUnit . SECONDS . sleep ( 1 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            isCancel  =   true ; \n         } \n     } \n     //构建水印分配器，学习测试直接使用系统时间了 \n     public   static   class   GoodsWatermark   implements   WatermarkStrategy < Goods >   { \n         @Override \n         public   TimestampAssigner < Goods >   createTimestampAssigner ( TimestampAssignerSupplier . Context  context )   { \n             return   ( element ,  recordTimestamp )   ->   System . currentTimeMillis ( ) ; \n         } \n         @Override \n         public   WatermarkGenerator < Goods >   createWatermarkGenerator ( WatermarkGeneratorSupplier . Context  context )   { \n             return   new   WatermarkGenerator < Goods > ( )   { \n                 @Override \n                 public   void   onEvent ( Goods  event ,   long  eventTimestamp ,   WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n\n                 @Override \n                 public   void   onPeriodicEmit ( WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n             } ; \n         } \n     } \n     //构建水印分配器，学习测试直接使用系统时间了 \n     public   static   class   OrderItemWatermark   implements   WatermarkStrategy < OrderItem >   { \n         @Override \n         public   TimestampAssigner < OrderItem >   createTimestampAssigner ( TimestampAssignerSupplier . Context  context )   { \n             return   ( element ,  recordTimestamp )   ->   System . currentTimeMillis ( ) ; \n         } \n         @Override \n         public   WatermarkGenerator < OrderItem >   createWatermarkGenerator ( WatermarkGeneratorSupplier . Context  context )   { \n             return   new   WatermarkGenerator < OrderItem > ( )   { \n                 @Override \n                 public   void   onEvent ( OrderItem  event ,   long  eventTimestamp ,   WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n                 @Override \n                 public   void   onPeriodicEmit ( WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n             } ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 #  Flink-End-to-End Exactly-Once \n 数据一致性语义分类 \n 数据一致性语义详解 \n \n \n \n 注意: \n Exactly-Once 更准确的理解 应该是: \n 数据只会被正确的处理一次! \n 而不是说数据只被处理一次,有可能多次,但只有最后一次是正确的,成功的! \n \n \n End-To-End Exactly-Once \n 表示从Source 到 Transformation 到 Sink 都能够保证Exactly-Once ! \n 如何实现局部的Exactly-Once \n 可以使用: \n 1.去重 \n \n 2.幂等 \n INSERT   INTO  t_student  ( id , `name` , age ) VALUES ( 9 , \'Gordon\' , 18 ) \n >   1062   -   Duplicate  entry  \'9\'   for  key \' PRIMARY \'\n >  时间 :   0.001 s\n \n 1 2 3 \n 3.分布式快照/Checkpoint---Flink使用的是这个 \n 如何实现End-To-End Exactly-Once \n \n Source: 如Kafka的offset 支持数据的replay/重放/重新传输 \n Transformation: 借助于Checkpoint \n Sink: Checkpoint + 两阶段事务提交 \n \n 两阶段事务提交 \n \n \n \n \n \n \n SourceOperater从Kafka消费消息/数据并记录offset \n TransformationOperater对数据进行处理转换并做Checkpoint \n SinkOperator将结果写入到Kafka \n \n 注意:在sink的时候会执行两阶段提交: \n 1.开启事务 \n 2.各个Operator执行barrier的Checkpoint, 成功则进行预提交 \n 3.所有Operator执行完预提交则执行真正的提交 \n 4.如果有任何一个预提交失败则回滚到最近的Checkpoint \n 代码演示 \n kafka主题flink-kafka1 ---\x3e \n Flink Source --\x3e \n Flink-Transformation做WordCount--\x3e \n 结果存储到kafka主题-flink-kafka2 \n //1.创建主题  \n / export / server / kafka / bin / kafka - topics . sh  -- zookeeper node1 : 2181   -- create  -- replication - factor  2   -- partitions  3   -- topic flink_kafka1\n / export / server / kafka / bin / kafka - topics . sh  -- zookeeper node1 : 2181   -- create  -- replication - factor  2   -- partitions  3   -- topic flink_kafka2\n //2.开启控制台生产者  \n / export / server / kafka / bin / kafka - console - producer . sh  -- broker - list node1 : 9092   -- topic flink_kafka1\n //3.开启控制台消费者  \n / export / server / kafka / bin / kafka - console - consumer . sh  -- bootstrap - server node1 : 9092   -- topic flink_kafka2\n \n 1 2 3 4 5 6 7 package   cn . itcast . feature ; \n\n import   org . apache . commons . lang3 . SystemUtils ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . common . restartstrategy . RestartStrategies ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . api . common . time . Time ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . runtime . state . filesystem . FsStateBackend ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . CheckpointConfig ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaConsumer ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaProducer ; \n import   org . apache . flink . streaming . connectors . kafka . internals . KeyedSerializationSchemaWrapper ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . Properties ; \n import   java . util . Random ; \n import   java . util . concurrent . TimeUnit ; \n\n /**\n * Author itcast\n * Desc 演示Flink的EndToEnd_Exactly_Once\n * 需求:\n * kafka主题flink-kafka1 ---\x3eFlink Source --\x3eFlink-Transformation做WordCount--\x3e结果存储到kafka主题-flink-kafka2\n */ \n public   class   Flink_Kafka_EndToEnd_Exactly_Once   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //开启Checkpoint \n         //===========类型1:必须参数============= \n         //设置Checkpoint的时间间隔为1000ms做一次Checkpoint/其实就是每隔1000ms发一次Barrier! \n        env . enableCheckpointing ( 1000 ) ; \n         if   ( SystemUtils . IS_OS_WINDOWS )   { \n            env . setStateBackend ( new   FsStateBackend ( "file:///D:/ckp" ) ) ; \n         }   else   { \n            env . setStateBackend ( new   FsStateBackend ( "hdfs://node1:8020/flink-checkpoint/checkpoint" ) ) ; \n         } \n         //===========类型2:建议参数=========== \n         //设置两个Checkpoint 之间最少等待时间,如设置Checkpoint之间最少是要等 500ms(为了避免每隔1000ms做一次Checkpoint的时候,前一次太慢和后一次重叠到一起去了) \n         //如:高速公路上,每隔1s关口放行一辆车,但是规定了两车之前的最小车距为500m \n        env . getCheckpointConfig ( ) . setMinPauseBetweenCheckpoints ( 500 ) ; //默认是0 \n         //设置如果在做Checkpoint过程中出现错误，是否让整体任务失败：true是  false不是 \n         //env.getCheckpointConfig().setFailOnCheckpointingErrors(false);//默认是true \n        env . getCheckpointConfig ( ) . setTolerableCheckpointFailureNumber ( 10 ) ; //默认值为0，表示不容忍任何检查点失败 \n         //设置是否清理检查点,表示 Cancel 时是否需要保留当前的 Checkpoint，默认 Checkpoint会在作业被Cancel时被删除 \n         //ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION：true,当作业被取消时，删除外部的checkpoint(默认值) \n         //ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION：false,当作业被取消时，保留外部的checkpoint \n        env . getCheckpointConfig ( ) . enableExternalizedCheckpoints ( CheckpointConfig . ExternalizedCheckpointCleanup . RETAIN_ON_CANCELLATION ) ; \n\n         //===========类型3:直接使用默认的即可=============== \n         //设置checkpoint的执行模式为EXACTLY_ONCE(默认) \n        env . getCheckpointConfig ( ) . setCheckpointingMode ( CheckpointingMode . EXACTLY_ONCE ) ; \n         //设置checkpoint的超时时间,如果 Checkpoint在 60s内尚未完成说明该次Checkpoint失败,则丢弃。 \n        env . getCheckpointConfig ( ) . setCheckpointTimeout ( 60000 ) ; //默认10分钟 \n         //设置同一时间有多少个checkpoint可以同时执行 \n        env . getCheckpointConfig ( ) . setMaxConcurrentCheckpoints ( 1 ) ; //默认为1 \n\n         //TODO ===配置重启策略: \n         //1.配置了Checkpoint的情况下不做任务配置:默认是无限重启并自动恢复,可以解决小问题,但是可能会隐藏真正的bug \n         //2.单独配置无重启策略 \n         //env.setRestartStrategy(RestartStrategies.noRestart()); \n         //3.固定延迟重启--开发中常用 \n        env . setRestartStrategy ( RestartStrategies . fixedDelayRestart ( \n                 3 ,   // 最多重启3次数 \n                 Time . of ( 5 ,   TimeUnit . SECONDS )   // 重启时间间隔 \n         ) ) ; \n         //上面的设置表示:如果job失败,重启3次, 每次间隔5s \n         //4.失败率重启--开发中偶尔使用 \n         /*env.setRestartStrategy(RestartStrategies.failureRateRestart(\n                3, // 每个测量阶段内最大失败次数\n                Time.of(1, TimeUnit.MINUTES), //失败率测量的时间间隔\n                Time.of(3, TimeUnit.SECONDS) // 两次连续重启的时间间隔\n        ));*/ \n         //上面的设置表示:如果1分钟内job失败不超过三次,自动重启,每次重启间隔3s (如果1分钟内程序失败达到3次,则程序退出) \n\n         //TODO 1.source-主题:flink-kafka1 \n         //准备kafka连接参数 \n         Properties  props1  =   new   Properties ( ) ; \n        props1 . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; //集群地址 \n        props1 . setProperty ( "group.id" ,   "flink" ) ; //消费者组id \n        props1 . setProperty ( "auto.offset.reset" ,   "latest" ) ; //latest有offset记录从记录位置开始消费,没有记录从最新的/最后的消息开始消费 /earliest有offset记录从记录位置开始消费,没有记录从最早的/最开始的消息开始消费 \n        props1 . setProperty ( "flink.partition-discovery.interval-millis" ,   "5000" ) ; //会开启一个后台线程每隔5s检测一下Kafka的分区情况,实现动态分区检测 \n         //props1.setProperty("enable.auto.commit", "true");//自动提交(提交到默认主题,后续学习了Checkpoint后随着Checkpoint存储在Checkpoint和默认主题中) \n         //props1.setProperty("auto.commit.interval.ms", "2000");//自动提交的时间间隔 \n         //使用连接参数创建FlinkKafkaConsumer/kafkaSource \n         //FlinkKafkaConsumer里面已经实现了offset的Checkpoint维护! \n         FlinkKafkaConsumer < String >  kafkaSource  =   new   FlinkKafkaConsumer < String > ( "flink_kafka1" ,   new   SimpleStringSchema ( ) ,  props1 ) ; \n        kafkaSource . setCommitOffsetsOnCheckpoints ( true ) ; //默认就是true//在做Checkpoint的时候提交offset到Checkpoint(为容错)和默认主题(为了外部工具获取)中 \n\n         //使用kafkaSource \n         DataStream < String >  kafkaDS  =  env . addSource ( kafkaSource ) ; \n\n         //TODO 2.transformation-做WordCount \n         SingleOutputStreamOperator < String >  result  =  kafkaDS . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n            private   Random  ran  =   new   Random ( ) ; \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                     int  num  =  ran . nextInt ( 5 ) ; \n                     if ( num  >   3 ) { \n                         System . out . println ( "随机异常产生了" ) ; \n                         throw   new   Exception ( "随机异常产生了" ) ; \n                     } \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) . keyBy ( t  ->  t . f0 ) \n           . sum ( 1 ) \n           . map ( new   MapFunction < Tuple2 < String ,   Integer > ,   String > ( )   { \n                     @Override \n                     public   String   map ( Tuple2 < String ,   Integer >  value )   throws   Exception   { \n                         return  value . f0  +   ":"   +  value . f1 ; \n                     } \n            } ) ; \n\n         //TODO 3.sink-主题:flink-kafka2 \n         Properties  props2  =   new   Properties ( ) ; \n        props2 . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; \n        props2 . setProperty ( "transaction.timeout.ms" ,   "5000" ) ; \n\n         FlinkKafkaProducer < String >  kafkaSink  =   new   FlinkKafkaProducer < > ( \n                 "flink_kafka2" ,                    // target topic \n                 new   KeyedSerializationSchemaWrapper ( new   SimpleStringSchema ( ) ) ,      // serialization schema \n                props2 ,                    // producer config \n                 FlinkKafkaProducer . Semantic . EXACTLY_ONCE ) ;   // fault-tolerance \n\n        result . addSink ( kafkaSink ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n\n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 #  Flink-异步IO-了解 \n 原理 \n \n API \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/asyncio.html \n \n 注意: 如果要使用异步IO, 对应Client有一定要求: \n 1.该Client要支持发送异步请求,如vertx \n 2. 如果Client不支持可以使用线程池来模拟异步请求 \n 代码演示 \n DROP TABLE IF EXISTS `t_category`;\nCREATE TABLE `t_category` (\n  `id` int(11) NOT NULL,\n  `name` varchar(255) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n-- ----------------------------\n-- Records of t_category\n-- ----------------------------\nINSERT INTO `t_category` VALUES (\'1\', \'手机\');\nINSERT INTO `t_category` VALUES (\'2\', \'电脑\');\nINSERT INTO `t_category` VALUES (\'3\', \'服装\');\nINSERT INTO `t_category` VALUES (\'4\', \'化妆品\');\nINSERT INTO `t_category` VALUES (\'5\', \'食品\');\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package   cn . itcast . feature ; \n\n import   io . vertx . core . AsyncResult ; \n import   io . vertx . core . Handler ; \n import   io . vertx . core . Vertx ; \n import   io . vertx . core . VertxOptions ; \n import   io . vertx . core . json . JsonObject ; \n import   io . vertx . ext . jdbc . JDBCClient ; \n import   io . vertx . ext . sql . SQLClient ; \n import   io . vertx . ext . sql . SQLConnection ; \n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . AsyncDataStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . async . ResultFuture ; \n import   org . apache . flink . streaming . api . functions . async . RichAsyncFunction ; \n import   org . apache . flink . streaming . api . functions . source . RichSourceFunction ; \n\n import   java . sql . * ; \n import   java . util . Collections ; \n import   java . util . List ; \n import   java . util . concurrent . ExecutorService ; \n import   java . util . concurrent . LinkedBlockingQueue ; \n import   java . util . concurrent . ThreadPoolExecutor ; \n import   java . util . concurrent . TimeUnit ; \n\n /**\n * 使用异步io的先决条件\n * 1.数据库(或key/value存储)提供支持异步请求的client。\n * 2.没有异步请求客户端的话也可以将同步客户端丢到线程池中执行作为异步客户端。\n */ \n public   class   ASyncIODemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //2.Source \n         //数据源中只有id \n         //DataStreamSource[1,2,3,4,5] \n         DataStreamSource < CategoryInfo >  categoryDS  =  env . addSource ( new   RichSourceFunction < CategoryInfo > ( )   { \n             private   Boolean  flag  =   true ; \n             @Override \n             public   void   run ( SourceContext < CategoryInfo >  ctx )   throws   Exception   { \n                 Integer [ ]  ids  =   { 1 ,   2 ,   3 ,   4 ,   5 } ; \n                 for   ( Integer  id  :  ids )   { \n                    ctx . collect ( new   CategoryInfo ( id ,   null ) ) ; \n                 } \n             } \n             @Override \n             public   void   cancel ( )   { \n                 this . flag  =   false ; \n             } \n         } ) ; \n         //3.Transformation \n\n\n         //方式一：Java-vertx中提供的异步client实现异步IO \n         //unorderedWait无序等待 \n         SingleOutputStreamOperator < CategoryInfo >  result1  =   AsyncDataStream \n                 . unorderedWait ( categoryDS ,   new   ASyncIOFunction1 ( ) ,   1000 ,   TimeUnit . SECONDS ,   10 ) ; \n\n         //方式二：MySQL中同步client+线程池模拟异步IO \n         //unorderedWait无序等待 \n         SingleOutputStreamOperator < CategoryInfo >  result2  =   AsyncDataStream \n                 . unorderedWait ( categoryDS ,   new   ASyncIOFunction2 ( ) ,   1000 ,   TimeUnit . SECONDS ,   10 ) ; \n\n         //4.Sink \n        result1 . print ( "方式一：Java-vertx中提供的异步client实现异步IO \\n" ) ; \n        result2 . print ( "方式二：MySQL中同步client+线程池模拟异步IO \\n" ) ; \n\n         //5.execute \n        env . execute ( ) ; \n     } \n } \n\n @Data \n @NoArgsConstructor \n @AllArgsConstructor \n class   CategoryInfo   { \n     private   Integer  id ; \n     private   String  name ; \n } \n\n //MySQL本身的客户端-需要把它变成支持异步的客户端:使用vertx或线程池 \n class   MysqlSyncClient   { \n     private   static   transient   Connection  connection ; \n     private   static   final   String   JDBC_DRIVER   =   "com.mysql.jdbc.Driver" ; \n     private   static   final   String   URL   =   "jdbc:mysql://localhost:3306/bigdata" ; \n     private   static   final   String   USER   =   "root" ; \n     private   static   final   String   PASSWORD   =   "root" ; \n\n     static   { \n         init ( ) ; \n     } \n\n     private   static   void   init ( )   { \n         try   { \n             Class . forName ( JDBC_DRIVER ) ; \n         }   catch   ( ClassNotFoundException  e )   { \n             System . out . println ( "Driver not found!"   +  e . getMessage ( ) ) ; \n         } \n         try   { \n            connection  =   DriverManager . getConnection ( URL ,   USER ,   PASSWORD ) ; \n         }   catch   ( SQLException  e )   { \n             System . out . println ( "init connection failed!"   +  e . getMessage ( ) ) ; \n         } \n     } \n\n     public   void   close ( )   { \n         try   { \n             if   ( connection  !=   null )   { \n                connection . close ( ) ; \n             } \n         }   catch   ( SQLException  e )   { \n             System . out . println ( "close connection failed!"   +  e . getMessage ( ) ) ; \n         } \n     } \n\n     public   CategoryInfo   query ( CategoryInfo  category )   { \n         try   { \n             String  sql  =   "select id,name from t_category where id = " +  category . getId ( ) ; \n             Statement  statement  =  connection . createStatement ( ) ; \n             ResultSet  rs  =  statement . executeQuery ( sql ) ; \n             if   ( rs  !=   null   &&  rs . next ( ) )   { \n                category . setName ( rs . getString ( "name" ) ) ; \n             } \n         }   catch   ( SQLException  e )   { \n             System . out . println ( "query failed!"   +  e . getMessage ( ) ) ; \n         } \n         return  category ; \n     } \n } \n\n /**\n * 方式一：Java-vertx中提供的异步client实现异步IO\n */ \n class   ASyncIOFunction1   extends   RichAsyncFunction < CategoryInfo ,   CategoryInfo >   { \n     private   transient   SQLClient  mySQLClient ; \n\n     @Override \n     public   void   open ( Configuration  parameters )   throws   Exception   { \n         JsonObject  mySQLClientConfig  =   new   JsonObject ( ) ; \n        mySQLClientConfig\n                 . put ( "driver_class" ,   "com.mysql.jdbc.Driver" ) \n                 . put ( "url" ,   "jdbc:mysql://localhost:3306/bigdata" ) \n                 . put ( "user" ,   "root" ) \n                 . put ( "password" ,   "root" ) \n                 . put ( "max_pool_size" ,   20 ) ; \n\n         VertxOptions  options  =   new   VertxOptions ( ) ; \n        options . setEventLoopPoolSize ( 10 ) ; \n        options . setWorkerPoolSize ( 20 ) ; \n         Vertx  vertx  =   Vertx . vertx ( options ) ; \n         //根据上面的配置参数获取异步请求客户端 \n        mySQLClient  =   JDBCClient . createNonShared ( vertx ,  mySQLClientConfig ) ; \n     } \n\n     //使用异步客户端发送异步请求 \n     @Override \n     public   void   asyncInvoke ( CategoryInfo  input ,   ResultFuture < CategoryInfo >  resultFuture )   throws   Exception   { \n        mySQLClient . getConnection ( new   Handler < AsyncResult < SQLConnection > > ( )   { \n             @Override \n             public   void   handle ( AsyncResult < SQLConnection >  sqlConnectionAsyncResult )   { \n                 if   ( sqlConnectionAsyncResult . failed ( ) )   { \n                     return ; \n                 } \n                 SQLConnection  connection  =  sqlConnectionAsyncResult . result ( ) ; \n                connection . query ( "select id,name from t_category where id = "   + input . getId ( ) ,   new   Handler < AsyncResult < io . vertx . ext . sql . ResultSet > > ( )   { \n                     @Override \n                     public   void   handle ( AsyncResult < io . vertx . ext . sql . ResultSet >  resultSetAsyncResult )   { \n                         if   ( resultSetAsyncResult . succeeded ( ) )   { \n                             List < JsonObject >  rows  =  resultSetAsyncResult . result ( ) . getRows ( ) ; \n                             for   ( JsonObject  jsonObject  :  rows )   { \n                                 CategoryInfo  categoryInfo  =   new   CategoryInfo ( jsonObject . getInteger ( "id" ) ,  jsonObject . getString ( "name" ) ) ; \n                                resultFuture . complete ( Collections . singletonList ( categoryInfo ) ) ; \n                             } \n                         } \n                     } \n                 } ) ; \n             } \n         } ) ; \n     } \n     @Override \n     public   void   close ( )   throws   Exception   { \n        mySQLClient . close ( ) ; \n     } \n\n     @Override \n     public   void   timeout ( CategoryInfo  input ,   ResultFuture < CategoryInfo >  resultFuture )   throws   Exception   { \n         System . out . println ( "async call time out!" ) ; \n        input . setName ( "未知" ) ; \n        resultFuture . complete ( Collections . singleton ( input ) ) ; \n     } \n } \n\n /**\n * 方式二：同步调用+线程池模拟异步IO\n */ \n class   ASyncIOFunction2   extends   RichAsyncFunction < CategoryInfo ,   CategoryInfo >   { \n     private   transient   MysqlSyncClient  client ; \n     private   ExecutorService  executorService ; //线程池 \n\n     @Override \n     public   void   open ( Configuration  parameters )   throws   Exception   { \n         super . open ( parameters ) ; \n        client  =   new   MysqlSyncClient ( ) ; \n        executorService  =   new   ThreadPoolExecutor ( 10 ,   10 ,   0L ,   TimeUnit . MILLISECONDS ,   new   LinkedBlockingQueue < Runnable > ( ) ) ; \n     } \n\n     //异步发送请求 \n     @Override \n     public   void   asyncInvoke ( CategoryInfo  input ,   ResultFuture < CategoryInfo >  resultFuture )   throws   Exception   { \n        executorService . execute ( new   Runnable ( )   { \n             @Override \n             public   void   run ( )   { \n                resultFuture . complete ( Collections . singletonList ( ( CategoryInfo )  client . query ( input ) ) ) ; \n             } \n         } ) ; \n     } \n\n\n     @Override \n     public   void   close ( )   throws   Exception   { \n     } \n\n     @Override \n     public   void   timeout ( CategoryInfo  input ,   ResultFuture < CategoryInfo >  resultFuture )   throws   Exception   { \n         System . out . println ( "async call time out!" ) ; \n        input . setName ( "未知" ) ; \n        resultFuture . complete ( Collections . singleton ( input ) ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 #  Flink-Streaming Flie Sink（新版本弃用，整合到File sink） \n 介绍 \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/streamfile_sink.html \n https://blog.csdn.net/u013220482/article/details/100901471 \n 代码演示 \n package   cn . itcast . feature ; \n\n\n import   org . apache . commons . lang3 . SystemUtils ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . common . serialization . SimpleStringEncoder ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . core . fs . Path ; \n import   org . apache . flink . runtime . state . filesystem . FsStateBackend ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . CheckpointConfig ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . sink . filesystem . OutputFileConfig ; \n import   org . apache . flink . streaming . api . functions . sink . filesystem . StreamingFileSink ; \n import   org . apache . flink . streaming . api . functions . sink . filesystem . rollingpolicies . DefaultRollingPolicy ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . concurrent . TimeUnit ; \n\n /**\n * Author itcast\n * Desc 演示Flink StreamingFileSink将流式数据写入到HDFS 数据一致性由Checkpoint + 两阶段提交保证\n */ \n public   class   StreamingFileSinkDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //开启Checkpoint \n         //===========类型1:必须参数============= \n         //设置Checkpoint的时间间隔为1000ms做一次Checkpoint/其实就是每隔1000ms发一次Barrier! \n        env . enableCheckpointing ( 1000 ) ; \n         if   ( SystemUtils . IS_OS_WINDOWS )   { \n            env . setStateBackend ( new   FsStateBackend ( "file:///D:/ckp" ) ) ; \n         }   else   { \n            env . setStateBackend ( new   FsStateBackend ( "hdfs://node1:8020/flink-checkpoint/checkpoint" ) ) ; \n         } \n         //===========类型2:建议参数=========== \n         //设置两个Checkpoint 之间最少等待时间,如设置Checkpoint之间最少是要等 500ms(为了避免每隔1000ms做一次Checkpoint的时候,前一次太慢和后一次重叠到一起去了) \n         //如:高速公路上,每隔1s关口放行一辆车,但是规定了两车之前的最小车距为500m \n        env . getCheckpointConfig ( ) . setMinPauseBetweenCheckpoints ( 500 ) ; //默认是0 \n         //设置如果在做Checkpoint过程中出现错误，是否让整体任务失败：true是  false不是 \n         //env.getCheckpointConfig().setFailOnCheckpointingErrors(false);//默认是true \n        env . getCheckpointConfig ( ) . setTolerableCheckpointFailureNumber ( 10 ) ; //默认值为0，表示不容忍任何检查点失败 \n         //设置是否清理检查点,表示 Cancel 时是否需要保留当前的 Checkpoint，默认 Checkpoint会在作业被Cancel时被删除 \n         //ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION：true,当作业被取消时，删除外部的checkpoint(默认值) \n         //ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION：false,当作业被取消时，保留外部的checkpoint \n        env . getCheckpointConfig ( ) . enableExternalizedCheckpoints ( CheckpointConfig . ExternalizedCheckpointCleanup . RETAIN_ON_CANCELLATION ) ; \n\n         //===========类型3:直接使用默认的即可=============== \n         //设置checkpoint的执行模式为EXACTLY_ONCE(默认) \n        env . getCheckpointConfig ( ) . setCheckpointingMode ( CheckpointingMode . EXACTLY_ONCE ) ; \n         //设置checkpoint的超时时间,如果 Checkpoint在 60s内尚未完成说明该次Checkpoint失败,则丢弃。 \n        env . getCheckpointConfig ( ) . setCheckpointTimeout ( 60000 ) ; //默认10分钟 \n         //设置同一时间有多少个checkpoint可以同时执行 \n        env . getCheckpointConfig ( ) . setMaxConcurrentCheckpoints ( 1 ) ; //默认为1 \n\n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n\n         //TODO 2.transformation \n         //注意:下面的操作将上面的2步合成了1步,直接切割单词并记为1返回 \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  wordAndOne  =  lines . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n\n         SingleOutputStreamOperator < String >  result  =  wordAndOne . keyBy ( t  ->  t . f0 ) . sum ( 1 ) \n                 . map ( new   MapFunction < Tuple2 < String ,   Integer > ,   String > ( )   { \n                     @Override \n                     public   String   map ( Tuple2 < String ,   Integer >  value )   throws   Exception   { \n                         return  value . f0  +   ":"   +  value . f1 ; \n                     } \n                 } ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //使用StreamingFileSink将数据sink到HDFS \n         OutputFileConfig  config  =   OutputFileConfig \n                 . builder ( ) \n                 . withPartPrefix ( "prefix" ) //设置文件前缀 \n                 . withPartSuffix ( ".txt" ) //设置文件后缀 \n                 . build ( ) ; \n\n         StreamingFileSink < String >  streamingFileSink  =   StreamingFileSink . \n                 forRowFormat ( new   Path ( "hdfs://node1:8020/FlinkStreamFileSink/parquet" ) ,   new   SimpleStringEncoder < String > ( "UTF-8" ) ) \n                 . withRollingPolicy ( \n                         DefaultRollingPolicy . builder ( ) \n                                 . withRolloverInterval ( TimeUnit . MINUTES . toMillis ( 15 ) ) //每隔15分钟生成一个新文件 \n                                 . withInactivityInterval ( TimeUnit . MINUTES . toMillis ( 5 ) ) //每隔5分钟没有新数据到来,也把之前的生成一个新文件 \n                                 . withMaxPartSize ( 1024   *   1024   *   1024 ) \n                                 . build ( ) ) \n                 . withOutputFileConfig ( config ) \n                 . build ( ) ; \n\n        result . addSink ( streamingFileSink ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 #  Flink-高级特性-Flie Sink \n \n package   cn . itcast . feature ; \n\n\n import   org . apache . commons . lang3 . SystemUtils ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . common . serialization . SimpleStringEncoder ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . connector . file . sink . FileSink ; \n import   org . apache . flink . core . fs . Path ; \n import   org . apache . flink . runtime . state . filesystem . FsStateBackend ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . CheckpointConfig ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . sink . filesystem . OutputFileConfig ; \n import   org . apache . flink . streaming . api . functions . sink . filesystem . bucketassigners . DateTimeBucketAssigner ; \n import   org . apache . flink . streaming . api . functions . sink . filesystem . rollingpolicies . DefaultRollingPolicy ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . concurrent . TimeUnit ; \n\n /**\n * Author itcast\n * Desc 演示Flink FileSink将批/流式数据写入到HDFS 数据一致性由Checkpoint + 两阶段提交保证\n */ \n public   class   FileSinkDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //开启Checkpoint \n         //===========类型1:必须参数============= \n         //设置Checkpoint的时间间隔为1000ms做一次Checkpoint/其实就是每隔1000ms发一次Barrier! \n        env . enableCheckpointing ( 1000 ) ; \n         if   ( SystemUtils . IS_OS_WINDOWS )   { \n            env . setStateBackend ( new   FsStateBackend ( "file:///D:/ckp" ) ) ; \n         }   else   { \n            env . setStateBackend ( new   FsStateBackend ( "hdfs://node1:8020/flink-checkpoint/checkpoint" ) ) ; \n         } \n         //===========类型2:建议参数=========== \n         //设置两个Checkpoint 之间最少等待时间,如设置Checkpoint之间最少是要等 500ms(为了避免每隔1000ms做一次Checkpoint的时候,前一次太慢和后一次重叠到一起去了) \n         //如:高速公路上,每隔1s关口放行一辆车,但是规定了两车之前的最小车距为500m \n        env . getCheckpointConfig ( ) . setMinPauseBetweenCheckpoints ( 500 ) ; //默认是0 \n         //设置如果在做Checkpoint过程中出现错误，是否让整体任务失败：true是  false不是 \n         //env.getCheckpointConfig().setFailOnCheckpointingErrors(false);//默认是true \n        env . getCheckpointConfig ( ) . setTolerableCheckpointFailureNumber ( 10 ) ; //默认值为0，表示不容忍任何检查点失败 \n         //设置是否清理检查点,表示 Cancel 时是否需要保留当前的 Checkpoint，默认 Checkpoint会在作业被Cancel时被删除 \n         //ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION：true,当作业被取消时，删除外部的checkpoint(默认值) \n         //ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION：false,当作业被取消时，保留外部的checkpoint \n        env . getCheckpointConfig ( ) . enableExternalizedCheckpoints ( CheckpointConfig . ExternalizedCheckpointCleanup . RETAIN_ON_CANCELLATION ) ; \n\n         //===========类型3:直接使用默认的即可=============== \n         //设置checkpoint的执行模式为EXACTLY_ONCE(默认) \n        env . getCheckpointConfig ( ) . setCheckpointingMode ( CheckpointingMode . EXACTLY_ONCE ) ; \n         //设置checkpoint的超时时间,如果 Checkpoint在 60s内尚未完成说明该次Checkpoint失败,则丢弃。 \n        env . getCheckpointConfig ( ) . setCheckpointTimeout ( 60000 ) ; //默认10分钟 \n         //设置同一时间有多少个checkpoint可以同时执行 \n        env . getCheckpointConfig ( ) . setMaxConcurrentCheckpoints ( 1 ) ; //默认为1 \n\n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n\n         //TODO 2.transformation \n         //注意:下面的操作将上面的2步合成了1步,直接切割单词并记为1返回 \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  wordAndOne  =  lines . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n\n         SingleOutputStreamOperator < String >  result  =  wordAndOne . keyBy ( t  ->  t . f0 ) . sum ( 1 ) \n                 . map ( new   MapFunction < Tuple2 < String ,   Integer > ,   String > ( )   { \n                     @Override \n                     public   String   map ( Tuple2 < String ,   Integer >  value )   throws   Exception   { \n                         return  value . f0  +   ":"   +  value . f1 ; \n                     } \n                 } ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //使用FileSink将数据sink到HDFS \n         OutputFileConfig  config  =   OutputFileConfig \n                 . builder ( ) \n                 . withPartPrefix ( "prefix" ) \n                 . withPartSuffix ( ".txt" ) \n                 . build ( ) ; \n\n         FileSink < String >  sink  =   FileSink \n                 . forRowFormat ( new   Path ( "hdfs://node1:8020/FlinkFileSink/parquet" ) ,   new   SimpleStringEncoder < String > ( "UTF-8" ) ) \n                 . withRollingPolicy ( \n                         DefaultRollingPolicy . builder ( ) \n                                 . withRolloverInterval ( TimeUnit . MINUTES . toMillis ( 15 ) ) \n                                 . withInactivityInterval ( TimeUnit . MINUTES . toMillis ( 5 ) ) \n                                 . withMaxPartSize ( 1024   *   1024   *   1024 ) \n                                 . build ( ) ) \n                 . withOutputFileConfig ( config ) \n                 . withBucketAssigner ( new   DateTimeBucketAssigner ( "yyyy-MM-dd--HH" ) ) \n                 . build ( ) ; \n\n        result . sinkTo ( sink ) ; \n\n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 #  Flink监控 \n https://blog.lovedata.net/8156c1e1.html \n 什么是Metrics \n Metrics分类 \n 代码 \n package   cn . itcast . metrics ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . metrics . Counter ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc 演示Flink-Metrics监控\n * 在Map算子中提供一个Counter,统计map处理的数据条数,运行之后再WebUI上进行监控\n */ \n public   class   MetricsDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n\n         //TODO 2.transformation \n         SingleOutputStreamOperator < String >  words  =  lines . flatMap ( new   FlatMapFunction < String ,   String > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < String >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( word ) ; \n                 } \n             } \n         } ) ; \n\n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  wordAndOne  =  words\n                 . map ( new   RichMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n                     Counter  myCounter ; //用来记录map处理了多少个单词 \n\n                     //对Counter进行初始化 \n                     @Override \n                     public   void   open ( Configuration  parameters )   throws   Exception   { \n                        myCounter  =   getRuntimeContext ( ) . getMetricGroup ( ) . addGroup ( "myGroup" ) . counter ( "myCounter" ) ; \n                     } \n                     //处理单词,将单词记为(单词,1) \n                     @Override \n                     public   Tuple2 < String ,   Integer >   map ( String  value )   throws   Exception   { \n                        myCounter . inc ( ) ; //计数器+1 \n                         return   Tuple2 . of ( value ,   1 ) ; \n                     } \n                 } ) ; \n\n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  wordAndOne . keyBy ( t  ->  t . f0 ) . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n // /export/server/flink/bin/yarn-session.sh -n 2 -tm 800 -s 1 -d \n // /export/server/flink/bin/flink run --class cn.itcast.metrics.MetricsDemo /root/metrics.jar \n // 查看WebUI \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 提交观察UI \n 1.打包 \n 2.提交到Yarn上运行 \n 3.查看监控指标 \n \n 4.也可以通过浏览器f12的找到url发送请求获取监控信息 \n \n 5.也可以通过代码发送请求获取监控信息 \n package   cn . itcast . metrics ; \n\n import   java . io . BufferedReader ; \n import   java . io . InputStreamReader ; \n import   java . net . URL ; \n import   java . net . URLConnection ; \n\n public   class   MetricsTest   { \n     public   static   void   main ( String [ ]  args )   { \n         //String result = sendGet("http://node1:8088/proxy/application_1609508087977_0010/jobs/558a5a3016661f1d732228330ebfaad5/vertices/cbc357ccb763df2852fee8c4fc7d55f2/metrics?get=0.Map.myGroup.myCounter"); \n         String  result  =   sendGet ( "http://node1:8088/proxy/application_1609508087977_0010/jobs/558a5a3016661f1d732228330ebfaad5" ) ; \n\n         System . out . println ( result ) ; \n     } \n\n     public   static   String   sendGet ( String  url )   { \n         String  result  =   "" ; \n         BufferedReader  in  =   null ; \n         try   { \n             String  urlNameString  =  url ; \n             URL  realUrl  =   new   URL ( urlNameString ) ; \n             URLConnection  connection  =  realUrl . openConnection ( ) ; \n             // 设置通用的请求属性 \n            connection . setRequestProperty ( "accept" ,   "*/*" ) ; \n            connection . setRequestProperty ( "connection" ,   "Keep-Alive" ) ; \n            connection . setRequestProperty ( "user-agent" ,   "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)" ) ; \n             // 建立实际的连接 \n            connection . connect ( ) ; \n            in  =   new   BufferedReader ( new   InputStreamReader ( connection . getInputStream ( ) ) ) ; \n             String  line ; \n             while   ( ( line  =  in . readLine ( ) )   !=   null )   { \n                result  +=  line ; \n             } \n         }   catch   ( Exception  e )   { \n             System . out . println ( "发送GET请求出现异常！"   +  e ) ; \n            e . printStackTrace ( ) ; \n         } \n         // 使用finally块来关闭输入流 \n         finally   { \n             try   { \n                 if   ( in  !=   null )   { \n                    in . close ( ) ; \n                 } \n             }   catch   ( Exception  e2 )   { \n                e2 . printStackTrace ( ) ; \n             } \n         } \n         return  result ; \n     } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 6.也可以整合三方工具对flink进行监控 \n https://blog.lovedata.net/8156c1e1.html \n Flink内存管理 \n \n #--—提交参数——— \n/export/server/flink/bin/flink run  -m  yarn-cluster  -yjm   1024   -ytm   1024   -p   6   -ys   2  streaming.jar  -c  dictionary_utils.OneDictForExecutor\n \n 1 2 \n Flink性能优化 \n 问题定位口诀 \n 一 压 二查 三指标 ，延迟吞吐是核心。 \n 时刻关注资源量，排查首先看 GC 。 \n 口诀解析 \n 常见性能问题 \n 1.序列化和反序列化 \n 2.数据倾斜 \n 3.频繁gc \n 4.外部系统 \n 5.大窗口 \n 经典场景调优 \n 数据去重 \n 数据倾斜 \n 内存调优 \n \n \n \n \n \n 实时场景常见问题： \n 1.数据延迟：（本身有动态背压机制credit-base，但是业务不接受延迟太久，还是会手动处理，重启。） \n ①确认本身的处理耗时情况，如果本身处理耗时不高，延迟高，那就是上游问题； \n ①本身耗时高： \n 是否访问量比平时大了，如果是则增加并发； \n 是否有访问外部系统，热点数据导致的部分线程处理速度变慢；--线程池和cache \n 2.数据倾斜： \n ①rebalance \n ②窗口计算有倾斜，可以加随机前缀分成两段聚合； \n ②使用水印不触发，也会导致数据堆积 \n 3.内存溢出 \n 窗口数据高峰时溢出，从数据结构、存储方式、压缩上面再考虑。 \n 1.复用对象 \n stream\n     . apply ( new   WindowFunction < WikipediaEditEvent ,   Tuple2 < String ,   Long > ,   String ,   TimeWindow > ( )   { \n         @Override \n         public   void   apply ( String  userName ,   TimeWindow  timeWindow ,   Iterable < WikipediaEditEvent >  iterable ,   Collector < Tuple2 < String ,   Long > >  collector )   throws   Exception   { \n             long  changesCount  =   . . . \n             // A new Tuple instance is created on every execution \n            collector . collect ( new   Tuple2 < > ( userName ,  changesCount ) ) ; \n         } \n     } \n \n 1 2 3 4 5 6 7 8 9 上面的代码可以优化为下面的代码: \n 可以避免Tuple2的重复创建 \n stream\n         . apply ( new   WindowFunction < WikipediaEditEvent ,   Tuple2 < String ,   Long > ,   String ,   TimeWindow > ( )   { \n     // Create an instance that we will reuse on every call \n     private   Tuple2 < String ,   Long >  result  =   new   Tuple < > ( ) ; \n     @Override \n     public   void   apply ( String  userName ,   TimeWindow  timeWindow ,   Iterable < WikipediaEditEvent >  iterable ,   Collector < Tuple2 < String ,   Long > >  collector )   throws   Exception   { \n         long  changesCount  =   . . . \n         // Set fields on an existing object instead of creating a new one \n        result . f0  =  userName ; \n         // Auto-boxing!! A new Long value may be created \n        result . f1  =  changesCount ; \n         // Reuse the same Tuple2 object \n        collector . collect ( result ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 2.数据倾斜 \n \n rebalance \n 自定义分区器 \n key+随机前后缀 \n 3.异步IO \n 4.合理调整并行度 \n 数据过滤之后可以减少并行度 \n 数据合并之后再处理之前可以增加并行度 \n 大量小文件写入到HDFS可以减少并行度 \n 5.序列化方式（不仅考虑计算内部，考虑输出给业务方的使用） \n 6.垃圾回收器 \n 7.调整内存 \n  1.ds.writeAsText("data/output/result1").setParallelism(1);\n 2.env.setParallelism(1);\n 3.提交任务时webUI或命令行参数  flink run  -p 10\n 4.配置文件flink-conf.yaml parallelism.default: 1\n \n 1 2 3 4 \n Flink Table&SQL \n 为什么需要Table&SQL \n \n 发展历史 \n 两种Table planners \n 1.旧的planner（ 创建环境时useOldPlanner） \n 2.Blink planner （创建环境时useBlinkPlanner()，新版本默认） \n \n 依赖 \n \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/ \n \n \n \n 程序结构 \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html \n \n \n \n 创建环境 \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html \n \n \n \n \n \n 创建表 \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html \n \n \n \n \n 查询 \n \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html \n \n \n \n \n 整合DataStream \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html \n 核心概念 \n \n \n \n 动态表/无界表 \n \n \n \n 连续查询/需要借助State \n Flink SQL空闲状态保留时间（idle state retention time） \n Flink SQL新手有可能犯的错误，笔者认为其中之一就是 忘记设置空闲状态保留时间导致状态爆炸 。 \n 为什么要设置 \n如果我们在数据流上进行分组查询，分组处理产生的结果(不仅仅是聚合结果)会作为中间状态存储下来。随着分组key的不断增加，状态自然也会不断膨胀。但是这些状态数据基本都有时效性，不必永久保留。例如，使用Top-N语法进行去重，重复数据的出现一般都位于特定区间内(例如一小时或一天内)，过了这段时间之后，对应的状态就不再需要了。Flink SQL提供的idle state retention time特性可以保证当状态中某个key对应的数据未更新的时间达到阈值时，该条状态被自动清理。设置方法是： \n tbEnv . getConfig ( ) . setIdleStateRetention ( Duration . ofDays ( 1 ) ) ; \n \n 1 注意setIdleStateRetentionTime()方法需要传入两个参数：状态的最小保留时间minRetentionTime和最大保留时间maxRetentionTime(根据实际业务决定)，且两者至少相差5分钟。如果minRetentionTime和maxRetentionTime的间隔设置太小，就会比较频繁地产生Timer与更新ValueState，维护Timer的成本会变大 \n 案例1 \n 将DataStream数据转Table和View然后使用sql进行统计查询 \n package   cn . itcast . sql ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . Table ; \n import   org . apache . flink . table . api . bridge . java . StreamTableEnvironment ; \n\n import   java . util . Arrays ; \n\n import   static   org . apache . flink . table . api . Expressions . $ ; \n\n /**\n * Author itcast\n * Desc 演示Flink Table&SQL 案例- 将DataStream数据转Table和View然后使用sql进行统计查询\n */ \n public   class   Demo01   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         EnvironmentSettings  settings  =   EnvironmentSettings . newInstance ( ) . useBlinkPlanner ( ) . inStreamingMode ( ) . build ( ) ; \n         StreamTableEnvironment  tenv  =   StreamTableEnvironment . create ( env ,  settings ) ; \n\n         //TODO 1.source \n         DataStream < Order >  orderA  =  env . fromCollection ( Arrays . asList ( \n                 new   Order ( 1L ,   "beer" ,   3 ) , \n                 new   Order ( 1L ,   "diaper" ,   4 ) , \n                 new   Order ( 3L ,   "rubber" ,   2 ) ) ) ; \n\n         DataStream < Order >  orderB  =  env . fromCollection ( Arrays . asList ( \n                 new   Order ( 2L ,   "pen" ,   3 ) , \n                 new   Order ( 2L ,   "rubber" ,   3 ) , \n                 new   Order ( 4L ,   "beer" ,   1 ) ) ) ; \n\n         //TODO 2.transformation \n         // 将DataStream数据转Table和View,然后查询 \n         Table  tableA  =  tenv . fromDataStream ( orderA ,  $ ( "user" ) ,  $ ( "product" ) ,  $ ( "amount" ) ) ; \n        tableA . printSchema ( ) ; \n         System . out . println ( tableA ) ; \n\n        tenv . createTemporaryView ( "tableB" ,  orderB ,  $ ( "user" ) ,  $ ( "product" ) ,  $ ( "amount" ) ) ; \n\n         //查询:tableA中amount>2的和tableB中amount>1的数据最后合并 \n         /*\nselect * from tableA where amount > 2\nunion\n select * from tableB where amount > 1\n         */ \n         String  sql  =   "select * from " + tableA + " where amount > 2 \\n"   + \n                 "union \\n"   + \n                 " select * from tableB where amount > 1" ; \n\n         Table  resultTable  =  tenv . sqlQuery ( sql ) ; \n        resultTable . printSchema ( ) ; \n         System . out . println ( resultTable ) ; //UnnamedTable$1 \n\n\n         //将Table转为DataStream \n         //DataStream<Order> resultDS = tenv.toAppendStream(resultTable, Order.class);//union all使用toAppendStream \n         DataStream < Tuple2 < Boolean ,   Order > >  resultDS  =  tenv . toRetractStream ( resultTable ,   Order . class ) ; //union使用toRetractStream \n         //toAppendStream → 将计算后的数据append到结果DataStream中去 \n         //toRetractStream  → 将计算后的新的数据在DataStream原数据的基础上更新true或是删除false \n         //类似StructuredStreaming中的append/update/complete \n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @NoArgsConstructor \n     @AllArgsConstructor \n     public   static   class   Order   { \n         public   Long  user ; \n         public   String  product ; \n         public   int  amount ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 案例2 \n 使用Table/DSL风格和SQL风格完成WordCount \n package   cn . itcast . sql ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . Table ; \n import   org . apache . flink . table . api . bridge . java . StreamTableEnvironment ; \n\n import   static   org . apache . flink . table . api . Expressions . $ ; \n\n /**\n * Author itcast\n * Desc 演示Flink Table&SQL 案例- 使用SQL和Table两种方式做WordCount\n */ \n public   class   Demo02   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         EnvironmentSettings  settings  =   EnvironmentSettings . newInstance ( ) . useBlinkPlanner ( ) . inStreamingMode ( ) . build ( ) ; \n         StreamTableEnvironment  tenv  =   StreamTableEnvironment . create ( env ,  settings ) ; \n\n         //TODO 1.source \n         DataStream < WC >  wordsDS  =  env . fromElements ( \n                 new   WC ( "Hello" ,   1 ) , \n                 new   WC ( "World" ,   1 ) , \n                 new   WC ( "Hello" ,   1 ) \n         ) ; \n\n         //TODO 2.transformation \n         //将DataStream转为View或Table \n        tenv . createTemporaryView ( "t_words" ,  wordsDS , $ ( "word" ) ,  $ ( "frequency" ) ) ; \n /*\nselect word,sum(frequency) as frequency\nfrom t_words\ngroup by word\n */ \n         String  sql  =   "select word,sum(frequency) as frequency\\n "   + \n                 "from t_words\\n "   + \n                 "group by word" ; \n\n         //执行sql \n         Table  resultTable  =  tenv . sqlQuery ( sql ) ; \n\n         //转为DataStream \n         DataStream < Tuple2 < Boolean ,  WC > >  resultDS  =  tenv . toRetractStream ( resultTable ,   WC . class ) ; \n         //toAppendStream → 将计算后的数据append到结果DataStream中去 \n         //toRetractStream  → 将计算后的新的数据在DataStream原数据的基础上更新true或是删除false \n         //类似StructuredStreaming中的append/update/complete \n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n         //new WC("Hello", 1), \n         //new WC("World", 1), \n         //new WC("Hello", 1) \n         //输出结果 \n         //(true,Demo02.WC(word=Hello, frequency=1)) \n         //(true,Demo02.WC(word=World, frequency=1)) \n         //(false,Demo02.WC(word=Hello, frequency=1)) \n         //(true,Demo02.WC(word=Hello, frequency=2)) \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @NoArgsConstructor \n     @AllArgsConstructor \n     public   static   class   WC   { \n         public   String  word ; \n         public   long  frequency ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 package   cn . itcast . sql ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . Table ; \n import   org . apache . flink . table . api . bridge . java . StreamTableEnvironment ; \n\n import   static   org . apache . flink . table . api . Expressions . $ ; \n\n /**\n * Author itcast\n * Desc 演示Flink Table&SQL 案例- 使用SQL和Table两种方式做WordCount\n */ \n public   class   Demo02_2   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         EnvironmentSettings  settings  =   EnvironmentSettings . newInstance ( ) . useBlinkPlanner ( ) . inStreamingMode ( ) . build ( ) ; \n         StreamTableEnvironment  tenv  =   StreamTableEnvironment . create ( env ,  settings ) ; \n\n         //TODO 1.source \n         DataStream < WC >  wordsDS  =  env . fromElements ( \n                 new   WC ( "Hello" ,   1 ) , \n                 new   WC ( "World" ,   1 ) , \n                 new   WC ( "Hello" ,   1 ) \n         ) ; \n\n         //TODO 2.transformation \n         //将DataStream转为View或Table \n         Table  table  =  tenv . fromDataStream ( wordsDS ) ; \n\n         //使用table风格查询/DSL \n         Table  resultTable  =  table\n                 . groupBy ( $ ( "word" ) ) \n                 . select ( $ ( "word" ) ,  $ ( "frequency" ) . sum ( ) . as ( "frequency" ) ) \n                 . filter ( $ ( "frequency" ) . isEqual ( 2 ) ) ; \n\n         //转换为DataStream \n         DataStream < Tuple2 < Boolean ,  WC > >  resultDS  =  tenv . toRetractStream ( resultTable ,   WC . class ) ; \n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @NoArgsConstructor \n     @AllArgsConstructor \n     public   static   class   WC   { \n         public   String  word ; \n         public   long  frequency ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 案例3 \n \n package   cn . itcast . sql ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . eventtime . WatermarkStrategy ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichSourceFunction ; \n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . Table ; \n import   org . apache . flink . table . api . bridge . java . StreamTableEnvironment ; \n import   org . apache . flink . types . Row ; \n\n import   java . time . Duration ; \n import   java . util . Random ; \n import   java . util . UUID ; \n import   java . util . concurrent . TimeUnit ; \n\n import   static   org . apache . flink . table . api . Expressions . $ ; \n\n /**\n * Author itcast\n * Desc 演示Flink Table&SQL 案例- 使用事件时间+Watermaker+window完成订单统计\n */ \n public   class   Demo03   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         EnvironmentSettings  settings  =   EnvironmentSettings . newInstance ( ) . useBlinkPlanner ( ) . inStreamingMode ( ) . build ( ) ; \n         StreamTableEnvironment  tenv  =   StreamTableEnvironment . create ( env ,  settings ) ; \n\n         //TODO 1.source \n         DataStreamSource < Order >  orderDS   =  env . addSource ( new   RichSourceFunction < Order > ( )   { \n             private   Boolean  isRunning  =   true ; \n             @Override \n             public   void   run ( SourceContext < Order >  ctx )   throws   Exception   { \n                 Random  random  =   new   Random ( ) ; \n                 while   ( isRunning )   { \n                     Order  order  =   new   Order ( UUID . randomUUID ( ) . toString ( ) ,  random . nextInt ( 3 ) ,  random . nextInt ( 101 ) ,   System . currentTimeMillis ( ) ) ; \n                     TimeUnit . SECONDS . sleep ( 1 ) ; \n                    ctx . collect ( order ) ; \n                 } \n             } \n\n             @Override \n             public   void   cancel ( )   { \n                isRunning  =   false ; \n             } \n         } ) ; \n\n         //TODO 2.transformation \n         //需求:事件时间+Watermarker+FlinkSQL和Table的window完成订单统计 \n         DataStream < Order >  orderDSWithWatermark  =  orderDS . assignTimestampsAndWatermarks ( WatermarkStrategy . < Order > forBoundedOutOfOrderness ( Duration . ofSeconds ( 5 ) ) \n                 . withTimestampAssigner ( ( order ,  recordTimestamp )   ->  order . getCreateTime ( ) ) \n         ) ; \n\n         //将DataStream--\x3eView/Table,注意:指定列的时候需要指定哪一列是时间 \n        tenv . createTemporaryView ( "t_order" , orderDSWithWatermark , $ ( "orderId" ) ,  $ ( "userId" ) ,  $ ( "money" ) ,  $ ( "createTime" ) . rowtime ( ) ) ; \n /*\nselect  userId, count(orderId) as orderCount, max(money) as maxMoney,min(money) as minMoney\nfrom t_order\ngroup by userId,\ntumble(createTime, INTERVAL \'5\' SECOND)\n */ \n         String  sql  =   "select userId, count(orderId) as orderCount, max(money) as maxMoney,min(money) as minMoney\\n "   + \n                 "from t_order\\n "   + \n                 "group by userId,\\n "   + \n                 "tumble(createTime, INTERVAL \'5\' SECOND)" ; \n\n         //执行sql \n         Table  resultTable  =  tenv . sqlQuery ( sql ) ; \n\n         DataStream < Tuple2 < Boolean ,   Row > >  resultDS  =  tenv . toRetractStream ( resultTable ,   Row . class ) ; \n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   Order   { \n         private   String  orderId ; \n         private   Integer  userId ; \n         private   Integer  money ; \n         private   Long  createTime ; //事件时间 \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 package   cn . itcast . sql ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . eventtime . WatermarkStrategy ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichSourceFunction ; \n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . Table ; \n import   org . apache . flink . table . api . Tumble ; \n import   org . apache . flink . table . api . bridge . java . StreamTableEnvironment ; \n import   org . apache . flink . types . Row ; \n\n import   java . time . Duration ; \n import   java . util . Random ; \n import   java . util . UUID ; \n import   java . util . concurrent . TimeUnit ; \n\n import   static   org . apache . flink . table . api . Expressions . $ ; \n import   static   org . apache . flink . table . api . Expressions . lit ; \n\n /**\n * Author itcast\n * Desc 演示Flink Table&SQL 案例- 使用事件时间+Watermaker+window完成订单统计-Table风格\n */ \n public   class   Demo03_2   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         StreamTableEnvironment  tblEnv  =   StreamTableEnvironment . create ( env ) ; \n\n         //TODO 1.source \n         DataStreamSource < Order >  orderDS   =  env . addSource ( new   RichSourceFunction < Order > ( )   { \n             private   Boolean  isRunning  =   true ; \n             @Override \n             public   void   run ( SourceContext < Order >  ctx )   throws   Exception   { \n                 Random  random  =   new   Random ( ) ; \n                 while   ( isRunning )   { \n                     Order  order  =   new   Order ( UUID . randomUUID ( ) . toString ( ) ,  random . nextInt ( 3 ) ,  random . nextInt ( 101 ) ,   System . currentTimeMillis ( ) ) ; \n                     TimeUnit . SECONDS . sleep ( 1 ) ; \n                    ctx . collect ( order ) ; \n                 } \n             } \n\n             @Override \n             public   void   cancel ( )   { \n                isRunning  =   false ; \n             } \n         } ) ; \n\n         //TODO 2.transformation \n         //需求:事件时间+Watermarker+FlinkSQL和Table的window完成订单统计 \n         DataStream < Order >  orderDSWithWatermark  =  orderDS . assignTimestampsAndWatermarks ( WatermarkStrategy . < Order > forBoundedOutOfOrderness ( Duration . ofSeconds ( 5 ) ) \n                 . withTimestampAssigner ( ( order ,  recordTimestamp )   ->  order . getCreateTime ( ) ) \n         ) ; \n\n         //将DataStream--\x3eView/Table,注意:指定列的时候需要指定哪一列是时间 \n        tenv . createTemporaryView ( "t_order" , orderDSWithWatermark , $ ( "orderId" ) ,  $ ( "userId" ) ,  $ ( "money" ) ,  $ ( "createTime" ) . rowtime ( ) ) ; \n         //Table table = tenv.fromDataStream(orderDSWithWatermark, $("orderId"), $("userId"), $("money"), $("createTime").rowtime()); \n         //table.groupBy().select(); \n /*\nselect  userId, count(orderId) as orderCount, max(money) as maxMoney,min(money) as minMoney\nfrom t_order\ngroup by userId,\ntumble(createTime, INTERVAL \'5\' SECOND)\n */ \n         Table  resultTable  =  tenv . from ( "t_order" ) \n                 . window ( Tumble . over ( lit ( 5 ) . second ( ) ) \n                         . on ( $ ( "createTime" ) ) \n                         . as ( "\n                            " ) ) \n                 . groupBy ( $ ( "tumbleWindow" ) ,  $ ( "userId" ) ) \n                 . select ( \n                        $ ( "userId" ) , \n                        $ ( "orderId" ) . count ( ) . as ( "orderCount" ) , \n                        $ ( "money" ) . max ( ) . as ( "maxMoney" ) , \n                        $ ( "money" ) . min ( ) . as ( "minMoney" ) \n                 ) ; \n\n         DataStream < Tuple2 < Boolean ,   Row > >  resultDS  =  tenv . toRetractStream ( resultTable ,   Row . class ) ; \n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   Order   { \n         private   String  orderId ; \n         private   Integer  userId ; \n         private   Integer  money ; \n         private   Long  createTime ; //事件时间 \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 #  Flink-高级特性-新特性-FlinkSQL整合Hive \n 1.介绍 \n \n 版本 \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/ \n \n 添加依赖和jar包和配置 \n < dependency > \n             < groupId > org . apache . flink < / groupId > \n             < artifactId > flink - connector - hive_2 . 12 < / artifactId > \n             < version > $ { flink . version } < / version > \n         < / dependency > \n         < dependency > \n             < groupId > org . apache . hive < / groupId > \n             < artifactId > hive - metastore < / artifactId > \n             < version > 2.1 .0 < / version > \n             < exclusions > \n                 < exclusion > \n                     < artifactId > hadoop - hdfs < / artifactId > \n                     < groupId > org . apache . hadoop < / groupId > \n                 < / exclusion > \n             < / exclusions > \n         < / dependency > \n         < dependency > \n             < groupId > org . apache . hive < / groupId > \n             < artifactId > hive - exec < / artifactId > \n             < version > 2.1 .0 < / version > \n         < / dependency > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 上传资料hive中的jar包到flink/lib中 \n \n \n FlinkSQL整合Hive-CLI命令行整合 \n 1.修改hive-site.xml \n <property>\n       <name>hive.metastore.uris</name>\n       <value>thrift://node3:9083</value>\n</property>\n \n 1 2 3 4 <?xml-stylesheet type="text/xsl" href="configuration.xsl"?> \n < configuration > \n     < property > \n         < name > javax.jdo.option.ConnectionUserName </ name > \n         < value > root </ value > \n     </ property > \n     < property > \n         < name > javax.jdo.option.ConnectionPassword </ name > \n         < value > 123456 </ value > \n     </ property > \n     < property > \n         < name > javax.jdo.option.ConnectionURL </ name > \n         < value > jdbc:mysql://node3:3306/hive?createDatabaseIfNotExist=true &amp; useSSL=false </ value > \n     </ property > \n     < property > \n         < name > javax.jdo.option.ConnectionDriverName </ name > \n         < value > com.mysql.jdbc.Driver </ value > \n     </ property > \n     < property > \n         < name > hive.metastore.schema.verification </ name > \n         < value > false </ value > \n     </ property > \n     < property > \n         < name > datanucleus.schema.autoCreateAll </ name > \n         < value > true </ value > \n     </ property > \n     < property > \n         < name > hive.server2.thrift.bind.host </ name > \n         < value > node3 </ value > \n     </ property > \n     < property > \n         < name > hive.metastore.uris </ name > \n         < value > thrift://node3:9083 </ value > \n     </ property > \n </ configuration > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 2.启动元数据服务 \n nohup /export/server/hive/bin/hive --service metastore & \n 3.修改flink/conf/sql-client-defaults.yaml \n catalogs:\n   - name: myhive\n     type: hive\n     hive-conf-dir: /export/server/hive/conf\n     default-database: default\n \n 1 2 3 4 5 4.分发 \n 5.启动flink集群 \n /export/server/flink/bin/start-cluster.sh \n 6.启动flink-sql客户端-hive在哪就在哪启 \n /export/server/flink/bin/sql-client.sh embedded \n 7.执行sql: \n show catalogs; \n use catalog myhive; \n show tables; \n select * from person; \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/ \n package   cn . itcast . feature ; \n\n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . TableEnvironment ; \n import   org . apache . flink . table . api . TableResult ; \n import   org . apache . flink . table . catalog . hive . HiveCatalog ; \n\n /**\n * Author itcast\n * Desc\n */ \n public   class   HiveDemo   { \n     public   static   void   main ( String [ ]  args ) { \n         //TODO 0.env \n         EnvironmentSettings  settings  =   EnvironmentSettings . newInstance ( ) . useBlinkPlanner ( ) . build ( ) ; \n         TableEnvironment  tableEnv  =   TableEnvironment . create ( settings ) ; \n\n         //TODO 指定hive的配置 \n         String  name             =   "myhive" ; \n         String  defaultDatabase  =   "default" ; \n         String  hiveConfDir  =   "./conf" ; \n\n         //TODO 根据配置创建hiveCatalog \n         HiveCatalog  hive  =   new   HiveCatalog ( name ,  defaultDatabase ,  hiveConfDir ) ; \n         //注册catalog \n        tableEnv . registerCatalog ( "myhive" ,  hive ) ; \n         //使用注册的catalog \n        tableEnv . useCatalog ( "myhive" ) ; \n\n         //向Hive表中写入数据 \n         String  insertSQL  =   "insert into person select * from person" ; \n         TableResult  result  =  tableEnv . executeSql ( insertSQL ) ; \n\n         System . out . println ( result . getJobClient ( ) . get ( ) . getJobStatus ( ) ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #  整合kafka和hive \n 案例4（掌握从datastream转sql，sql转datastream） \n \n import   org . apache . commons . lang3 . time . FastDateFormat ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . common . typeinfo . TypeInformation ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . configuration . RestOptions ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . Table ; \n import   org . apache . flink . table . api . TableResult ; \n import   org . apache . flink . table . api . bridge . java . StreamTableEnvironment ; \n import   org . apache . flink . table . catalog . hive . HiveCatalog ; \n import   org . apache . flink . types . Row ; \n\n import   java . io . Serializable ; \n import   java . sql . Timestamp ; \n\n public   class   KafkaToHive   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         Configuration  configuration  =   new   Configuration ( ) ; \n        configuration . setString ( RestOptions . BIND_PORT , "8081-8089" ) ;   //指定 Flink Web UI 端口为9091 \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( configuration ) ; \n         EnvironmentSettings  settings  =   EnvironmentSettings . newInstance ( ) . inStreamingMode ( ) . useBlinkPlanner ( ) . build ( ) ; \n         StreamTableEnvironment  tbEnv  =   StreamTableEnvironment . create ( env , settings ) ; \n         //写hive可以不用那么checkpoint可以设置大点，可是没到时间点也会ck，是根据数量？ \n        env . enableCheckpointing ( 10 * 60 * 1000 ,   CheckpointingMode . EXACTLY_ONCE ) ; \n        env . setParallelism ( 3 ) ; \n         //socket->producer \n         SingleOutputStreamOperator < Order >  socket  =  env . socketTextStream ( "node1" ,   9999 ) . rebalance ( ) . map ( new   RichMapFunction < String ,   Order > ( )   { \n             @Override \n             public   Order   map ( String  value )   throws   Exception   { \n                 String [ ]  timeAndWord  =  value . split ( "," ) ; \n                 Timestamp  timestamp  =   Timestamp . valueOf ( timeAndWord [ 0 ] ) ; \n                 Order  order  =   new   Order ( ) ; \n                order . setOrderId ( timeAndWord [ 1 ] ) ; \n                order . setMoney ( 1L ) ; \n                order . setEvenTime ( timestamp . getTime ( ) ) ; \n                 return  order ; \n             } \n         } ) ; \n        tbEnv . createTemporaryView ( "socket" , socket ) ; \n        tbEnv . executeSql ( "DROP TABLE IF EXISTS ODS" ) ; \n         TableResult  odsResult  =  tbEnv . executeSql ( "CREATE TABLE ODS (\\n"   + \n                 "  `orderId` STRING,\\n"   + \n                 "  `userId` INT,\\n"   + \n                 "  `money` BIGINT,\\n"   + \n                 "  `evenTime` BIGINT\\n"   + \n                 ") WITH (\\n"   + \n                 "  \'connector\' = \'kafka\',\\n"   + \n                 "  \'topic\' = \'KafkaWordCount\',\\n"   + \n                 "  \'properties.bootstrap.servers\' = \'node1:9092\',\\n"   + \n                 "  \'format\' = \'json\',\\n"   + \n                 "  \'sink.partitioner\' = \'round-robin\'\\n"   + \n                 ") " ) ; \n         String  sql = "insert into ODS select orderId,userId,money,evenTime from socket" ; \n         TableResult  result  =  tbEnv . executeSql ( sql ) ; \n\n         //kafka->hive \n        tbEnv . executeSql ( "DROP TABLE IF EXISTS DW" ) ; \n         //默认is_generic\'=\'false\' 创建的 \n         /**\n         * 可以让Flink使用Hive Catalog存储Flink SQL 元数据。\n         * 可以在Hive命令行中使用DESCRIBE FORMATTED命令查看表的元数据，\n         * 如果是is_generic=true代表是Flink专用表，这种表只能由Flink读写使用，不要用Hive去读写。\n         * 也可以直接使用Flink读写Hive表数据。\n         */ \n         TableResult  dwResult  =  tbEnv . executeSql ( "CREATE TABLE DW (\\n"   + \n                 "  `orderId` STRING,\\n"   + \n                 "  `userId` INT,\\n"   + \n                 "  `money` BIGINT,\\n"   + \n                 "  `evenTime` BIGINT\\n"   + \n                 ") WITH (\\n"   + \n                 "  \'connector\' = \'kafka\',\\n"   + \n                 "  \'topic\' = \'KafkaWordCount\',\\n"   + \n                 "  \'properties.bootstrap.servers\' = \'node1:9092\',\\n"   + \n                 "  \'properties.group.id\' = \'sink_to_hive\',\\n"   + \n                 "  \'scan.startup.mode\' = \'group-offsets\',\\n"   + \n                 "  \'format\' = \'json\'\\n"   + \n                 ")" ) ; \n\n         //写hive \n         //TODO 指定hive的配置 \n         String  name             =   "myhive" ; \n         String  defaultDatabase  =   "default" ; \n         String  hiveConfDir  =   "E:\\\\BigData\\\\workspace\\\\Flink\\\\Flink_study\\\\src\\\\main\\\\resources" ; \n\n         //TODO 根据配置创建hiveCatalog,连接hive使用的是beeline thiftserver服务方式 \n         HiveCatalog  hive  =   new   HiveCatalog ( name ,  defaultDatabase ,  hiveConfDir ) ; \n         //注册catalog \n        tbEnv . registerCatalog ( "myhive" ,  hive ) ; \n         //使用注册的catalog \n        tbEnv . useCatalog ( "myhive" ) ; \n         //可以设定使用hive sql \n        tbEnv . executeSql ( "DROP TABLE IF EXISTS OrderTb" ) ; \n         TableResult   OrderTb   =  tbEnv . executeSql ( "create table if not EXISTS OrderTb("   + \n                 "  `orderId` STRING,\\n"   + \n                 "  `userId` INT,\\n"   + \n                 "  `money` BIGINT,\\n"   + \n                 "  `evenTime` BIGINT\\n"   + \n                 ")WITH (\\n"   + \n                 "  \'is_generic\'=\'false\' )" ) ; \n         //Insert overwrite is not supported for streaming write. \n        tbEnv . executeSql ( "insert into OrderTb select orderId,userId,money,evenTime from default_catalog.default_database.DW" ) ; \n\n         Table  table  =  tbEnv . sqlQuery ( "select * from default_catalog.default_database.DW " ) ; \n        tbEnv . toRetractStream ( table ,   TypeInformation . of ( Row . class ) ) . print ( ) ; \n        env . execute ( ) ; \n\n     } \n     public   static   class   Order   implements   Serializable   { \n         private   String  orderId ; \n         private   Integer  userId ; \n         private   Long  money ; \n         private   Long  evenTime ; \n\n         public   Order ( String  orderId ,   Integer  userId ,   Long  money ,   Long  evenTime )   { \n             this . orderId  =  orderId ; \n             this . userId  =  userId ; \n             this . money  =  money ; \n             this . evenTime  =  evenTime ; \n         } \n\n         public   Order ( )   { \n         } \n\n         public   String   getOrderId ( )   { \n             return  orderId ; \n         } \n\n         public   void   setOrderId ( String  orderId )   { \n             this . orderId  =  orderId ; \n         } \n\n         public   Integer   getUserId ( )   { \n             return  userId ; \n         } \n\n         public   void   setUserId ( Integer  userId )   { \n             this . userId  =  userId ; \n         } \n\n         public   Long   getMoney ( )   { \n             return  money ; \n         } \n\n         public   void   setMoney ( Long  money )   { \n             this . money  =  money ; \n         } \n\n         public   Long   getEvenTime ( )   { \n             return  evenTime ; \n         } \n\n         public   void   setEvenTime ( Long  evenTime )   { \n             this . evenTime  =  evenTime ; \n         } \n\n         @Override \n         public   String   toString ( )   { \n             FastDateFormat  df  =   FastDateFormat . getInstance ( "HH:mm:ss" ) ; \n             return   "Order{"   + \n                     "orderId=\'"   +  orderId  +   \'\\\'\'   + \n                     ", userId="   +  userId  + \n                     ", money="   +  money  + \n                     ", evenTime="   +  df . format ( evenTime )   + \n                     \'}\' ; \n         } \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 #  Flink-练习-双十一实时交易大屏-掌握 \n 需 求 \n \n 数据 \n /**\n     * 自定义数据源实时产生订单数据Tuple2<分类, 金额>\n     */ \n     public   static   class   MySource   implements   SourceFunction < Tuple2 < String ,   Double > >   { \n         private   boolean  flag  =   true ; \n         private   String [ ]  categorys  =   { "女装" ,   "男装" ,   "图书" ,   "家电" ,   "洗护" ,   "美妆" ,   "运动" ,   "游戏" ,   "户外" ,   "家具" ,   "乐器" ,   "办公" } ; \n         private   Random  random  =   new   Random ( ) ; \n\n         @Override \n         public   void   run ( SourceContext < Tuple2 < String ,   Double > >  ctx )   throws   Exception   { \n             while   ( flag )   { \n                 //随机生成分类和金额 \n                 int  index  =  random . nextInt ( categorys . length ) ; //[0~length) ==> [0~length-1] \n                 String  category  =  categorys [ index ] ; //获取的随机分类 \n                 double  price  =  random . nextDouble ( )   *   100 ; //注意nextDouble生成的是[0~1)之间的随机小数,*100之后表示[0~100)的随机小数 \n                ctx . collect ( Tuple2 . of ( category ,  price ) ) ; \n                 Thread . sleep ( 20 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 实现步骤 \n 1.env\n2.source\n3.transformation--预聚合\n3.1定义大小为一天的窗口,第二个参数表示中国使用的UTC+08:00时区比UTC时间早\nkeyBy(t->t.f0)\nwindow(TumblingProcessingTimeWindows.of(Time.days(1), Time.hours(-8))\n3.2定义一个1s的触发器\n.trigger(ContinuousProcessingTimeTrigger.of(Time.seconds(1)))\n3.3聚合结果.aggregate(new PriceAggregate(), new WindowResult());\n3.4看一下聚合的结果\nCategoryPojo(category=男装, totalPrice=17225.26, dateTime=2020-10-20 08:04:12)\n4.sink-使用上面预聚合的结果,实现业务需求:\ntempAggResult.keyBy(CategoryPojo::getDateTime)\n//每秒钟更新一次统计结果\n                .window(TumblingProcessingTimeWindows.of(Time.seconds(1)))   \n//在ProcessWindowFunction中实现该复杂业务逻辑\n             \t.process(new WindowResultProcess());\n4.1.实时计算出当天零点截止到当前时间的销售总额\n4.2.计算出各个分类的销售top3\n4.3.每秒钟更新一次统计结果\n5.execute\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 代码实现 \n \n package   cn . itcast . action ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . commons . lang3 . StringUtils ; \n import   org . apache . commons . lang3 . time . FastDateFormat ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . AggregateFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . SourceFunction ; \n import   org . apache . flink . streaming . api . functions . windowing . ProcessWindowFunction ; \n import   org . apache . flink . streaming . api . functions . windowing . WindowFunction ; \n import   org . apache . flink . streaming . api . windowing . assigners . TumblingProcessingTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n import   org . apache . flink . streaming . api . windowing . triggers . ContinuousProcessingTimeTrigger ; \n import   org . apache . flink . streaming . api . windowing . windows . TimeWindow ; \n import   org . apache . flink . util . Collector ; \n\n import   java . math . BigDecimal ; \n import   java . math . RoundingMode ; \n import   java . util . List ; \n import   java . util . PriorityQueue ; \n import   java . util . Queue ; \n import   java . util . Random ; \n import   java . util . stream . Collectors ; \n\n /**\n * Author itcast\n * Desc\n * 1.实时计算出当天零点截止到当前时间的销售总额 11月11日 00:00:00 ~ 23:59:59\n * 2.计算出各个分类的销售top3\n * 3.每秒钟更新一次统计结果\n */ \n public   class   DoubleElevenBigScreem   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n        env . setParallelism ( 1 ) ; //方便观察 \n         //TODO 2.source \n         DataStream < Tuple2 < String ,   Double > >  orderDS  =  env . addSource ( new   MySource ( ) ) ; \n\n         //TODO 3.transformation--初步聚合:每隔1s聚合一下截止到当前时间的各个分类的销售总金额 \n         DataStream < CategoryPojo >  tempAggResult  =  orderDS\n                 //分组 \n                 . keyBy ( t  ->  t . f0 ) \n                 //如果直接使用之前学习的窗口按照下面的写法表示: \n                 //表示每隔1天计算一次 \n                 //.window(TumblingProcessingTimeWindows.of(Time.days(1))); \n                 //表示每隔1s计算最近一天的数据,但是11月11日 00:01:00运行计算的是: 11月10日 00:01:00~11月11日 00:01:00 ---不对! \n                 //.window(SlidingProcessingTimeWindows.of(Time.days(1),Time.seconds(1))); \n                 //*例如中国使用UTC+08:00，您需要一天大小的时间窗口， \n                 //*窗口从当地时间的00:00:00开始，您可以使用{@code of(时间.天(1),时间.hours(-8))}. \n                 //下面的代码表示从当天的00:00:00开始计算当天的数据,缺一个触发时机/触发间隔 \n                 //3.1定义大小为一天的窗口,第二个参数表示中国使用的UTC+08:00时区比UTC时间早 \n                 . window ( TumblingProcessingTimeWindows . of ( Time . days ( 1 ) ,   Time . hours ( - 8 ) ) ) \n                 //3.2自定义触发时机/触发间隔 \n                 . trigger ( ContinuousProcessingTimeTrigger . of ( Time . seconds ( 1 ) ) ) \n                 //.sum()//简单聚合 \n                 //3.3自定义聚合和结果收集 \n                 //aggregate(AggregateFunction<T, ACC, V> aggFunction,WindowFunction<V, R, K, W> windowFunction) \n                 . aggregate ( new   PriceAggregate ( ) ,   new   WindowResult ( ) ) ; //aggregate支持复杂的自定义聚合 \n         //3.4看一下聚合的结果 \n        tempAggResult . print ( "初步聚合的各个分类的销售总额" ) ; \n         //初步聚合的各个分类的销售总额> DoubleElevenBigScreem.CategoryPojo(category=游戏, totalPrice=563.8662504982619, dateTime=2021-01-19 10:31:40) \n         //初步聚合的各个分类的销售总额> DoubleElevenBigScreem.CategoryPojo(category=办公, totalPrice=876.5216500403918, dateTime=2021-01-19 10:31:40) \n\n         //TODO 4.sink-使用上面初步聚合的结果(每隔1s聚合一下截止到当前时间的各个分类的销售总金额),实现业务需求: \n        tempAggResult . keyBy ( CategoryPojo :: getDateTime ) \n                 . window ( TumblingProcessingTimeWindows . of ( Time . seconds ( 1 ) ) ) //每隔1s进行最终的聚合并输出结果 \n                 //.sum//简单聚合 \n                 . process ( new   FinalResultWindowProcess ( ) ) ; //在ProcessWindowFunction中实现该复杂业务逻辑 \n\n         //TODO 5.execute \n        env . execute ( ) ; \n     } \n\n     /**\n     * 自定义数据源实时产生订单数据Tuple2<分类, 金额>\n     */ \n     public   static   class   MySource   implements   SourceFunction < Tuple2 < String ,   Double > >   { \n         private   boolean  flag  =   true ; \n         private   String [ ]  categorys  =   { "女装" ,   "男装" ,   "图书" ,   "家电" ,   "洗护" ,   "美妆" ,   "运动" ,   "游戏" ,   "户外" ,   "家具" ,   "乐器" ,   "办公" } ; \n         private   Random  random  =   new   Random ( ) ; \n\n         @Override \n         public   void   run ( SourceContext < Tuple2 < String ,   Double > >  ctx )   throws   Exception   { \n             while   ( flag )   { \n                 //随机生成分类和金额 \n                 int  index  =  random . nextInt ( categorys . length ) ; //[0~length) ==> [0~length-1] \n                 String  category  =  categorys [ index ] ; //获取的随机分类 \n                 double  price  =  random . nextDouble ( )   *   100 ; //注意nextDouble生成的是[0~1)之间的随机小数,*100之后表示[0~100)的随机小数 \n                ctx . collect ( Tuple2 . of ( category ,  price ) ) ; \n                 Thread . sleep ( 20 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n     } \n\n     /**\n     * 自定义聚合函数,指定聚合规则\n     * AggregateFunction<IN, ACC, OUT>\n     */ \n     private   static   class   PriceAggregate   implements   AggregateFunction < Tuple2 < String ,   Double > ,   Double ,   Double >   { \n         //初始化累加器 \n         @Override \n         public   Double   createAccumulator ( )   { \n             return   0D ; //D表示double,L表示Long \n         } \n\n         //把数据累加到累加器上 \n         @Override \n         public   Double   add ( Tuple2 < String ,   Double >  value ,   Double  accumulator )   { \n             return  value . f1  +  accumulator ; \n         } \n\n         //获取累加结果 \n         @Override \n         public   Double   getResult ( Double  accumulator )   { \n             return  accumulator ; \n         } \n\n         //合并各个subtask的结果 \n         @Override \n         public   Double   merge ( Double  a ,   Double  b )   { \n             return  a  +  b ; \n         } \n     } \n\n     /**\n     * 自定义窗口函数,指定窗口数据收集规则\n     * WindowFunction<IN, OUT, KEY, W extends Window>\n     */ \n     private   static   class   WindowResult   implements   WindowFunction < Double ,   CategoryPojo ,   String ,   TimeWindow >   { \n         private   FastDateFormat  df  =   FastDateFormat . getInstance ( "yyyy-MM-dd HH:mm:ss" ) ; \n         @Override \n         //void apply(KEY key, W window, Iterable<IN> input, Collector<OUT> out) \n         public   void   apply ( String  category ,   TimeWindow  window ,   Iterable < Double >  input ,   Collector < CategoryPojo >  out )   throws   Exception   { \n             long  currentTimeMillis  =   System . currentTimeMillis ( ) ; \n             String  dateTime  =  df . format ( currentTimeMillis ) ; \n             Double  totalPrice  =  input . iterator ( ) . next ( ) ; \n            out . collect ( new   CategoryPojo ( category , totalPrice , dateTime ) ) ; //转化成同一秒，但是万一到后面数据量大，都能在一秒内完成计算吗？转化成天安全点 \n         } \n     } \n\n     /**\n     * 用于存储聚合的结果\n     */ \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   CategoryPojo   { \n         private   String  category ; //分类名称 \n         private   double  totalPrice ; //该分类总销售额 \n         private   String  dateTime ; // 截止到当前时间的时间,本来应该是EventTime,但是我们这里简化了直接用当前系统时间即可 \n     } \n\n     /**\n     * 自定义窗口完成销售总额统计和分类销售额top3统计并输出\n     * abstract class ProcessWindowFunction<IN, OUT, KEY, W extends Window>\n     */ \n     private   static   class   FinalResultWindowProcess   extends   ProcessWindowFunction < CategoryPojo ,   Object ,   String ,   TimeWindow >   { \n         //注意: \n         //下面的key/dateTime表示当前这1s的时间 \n         //elements:表示截止到当前这1s的各个分类的销售数据 \n         @Override \n         public   void   process ( String  dateTime ,   Context  context ,   Iterable < CategoryPojo >  elements ,   Collector < Object >  out )   throws   Exception   { \n             //1.实时计算出当天零点截止到当前时间的销售总额 11月11日 00:00:00 ~ 23:59:59 \n             double  total  =   0D ; //用来记录销售总额 \n             //2.计算出各个分类的销售top3:如: "女装": 10000 "男装": 9000 "图书":8000 \n             //注意:这里只需要求top3,也就是只需要排前3名就行了,其他的不用管!当然你也可以每次对进来的所有数据进行排序,但是浪费! \n             //所以这里直接使用小顶堆完成top3排序: \n             //70 \n             //80 \n             //90 \n             //如果进来一个比堆顶元素还有小的,直接不要 \n             //如果进来一个比堆顶元素大,如85,直接把堆顶元素删掉,把85加进去并继续按照小顶堆规则排序,小的在上面,大的在下面 \n             //80 \n             //85 \n             //90 \n             //创建一个小顶堆 \n             Queue < CategoryPojo >  queue  =   new   PriorityQueue < > ( 3 , //初识容量 \n                     //正常的排序,就是小的在前,大的在后,也就是c1>c2的时候返回1,也就是升序,也就是小顶堆 \n                     ( c1 ,  c2 )   ->  c1 . getTotalPrice ( )   >=  c2 . getTotalPrice ( )   ?   1   :   - 1 ) ; \n             for   ( CategoryPojo  element  :  elements )   { \n                 double  price  =  element . getTotalPrice ( ) ; \n                total  +=  price ; \n                 if ( queue . size ( ) <   3 ) { \n                    queue . add ( element ) ; //或offer入队 \n                 } else { \n                     if ( price  >=  queue . peek ( ) . getTotalPrice ( ) ) { //peek表示取出堆顶元素但不删除 \n                         //queue.remove(queue.peek()); \n                        queue . poll ( ) ; //移除堆顶元素 \n                        queue . add ( element ) ; //或offer入队 \n                     } \n                 } \n             } \n             //代码走到这里那么queue存放的就是分类的销售额top3,但是是升序.需要改为逆序然后输出 \n             List < String >  top3List  =  queue . stream ( ) \n                     . sorted ( ( c1 ,  c2 )   ->  c1 . getTotalPrice ( )   >=  c2 . getTotalPrice ( )   ?   - 1   :   1 ) \n                     . map ( c  ->   "分类:"   +  c . getCategory ( )   +   " 金额:"   +  c . getTotalPrice ( ) ) \n                     . collect ( Collectors . toList ( ) ) ; \n\n             //3.每秒钟更新一次统计结果-也就是直接输出 \n             double  roundResult  =   new   BigDecimal ( total ) . setScale ( 2 ,   RoundingMode . HALF_UP ) . doubleValue ( ) ; //四舍五入保留2位小数 \n             System . out . println ( "时间: " + dateTime  + " 总金额 :"   +  roundResult ) ; \n\n             System . out . println ( "top3: \\n"   +   StringUtils . join ( top3List , "\\n" ) ) ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 #  Flink-练习-订单自动好评-掌握 \n 需求 \n \n 数据 \n 思考：开窗口计算？ \n 按照一定规则收集数据计算，规则是针对所有数据，跟单条数据无关 \n 每个订单有每个订单的倒计时开始时间都不一样，不能用窗口计算。 \n 计算的触发和结束由每条数据/key自己决定，不能用窗口，得自己建立一个状态保存。 \n 思考： \n 1.进来一条数据就判断是否好评，还是都先保存状态倒计时，然后到时间再统一判断是否超时。 \n 结合实际情况，如果评价超时多，那就后判断，但是已经好评的就会保存state，占用空间，这是牺牲空间换时间。如果都是积极评价的那选择先判断，牺牲时间换空间思路。 \n 2.状态用valuestate 还是mapstate？ \n 用valuestate就行，不需要获取订单完成时间，倒计时已经记录了时间 \n 3.现实好评都是隔很多天比如15天，这样存储的状态就大了，没必要实时搞，可以按小时/天去更新状态。 \n 4.状态过期处理 \n /**\n     * 自定义source实时产生订单数据Tuple3<用户id,订单id, 订单生成时间>\n     */ \n     public   static   class   MySource   implements   SourceFunction < Tuple3 < String ,   String ,   Long > >   { \n         private   boolean  flag  =   true ; \n         @Override \n         public   void   run ( SourceContext < Tuple3 < String ,   String ,   Long > >  ctx )   throws   Exception   { \n             Random  random  =   new   Random ( ) ; \n             while   ( flag )   { \n                 String  userId  =  random . nextInt ( 5 )   +   "" ; \n                 String  orderId  =   UUID . randomUUID ( ) . toString ( ) ; \n                 long  currentTimeMillis  =   System . currentTimeMillis ( ) ; \n                ctx . collect ( Tuple3 . of ( userId ,  orderId ,  currentTimeMillis ) ) ; \n                 Thread . sleep ( 500 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 实现步骤 \n 1.env\n2.source\n3.transformation\n设置经过interval毫秒用户未对订单做出评价，自动给与好评.为了演示方便，设置5s的时间\nlong interval = 5000L;\n分组后使用自定义KeyedProcessFunction完成定时判断超时订单并自动好评\ndataStream.keyBy(0).process(new TimerProcessFuntion(interval));\n3.1定义MapState类型的状态，key是订单号，value是订单完成时间\n3.2创建MapState\nMapStateDescriptor<String, Long> mapStateDesc =\n            new MapStateDescriptor<>("mapStateDesc", String.class, Long.class);\n            mapState = getRuntimeContext().getMapState(mapStateDesc);\n3.3注册定时器\nmapState.put(value.f0, value.f1);\nctx.timerService().registerProcessingTimeTimer(value.f1 + interval);\n3.4定时器被触发时执行并输出结果\n4.sink\n5.execute\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 代码实现 \n package   cn . itcast . action ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . state . MapState ; \n import   org . apache . flink . api . common . state . MapStateDescriptor ; \n import   org . apache . flink . api . java . tuple . Tuple3 ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . KeyedProcessFunction ; \n import   org . apache . flink . streaming . api . functions . source . SourceFunction ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . Iterator ; \n import   java . util . Map ; \n import   java . util . Random ; \n import   java . util . UUID ; \n\n /**\n * Author itcast\n * Desc\n */ \n public   class   OrderAutomaticFavorableComments   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n        env . setParallelism ( 1 ) ; \n\n         //TODO 2.source \n         //Tuple3<用户id,订单id,订单生成时间> \n         DataStream < Tuple3 < String ,   String ,   Long > >  orderDS  =  env . addSource ( new   MySource ( ) ) ; \n\n         //TODO 3.transformation \n         //设置经过interval毫秒用户未对订单做出评价，自动给与好评.为了演示方便，设置5s的时间 \n         long  interval  =   5000L ; //5s \n         //分组后使用自定义KeyedProcessFunction完成定时判断超时订单并自动好评 \n        orderDS . keyBy ( t  ->  t . f0 ) \n                 . process ( new   TimerProcessFunction ( interval ) ) ; \n\n         //TODO 4.sink \n\n         //TODO 5.execute \n        env . execute ( ) ; \n     } \n\n     /**\n     * 自定义source实时产生订单数据Tuple3<用户id,订单id, 订单生成时间>\n     */ \n     public   static   class   MySource   implements   SourceFunction < Tuple3 < String ,   String ,   Long > >   { \n         private   boolean  flag  =   true ; \n\n         @Override \n         public   void   run ( SourceContext < Tuple3 < String ,   String ,   Long > >  ctx )   throws   Exception   { \n             Random  random  =   new   Random ( ) ; \n             while   ( flag )   { \n                 String  userId  =  random . nextInt ( 5 )   +   "" ; \n                 String  orderId  =   UUID . randomUUID ( ) . toString ( ) ; \n                 long  currentTimeMillis  =   System . currentTimeMillis ( ) ; \n                ctx . collect ( Tuple3 . of ( userId ,  orderId ,  currentTimeMillis ) ) ; \n                 Thread . sleep ( 500 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n     } \n\n     /**\n     * 自定义ProcessFunction完成订单自动好评\n     * 进来一条数据应该在interval时间后进行判断该订单是否超时是否需要自动好评\n     * abstract class KeyedProcessFunction<K, I, O>\n     */ \n     private   static   class   TimerProcessFunction   extends   KeyedProcessFunction < String ,   Tuple3 < String ,   String ,   Long > ,   Object >   { \n         private   long  interval ; //订单超时时间 传进来的是5000ms/5s \n\n         public   TimerProcessFunction ( long  interval )   { \n             this . interval  =  interval ; \n         } \n\n         //-0.准备一个State来存储订单id和订单生成时间 \n         private   MapState < String ,   Long >  mapState  =   null ; \n\n         //-1.初始化 \n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n             MapStateDescriptor < String ,   Long >  mapStateDescriptor  =   new   MapStateDescriptor < > ( "mapState" ,   String . class ,   Long . class ) ; \n            mapState  =   getRuntimeContext ( ) . getMapState ( mapStateDescriptor ) ; \n         } \n\n         //-2.处理每一条数据并存入状态并注册定时器 \n         @Override \n         public   void   processElement ( Tuple3 < String ,   String ,   Long >  value ,   Context  ctx ,   Collector < Object >  out )   throws   Exception   { \n             //Tuple3<用户id,订单id, 订单生成时间> value里面是当前进来的数据里面有订单生成时间 \n             //把订单数据保存到状态中 \n            mapState . put ( value . f1 ,  value . f2 ) ; //xxx,2020-11-11 00:00:00 ||xx,2020-11-11 00:00:01 \n             //该订单在value.f2 + interval时过期/到期,这时如果没有评价的话需要系统给与默认好评 \n             //注册一个定时器在value.f2 + interval时检查是否需要默认好评 \n            ctx . timerService ( ) . registerProcessingTimeTimer ( value . f2  +  interval ) ; //2020-11-11 00:00:05  || 2020-11-11 00:00:06 \n         } \n\n         //-3.执行定时任务 \n         @Override \n         public   void   onTimer ( long  timestamp ,   OnTimerContext  ctx ,   Collector < Object >  out )   throws   Exception   { \n             //检查历史订单数据(在状态中存储着) \n             //遍历取出状态中的订单数据 \n             Iterator < Map . Entry < String ,   Long > >  iterator  =  mapState . iterator ( ) ; \n             while   ( iterator . hasNext ( ) )   { \n                 Map . Entry < String ,   Long >  map  =  iterator . next ( ) ; \n                 String  orderId  =  map . getKey ( ) ; \n                 Long  orderTime  =  map . getValue ( ) ; \n                 //先判断是否好评--实际中应该去调用订单评价系统看是否好评了,我们这里写个方法模拟一下 \n                 if   ( ! isFavorable ( orderId ) )   { //该订单没有给好评 \n                     //判断是否超时--不用考虑进来的数据是否过期,该种方式少访问一次外部好评系统 \n                     if   ( System . currentTimeMillis ( )   -  orderTime  >=  interval )   { \n                         System . out . println ( "orderId:"   +  orderId  +   "该订单已经超时未评价,系统自动给与好评!...." ) ; \n                         //移除状态中的数据,避免后续重复判断 \n                        iterator . remove ( ) ; \n                        mapState . remove ( orderId ) ; \n                     } \n                 }   else   { \n                     System . out . println ( "orderId:"   +  orderId  +   "该订单已经评价...." ) ; \n                     //移除状态中的数据,避免后续重复判断 \n                    iterator . remove ( ) ; \n                    mapState . remove ( orderId ) ; \n                 } \n             } \n         } \n\n         //自定义一个方法模拟订单系统返回该订单是否已经好评 \n         public   boolean   isFavorable ( String  orderId )   { \n             return  orderId . hashCode ( )   %   2   ==   0 ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 '},{title:"spark",frontmatter:{title:"spark",date:"2019-09-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["计算引擎"],tags:["离线计算","sparksql"]},regularPath:"/%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/spark.html",relativePath:"计算引擎/spark.md",key:"v-9f20b3be",path:"/2019/09/08/spark/",headers:[{level:2,title:"Spark入门",slug:"spark入门"},{level:2,title:"Spark的部署回顾",slug:"spark的部署回顾"},{level:3,title:"local模式",slug:"local模式"},{level:3,title:"Standalone",slug:"standalone"},{level:3,title:"StandaloneHA",slug:"standaloneha"},{level:3,title:"SparkOnYarn",slug:"sparkonyarn"},{level:2,title:"SparkCore--RDD",slug:"sparkcore-rdd"},{level:3,title:"RDD是什么",slug:"rdd是什么"},{level:3,title:"RDD的五大属性",slug:"rdd的五大属性"},{level:3,title:"RDD的算子分类",slug:"rdd的算子分类"},{level:3,title:"实战案例",slug:"实战案例"},{level:3,title:"RDD依赖关系",slug:"rdd依赖关系"},{level:3,title:"RDD缓存",slug:"rdd缓存"},{level:3,title:"RDD的CheckPoint",slug:"rdd的checkpoint"},{level:3,title:"RDD的DAG",slug:"rdd的dag"},{level:3,title:"共享变量",slug:"共享变量"},{level:3,title:"Kyro序列化",slug:"kyro序列化"},{level:3,title:"内存模型",slug:"内存模型"},{level:3,title:"Spark的shuffle",slug:"spark的shuffle"},{level:2,title:"SparkSQL引入",slug:"sparksql引入"},{level:3,title:"什么是sparksql",slug:"什么是sparksql"},{level:3,title:"SparkSQL和Hive的关系(发展历程)",slug:"sparksql和hive的关系-发展历程"},{level:3,title:"SparkSQL的数据结构",slug:"sparksql的数据结构"},{level:3,title:"电影评分案例(掌握)",slug:"电影评分案例-掌握"},{level:3,title:"Spark SQL整合Hive",slug:"spark-sql整合hive"},{level:3,title:"SparkSQL的UDF函数(必须掌握)",slug:"sparksql的udf函数-必须掌握"},{level:3,title:"SparkSQL函数",slug:"sparksql函数"},{level:3,title:"SparkSQL底层如何执行解析成RDD",slug:"sparksql底层如何执行解析成rdd"},{level:2,title:"SparkStreaming引入--RDD--DStream",slug:"sparkstreaming引入-rdd-dstream"},{level:3,title:"流数据的处理模式",slug:"流数据的处理模式"},{level:3,title:"SparkStreaming数据结构",slug:"sparkstreaming数据结构"},{level:3,title:"DStream两种算子",slug:"dstream两种算子"},{level:3,title:"SparkStreaming初体验-无状态",slug:"sparkstreaming初体验-无状态"},{level:3,title:"状态计算",slug:"状态计算"},{level:3,title:"SparkStreaming和SparkSQL整合",slug:"sparkstreaming和sparksql整合"},{level:3,title:"SparkStreaming和Kafka整合",slug:"sparkstreaming和kafka整合"},{level:2,title:"StructuredStreamig引入",slug:"structuredstreamig引入"},{level:3,title:"StructuredStreamig定义",slug:"structuredstreamig定义"},{level:3,title:"StructuredStreaming与sparkstreaming区别",slug:"structuredstreaming与sparkstreaming区别"},{level:3,title:"结构化流编程模型",slug:"结构化流编程模型"},{level:3,title:"入门案例",slug:"入门案例"},{level:3,title:"StructuredStreaming的Kakfa整合",slug:"structuredstreaming的kakfa整合"},{level:3,title:"事件时间--水印机制",slug:"事件时间-水印机制"},{level:3,title:"StructuredStreaming的Continue的连续处理机制",slug:"structuredstreaming的continue的连续处理机制"}],lastUpdated:"2023-6-24 2:06:37 ├F10: AM┤",lastUpdatedTimestamp:1687543597e3,content:' Spark入门 \n \n \n 四代计算引擎 \n \n 第一代引擎：MR \n 第二代引擎：Hive（MR,Spark,Tez） 部分支持DAG(有向无环图) \n 第三代引擎：Spark和Impala(完全支持DAG) \n 第四代计算引擎：批流统一FLink(完全支持DAG) \n \n \n \n 技术发展： \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 面试题：Hadoop的基于进程的计算和Spark基于线程方式优缺点？ \n \n \n 只需要回答进程和线程的区别 \n \n \n 线程基本概念 \n l 线程是CPU的基本调度单位 \n l 一个进程一般包含多个线程, 一个进程下的多个线程共享进程的资源 \n l 不同进程之间的线程相互不可见 \n l 线程不能独立执行 \n l 一个线程可以创建和撤销另外一个线程 \n Spark的部署回顾 \n local模式 \n \n \n \n \n \n 如何安装？ \n \n local模式，开箱即用模式，使用测试模式下 \n \n \n \n \n 如何spark-shell应用？ \n \n bin/spark-shell --master local[3] \n \n \n \n wordcount \n \n 本地文件执行wordcount \n \n hdfs文件执行wordcount \n \n 了解Spark的任务流程 \n \n 深入原理：了解Shuffle \n \n 理解：为什么会生成两个文件夹？ \n \n \n \n \n 如何提交任务？ \nSpark-local模式提交任务\n    bin/spark-submit \\\n    --master local[3] \\\n    --class org.apache.spark.examples.SparkPi \\\n    /export/server/spark/examples/jars/spark-examples_2.11-2.4.5.jar \\\n    10\n \n 1 2 3 4 5 6 #  Standalone \n \n \n 2-Standalone模式学会查看 \n \n \n \n \n \n 如何安装？ \n \n 根据架构安装 \n \n 配置文件中需要更改：\n \n 1-指定谁是Master，谁是Worker \n 2-指定Master的通信地址，7077，指定Master的WebUi地址，8080 \n 3-可选项指定WOrker通信地址和Worker的WebUi地址 \n 4-配置Spark的历史日志服务器\n \n 为什么配置？因为如果执行Spark-Shell启动4040在关闭当前应用窗口之后无法查看UI \n 如何配置？需要将Spark的历史日志服务器的日志写入到HDFS分布式文件系统 \n \n \n \n \n 配置： \n \n \n \n \n 查看WebUi \n \n \n \n \n 如何spark-shell？ \n \n bin/spark-shell --master spark://node1:7077 \n \n \n \n \n 如何提交Spark任务？ \n \n \n \n \n \n     bin/spark-submit  \\ \n     --master  spark://node1:7077  \\ \n     --class  org.apache.spark.examples.SparkPi  \\ \n    /export/server/spark/examples/jars/spark-examples_2.11-2.4.5.jar  \\ \n     10 \n \n 1 2 3 4 5 \n StandaloneHA \n \n \n 3-standaloneHA模式 \n \n \n 1-架构 \n \n \n \n \n \n 2-如何搭建 \n \n 1-首先注释spark-env.sh的master节点的部分(因为zk选举) \n 2-增加zk选举的配置项 \n 3-进一步分发即可 \n \n \n \n \n \n \n 3-如何执行spark-shell(测试) \n \n \n bin/spark-shell --master spark://node1:7077,node2:7077 \n \n \n 启动遇到的问题 \n \n \n \n \n \n 解决：在node2上启动master \n \n \n \n \n \n 正常启动 \n \n \n \n \n \n 可以执行简单的rdd创建 \n \n \n \n \n \n 4-如何提交任务(实际生产环境) \n \n \n \n \n     bin/spark-submit \\\n    --master spark://node1:7077,node2:7077 \\\n    --class org.apache.spark.examples.SparkPi \\\n    /export/server/spark/examples/jars/spark-examples_2.11-2.4.5.jar \\\n    10\n \n 1 2 3 4 5 \n \n \n \n \n 观察现象： \n SparkOnYarn \n \n \n 使用的Yarn负责资源管理和调度 \n \n \n Driver申请资源-----AppMaster应用管理器----ResourceManager ---- -NodeManager----Container----Task任务 \n \n \n （1）搭建Yarn环境 \n \n 1-首先需要在spark-env.sh中配置HADOOP_CONF和Yarn_CONF \n 2-需要关闭内存检查 \n \n \n \n 3-需要整合Yarn的历史日志服务器和Spark的历史日志服务器\n \n 为什么整合？\n \n 答案：因为Yarn的历史日志服务器中的历史Job无法查看具体spark的任务使用了多少executor和内存 \n \n \n \n \n \n \n \n \n \n \n \n \n \n （2）提交Yarn任务 \n \n \n   bin/spark-submit \\\n  --master yarn \\\n  --class org.apache.spark.examples.SparkPi \\\n  /export/server/spark/examples/jars/spark-examples_2.11-2.4.5.jar \\\n  10\n \n 1 2 3 4 5 \n \n \n \n \n （3）查看监控---8088，历史日志服务器19888 \n \n \n \n \n \n \n \n \n 重难点知识 \n \n Spark部署模式 \n SparkOnYarn的两种deploymode模式原理 \n 掌握基于Scala的WOrdcount的IDEA编码 \n 学会提交Jar包集群跑 \n 需要形成属于自己的脚本 \n Spark的两种Mode模式(集群模式均可用) \n \n \n 集群模式：standalone和HA和Yarn \n \n \n 本质区别：Cluster和Client模式最最本质的区别是：Driver程序运行在哪里。 \n \n \n \n \n \n 第一种Client：driver启动在本地 \n \n \n \n \n \n   bin/spark-submit \\\n  --master spark://node1:7077 \\\n  --deploy-mode client \\\n  --class org.apache.spark.examples.SparkPi \\\n  /export/server/spark-2.4.5-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.5.jar \\\n  10\n \n 1 2 3 4 5 6 \n \n \n \n \n 第二种Cluster：driver启动在由Master指定的worker节点上 \nSpark-standalone模式提交任务-cluster \n  bin/spark-submit  \\ \n   --master  spark://node1:7077  \\ \n  --deploy-mode cluster  \\ \n   --class  org.apache.spark.examples.SparkPi  \\ \n  /export/server/spark-2.4.5-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.5.jar  \\ \n   10 \n \n 1 2 3 4 5 6 7 \n \n \n \n \n \n \n \n \n \n \n \n \n \n 总结： \n \n 本质：driver启动在哪里，如果启动在client端客户端直接看到结果，否则需要worker节点日志查看 \n Spark的OnYarn原理详解 \n Client \n \n \n \n \n 1-首先Driver启动在Client端的 \n \n \n 2-向ResourceManager申请启动AppMaster \n \n \n 3-由AppMaster向ResourceManager申请资源 \n \n \n 4-由ResourceManager(返回资源列表到AppMaster在)指定NodeManager启动Executor进程 \n \n \n 5-Executor就是资源反向注册到Driver端 \n \n \n 6-Driver端继续执行计算任务-----DAGScheduler和TaskScheduler \n \n \n 注意：执行到****Action算子时，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行****。 \n \n \n \n \n \n 命令 \n \n \n   bin/spark-submit  \\ \n   --master   yarn   \\ \n  --deploy-mode client  \\ \n   --class  org.apache.spark.examples.SparkPi  \\ \n  /export/server/spark/examples/jars/spark-examples_2.11-2.4.5.jar  \\ \n   10 \n \n 1 2 3 4 5 6 \n \n \n \n Cluster原理 \n \n \n \n \n \n \n \n \n \n \n \n \n   bin/spark-submit  \\ \n   --master   yarn   \\ \n  --deploy-mode cluster  \\ \n   --class  org.apache.spark.examples.SparkPi  \\ \n  /export/server/spark/examples/jars/spark-examples_2.11-2.4.5.jar  \\ \n   10 \n \n 1 2 3 4 5 6 \n \n 如何查询结果（conf 底下的log4j.properties.tmplete需复制一份log4j.properties,否则看不到stdout的东西）： \n \n \n \n \n \n \n \n \n \n \n \n 查看Container容器和Executor的关系 \n \n \n 答案：Executor是运行在Continer里面 \n \n \n Executor是进程，Task是线程，最终执行计算的是一个线程执行一个分区的Task任务 \n Spark的IDEA编程指南 \n 创建项目 \n \n \n 步骤： \n \n \n 1-首先明确项目名称和包名 \n \n bigdata-spark_2.11 \n spark-chapter01_2.11 \n \n \n \n \n \n \n 2-加载pom文件，记得修改本地maven库一直使用的库 \n \n \n    < properties > \n         < encoding > UTF-8 </ encoding > \n         < scala.version > 2.11.12 </ scala.version > \n         < scala.binary.version > 2.11 </ scala.binary.version > \n         < spark.version > 2.4.5 </ spark.version > \n         < hadoop.version > 2.7.5 </ hadoop.version > \n     </ properties > \n  \n     < dependencies > \n         < dependency > \n             < groupId > org.scala-lang </ groupId > \n             < artifactId > scala-library </ artifactId > \n             < version > ${scala.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-core_${scala.binary.version} </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.hadoop </ groupId > \n             < artifactId > hadoop-client </ artifactId > \n             < version > ${hadoop.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > junit </ groupId > \n             < artifactId > junit </ artifactId > \n             < version > 4.10 </ version > \n             < scope > provided </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > mysql </ groupId > \n             < artifactId > mysql-connector-java </ artifactId > \n             < version > 5.1.38 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > com.alibaba </ groupId > \n             < artifactId > fastjson </ artifactId > \n             < version > 1.2.47 </ version > \n         </ dependency > \n         \x3c!-- 后续使用 --\x3e \n         \x3c!--SparkSQL+ Hive依赖--\x3e \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-hive_2.11 </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-hive-thriftserver_2.11 </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n         \x3c!-- spark-streaming--\x3e \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-streaming_2.11 </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n         \x3c!-- SparkMlLib机器学习模块,里面有ALS推荐算法--\x3e \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-mllib_2.11 </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n         \x3c!--spark-streaming+Kafka依赖--\x3e \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-streaming-kafka-0-10_2.11 </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n         \x3c!--StructuredStreaming+Kafka依赖--\x3e \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-sql-kafka-0-10_2.11 </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n  \n         < dependency > \n             < groupId > com.hankcs </ groupId > \n             < artifactId > hanlp </ artifactId > \n             < version > portable-1.7.7 </ version > \n         </ dependency > \n         \x3c!-- Redis客户端工具--\x3e \n         < dependency > \n             < groupId > redis.clients </ groupId > \n             < artifactId > jedis </ artifactId > \n             < version > 2.9.0 </ version > \n         </ dependency > \n     </ dependencies > \n  \n     < build > \n         < outputDirectory > target/classes </ outputDirectory > \n         < testOutputDirectory > target/test-classes </ testOutputDirectory > \n         < resources > \n             < resource > \n                 < directory > ${project.basedir}/src/main/resources </ directory > \n             </ resource > \n         </ resources > \n         \x3c!-- Maven 编译的插件 --\x3e \n         < plugins > \n             \x3c!-- 指定编译java的插件 --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.0 </ version > \n                 < configuration > \n                     < source > 1.8 </ source > \n                     < target > 1.8 </ target > \n                     < encoding > UTF-8 </ encoding > \n                 </ configuration > \n             </ plugin > \n             \x3c!-- 指定编译scala的插件 --\x3e \n             < plugin > \n                 < groupId > net.alchim31.maven </ groupId > \n                 < artifactId > scala-maven-plugin </ artifactId > \n                 < version > 3.2.0 </ version > \n                 < executions > \n                     < execution > \n                         < goals > \n                             < goal > compile </ goal > \n                             < goal > testCompile </ goal > \n                         </ goals > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n         </ plugins > \n     </ build > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 #  Scala编程指南[掌握] \n \n \n 3-构建基础Scala的WordCount版本 \n \n \n 需求：使用IDEA实现wordcount案例 \n \n \n 步骤： \n \n 1-首先创建SparkContent上下文环境 \n 2-从外部文件数据源读取数据 \n 3-执行flatmap执行扁平化操作 \n 4-执行map转化操作，(word,1) \n 5-reduceByKey(预聚合) \n 6-输出到文件系统或打印即可 \n \n \n \n 结果： \n \n \n    package   cn . itcast . sparkbase \n  \n   import   org . apache . spark . rdd . RDD\n   import   org . apache . spark . { SparkConf ,  SparkContext } \n  \n   /**\n   * DESC:\n   * 1-首先创建SparkContent上下文环境\n   * 2-从外部文件数据源读取数据\n   * 3-执行flatmap执行扁平化操作\n   * 4-执行map转化操作，(word,1)\n   * 5-reduceByKey(预聚合)\n   * 6-输出到文件系统或打印即可\n   */ \n   object  _01SparkWordCount  { \n     def  main ( args :  Array [ String ] ) :   Unit   =   { \n       //* 1-首先创建SparkContext上下文环境 \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( "_01SparkWordCount" ) . setMaster ( "local[*]" ) \n       val  sc :  SparkContext  =   new  SparkContext ( conf ) \n       //sc.setLogLevel("WARN") \n       //* 2-从外部文件数据源读取数据 \n       val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/words.txt" ) \n       //println(s"count value is:${fileRDD.count()}")//count value is:2 \n       //* 3-执行flatmap执行扁平化操作 \n       val  valueRDD :  RDD [ String ]   =  fileRDD . flatMap ( x  =>  x . split ( "\\\\s+" ) ) \n       //* 4-执行map转化操作，(word,1) \n       val  mapRDD :  RDD [ ( String ,   Int ) ]   =  valueRDD . map ( x  =>   ( x ,   1 ) ) \n       //* 5-reduceByKey(预聚合) \n       val  resultRDD :  RDD [ ( String ,   Int ) ]   =  mapRDD . reduceByKey ( ( a ,  b )   =>  a  +  b ) \n       //* 6-输出到文件系统或打印即可 \n      resultRDD . foreach ( x => println ( x ) )   //这里是直接使用foreach对RDD进行打印 \n      println ( "=============================" ) \n      resultRDD . collect ( ) . foreach ( println ( _ ) ) //能否转换为集合在打印 \n      println ( "=============================" ) \n      resultRDD . saveAsTextFile ( "data/baseoutput/output-1" ) \n      sc . stop ( ) \n     } \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 Java编程指南[了解] \n \n \n 4-构建基础Java的WordCount版本[了解] \n \n \n 1-需求：使用Java语言完成Wordcount代码 \n \n \n 2-步骤： \n \n 1-首先创建SparkContent上下文环境 \n 2-从外部文件数据源读取数据 \n 3-执行flatmap执行扁平化操作 \n 4-执行map转化操作，(word,1) \n 5-reduceByKey(预聚合) \n 6-输出到文件系统或打印即可 \n \n \n \n 3-结果或代码 \n \n \n \n \n      package   cn . itcast . sparkbase ; \n    \n     import   org . apache . spark . SparkConf ; \n     import   org . apache . spark . api . java . JavaPairRDD ; \n     import   org . apache . spark . api . java . JavaRDD ; \n     import   org . apache . spark . api . java . JavaSparkContext ; \n     import   scala . Tuple2 ; \n    \n     import   java . util . Arrays ; \n     import   java . util . List ; \n    \n     /**\n     * DESCRIPTION:\n     * 1-首先创建SparkContent上下文环境\n     * 2-从外部文件数据源读取数据\n     * 3-执行flatmap执行扁平化操作\n     * 4-执行map转化操作，(word,1)\n     * 5-reduceByKey(预聚合)\n     * 6-输出到文件系统或打印即可\n     */ \n     public   class  _01SparkFirst  { \n         public   static   void   main ( String [ ]  args )   { \n             //1-首先创建SparkContent上下文环境 \n             SparkConf  conf  =   new   SparkConf ( ) . setAppName ( "_01SparkFirst" ) . setMaster ( "local[*]" ) ; \n             JavaSparkContext  jsc  =   new   JavaSparkContext ( conf ) ; \n             //2-从外部文件数据源读取数据 \n             JavaRDD < String >  fileRDD  =  jsc . textFile ( "data/baseinput/words.txt" ) ; \n             //3-执行flatmap执行扁平化操作(学习看源码) \n             JavaRDD < String >  flatMapRDD  =  fileRDD . flatMap ( x  ->   Arrays . asList ( x . split ( "\\\\s+" ) ) . iterator ( ) ) ; \n             //4-执行map转化操作，(word,1)(学习看源码) \n             JavaPairRDD < String ,   Integer >  mapRDD  =  flatMapRDD . mapToPair ( x  ->   new   Tuple2 < > ( x ,   1 ) ) ; \n             //5-reduceByKey(预聚合) \n             JavaPairRDD < String ,   Integer >  resultRDD  =  mapRDD . reduceByKey ( ( a ,  b )   ->  a  +  b ) ; \n             //6-输出到文件系统或打印即可 \n             List < Tuple2 < String ,   Integer > >  collectResult  =  resultRDD . collect ( ) ; \n            collectResult . forEach ( System . out :: println ) ; \n             //关闭连接 \n            jsc . stop ( ) ; \n         } \n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n \n 总结1： \n \n setAppName撰写this.getClass.getSimpleName.stripSuffix("$")方式 \n \n \n \n 总结2： \n \n \n     //这里设置线程休眠为了能够看到WebUI的展示的DAG等监控界面\n    Thread.sleep(100*1000)\n \n 1 2 \n \n \n \n \n 总结3： \n \n Spark划分为两大角色：Driver和Executor \n \n Driver是负责应用管理者，申请资源和执行dag的计算 \n Executor利用线程执行对应分区的计算，一个task需要一个线程执行 \n \n 传统意义上的几核几线程，实质上指的是PC机器的线程数，比如我的机器6和12线程，线程个数最多可以使用12个 \n \n \n \n Job和DAG的关系 \n \n 代码中那些事在Driver端进行的，那些事Executor端进行的 \n 打包上传后提交任务 \n \n \n 后续再讲解打包上传 \n \n \n 需求：需要实现代码并提交到集群上运行 \n \n \n 步骤； \n \n \n 1-首先pom实现package打包 \n \n 直接package \n \n \n \n 2-上传虚拟机中 \n \n 直接拖拽 \n \n \n \n 3-执行任务 \n自己实现的代码放在集群上跑任务，尝试使用Spark Yarn-client模式 \n    bin/spark-submit  \\ \n     --master   yarn   \\ \n    --deploy-mode client  \\ \n     --class  cn.itcast.sparkbase._01SparkWordCountTar  \\ \n    /export/data/spark-base_2.11-1.0.0.jar  \\ \n    hdfs://node1.itcast.cn:8020/wordcount/input  \\ \n    hdfs://node1.itcast.cn:8020/wordcount/output-8\n \n 1 2 3 4 5 6 7 8 \n \n 查看结果 \n \n \n \n \n \n 该任务需要掌握 \n SparkSubmit提交任务的参数 \n \n 下面的重点掌握 \n \n   YARN：\n   --master  MASTER_URL集群资源管理器   spark://host:port,host1:port, mesos://host:port,  yarn \n                              k8s://https://host:port, or  local   ( Default: local [ * ] ) .\n  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally  ( "client" )  or\n                              on one of the worker machines inside the cluster  ( "cluster" ) \n                               ( Default: client ) .\n   --class  CLASS_NAME          Your application\'s main class  ( for Java / Scala apps ) .\n   --jars  JARS                 Comma-separated list of jars to include on the driverand executor classpaths.\n   --conf   PROP = VALUE           Arbitrary Spark configuration property.\n关键部分，Driver端和Executor端的配置 \nDriver申请资源执行计算任务 \n  --driver-memory MEM         Driver端的内存默认1G Memory  for  driver  ( e.g. 1000M, 2G )   ( Default: 1024M ) .\n  --driver-cores NUM          Number of cores used by the driver, only  in  cluster mode ( Default:  1 ) .\n                              这里注意，为什么local不需要使用driver-cores，因为使用local [ * ] 模拟本机多线程\nExecutor是真正执行资源和计算任务的 \n  --num-executors NUM         Number of executors to launch  ( Default:  2 ) .启动多少个executors，默认2个\n                              If dynamic allocation is enabled, the initial number of\n                              executors will be at least NUM.还可以动态开启\n  --executor-memory MEM       每个Executor的内存，默认1G  Memory per executor  ( e.g. 1000M, 2G )   ( Default: 1G ) .\n  --executor-cores NUM        每个executor有多少cores，yarn默认为1 \n                          Number of cores per executor.  ( Default:  1   in  YARN mode,\n                          or all available cores on the worker  in  standalone mode ) \n   --queue  QUEUE_NAME          The YARN queue to submit to  ( Default:  "default" ) .\n  \n  如果有一个需求的数据量，需要满足Executor端的内存一定超越给定的数据量，\n  cpu-cores越多越好 ( cpu-cores模拟的线程，每个线程执行1个分区的数据，如果业务数据分区越多，开启cpucores越多 ) \n  bin/spark-submit  \\ \n   --master   yarn   \\ \n   --deploy_mode  cluster  \\ \n  --driver-memory 2g  \\  \n  --driver-cores  2   \\ \n  --num-executors  10   \\ \n  --executor-memory 2g  \\ \n  --executor-cores  3   \\ \n   --class  cn.itcast.apple.mainclass  \\ \n  jar包路径  \\ \n  程序需要参数\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 \n IDEA直接读取HDFS文件 \n \n \n 需求：在IDEA中直接读取HDFS文件，如何实现 \n \n \n 步骤： \n \n 1-远程连接Hadoop集群 \n 2-直接拷贝core-site.xml和hdfs-site.xml配置文件(为了使用相对路径) \n 3-拷贝到对应module模块的resource目录下 \n 4-直接写相对路径即可读取 \n \n \n \n 操作连接服务器的步骤 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 用groupbykey的方法实现wordcount案例 \n \n \n \n \n \n 排序的操作，这里截图使用的是top方法 \n \n \n \n \n \n 所有代码 \n \n \n    package   cn . itcast . sparkbase \n  \n   import   org . apache . spark . rdd . RDD\n   import   org . apache . spark . { SparkConf ,  SparkContext } \n  \n   /**\n   * DESC:\n   * 1-首先需要创建SparkContext，引入SparkConf\n   * 2-读取外部数据文件到RDD\n   * 3-RDD的转换\n   * 4-将结果数据保存在HDFS中\n   */ \n   object  _01SparkWordCount  { \n     def  main ( args :  Array [ String ] ) :   Unit   =   { \n       //1-首先需要创建SparkContext，引入SparkConf \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  sc  =   new  SparkContext ( conf ) \n      sc . setLogLevel ( "WARN" ) \n       //2-读取外部数据文件到RDD \n       val  fileRDD :  RDD [ String ]   =  sc . textFile ( "hdfs://node1:8020/wordcount/input/" ) \n       //fileRDD.foreach(println(_)) \n       //3-RDD的转换 \n       val  flatMapRDD :  RDD [ String ]   =  fileRDD\n         . filter ( line  =>  line  !=   null   &&  line . trim . length  >   0 ) \n         . flatMap ( _ . split ( "\\\\s+" ) ) \n       val  resultRDD :  RDD [ ( String ,   Int ) ]   =  flatMapRDD . map ( ( _ ,   1 ) ) . groupByKey ( ) . map ( x  => ( x . _1 , x . _2 . sum )   ) \n       //将结果数据收集到Driver端-all the data is loaded into the driver\'s memory. \n       val  resultArray :  Array [ ( String ,   Int ) ]   =  resultRDD . collect ( ) \n      println ( resultArray . toBuffer )   //打印方法 \n       //Buffer((hello,4), (me,3), (you,2), (her,1)) \n       //4-排序操作,因为take是action算子，如果返回值是rdd的化验证操作是Transformation操作 \n      println ( "sotyby oprations is:================" ) \n      resultRDD . sortBy ( _ . _2 , true ) . take ( 3 ) . foreach ( println ( _ ) ) \n      println ( "sotybykey oprations is:================" ) \n       //Take the first num elements of the RDD. \n      resultRDD . map ( _ . swap ) . sortByKey ( true ) . take ( 3 ) . foreach ( println ( _ ) ) \n      println ( "top oprations is:================" ) \n       //Returns the top k (largest) elements from this RDD \n      resultRDD . top ( 3 ) ( Ordering . by ( x => x . _2 ) ) . foreach ( println ( _ ) ) \n       //4-将结果数据保存在HDFS中 \n       //flatMapRDD.foreach(println(_)) \n      sc . stop ( ) \n     } \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 #  SparkCore--RDD \n 重难点知识 \n \n \n 使用IDEA完成HDFS文件读取 \n \n \n RDD的引入 \n \n \n RDD的特性--面试必问[重点] \n \n \n RDD的创建 \n \n \n RDD的转换算子 \n \n \n RDD的行动算子 \n \n \n RDD的案例实战[重点] \n RDD是什么 \n RDD*(Resilient Distributed Dataset)*是弹性分布式数据集，是 不可变，可分区，可并行计算 的集合。 \n \n \n 什么是弹性：数据可以存储在磁盘也可以存储在内存中 \n \n \n 什么是分布式：分布式计算和分布式存储 \n \n \n 什么是数据集：数据构成的集合 \n \n \n 不可变：RDD一旦创建就不能改变，不能转换 \n \n \n 可分区：RDD是可以划分为不同分区partition \n \n \n 可并行计算的集合：基于内存的可并行计算集合 \n \n \n \n \n \n 源码分析 \n RDD的五大属性 \n \n 1-分区列表，每一个RDD都是不同分区构成的 \n 2-计算函数：每个RDD的分区都有计算函数作用 \n 3-依赖关系：每个RDD有一定依赖关系 \n 4-可选分区器：也就是RDD有分区器，默认是Hash-Partitioner \n 5-可选位置优先性：移动计算不要移动存储 \n \n \n \n 稍后以WordCount案例梳理五大属性 \n 1-如何查看分区个数 \n \n 2-如何查看分区器 \n \n \n 补充图示 \n \n \n RDD创建的方法 \n \n \n 三种方法的创建 \n \n \n \n \n \n 增加分区的理解 \n \n \n \n \n \n \n \n \n IDEA中查看 \n \n \n \n \n \n \n \n \n 点击源码查看的过程，得到结论是makerdd或parallise都是根据totalcpucores和2比较最大值 \n \n \n 如果直接覆盖makerdd或parallise的第二个分区个数的参数，改变该取值 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 思考：spark.default.parallelism啥东西 \n \n \n \n \n \n \n \n \n 如何在代码中设置上述参数 \n \n \n \n \n \n 查看textFile如何控制并行度？ \n \n \n \n \n \n \n \n \n \n \n \n 如何使用Spark的rdd读取很多小文件？ \n \n \n 1-数据集含义：userid-moviesid-rating-timestamp，一个用户在什么时间给什么电影评分 \n \n \n \n \n \n 2-sc.textFile遇到小文件没有办法很好合并小文件的，即便重写第二个参数也没有作用 \n \n \n Spark中提供了小文件的处理方案sc.wholetextFile的方式，不会根据文件多少得到多少分区 \n \n \n package   cn . itcast . sparkbase . rddopration \n\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . { SparkConf ,  SparkContext } \n\n /**\n * DESC:\n */ \n object  _02SparkRDDFirst  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-准备环境 \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setMaster ( "local[*]" ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . set ( "spark.default.parallelism" ,   "4" ) \n     val  sc  =   new  SparkContext ( conf ) \n     //2-准备读取外部文件系统 \n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/words.txt" ) \n     //sc.parallelize(1 to 10) \n     //3-查看分区个数 \n    println ( "partitons length:"   +  fileRDD . getNumPartitions )   //2 \n    println ( s "partiiton length is: ${ fileRDD . partitions . length } " )   //2 \n     //读取一个文件夹下的多个文件 \n     //textFile在读取小文件的时候，会参考小文件的个数，文件个数越多，分区个数越多 \n     val  filesRDD1 :  RDD [ String ]   =  sc . textFile ( "data/baseinput/ratings100/" ) \n    println ( "partitons length:"   +  filesRDD1 . getNumPartitions )   //100 \n     //Spark读取小文件的方法 \n     val  filesRDD2 :  RDD [ ( String ,   String ) ]   =  sc . wholeTextFiles ( "data/baseinput/ratings100/" ) \n     //println("partitons length:" + filesRDD2.getNumPartitions) //2 \n    filesRDD2 . take ( 3 ) . foreach ( println ( _ ) ) \n     //23106,5214,3.0,1043445719 \n\n    sc . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \n \n 补充案例 \n \n \n \n \n \n \n \n \n \n 用textFile时，它的partition的数量是与文件夹下的文件数量（实例中用3个xxx.log文件）相关，一个文件就是一个partition(既然3个文件就是：partition=3)，文件大小均匀的情况下，可以这么估算，具体按照hadoop的文件切片规则（goalsize->splitsize->1.1split每个大文件会进行切片）。 \n \n \n textFile分片源码 \n \n \n \n \n \n wholeTextFiles通常用于读取许多小文件的需求，切片时会进行小文件的合并（maxSplitSize->累计切片）。 \n \n \n wholeTextFile分片源码https://blog.csdn.net/m0_37817767/article/details/125779863 \n \n \n \n spark wholeTextFile合并小文件的过程较为简单，mapreduce的CombineTextInputFormat合并过程较为复杂。 \n https://blog.csdn.net/qq_35241080/article/details/106065442 \n RDD的算子分类 \n \n Transmormation转化算子 \n Action行动算子，使用action算子不会返回rdd，且会将结果返回给driver端。 \n RDD的Transformation算子 \n \n \n 第一种：RDD的单Value类型RDD \n \n \n 第二种：RDD的双Value的RDD \n \n \n 第三种：RDD的Key和value类型的RDD \n \n \n RDD单Value类型算子 \n map算子 \n \n 1-含义：经过fun的操作得到新的RDD \n 2-操作 \n \n \n filter算子 \n \n 1-含义：经过fun的操作得到新的RDD，元素需要满足条件 \n 2-操作 \n \n \n flatMap算子 \n \n 1-含义：扁平化 \n 2-操作 \n \n \n mapPartitions算子 \n \n 1-含义 \n \n 2-操作 \n \n mapParittionWithIndex算子 \n \n 1-含义 \n \n 2-操作 \n \n sample算子 \n \n 1-含义 \n \n seed：保证每次随机切分的数据的可重复性 \n 2-操作 \n \n \n glom算子 \n \n 1-含义：查看每个分区的内容 \n 2-操作 \n \n \n sortBy算子 \n \n 1-含义：根据key或者value进行排序 \n 2-操作： \n \n \n coalese算子（合并） \n \n 如果缩减分区直接给定缩减到的数量，扩分区需要需要开启Shuffle为true \n 1-含义 \n \n 2-操作 \n \n \n repartition算子 \n \n \n 1-含义：重分区，调用coalesce(numPartitions,shuffle=true),因此缩减分区尽量用coalesce。 \n \n \n 2-操作 \n \n \n \n \n \n 源码： \n \n \n \n \n \n \n \n \n repartitionAndSortWithinPartitions 重分区的时候就排序,比分完区再排序高效 \n \n \n RDD的双Value类型算子 \n 集合的交集并集补集 \n union并集 \n intersection交集 \n distinct去重 \n subtract差集 \n \n zip拉链 \n 需要保持元素和分区个数一致 \n \n \n RDD的Key和Value的算子 \n partitionBy： \n \n 1-含义： \n \n 2-操作： \n \n \n ****注意：Spark采用的分区有三种****：第一、水平分区，也就是sc.makerdd按照下标元素划分，第二、Hash划分根据数据确定性划分到某个分区，一般只给定分区数。第三、Range分区该方法一般按照元素大小进行划分不同区域，每个分区表示一个数据区域，如数组中每个数是[0,100]之间的随机数，Range划分首先将区域划分为10份，然后将数组中每个数字分发到不同的分区，比如将18分到(10,20]的分区，最后对每个分区进行排序。 \n \n reduceByKey \n \n 1-含义：在shuffle前进行预聚合，减少shuffle拉取的数据量 \n \n 2-操作： \n \n \n groupByKey \n \n \n 1-含义： \n \n \n \n \n \n 2-操作： \n \n \n \n \n \n 了解区别 \n \n \n \n reduceByKey：按照key进行聚合，在shuffle之前有combine（预聚合）操作，返回结果是RDD[k,v]. \n groupByKey：按照key进行分组，直接进行shuffle。 \n 开发指导：reduceByKey比groupByKey，建议使用。但是需要注意是否会影响业务逻辑。 \n \n \n \n 需求：使用GroupByKey的源码---使用底层源码实现GroupByKey \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n package   cn . itcast . sparkbase . base \n\n import   org . apache . spark . rdd . { RDD ,  ShuffledRDD } \n import   org . apache . spark . { Aggregator ,  SparkConf ,  SparkContext } \n\n import   scala . collection . mutable . ArrayBuffer\n\n /**\n * DESC:\n */ \n object  _01groupByKeyOperation  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //* 1-首先创建SparkContext上下文环境 \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n     val  sc :  SparkContext  =   new  SparkContext ( conf ) \n     //* 2创建数据，生成RDD \n     val  filerdd :  RDD [ String ]   =  sc . makeRDD ( Array ( "hello you" ,   "hello me hello she" ,   "hello spark" ) ) \n     val  flatmapRDD :  RDD [ String ]   =  filerdd . flatMap ( _ . split ( "\\\\s+" ) ) \n     val  mapRDD :  RDD [ ( String ,   Int ) ]   =  flatmapRDD . map ( x  =>   ( x ,   1 ) ) \n     //* 3使用groupBy完成分组 \n     val  groupRDD1 :  RDD [ ( String ,  Iterable [ Int ] ) ]   =  mapRDD . groupByKey ( ) \n    groupRDD1 . collect ( ) . foreach ( println ( _ ) ) \n     //(me,CompactBuffer(1)) \n     //(spark,CompactBuffer(1)) \n     //(you,CompactBuffer(1)) \n     //(she,CompactBuffer(1)) \n     //(hello,CompactBuffer(1, 1, 1, 1)) \n     //CompactBuffer是spark模仿arraybuffer构建缓冲容器 \n     //换一种方法模拟gropuByKey实现相同key的value的分组，使用源码的方法 \n     //1-首先CompactBuffer存放分组的结果可以防止的ArrayBuffer \n     //val createCombiner = (v: V) => CompactBuffer(v) \n     //val mergeValue = (buf: CompactBuffer[V], v: V) => buf += v \n     //val mergeCombiners = (c1: CompactBuffer[V], c2: CompactBuffer[V]) => c1 ++= c2 \n     /*\n    key value 存放value的arraybuffer\n     */ \n     val  createCombiner  =   ( v :   Int )   =>  ArrayBuffer ( v ) \n     val  mergeValue  =   ( buf :  ArrayBuffer [ Int ] ,  v :   Int )   =>  buf  +=  v\n     val  mergeCombiners  =   ( c1 :  ArrayBuffer [ Int ] ,  c2 :  ArrayBuffer [ Int ] )   =>  c1  ++ =  c2\n     val  valueRDD :  ShuffledRDD [ String ,   Int ,  ArrayBuffer [ Int ] ]   =   new  ShuffledRDD [ String ,   Int ,  ArrayBuffer [ Int ] ] ( mapRDD ,   new  org . apache . spark . HashPartitioner ( 4 ) ) \n       . setAggregator ( new  Aggregator ( \n        createCombiner , \n        mergeValue , \n        mergeCombiners ) ) \n       . setMapSideCombine ( false ) \n    println ( "source code create groupBykey" ) \n    valueRDD . foreach ( println ( _ ) ) \n    sc . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别？ \n 这四个算子底层调用的都是同一个方法combineByKeyWithClassTag只不过他们的参数传值不同： \n reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同 \nFoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同 \nAggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同 \nCombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则可以不相同。 \n CombineByKey 与combineByKeyWithClassTag是一样的，为了兼容所以保留着\n This  method is here  for  backward  compatibility .  It  does not provide combiner classtag information  to   the  shuffle . \njvm的类型擦除问题，泛型 T ， K , U , V 的类型信息 , 类 Java 语言只能在编译的阶段获取到类型参数，一旦代码被送入 JVM 运行，就会被擦除 . \n  1. 只要在定义泛型函数或泛型类的时候，在泛型标识后加上 : ClassTag 关键字，并且导入  scala . reflect . ClassTag  包即可\n 2. 或者使用 : Manifest 关键字，这个不需要导包\n \n 1 2 3 4 5 用法举例： \nFoldByKey（同一分区同一key与初始值计算一次，n个分区同一key与初始值计算n次） \n \n aggreateByKey 不同分区的最大值相加 \n 为避免reduceByKey内存问题，可用aggregateByKey。 \n To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.\n \n 1 \n combineByKey（可用来求平均值） \n \n \n \n sortByKey \n 思考 ：sortBy是全局排序吗？是。 \nrdd. sortBy( _._2 ,false). foreach(println) //虽然sortBy是全局排序，但由于不止一个分区，foreach输出的时候分区的先后顺序随机，又把全局排序后的数据打乱了。 \n 按照RangePartitioner shuffle会有倾斜的问题 \n \n \n 1-含义： \n \n 2-操作： \n \n \n join \n \n 1-含义： \n \n 2-操作： \n \n \n cogroup \n \n 1-含义： \n \n 2-操作： \n \n \n cartisian \n \n 1-含义： \n \n 2-操作： \n \n \n mapvalue \n \n 1-含义： \n 对Value进行操作 \n 2-操作： \n RDD的Action算子 \n \n \n reduce \n \n \n collect（拉取的分区结果数据很大的情况下，会造成driver端的内存溢出，可以foreach打印出来，或者saveasTextFile保存到硬盘查看） \n \n \n count \n \n \n first \n \n \n take（从第一个分区开始，满足要求就不再遍历获取，collect） \n \n \n \n \n \n takeSample \n \n \n \n \n \n takeorder \n \n \n aggreate \n \n \n fold \n \n \n \n \n \n countByKey \n \n \n \n \n \n foreach \n \n \n rdd.foreach方法在执行的过程中的打印方法是在Executor中执行的，每个Executor在执行完自己的逻辑之后就执行foreach进行打印，因此在本地多线程执行的时候，可能List(3,4)是有可能先执行完成，所以会存在顺序错乱的情况。多线程，多分区才会有这种情况。而value.collect().foreach(println)这种写法的print是在数据采集到Driver之后，在Driver端打印的。所以顺序不会乱 \n \n \n \n 常见的RDD统计操作 \n \n \n \n \n \n RDD的关键算子练习 \n package   cn . itcast . sparkbase . base \n\n import   org . apache . spark . { SparkConf ,  SparkContext ,  TaskContext } \n import   org . junit . Test\n\n /**\n * DESC:\n */ \n class  _02RDDTest  { \n\n   private   val  sc  =   new  SparkContext ( new  SparkConf ( ) . setAppName ( "_02RDDTest" ) . setMaster ( "local[*]" ) ) \n\n   @Test \n   def  test01 ( ) :   Unit   =   { \n    sc . parallelize ( Seq ( 1 ,   2 ,   3 ,   4 ,   5 ) ) \n       . map ( x  =>  x  *   2 ) \n       . foreach ( println ( _ ) ) \n\n    sc . parallelize ( Seq ( 1 ,   2 ,   3 ,   4 ,   5 ) ) \n       . filter ( x  =>  x  >   3 ) \n       . foreach ( println ( _ ) ) \n\n   } \n\n   @Test \n   def  test02 :   Unit   =   { \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       . map ( x  =>   ( x  *   2 ) ) \n       . foreach ( println ( _ ) ) \n     //如果直接使用foreach是无法作为iteratale返回 \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       //f: Iterator[T] => Iterator[U], \n       . mapPartitions ( iter  =>   { \n        iter . foreach ( println ( _ ) ) \n        iter\n       } ) . collect ( ) \n     //执行每个分区的元素乘以2 \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       //f: Iterator[T] => Iterator[U], \n       . mapPartitions ( iter  =>   { \n         val  iterator :  Iterator [ Int ]   =  iter . map ( item  =>  item  *   2 ) \n        iterator\n       } ) . collect ( ) . foreach ( println ( _ ) ) \n\n    println ( "上面的等价写法" ) \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       //f: Iterator[T] => Iterator[U], \n       . mapPartitions ( iter  =>   { \n        iter . map ( item  =>  item  *   2 ) \n       } ) . collect ( ) . foreach ( println ( _ ) ) \n   } \n\n   @Test \n   def  test03 :   Unit   =   { \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       // f: (Int几号分区, Iterator[T]) => Iterator[U] \n       . mapPartitionsWithIndex ( ( index ,  iter )   =>   { \n        println ( "index:"   +  index ) \n        iter . map ( _  *   2 ) \n       } ) . collect ( ) . foreach ( println ( _ ) ) \n    println ( "改进的方法" ) \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       // f: (Int几号分区, Iterator[T]) => Iterator[U] \n       . mapPartitionsWithIndex ( ( index ,  iter )   =>   { \n        iter . map ( x  =>   "index is:"   +  index  +   "\\tvalue is:"   +  x  *   2 ) \n       } ) . collect ( ) . foreach ( println ( _ ) ) \n    println ( "想用mapPartition的方法实现分区有哪些元素" ) \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       . mapPartitions ( iter  =>   { \n        println ( "partitionID:" ,  TaskContext . getPartitionId ( ) ) \n        iter . map ( x  =>  x  *   2 ) \n       } ) . collect ( ) . foreach ( println ( _ ) ) \n\n    println ( "想用mapPartition的方法实现分区有哪些元素,改进方法" ) \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       . mapPartitions ( iter  =>   { \n        iter . map ( x  =>   "partitionID:"   +  TaskContext . getPartitionId ( )   +   "\\tvalue is:"   +  x  *   2 ) \n       } ) . collect ( ) . foreach ( println ( _ ) ) \n     //想用mapPartition的方法实现分区有哪些元素,改进方法 \n     //partitionID:0  value is:2 \n     //partitionID:0  value is:4 \n     //partitionID:0  value is:6 \n     //partitionID:1  value is:8 \n     //partitionID:1  value is:10 \n     //partitionID:1  value is:12 \n   } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 总结 \n \n 使用IDEA完成HDFS文件读取\n \n 将core-site.xml和hdfs-site.xml加入配置 \n \n \n RDD的引入\n \n RDD是弹性分布式数据集 \n RDD特点：不可变可分区可并行计算的集合 \n RDD五种属性 \n \n \n RDD的特性--面试必问[重点]\n \n 1-分区列表 \n 2-作用函数 \n 3-依赖关系 \n 4-key-value类型分区器 \n 5-位置优先性 \n \n \n RDD的创建\n \n 三种 \n 小文件读取 \n \n \n RDD的转换算子\n \n 单value类型\n \n map \n filter \n repartiton \n colasese \n glom \n \n \n 双value类型\n \n 集合交并补 \n distinct \n zip \n \n \n key-value类型\n \n 大家晚上巩固groupByKey，combineBykey \n \n \n \n \n RDD的行动算子\n \n collect \n take \n \n \n RDD的案例实战[重点] \n \n 案例1:groupByKey(要求和需要再sum) \n \n \n \n \n \n    @Test \n   def  test01 ( ) :   Unit   =   { \n     val  value :  RDD [ ( String ,   Int ) ]   =  sc . parallelize ( Seq ( ( "a" ,   1 ) ,   ( "a" ,   1 ) ,   ( "b" ,   1 ) ) ) \n    value\n       . groupByKey ( ) \n       . collect ( ) \n       . foreach ( println ( _ ) ) \n     //(a,CompactBuffer(1, 1)) \n     //(b,CompactBuffer(1)) \n    value\n       . groupByKey ( ) \n       . map ( x  =>   ( x . _1 ,  x . _2 . sum ) ) \n       . collect ( ) \n       . foreach ( println ( _ ) ) \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 案例2:combineBykey(平均值) \n \n \n \n    @Test \n   def  test02 :   Unit   =   { \n     val  rdd :  RDD [ ( String ,   Double ) ]   =  sc . parallelize ( Seq ( \n       ( "zhangsan" ,   99.0 ) , \n       ( "zhangsan" ,   96.0 ) , \n       ( "lisi" ,   97.0 ) , \n       ( "lisi" ,   98.0 ) , \n       ( "zhangsan" ,   97.0 ) ) \n     ) \n     // createCombiner: Value 元素=> C 对元素的操作, \n     // mergeValue: (C, Value) => C,需要将上一个初始值和新的value进行合并 \n     // mergeCombiners: (C, C) => C, \n     val  createCombiner  =   ( curr :   Double )   =>   ( curr ,   1 ) \n     val  mergeValue  =   ( curr :   ( Double ,   Int ) ,  nextValue :   Double )   =>   ( curr . _1  +  nextValue ,  curr . _2  +   1 ) \n     val  mergeCombiners  =   ( curr1 :   ( Double ,   Int ) ,  agg1 :   ( Double ,   Int ) )   =>   ( curr1 . _1  +  agg1 . _1 ,  curr1 . _2  +  agg1 . _2 ) \n     val  valueRDD :  RDD [ ( String ,   ( Double ,   Int ) ) ]   =  rdd . combineByKey ( createCombiner ,  mergeValue ,  mergeCombiners ) \n  \n    valueRDD . foreach ( println ( _ ) ) \n     // x._1      x._2._1 \n     //(zhangsan,(292.0,3)) \n     //(lisi,(195.0,2)) \n    valueRDD . map ( x  =>   ( x . _1 ,  x . _2 . _1  /  x . _2 . _2 ) ) . foreach ( println ( _ ) ) //平均值 \n     //(zhangsan,(292.0,3)) \n     //(zhangsan,97.33333333333333) \n     //(lisi,97.5) \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 案例3:foldByKey \n \n \n \n \n \n    @Test \n   def  test03 :   Unit   =   { \n     // \n    sc . parallelize ( Seq ( ( "a" ,   1 ) ,   ( "a" ,   1 ) ,   ( "b" ,   1 ) ) ) \n       . aggregateByKey ( 0 ) ( _  +  _ ,  _  +  _ ) \n       . collect ( ) \n       . foreach ( println ( _ ) ) \n  \n    sc . parallelize ( Seq ( ( "a" ,   1 ) ,   ( "a" ,   1 ) ,   ( "b" ,   1 ) ) ) \n       . foldByKey ( 0 ) ( _  +  _ ) \n       . collect ( ) \n       . foreach ( println ( _ ) ) \n  \n    sc . parallelize ( Seq ( ( "a" ,   1 ) ,   ( "a" ,   1 ) ,   ( "b" ,   1 ) ) ) \n       . foldByKey ( 0 ) ( ( a ,  b )   =>  a  +  b ) \n       . collect ( ) \n       . foreach ( println ( _ ) ) \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \n collectAsMap \n \n    @Test \n   def  test04 ( ) :   Unit   = { \n     val  rdd  =  sc . parallelize ( List ( ( "a" ,   1 ) ,   ( "a" ,   3 ) ,   ( "b" ,   2 ) ) ) \n    rdd . collectAsMap ( ) . foreach ( println ( _ ) ) \n   } \n \n 1 2 3 4 5 \n randomSplit \n \n    @Test \n   def  test04 ( ) :   Unit   = { \n     val  rdd  =  sc . parallelize ( List ( ( "a" ,   1 ) ,   ( "a" ,   3 ) ,   ( "b" ,   2 ) ) ) \n    rdd . collectAsMap ( ) . foreach ( println ( _ ) ) \n     //Randomly splits this RDD with the provided weights. \n     //weights: Array[Double], \n     //seed: Long = Utils.random.nextLong \n     val  array :  Array [ RDD [ ( String ,   Int ) ] ]   =  rdd . randomSplit ( Array ( 0.6 ,   0.4 ) ,   123L ) \n     val  traingSet :  RDD [ ( String ,   Int ) ]   =  array ( 0 ) \n     val  testSet :  RDD [ ( String ,   Int ) ]   =  array ( 1 ) \n    println ( "=======================" ) \n    traingSet . collect ( ) . foreach ( println ( _ ) ) \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 \n join \n \n    @Test \n   def  testJoin ( ) :   Unit   = { \n     // 模拟数据集 \n     val  empRDD :  RDD [ ( Int ,   String ) ]   =  sc . parallelize ( \n      Seq ( ( 1001 ,   "zhangsan" ) ,   ( 1002 ,   "lisi" ) ,   ( 1003 ,   "wangwu" ) ,   ( 1004 ,   "zhangliu" ) ) \n     ) \n     val  deptRDD :  RDD [ ( Int ,   String ) ]   =  sc . parallelize ( \n      Seq ( ( 1001 ,   "sales" ) ,   ( 1002 ,   "tech" ) ) \n     ) \n    empRDD . join ( deptRDD ) . foreach ( println ( _ ) ) \n    println ( "============" ) \n    empRDD . leftOuterJoin ( deptRDD ) . foreach ( println ( _ ) ) \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 重难点知识 \n \n 案例代码实战1：Spark实战PV-UV-TOPK \n 案例代码实战2：Spark实战区域热点查询 \n 案例代码实战3：Spark实战搜狗分词统计查询 \n RDD依赖关系 \n RDD缓存 \n RDD的DAG \n Spark的Shuffle \n Spark的执行流程分析 \n 回顾总结spark调度 \n 实战案例 \n 实战1：Spark实战PV、UV、访问来源TOPN \n \n 需求：读取日志文件，实现PV，UV，TOPN \n \n   日志格式：\n  127.0.0.1 - - [28/Nov/2019:08:37:25 +0800] "GET / HTTP/1.1" 200 57621 "-" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36"\n  1、pv的全称是page view，译为页面浏览量或点击量，通常是衡量一个网站甚至一条网络新闻的指标。PV反映的是浏览某网站的页面数，所以每刷新一次也算一次。就是说PV与来访者的数量成正比，但PV并不是页面的来访者数量，而是网站被访问的页面数量。\n  \n  2、uv的全称是unique view，译为通过互联网访问、浏览这个网页的自然人，访问网站的一台电脑客户端被视为一个访客，在同一天内相同的客户端只被计算一次。\n \n 1 2 3 4 5 \n \n \n 步骤： \n \n 实现PV-PageView： \n 1-首先读取数据 \n 2-通过map转换函数对于每一行数据都统计为一个PV \n 3-需要通过reduceByKey累加计算(AggerateByKey，FoldByKey) \n 4-实现PV排序操作 \n 5-打印输出或保存起来 \n 实现UV-UserView \n 1-读取数据 \n 2-通过map转换函数将IP地址选择出来 \n 3-使用distinct去重，并实现统计 \n 4-使用reduceByKey实现相同key的value的统计 \n 5-实现UV的排序输出 \n 6-saveAsTextFile \n 实现TopK-排名前几位 \n 1-读取数据 \n 2-通过map选择出用户点击的URL或APP等，X(10) \n 3-进一步实现过滤统计 \n 4-实现TopK输出 \n 5-saveAsTextFile \n \n \n \n 代码 \n \n \n    package   cn . itcast . sparkbase . pro \n  \n   import   org . apache . spark . rdd . RDD\n   import   org . apache . spark . { SparkConf ,  SparkContext } \n  \n   /**\n   * DESC:\n   * 实现PV-PageView：\n   * 1-首先读取数据\n   * 2-通过map转换函数对于每一行数据都统计为一个PV\n   * 3-需要通过reduceByKey累加计算(AggerateByKey，FoldByKey)\n   * 4-实现PV排序操作\n   * 5-打印输出或保存起来\n   * 实现UV-UserView\n   */ \n   object  _01SparkPvUvTopK  { \n     def  main ( args :  Array [ String ] ) :   Unit   =   { \n       // 实现PV-PageView： \n       val  sc :  SparkContext  =   { \n         val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n         val  sc  =   new  SparkContext ( conf ) \n        sc\n       } \n       // 1-首先读取数据 \n       val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/access.log" ) \n       //fileRDD.take(5).foreach(println(_)) \n       //194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] "GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1" 304 0 "-" "Mozilla/4.0 (compatible;)" \n       // 2-通过map转换函数对于每一行数据都统计为一个PV \n       val  mapRDD :  RDD [ ( String ,   Int ) ]   =  fileRDD . map ( x  =>   ( "PV" ,   1 ) ) \n       //mapRDD.take(3).foreach(println(_)) \n       // 3-需要通过reduceByKey累加计算(AggerateByKey，FoldByKey) \n       val  resuleRDD :  RDD [ ( String ,   Int ) ]   =  mapRDD . reduceByKey ( _  +  _ ) \n       // 5-打印输出或保存起来 \n      resuleRDD . foreach ( println ( _ ) )   //(PV,14619) \n       // 实现UV-UserView \n       // 1-读取数据 -已经读取 \n       // 2-通过map转换函数将IP地址选择出来 \n       val  ipValue :  RDD [ String ]   =  fileRDD . map ( x  =>  x . split ( "\\\\s+" ) ) . map ( x  =>  x ( 0 ) ) \n       // 3-使用distinct去重，并实现统计 \n       val  uvValue :  RDD [ ( String ,   Int ) ]   =  ipValue . distinct ( ) . map ( x  =>   ( "UV" ,   1 ) ) \n       // 4-使用reduceByKey实现相同key的value的统计 \n       val  uvResult :  RDD [ ( String ,   Int ) ]   =  uvValue . reduceByKey ( _  +  _ ) \n      uvResult . foreach ( println ( _ ) )   //(UV,1050) \n       // 5-实现UV的排序输出 \n       //实现TopK-排名前几位 \n       //1-读取数据--已经读取 \n       //2-通过map选择出用户点击的URL或APP等，X(10) \n       val  value1RDD :  RDD [ ( String ,   Int ) ]   =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . filter ( x  =>  x . length  >   10 ) . map ( x  =>   ( x ( 10 ) ,   1 ) ) \n       //value1RDD.take(20).foreach(println(_)) \n       //3-进一步实现过滤统计 \n       val  result1RDD :  RDD [ ( String ,   Int ) ]   =  value1RDD . reduceByKey ( _  +  _ ) . sortBy ( x  =>  x . _2 ,   false ) . filter ( x  =>  x . _1  !=   "\\"-\\"" ) \n       //如果直接执行sortBy或sortBykey可能得到结果没有办法全局排序的，这时候可以增加collect收集到driver端 \n      result1RDD . take ( 5 ) . foreach ( println ( _ ) ) \n       //("http://blog.fens.me/category/hadoop-action/",547) \n       //("http://blog.fens.me/",377) \n       //("http://blog.fens.me/wp-admin/post.php?post=2445&action=edit&message=10",360) \n       //("http://blog.fens.me/r-json-rjson/",274) \n       //("http://blog.fens.me/angularjs-webstorm-ide/",271) \n       //4-实现TopK输出 \n      sc . stop ( ) \n     } \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 \n \n 总结 \n \n 学会使用对应RDD实战 \n 实战2：Spark实战区域热点查询 \n \n \n 需求： \n \n 需要通过日志信息（运行商或者网站自己生成）和城市ip段信息来判断用户的ip段，统计热点经纬 \n \n \n \n 数据集： \n \n \n \n \n \n 用户IP日志信息：记录的是用户IP \n \n \n \n \n     20090121000132095572000|125.213.100.12|show.51.com|/\n    关注的就是第二个字段\n \n 1 2 \n 城市IP端信息：需要对某一个区域根据IP划分 \n \n     1.0.1.0|1.0.3.255|16777472|16778239|亚洲|中国|福建|福州||电信|350100|China|CN|119.306239|26.075302\n \n 1 \n \n 125.213.100.12 对比 1.0.1.0|1.0.3.255  无法比较大小 \n \n \n 方法：1.0.1.0|1.0.3.255部分已经转化为Long类型数据16777472|16778239 \n \n 如果125.213.100.12 转化为Long类型数据，可以拿到该数据和IP.txt数据进行对比 \n 如何对比？一个数字在一个连续数字区域中？这里选择的是 二分查找法 \n \n \n \n \n 统计：如果一个用户IP位于城市IP端之内，可以实现统计分析 \n \n \n \n \n \n \n \n \n 步骤分析： \n \n 1-创建SparkContext环境 \n 2-读取两个文件，ip和用户地址 \n 3-将用户IP对比IP信息段\n \n 3-1首先将用户ip转化为long类型 \n 3-2将long类型ip放在ip地址信息段中进行查询（二分查找法） \n 3-3根据查找的索引下标，得到经度和维度 \n \n \n 4-根据经度和维度统计区域热度 \n \n \n \n 代码： \n \n \n    package   cn . itcast . sparkbase . pro \n  \n   import   org . apache . spark . broadcast . Broadcast\n   import   org . apache . spark . rdd . RDD\n   import   org . apache . spark . { SparkConf ,  SparkContext } \n  \n   /**\n   * DESC:\n   * 1-创建SparkContext环境\n   * 2-读取两个文件，ip和用户地址\n   * 3-将用户IP对比IP信息段\n   * 3-1首先将用户ip转化为long类型\n   * 3-2将long类型ip放在ip地址信息段中进行查询（二分查找法）\n   * 3-3根据查找的索引下标，得到经度和维度\n   * 4-根据经度和维度统计区域热度\n   */ \n   object  _05IPCheck  { \n   / \n     def  binarySearch ( ipLong :   Long ,  boradcastValue :  Array [ ( String ,   String ,   String ,   String ) ] ) :   Int   =   { \n       var  start  =   0 \n       var  end  =  boradcastValue . length  -   1 \n  \n       while   ( start  <=  end )   { \n         var  middle  =   ( start  +  end )   /   2 \n         //查找出结果--中间位置的startip \n         if   ( ipLong  >=  boradcastValue ( middle ) . _1 . toLong  &&  ipLong  <=  boradcastValue ( middle ) . _2 . toLong )   { \n           return  middle\n         } \n         //在右边查找 \n         if   ( ipLong  >  boradcastValue ( middle ) . _2 . toLong )   { \n          start  =  middle  +   1 \n         } \n         //在左边查找 \n         if   ( ipLong  <  boradcastValue ( middle ) . _1 . toLong )   { \n          end  =  middle  -   1 \n         } \n       } \n       - 1 \n     } \n  \n     def  ipToLong ( ip :   String ) :   Long   =   { \n       //注意:IP个原始面貌: \n       //10111111.10111010.11110000.11110000 \n       val  ipArr :  Array [ Int ]   =  ip . split ( "[.]" ) . map ( s  =>  Integer . parseInt ( s ) ) \n       var  ipnum  =   0L \n       for   ( i  <-  ipArr )   { \n         //<<表示位运算左移 ,0L左移之后还是0L,二进制形式:00000000.00000000.00000000.00000000 \n         //其他数,左移之后,后面补0 \n         //|表示位运算或,或的特点是,与0进行或,返回本身 \n         //第一次: \n         //00000000.00000000.00000000.10111111 \n         //00000000.00000000.00000000.00000000 \n         //00000000.00000000.00000000.10111111 \n         //第二次: \n         //00000000.00000000.00000000.10111010 \n         //00000000.00000000.10111111.00000000 \n         //00000000.00000000.10111111.10111010 \n         //第三次: \n         //00000000.00000000.00000000.11110000 \n         //00000000.10111111.10111010.00000000 \n         //00000000.10111111.10111010.11110000 \n         //第四次: \n         //0000 \n           0.00000000 .00000000 .11110000 \n         //10111111.10111010.11110000.00000000 \n         //10111111.10111010.11110000.11110000 \n        ipnum  =  i  |   ( ipnum  <<   8 ) \n       } \n      ipnum\n     } \n  \n     def  main ( args :  Array [ String ] ) :   Unit   =   { \n       //1-创建SparkContext环境 \n       val  sc :  SparkContext  =   { \n         val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n         val  sc  =   new  SparkContext ( conf ) \n        sc\n       } \n       //2-读取两个文件，ip和用户地址 \n       val  userIPRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/ip/20190121000132.394251.http.format" ) \n       val  userSplitRDD :  RDD [ String ]   =  userIPRDD . map ( _ . split ( "\\\\|" ) ) . map ( x  =>  x ( 1 ) ) \n       //userSplitRDD.take(5).foreach(println(_))//125.213.100.123 \n  \n       val  ipRangeRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/ip/ip.txt" ) \n       //223.247.0.0|223.247.7.255|3757506560|3757508607|亚洲|中国|安徽|池州||电信|341700|China|CN|117.489157|30.656037 \n       //开始ip的起始long类型地址，结束ip的long类型地址，经度，维度 \n       val  ipRDD :  RDD [ ( String ,   String ,   String ,   String ) ]   =  ipRangeRDD . map ( _ . split ( "\\\\|" ) ) . map ( x  =>   ( x ( 2 ) ,  x ( 3 ) ,  x ( x . length  -   2 ) ,  x ( x . length  -   1 ) ) ) \n       //ipRDD.take(5).foreach(println(_))//(16777472,16778239,119.306239,26.075302) \n       val  broadcastIpValue :  Broadcast [ Array [ ( String ,   String ,   String ,   String ) ] ]   =  sc . broadcast ( ipRDD . collect ( ) ) \n       //broadcastIpValue.value//直接引用他的值 \n       //3-将用户IP对比IP信息段 \n       val  resultRDD :  RDD [ ( ( String ,   String ) ,   Int ) ]   =  userSplitRDD . mapPartitions ( iter  =>   { \n         val  boradcastValue :  Array [ ( String ,   String ,   String ,   String ) ]   =  broadcastIpValue . value\n        iter . map ( ip  =>   { \n           //* 3-1首先将用户ip转化为long类型 \n           val  ipLong :   Long   =  ipToLong ( ip ) \n           //* 3-2将long类型ip放在ip地址信息段中进行查询（二分查找法） \n           val  index :   Int   =  binarySearch ( ipLong ,  boradcastValue ) \n           //* 3-3根据查找的索引下标，得到经度和维度 \n           ( ( boradcastValue ( index ) . _3 ,  boradcastValue ( index ) . _4 ) ,   1 ) \n         } ) \n       } )   //end mapParttion \n       //4-根据经度和维度统计区域热度 \n      resultRDD . reduceByKey ( _  +  _ ) . sortBy ( _ . _2 ,   false ) . take ( 5 ) . foreach ( println ( _ ) ) \n       //((108.948024,34.263161),1824) \n       //((116.405285,39.904989),1535) \n       //((106.504962,29.533155),400) \n       //((114.502461,38.045474),383) \n       //((106.57434,29.60658),177) \n      sc . stop ( ) \n     } \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 \n \n 总结： \n \n 直接利用ip转化为long类型的额工具类 \n 注意使用二分查找方法 \n 需要使用广播变量 \n 实战3：Spark实战搜狗分词统计查询 \n \n \n 需求：搜狗词库数据集，首先对搜狗的收集的词条需要分词操作，进行对应的统计 \n \n \n http://download.labs.sogou.com/dl/sogoulabdown/SogouQ/SogouQ.reduced.zip \n \n \n \n \n \n 分词： 我是黑马程序员的一匹黑马  我/是/黑马/程序员/一匹/黑马  TFIDF Word2Vec \n \n \n 引入HanLP的依赖 \n \n \n \n \n      < dependency > \n         < groupId > com.hankcs </ groupId > \n         < artifactId > hanlp </ artifactId > \n         < version > portable-1.7.7 </ version > \n     </ dependency > \n \n 1 2 3 4 5 \n \n 读取数据 \n \n \n 搜索关键词统计 \n \n \n 用户搜索点击统计 \n \n \n 搜索时间段统计 \n \n \n \n \n \n 步骤 \n \n 读取数据：sc.textFile,数据处理，这里可以使用map或mappartition，map(x=>x.split("\\\\s+") \n 搜索关键词统计:首先拿到查询词，对查询词进行分词，根据分词进行统计 \n 用户搜索点击统计：需要拿到用户ID和用户查询词，然后根据(用户ID，用户查询词)进行统计 \n 搜索时间段统计：获取用户的搜索的时间段，根据时间段统计排序 \n \n \n \n 代码 \n \n \n 1-分词代码 \n \n \n package   cn . itcast . sparkbase . pro \n\n import   java . util \n\n import   com . hankcs . hanlp . HanLP\n import   com . hankcs . hanlp . seg . common . Term\n import   com . hankcs . hanlp . tokenizer . StandardTokenizer\n\n /**\n * DESC:\n */ \n import   scala . collection . JavaConverters . _\n\n object  _02HanLp  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n\n     val  terms :  util . List [ Term ]   =  HanLP . segment ( "我是黑马程序员的一匹黑马" ) \n    println ( terms )   //[我/r, 是/v, 黑马/n, 程序员/n, 的/uj, 一/m, 匹/q, 黑马/n] \n    println ( terms . asScala . map ( _ . word . trim ) )   //ArrayBuffer(我, 是, 黑马, 程序员, 的, 一, 匹, 黑马) \n\n     val  terms1 :  util . List [ Term ]   =  StandardTokenizer . segment ( "我是黑马程序员的一匹黑马" ) \n    println ( terms1 . asScala . map ( _ . word . trim ) ) \n\n     //00:00:00 2982199073774412 [360安全卫士]  8 3  download.it.com.cn/softweb/software/firewall/antivirus/20067/17938.html \n     val  arr :  Array [ String ]   =   """00:00:00 2982199073774412 [360安全卫士]  8 3  download.it.com.cn/softweb/software/firewall/antivirus/20067/17938.html""" . split ( "\\\\s+" ) \n    println ( arr ( 2 ) . replaceAll ( "\\\\[|\\\\]" ,   "" ) ) //360安全卫士 \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \n 搜索关键词统计 \n \n package   cn . itcast . sparkbase . pro \n\n import   java . util \n\n import   com . hankcs . hanlp . HanLP\n import   com . hankcs . hanlp . seg . common . Term\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . { SparkConf ,  SparkContext } \n\n import   scala . collection . JavaConverters . _\n import   scala . collection . mutable \n\n /**\n * DESC:\n * 首先完成搜狗词库的数据读取\n * 1-创建SparkContext\n * 2-读取数据\n */ \n object  _04SougouCount  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-创建SparkContext \n     val  sc :  SparkContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  sc  =   new  SparkContext ( conf ) \n      sc\n     } \n     //2-读取数据 \n     val  sougouRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sougu/SogouQ.reduced" ) \n    println ( s "sougouRDD count value is: ${ sougouRDD . count ( ) } " )   //1724264 \n     //3-引入样例类，可以基于样例类更好解析各个搜狗的字段 \n     val  recordRDD :  RDD [ SogouRecord ]   =  sougouRDD\n       //这里的过滤需要处理，需要对有缺失字段的需要使用length判断长度 \n       . filter ( line  =>  line  !=   null   &&  line . trim . split ( "\\\\s+" ) . length  ==   6 ) \n       . mapPartitions ( iter  =>   { \n        iter . map ( record  =>   { \n           val  arr :  Array [ String ]   =  record . split ( "\\\\s+" ) \n          SogouRecord ( arr ( 0 ) ,  arr ( 1 ) ,  arr ( 2 ) . replaceAll ( "\\\\[|\\\\]" ,   "" ) ,  arr ( 3 ) . toInt ,  arr ( 4 ) . toInt ,  arr ( 5 ) ) \n         } ) \n       } ) \n       //打印信息 \n    recordRDD . take ( 5 ) . foreach ( println ( _ ) ) \n     //SogouRecord(00:00:00,2982199073774412,360安全卫士,8,3,download.it.com.cn/softweb/software/firewall/antivirus/20067/17938.html) \n     //SogouRecord(00:00:00,07594220010824798,哄抢救灾物资,1,1,news.21cn.com/social/daqian/2008/05/29/4777194_1.shtml) \n     //3-搜索关键词统计 \n     val  valueRDD :  RDD [ String ]   =  recordRDD . mapPartitions ( iter  =>   { \n      iter . flatMap ( record  =>   { \n         val  terms :  util . List [ Term ]   =  HanLP . segment ( record . queryWords ) \n        terms . asScala . map ( _ . word . trim ) \n       } ) \n     } )   //end  mappartition \n     //valueRDD.take(5).foreach(println(_)) \n     val  keyWordCount :  RDD [ ( String ,   Int ) ]   =  valueRDD . map ( x  =>   ( x ,   1 ) ) . reduceByKey ( _  +  _ ) . sortBy ( _ . _2 ,   false ) \n    println ( "========搜索关键词统计============" ) \n    keyWordCount . take ( 5 ) . foreach ( println ( _ ) ) \n     //(+,193939) \n     //(的,93246) \n     //(.,90985) \n     //(地震,88451) \n     //(救灾,69662) \n    \n\n  sc . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 #  RDD依赖关系 \n 案例 \n 为什么有依赖 \n \n \n 原因有哪些？ \n \n 1-依赖关系可以进行RDD的 容错 ，某一个RDD出错，可以顺着依赖链重建RDD \n 2-RDD的依赖可以构建RDD的血缘关系，也是方便容错 \n 3-依赖关系是为了RDD实现并行计算的,只有shuffle依赖无法实现并行计算 \n \n 相邻两个RDD的关系称之为依赖关系，多个连续的RDD的依赖关系，称之为血缘 \n \n \n 依赖关系是Spark借助于DAG有向无环图实现的RDD的容错方法，基于依赖关系构建RDD或Spark的血缘关系 \n 依赖关系分为几类？ \n \n \n \n \n \n \n \n \n \n 窄依赖： \n \n 一个子RDD依赖于一个父RDD，就是窄依赖(错误) \n 一个父RDD对应一个子RDD，就是窄依赖 \n \n \n \n 宽依赖： \n \n 一个子RDD依赖于多个父RDD，就是宽依赖 (错误) \n 一个父RDD对应多个子RDD，就是宽依赖 \n \n \n \n shuffle \n \n 发生宽依赖，重新分桶，一定会伴随shuffle，但是发生shuffle不一定是宽依赖，比如缩减到一个分区。所有父RDD都只有这个子RDD，窄依赖的父RDD是干爹之一就会shuffle，否则不会。 \n \n \n \n 总结： \n \n 区分宽依赖还是窄依赖，看父RDD会不会被子RDD分享，被分享就是宽依赖，否则就是窄依赖。 \n 区分是否发生shuffle,父RDD的数据会不会被重新分桶 (分到一个或多个桶,可能shuffle前后分区数一样.) \n RDD缓存 \n \n 什么是缓存\n \n Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或者缓存数据集。 \n \n \n 为什么需要缓存？ \n \n 对于某些比较 昂贵的算子 可以将计算结果缓存起来，重复利用 加快计算速度 \n 如果将数据缓存起来可以进行 容错 ，因为缓存可以将数据保存在内存或磁盘中 \n \n \n 缓存有几种分类？ \n \n 两种：Cache和Persist \n 缓存的级别有哪些？\n \n \n \n 1-内存 \n 2-磁盘 \n 3-堆外内存(off_heap，不受限于jvm管理) \n 4-序列化---网络传输 \n 5-副本---容错 \n \n \n \n \n 缓存如何选择？\n \n 尽量选择内存，如果内存放不下可以尝试序列化，除非算子昂贵可以放在磁盘，如果容错恢复增加副本机制 \n \n \n \n 缓存怎么使用？\n \n cache \n persist \n unpersist \n 经过缓存的数据明显加快计算速度，一般用于昂贵算子，如shuffle算子的缓存 \n \n \n \n 缓存有什么问题？\n \n 缓存的数据可能存在丢失的情况，考虑使用非易失介质如HDFS分布式文件系统 \n 引出checkpoint检查点机制 \n RDD的CheckPoint \n \n 什么是checkpoint机制？\n \n checkpoint是Spark的重要 容错 机制 \n 因为cache或persist会存在丢失缓存数据的情况，所以提出检查点机制，将数据放在HDFS中，之后就可以从checkpoint所保存的HDFS路径中读取 \n \n \n 检查点机制的基本原理？\n \n Checkpoint会 斩断依赖链 ,因为Checkpoint会把结果保存在HDFS这类存储中,更加的 安全可靠,一般不需要回溯依赖链 ,而cache和persist不会斩断依赖链. \n \n \n 怎么使用checkpoint检查点机制？\n \n sc.setCheckpontDir(“hdfs:///”)设置将RDD的依赖关系保存在HDFS的那个路径中 \n rdd1.checkpoint() 将rdd保存在上述设置的HDFS中 \n \n \n checkpoint和缓存的区别和联系？ \n \n \n \n 实验： \n \n \n \n \n 如果删除checkpoint的一个hdfs文件，会报错，文件不存在 \n \n Spark如何实现容错机制的？思考\n \n 结果： \n 1-如果Spark中将数据缓存在内存中，也就是cache和persist，首先从内存中寻找这部分数据 \n 2-如果内存没有数据的话，再从checkpoint所保存的HDFS中寻找 \n 3-如果上述都没有做，直接利用RDD的血缘关系实现容错，也就是重复从rdd1计算得到rdd2在得到rdd3 \n 分析： \n \n 源码 \n \n \n \n \n \n 完成 \n RDD的DAG \n \n \n \n \n \n \n \n \n 为什么需要有DAG？ \n \n Spark作为第三代计算引擎，通过有向无环图构建Spark的任务。 \n \n \n \n 什么是DAG？ \n \n 有向无环图，Spark或Impala或Flink都是基于DAG构建任务 \n 通过4040端口查看DAG有向无环图 \n \n \n \n DAG 如何划分Stage ？ \n \n 划分Stages的依据是发生宽依赖 ，从当前job的最后一个算子往前推，遇到宽依赖，那么当前在这个批次中的所有算子操作都划分成一个stage \n \n \n \n DAG划分Stage优势是什么？ \n \n 同一个Stage内可以并行计算，这样能够加快计算速度 \n \n \n \n Spark调度流程分析 \n spark的调度分为资源申请和任务调度。 \n 如下图 1~10为资源申请，12~19为任务调度。 \n 12其实在driver启动后初始化就创建了DAGSchedluer和TaskScheduler，TaskScheduler通过后台进程去向ResourceManager申请资源。 \n \n \n \n 粗粒度资源申请(Spark） \n 在 Application执行之前，将所有的资源申请完毕，当资源申请成功后，才会进行任务的调度，当所有的task执行完成后，才会释放这部分资源。 \n 优点：在Application执行之前，所有的资源都申请完毕，每一个task运行时直接使用资源就可以了，不需要task运行时在执行前自己去申请资源，task启动就快了，task执行快了，stage执行就快了，job就快了，application执行就快了。 \n 缺点：直到最后一个task执行完成才会释放资源，集群的资源无法充分利用。当数据倾斜时更严重。 \n 细粒度资源申请（MapReduce） \n Application执行之前不需要先去申请资源，而是直接执行，让job中的每一个task在执行前自己去申请资源，task执行完成就释放资源。 \n 优点：集群的资源可以充分利用。 \n 缺点：task自己去申请资源，task启动变慢，Application的运行就相应的变慢了。 \n 共享变量 \n 广播变量 \n \n \n \n \n \n \n \n \n 关键点： \n \n \n 1-广播变量，是在driver端定义的，executor端拥有副本，在executor端是不能改变广播变量的值 \n \n \n 2-广播变量获取的时候是从BlockManager中获取数据，如果本地没有从Driver端获取变量副本 \n \n \n 3-如何使用：sc.broadcast(map.collect) \n \n \n 案例： \n \n \n package   cn . itcast . sparkbase . base \n\n import   org . apache . spark . broadcast . Broadcast\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . { SparkConf ,  SparkContext } \n\n /**\n * DESC:\n */ \n object  _04broadcast  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n\n     //申请资源 \n     val  sc : SparkContext = { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  sc  =   new  SparkContext ( conf ) \n      sc\n     } \n     //创建RDD \n     val  kvFruit :  RDD [ ( Int ,   String ) ]   =  sc . parallelize ( List ( ( 1 , "apple" ) , ( 2 , "orange" ) , ( 3 , "banana" ) , ( 4 , "grape" ) ) ) \n     val  fruitMap :  collection . Map [ Int ,   String ]   = kvFruit . collectAsMap\n     //fruitMap.foreach(println(_)) \n     //需求：根据水果的编号查找水果的名称 \n     val  fruitsIds :  RDD [ Int ]   =  sc . parallelize ( Array ( 2 ,   4 ,   1 ,   3 ) ) \n    fruitsIds . map ( x => fruitMap ( x ) ) . collect ( ) . foreach ( println ( _ ) ) \n     //改进：如果水果很多，那么每个水果都需要拉取fruitMap变量进行对比得到水果名称 \n     val  broadMap :  Broadcast [ collection . Map [ Int ,   String ] ]   =  sc . broadcast ( fruitMap ) \n    fruitsIds . map ( x => broadMap . value ( x ) ) . collect ( ) . foreach ( println ( _ ) ) \n     //orange \n     //grape \n     //apple \n     //banana \n    sc . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #  累加器 \n \n 共享变量-累加器： \n \n \n Accumulator只提供了累加的功能，只能累加，不能减少。 \n \n \n 累加器只能在Driver端构建，并只能从Driver端读取结果，在Task端只能进行累加。 \n \n \n scala的累加 \n rdd的累加问题 \n 累加器 \n \n package   cn . itcast . sparkbase . base \n\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . util . LongAccumulator\n import   org . apache . spark . { Accumulator ,  SparkConf ,  SparkContext } \n\n /**\n * DESC:\n */ \n object  _05accumulate  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //申请资源 \n     val  sc :  SparkContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  sc  =   new  SparkContext ( conf ) \n      sc\n     } \n     //scala的加法 \n     var  counter1  =   0 \n     val  seq  =  Seq ( 1 ,   2 ,   3 ) \n    seq . map ( x  =>  counter1  +=  x ) \n    println ( "counter result is:"   +  counter1 ) \n     //rdd的加法-0--为什么会出现现象？因为变量在driver端定义，将数据发送到executor执行累加，但是执行完累加后结果并没返回driver \n     var  counter2  =   0 \n     val  rdd1 :  RDD [ Int ]   =  sc . parallelize ( seq ) \n    rdd1 . foreach ( x  =>  counter2  +=  x ) \n    println ( counter2 ) \n     //提出了在driver端和executor端共享当前变量 \n     //累加器也是在action操作的时候触发 \n     val  acc :  Accumulator [ Int ]   =  sc . accumulator ( 0 ) \n    rdd1 . foreach ( x => acc += x ) \n    println ( acc ) \n     //sc.accumulator该方法2.0已经弃用，可以直接使用sc.longAccumulator \n     val  acc_count :  LongAccumulator  =  sc . longAccumulator ( "acc_count" ) \n    rdd1 . foreach ( x => acc_count . add ( x ) ) \n    println ( acc_count ) //LongAccumulator(id: 51, name: Some(acc_count), value: 6) \n    println ( acc_count . value ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 累加器重复累加问题 \n Spark中的一系列transform操作都会构造成一长串的任务链，此时就需要通过一个action操作来触发（lazy的特性），accumulator也是如此。 \n 因此在一个action操作之后，调用value方法查看，是没有任何变化 \n 第一次action操作之后，调用value方法查看，变成了5 \n 第二次action操作之后，调用value方法查看，变成了10 \n 原因就在于第二次action操作的时候，又执行了一次累加器的操作，同个累加器，在原有的基础上又加了5，从而变成了10 \n 解决方案 \n通过上述的现象描述，我们可以很快知道解决的方法：只进行一次action操作。基于此，我们只要避免重复计算（网上很多说法是说斩断依赖链，cache不会斩断依赖链，容错或者重复调用会依次判断内存、checkpoint里面有没有，没有才会根据依赖链重复计算），即使用cache、persist。这样操作之后，那么后续的累加器操作就不会重复累加。 \n 共享变量案例 \n \n \n 需求：包括非单词组合，统计数据词频时过滤非单词的符合并且统计总的格式。 \n \n \n 分析：首先过滤单词，词频统计、 \n \n \n 步骤： \n \n 1-读取数据 \n 2-使用广播变量，定义的一组非单词组合的list或map广播到executor执行 \n 2-过滤出来，使用累加器统计非单词的组合 \n 3-使用wordcout统计单词综合 \n \n \n \n 代码 \n \n \n package   cn . itcast . sparkbase . base \n\n import   org . apache . spark . broadcast . Broadcast\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . util . LongAccumulator\n import   org . apache . spark . { SparkConf ,  SparkContext } \n\n /**\n * DESC:\n * * 1-读取数据\n * * 2-使用广播变量，定义的一组非单词组合的list或map广播到executor执行\n * * 2-过滤出来，使用累加器统计非单词的组合\n * * 3-使用wordcout统计单词综合\n */ \n object  _06acc_broadcast  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //申请资源 \n     val  sc :  SparkContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  sc  =   new  SparkContext ( conf ) \n      sc\n     } \n     // 1-读取数据 \n     //定义广播变量，因为每个单词都需要查看是否是费单词序列 \n     val  list :  List [ String ]   =  List ( "," ,   "." ,   "!" ,   "#" ,   "$" ,   "%" ) \n     val  broadcastList :  Broadcast [ List [ String ] ]   =  sc . broadcast ( list ) \n     //定义累加器 \n     val  acc_count :  LongAccumulator  =  sc . longAccumulator ( "acc_count" ) \n\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/words1.txt" ) \n     val  wordscount :  RDD [ String ]   =  fileRDD\n       . filter ( line  =>  line  !=   null   &&  line . trim . length  >   0 ) \n       . flatMap ( line  =>  line . split ( "\\\\s+" ) ) \n       . filter ( word  =>   { \n         val  listValue :  List [ String ]   =  broadcastList . value\n         val  isFlag :   Boolean   =  listValue . contains ( word ) \n         if   ( isFlag )   { \n          acc_count . add ( 1L ) \n         } \n         ! isFlag\n       } ) \n     // 2-使用广播变量，定义的一组非单词组合的list或map广播到executor执行 \n     // 2-过滤出来，使用累加器统计非单词的组合 \n     // 3-使用wordcout统计单词综合 \n    println ( "wordcount的结果" ) \n    wordscount . map ( ( _ ,   1 ) ) . reduceByKey ( _  +  _ ) . collect ( ) . foreach ( println ( _ ) ) \n    println ( "非单词的组合：" ,  acc_count . value ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \n \n \n \n \n 通过睡眠Thread.sleep睡眠查看WebUi \n Kyro序列化 \n \n \n \n \n \n \n \n \n 什么kyro序列化？ \n \n \n \n \n \n \n \n \n 如何使用kryo序列化？ \n \n \n \n \n \n kryo序列化是java序列化的10x，kryo序列化并不是支持所有的类型，对于一部分类型需要注册，其他基础类直接设置 \n \n \n \n \n \n 配置说明： \n \n \n   1.   spark.serializer：序列化时用的类，需要申明为org.apache.spark.serializer.KryoSerializer。这个设置不仅控制各个worker节点之间的混洗数据序列化格式，同时还控制RDD存到磁盘上的序列化格式及广播变量的序列化格式。 \n   2.   spark.kryoserializer.buffer：每个Executor中的每个core对应着一个序列化buffer。如果你的对象很大，可能需要增大该配置项。其值不能超过spark.kryoserializer.buffer.max \n   3.   spark.kryoserializer.buffer.max：允许使用序列化buffer的最大值 \n   4.   spark.kryo.classesToRegister：向Kryo注册自定义的的类型，类名间用逗号分隔 \n   5.   spark.kryo.referenceTracking：跟踪对同一个对象的引用情况，这对发现有循环引用或同一对象有多个副本的情况是很有用的。设置为false可以提高性能 \n   6.   spark.kryo.registrationRequired：是否需要在Kryo登记注册？如果为true，则序列化一个未注册的类时会抛出异常 \n   7.   spark.kryo.registrator：为Kryo设置这个类去注册你自定义的类。最后，如果你不注册需要序列化的自定义类型，Kryo也能工作，不过每一个对象实例的序列化结果都会包含一份完整的类名，这有点浪费空间 \n   8.   spark.kryo.unsafe：如果想更加提升性能，可以使用Kryo unsafe方式 \n \n 1 2 3 4 5 6 7 8 kryo的使用 \n    //刚开始的时候一直序列化不成功，应该是找不到类，java+scala混编，把scala设置成src了，去掉pom的这个配置就可以了。 \n   //第一种方式 \n   //实现一个KryoRegistrator注册类，在该类里面对自定义的序列化类进行注册，然后在conf里面配置该类 \n  public  class  MyKryoRegistrator implements KryoRegistrator  { \n       @Override \n      public void registerClasses (  Kryo kryo )   { \n          kryo . register ( User . class ) ; \n          kryo . register ( User [ ] . class ) ; \n          kryo . register ( scala . collection . convert . Wrappers$ . class ) ; \n       } \n   } \n   @Data \n  public  class  User implements Serializable  { \n          int id ; \n           String  name ; \n           String  city ; \n          List < String >  hobby ; \n   } \n  \n   //  在conf配置如下 \n   val  conf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName ) \n        conf\n         . set ( "spark.serializer" ,   "org.apache.spark.serializer.KryoSerializer" ) \n         . set ( "spark.kryo.registrationRequired" ,   "true" ) \n         . set ( "spark.kryo.registrator" , classOf [ MyKryoRegistrator ] . getName ) \n      \n  \n       val  sc  =   new  SparkContext ( conf ) \n  \n   //第二种方式 \n   conf\n         . set ( "spark.serializer" ,   "org.apache.spark.serializer.KryoSerializer" ) \n         . registerKryoClasses ( Array ( classOf [ User ] ) ) \n  \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 声明序列化为kryo \n \n \n \n \n 扩展2：groupBy什么时候是窄依赖或宽依赖？ \n 内存模型 \n \n \n 问题1： \n \n Spark和Flink不一样，Flink的内存对用户无法操作，Spark的内存用户可以配置 \n SparkCore的内存模型 \n \n 问题2： \n Spark的shuffle \n \n \n Spark不同版本的shuffle \n \n 1.2之前HashShuffleManager \n 1.2之后SortShuffleManager \n \n \n \n Shuffle阶段 \n \n shuffle write：mapper阶段，上一个stage得到最后的结果写出 \nshuffle read ：reduce阶段，下一个stage拉取上一个stage进行合并 \n \n \n \n HashShuffleManager \n \n 未经优化的hashShuffleManager \n \n \n \n 优化的hashShuffleManager\n \n shuffleFileGroup \n \n \n \n \n \n \n \n \n \n \n \n \n SortShuffleManager \n \n \n \n \n \n bypassMerge机制 \n \n shuffle write task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时(****默认为200****) \n 不能是聚合类的算子，比如reduceByKey，本质是map side combine是否为true \n \n \n \n \n 普通机制 \n \n \n \n \n \n （1）该模式下，数据会先写入一个内存数据结构中(默认5M)，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存;如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。 \n （2）   接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。 \n 注意： \n shuffle中的定时器：定时器会检查内存数据结构的大小，如果内存数据结构空间不够，那么会申请额外的内存，申请的大小满足如下公式： \n applyMemory=nowMenory*2-oldMemory \n 申请的内存=当前的数据内存情况*2-上一次的内嵌情况 \n 意思就是说内存数据结构的大小的动态变化，如果存储的数据超出内存数据结构的大小，将申请内存数据结构存储的数据*2-内存数据结构的设定值的内存大小空间。申请到了，内存数据结构的大小变大，内存不够，申请不到，则发生溢写。 \n 由于Spark是一个内存计算框架，没有办法严格控制Executor内存，只能采用监控方式监控内存，内存初始值为5M，当超出的时候，如5.02M，监控内存数据的对象会去申请5.02*2-5=5.04M内存，如果申请到了就不需要溢写，否则会发生溢写。 \n 区别： \n （a）Spark内存数据初始值为5M，他可以申请扩大，而MR固定的Buffer为100M \n （b）溢写磁盘文件还带有索引文件，索引文件是对磁盘文件的描述，还记录每个分区的起始位置 start offset 和终止位置 end offset 。 \n （3）排序 \n 在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。 \n （4）溢写 \n 排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过 Java 的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。 \n （5）merge \n 一个task 将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并成1个磁盘文件，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为Reduce端的stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。 \n SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。 \n \n \n \n \n \n \n bypass和普通机制的区别 \n 第一，磁盘写机制不同; \n 第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。 \n 两个最终都是会产生2M(map task number)个磁盘小文件。 \n **总结：**SortShuffle也分为普通机制和bypass机制，普通机制在内存数据结构(默认为5M)完成排序，而当shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。或者算子不是聚合类的shuffle算子(比如reduceByKey)的时候会触发SortShuffle的bypass机制，SortShuffle的bypass机制不会进行排序，极大的提高了其性能。 \n \n \n 页面监控： \n \n \n \n \n \n 补充：Shuffle类算子 \n \n \n 下面列出Shuffle类不同分类算子\n去重\n def  distinct ( ) \n def  distinct ( numPartitions :   Int ) \n聚合\n def  reduceByKey ( func :   ( V ,  V )   =>  V ,  numPartitions :   Int ) :  RDD [ ( K ,  V ) ] \n def  reduceByKey ( partitioner :  Partitioner ,  func :   ( V ,  V )   =>  V ) :  RDD [ ( K ,  V ) ] \n def  groupBy [ K ] ( f :  T  =>  K ,  p :  Partitioner ) : RDD [ ( K ,  Iterable [ V ] ) ] \n def  groupByKey ( partitioner :  Partitioner ) : RDD [ ( K ,  Iterable [ V ] ) ] \n def  aggregateByKey [ U :  ClassTag ] ( zeroValue :  U ,  partitioner :  Partitioner ) :  RDD [ ( K ,  U ) ] \n def  aggregateByKey [ U :  ClassTag ] ( zeroValue :  U ,  numPartitions :   Int ) :  RDD [ ( K ,  U ) ] \n def  combineByKey [ C ] ( createCombiner :  V  =>  C ,  mergeValue :   ( C ,  V )   =>  C ,  mergeCombiners :   ( C ,  C )   =>  C ) :  RDD [ ( K ,  C ) ] \n def  combineByKey [ C ] ( createCombiner :  V  =>  C ,  mergeValue :   ( C ,  V )   =>  C ,  mergeCombiners :   ( C ,  C )   =>  C ,  numPartitions :   Int ) :  RDD [ ( K ,  C ) ] \n def  combineByKey [ C ] ( createCombiner :  V  =>  C ,  mergeValue :   ( C ,  V )   =>  C ,  mergeCombiners :   ( C ,  C )   =>  C ,  partitioner :  Partitioner ,  mapSideCombine :   Boolean   =   true ,  serializer :  Serializer  =   null ) :  RDD [ ( K ,  C ) ] \n排序\n def  sortByKey ( ascending :   Boolean   =   true ,  numPartitions :   Int   =   self . partitions . length ) :  RDD [ ( K ,  V ) ] \n def  sortBy [ K ] ( f :   ( T )   =>  K ,  ascending :   Boolean   =   true ,  numPartitions :   Int   =   this . partitions . length ) ( implicit  ord :  Ordering [ K ] ,  ctag :  ClassTag [ K ] ) :  RDD [ T ] \n重分区\n def  coalesce ( numPartitions :   Int ,  shuffle :   Boolean   =   false ,  partitionCoalescer :  Option [ PartitionCoalescer ]   =  Option . empty ) \n def  repartition ( numPartitions :   Int ) ( implicit  ord :  Ordering [ T ]   =   null ) \n集合或者表操作\n def  intersection ( other :  RDD [ T ] ) :  RDD [ T ] \n def  intersection ( other :  RDD [ T ] ,  partitioner :  Partitioner ) ( implicit  ord :  Ordering [ T ]   =   null ) :  RDD [ T ] \n def  intersection ( other :  RDD [ T ] ,  numPartitions :   Int ) :  RDD [ T ] \n def  subtract ( other :  RDD [ T ] ,  numPartitions :   Int ) :  RDD [ T ] \n def  subtract ( other :  RDD [ T ] ,  p :  Partitioner ) ( implicit  ord :  Ordering [ T ]   =   null ) :  RDD [ T ] \n def  subtractByKey [ W :  ClassTag ] ( other :  RDD [ ( K ,  W ) ] ) :  RDD [ ( K ,  V ) ] \n def  subtractByKey [ W :  ClassTag ] ( other :  RDD [ ( K ,  W ) ] ,  numPartitions :   Int ) :  RDD [ ( K ,  V ) ] \n def  subtractByKey [ W :  ClassTag ] ( other :  RDD [ ( K ,  W ) ] ,  p :  Partitioner ) :  RDD [ ( K ,  V ) ] \n def  join [ W ] ( other :  RDD [ ( K ,  W ) ] ,  partitioner :  Partitioner ) :  RDD [ ( K ,   ( V ,  W ) ) ] \n def  join [ W ] ( other :  RDD [ ( K ,  W ) ] ) :  RDD [ ( K ,   ( V ,  W ) ) ] \n def  join [ W ] ( other :  RDD [ ( K ,  W ) ] ,  numPartitions :   Int ) :  RDD [ ( K ,   ( V ,  W ) ) ] \n def  leftOuterJoin [ W ] ( other :  RDD [ ( K ,  W ) ] ) :  RDD [ ( K ,   ( V ,  Option [ W ] ) ) ] \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \n \n *spark.shuffle.file.buffer*   默认32K，调整64K \n \n \n *spark.reducer.maxSizeInFlight：*   默认48M \n \n \n *spark.shuffle.io.maxRetries*  ：重试次数 \n \n \n ****spark.shuffle.io.retryWait：****：重试时间 \n \n \n 内存结构：Array和Map，5M \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 补充源码 \n SparkSQL引入 \n \n SparkCore撰写代码非常复杂，引入SparkSQL处理结构化数据 \n SparkSQL基于Hive之上做了改进 \n 什么是sparksql \n SparkSQL和Hive的关系(发展历程) \n \n \n SparkSQL引入Hive的发展历程: \n \n \n Hive \n \n \n \n \n \n Shark \n \n \n HQL on MapRedduce -> \n HQL on MapReduce -> HQL on Spark \n \n \n SparkSQL \n \n \n \n \n \n 历史发展 \n SparkSQL的数据结构 \n \n \n SparkCore数据结构：RDD \n \n \n SparkSQL数据结构：DataFrame和DataSet \n \n \n 思考：三种数据结构关系 \n \n \n \n 在2.0中DataFrame = DataSet[Row] \n DataFrame是在运行时候进行类型的检查，而Dataset是在编译的时候进行类型检查，DataSet安全性更好 \n \n \n SparkSQL的DataFrame创建的四种方法 \n \n SparkSession=sqlcontext+hivecontext+sparkcontext \n \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . sql . { Dataset ,  SparkSession } \n\n /**\n * DESC:\n * 使用SparkSession应用程序入口\n */ \n object  _01SparkSession  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //现在我们使用SparkSession \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //读取文件 \n     val  sc :  SparkContext  =  spark . sparkContext\n    sc . setLogLevel ( "WARN" ) \n     // \n     val  valueDS :  Dataset [ String ]   =  spark . read . textFile ( "data/baseinput/words.txt" ) \n    println ( "counts:"   +  valueDS . count ( ) ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \n \n RDD转DF的方式 \n \n \n 1-RDD配合样例类实现转化为DF \n \n \n \n \n \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n\n /**\n * DESC:\n */ \n case   class  People ( id :   Int ,  name :   String ,  age :   Int ) \n\n object  _02toDFWay1  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //申请资源 \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //For implicit conversions from RDDs to DataFrames \n     import   spark . implicits . _\n     //读取数据 \n     val  sc :  SparkContext  =  spark . sparkContext\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sql/people1.txt" ) \n     //切分后套入person类 \n     val  peopleRDD :  RDD [ People ]   =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . map ( x  =>  People ( x ( 0 ) . toInt ,  x ( 1 ) ,  x ( 2 ) . toInt ) ) \n     //引入隐式反馈转化为df \n     val  peopleDF :  DataFrame  =  peopleRDD . toDF ( ) \n     //打印输出 \n    peopleDF . show ( ) \n     //+---+--------+---+ \n     //| id|    name|age| \n     //+---+--------+---+ \n     //|  1|zhangsan| 20| \n     //|  2|    lisi| 29| \n     //|  3|  wangwu| 25| \n     //如何打印scheme \n    peopleDF . printSchema ( ) \n     //root \n     // |-- id: integer (nullable = false) \n     // |-- name: string (nullable = true) \n     // |-- age: integer (nullable = false) \n     //关闭资源 \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \n \n 2-spark直接读取数据文件转化为DF(了解) \n \n \n \n \n \n \n \n 4-RDD配合Row+StructType转化为DF \n \n \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . types . { DataTypes ,  StructType } \n import   org . apache . spark . sql . { DataFrame ,  Row ,  SparkSession } \n\n /**\n * DESC:\n */ \n\n object  _04toDFWay3  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //申请资源 \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //For implicit conversions from RDDs to DataFrames \n     import   spark . implicits . _\n     //读取数据 \n     val  sc :  SparkContext  =  spark . sparkContext\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sql/people1.txt" ) \n     //一个row对象就是一行数据 \n     val  peopleRDD :  RDD [ Row ]   =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . map ( x  =>  Row ( x ( 0 ) . toInt ,  x ( 1 ) ,  x ( 2 ) . toInt ) ) \n     //需要引入structedFiled \n     val  schema :  StructType  =   new  StructType ( ) \n       . add ( "id" ,  DataTypes . IntegerType ,   true ) \n       . add ( "name" ,   "string" ,   true ) \n       . add ( "age" ,   "int" ,   true ) \n     //打印输出 \n     val  peopleDF :  DataFrame  =  spark . createDataFrame ( peopleRDD ,  schema ) \n    peopleDF . show ( 2 , false ) \n     //+---+--------+---+ \n     //| id|    name|age| \n     //+---+--------+---+ \n     //|  1|zhangsan| 20| \n     //|  2|    lisi| 29| \n     //|  3|  wangwu| 25| \n     //如何打印scheme \n    peopleDF . printSchema ( ) \n     //root \n     // |-- id: integer (nullable = false) \n     // |-- name: string (nullable = true) \n     // |-- age: integer (nullable = false) \n     //关闭资源 \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \n 方式2 \n \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . types . { DataTypes ,  IntegerType ,  LongType ,  StringType ,  StructField ,  StructType } \n import   org . apache . spark . sql . { DataFrame ,  Row ,  SparkSession } \n\n /**\n * DESC:\n */ \n\n object  _04toDFWay4  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //申请资源 \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //For implicit conversions from RDDs to DataFrames \n     //读取数据 \n     val  sc :  SparkContext  =  spark . sparkContext\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sql/people1.txt" ) \n     //一个row对象就是一行数据 \n     val  peopleRDD :  RDD [ Row ]   =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . map ( x  =>  Row ( x ( 0 ) . toInt ,  x ( 1 ) ,  x ( 2 ) . toInt ) ) \n     //需要引入structedFiled \n\n     /* val schema: StructType = StructType(Array(\n       StructField("id", DataTypes.IntegerType, true),\n       StructField("name", DataTypes.StringType, true),\n       StructField("age", DataTypes.IntegerType, true)\n     ))*/ \n\n     val  schema :  StructType  =  StructType ( \n      StructField ( "f1" ,  IntegerType ,   true )   :: \n        StructField ( "f2" ,  StringType ,   false )   :: \n        StructField ( "f3" ,  IntegerType ,   false )   :: \n        Nil ) \n     //打印输出 \n     val  peopleDF :  DataFrame  =  spark . createDataFrame ( peopleRDD ,  schema ) \n    peopleDF . show ( 2 ,   false ) \n     //+---+--------+---+ \n     //| id|    name|age| \n     //+---+--------+---+ \n     //|  1|zhangsan| 20| \n     //|  2|    lisi| 29| \n     //|  3|  wangwu| 25| \n     //如何打印scheme \n    peopleDF . printSchema ( ) \n     //root \n     // |-- id: integer (nullable = false) \n     // |-- name: string (nullable = true) \n     // |-- age: integer (nullable = false) \n     //关闭资源 \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 \n 5-直接toDF \n \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n\n /**\n * DESC:\n */ \n\n object  _03toDFWay2  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //申请资源 \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //For implicit conversions from RDDs to DataFrames \n     import   spark . implicits . _\n     //读取数据 \n     val  sc :  SparkContext  =  spark . sparkContext\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sql/people1.txt" ) \n     //切分后套入person类 \n     val  peopleRDD  =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . map ( x  =>   ( x ( 0 ) . toInt ,  x ( 1 ) ,  x ( 2 ) . toInt ) ) \n     //引入隐式反馈转化为df \n     val  peopleDF :  DataFrame  =  peopleRDD . toDF ( "id" , "name" , "age" ) \n     //打印输出 \n    peopleDF . show ( ) \n     //+---+--------+---+ \n     //| id|    name|age| \n     //+---+--------+---+ \n     //|  1|zhangsan| 20| \n     //|  2|    lisi| 29| \n     //|  3|  wangwu| 25| \n     //如何打印scheme \n    peopleDF . printSchema ( ) \n     //root \n     // |-- id: integer (nullable = false) \n     // |-- name: string (nullable = true) \n     // |-- age: integer (nullable = false) \n     //关闭资源 \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 \n 补充： \n \n \n \n 样例类的rdd到df使用比较多的，特别是字段多的时候 \n 如果需要动态增加字段，可以使用strucedType的方式 \n \n SparkSQL的DataFrame的花式操作 \n DSL案例 \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n\n /**\n * DESC:\n */ \n case   class  People1 ( id :   Int ,  name :   String ,  age :   Int ) \n\n object  _06DataFrameOpration  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //申请资源 \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //For implicit conversions from RDDs to DataFrames \n     import   spark . implicits . _\n     //读取数据 \n     val  sc :  SparkContext  =  spark . sparkContext\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sql/people1.txt" ) \n     //切分后套入person类 \n     val  peopleRDD :  RDD [ People1 ]   =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . map ( x  =>  People1 ( x ( 0 ) . toInt ,  x ( 1 ) ,  x ( 2 ) . toInt ) ) \n     //引入隐式反馈转化为df \n     val  peopleDF :  DataFrame  =  peopleRDD . toDF ( ) \n     //打印输出 \n    peopleDF . show ( ) \n     //+---+--------+---+ \n     //| id|    name|age| \n     //+---+--------+---+ \n     //|  1|zhangsan| 20| \n     //|  2|    lisi| 29| \n     //|  3|  wangwu| 25| \n     //如何打印scheme \n    peopleDF . printSchema ( ) \n     //root \n     // |-- id: integer (nullable = false) \n     // |-- name: string (nullable = true) \n     // |-- age: integer (nullable = false) \n     //Spark中提供了查询的方式 \n     //DSL基于领域查询语言 \n     //需求1-能否将name字段筛选出来 \n    peopleDF . select ( "name" ) . show ( ) \n    peopleDF . select ( col ( "name" ) ) . show ( ) \n    peopleDF . select ( column ( "name" ) ) . show ( ) \n    peopleDF . select ( \'name ) . show ( ) \n     //需求2-能否将name，age字段筛选出来 \n    peopleDF . select ( "name" ,   "age" ) . show ( ) \n    peopleDF . select ( col ( "name" ) ,  col ( "age" ) ) . show ( ) \n    peopleDF . select ( column ( "name" ) ,  column ( "age" ) ) . show ( ) \n    peopleDF . select ( \'name, \' age ) . show ( ) \n     //组合写法 \n    peopleDF . select ( col ( "name" ) ,  column ( "age" ) ) . show ( ) \n    peopleDF . select ( col ( "name" ) ,   \'age ) . show ( ) \n     //peopleDF.select("name",\'age).show() \n     //需求3-能否将age字段筛选出来+1 \n     //peopleDF.select(\'name,\'age+1).show() \n     //peopleDF.select("name","age"+1).show() \n    peopleDF . select ( col ( "name" ) ,  col ( "age" )   +   1 ) . show ( ) \n    peopleDF . select ( col ( "name" ) ,  column ( "age" )   +   1 ) . show ( ) \n    peopleDF . select ( \'name, \' age  +   1 ) . show ( ) \n     //需求4：过滤age大于等于25的，使用filter方法过滤 \n    peopleDF . filter ( col ( "age" )   >   25 ) . show ( ) \n    peopleDF . filter ( \'age   >   25 ) . show ( ) \n    println ( peopleDF . filter ( \'age   >   25 ) . count ( ) ) \n     //需求5：按年龄进行分组并统计相同年龄的人数 \n     val  re1 :  DataFrame  =  peopleDF . groupBy ( "age" ) . count ( )   //这里通过count操作之后，schem中会增加count的名称 \n    re1 . show ( ) \n    re1 . printSchema ( ) \n    re1 . orderBy ( "count" ) . show ( ) \n    re1 . orderBy ( \'count ) . show ( ) \n     //完整的方案 \n    peopleDF . groupBy ( "age" ) . count ( ) . orderBy ( "count" ) . show ( ) \n     //SQL结构化查询语言 \n     //关闭资源 \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 SQL案例 \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n\n /**\n * DESC:\n */ \n\n object  _06_1DataFrameOpration  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //申请资源 \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //For implicit conversions from RDDs to DataFrames \n     import   spark . implicits . _\n     //读取数据 \n     val  sc :  SparkContext  =  spark . sparkContext\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sql/people1.txt" ) \n     //切分后套入person类 \n     val  peopleRDD :  RDD [ People1 ]   =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . map ( x  =>  People1 ( x ( 0 ) . toInt ,  x ( 1 ) ,  x ( 2 ) . toInt ) ) \n     //引入隐式反馈转化为df \n     val  peopleDF :  DataFrame  =  peopleRDD . toDF ( ) \n     //打印输出 \n     //SQL \n    peopleDF . createOrReplaceTempView ( "peopleTable" ) \n     //需求1-能否将name字段筛选出来 \n    spark . sql ( "select name from peopleTable" ) . show ( ) \n     //需求2-能否将name，age字段筛选出来 \n    spark . sql ( "select name,age from peopleTable" ) . show ( ) \n     //需求3-查询年龄最大的前两名 \n    spark . sql ( "select * from peopleTable order by age desc limit 2" ) . show ( ) \n    spark . sql ( \n       """\n        |select *\n        |from peopleTable\n        |order by age desc\n        |limit 2\n        |""" . stripMargin ) . show ( ) \n     //SQL结构化查询语言 \n     //关闭资源 \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 SparkSQL的DataSet \n \n \n wordcount \n \n \n DSL: \n \n \n package   cn . itcast . sparksql . wordcount \n\n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  Row ,  SparkSession } \n\n /**\n * DESC:\n */ \n object  _01DSL  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //现在我们使用SparkSession \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     //读取数据 \n     val  df1 :  DataFrame  =  spark . read . text ( "data/baseinput/words.txt" ) \n     val  ds1 :  Dataset [ String ]   =  spark . read . textFile ( "data/baseinput/words.txt" ) \n     //这里只能使用ds \n     //df1.as[String].flatMap(_.split("\\\\s+")) \n     val  value :  Dataset [ String ]   =  ds1 . flatMap ( _ . split ( "\\\\s+" ) ) \n    value . show ( ) \n     //+----------+ \n     //|     value| \n     //+----------+ \n     //|     hello| \n     //|     spark| \n     //|     hello| \n     //|     flink| \n     //|     hello| \n     //dsl \n     val  result :  Dataset [ Row ]   =  value . groupBy ( "value" ) . count ( ) . orderBy ( \'count . desc ) \n    result . show ( ) \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 \n SQL: \n \n package   cn . itcast . sparksql . wordcount \n\n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  Row ,  SparkSession } \n\n /**\n * DESC:\n */ \n object  _02SQL  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //现在我们使用SparkSession \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     //读取数据 \n     val  df1 :  DataFrame  =  spark . read . text ( "data/baseinput/words.txt" ) \n     val  ds1 :  Dataset [ String ]   =  spark . read . textFile ( "data/baseinput/words.txt" ) \n     //这里只能使用ds \n     //df1.as[String].flatMap(_.split("\\\\s+")) \n     val  value :  Dataset [ String ]   =  ds1 . flatMap ( _ . split ( "\\\\s+" ) ) \n    value . show ( ) \n     //+----------+ \n     //|     value| \n     //+----------+ \n     //|     hello| \n     //|     spark| \n     //|     hello| \n     //|     flink| \n     //|     hello| \n     //sql \n    value . createOrReplaceTempView ( "table" ) \n     val  result :  DataFrame  =  spark . sql ( \n       """\n        |select  value,count(value) as counts\n        |from table\n        |group by value\n        |order by counts desc\n        |""" . stripMargin ) \n     //+----------+-----+ \n     //|     value|count| \n     //+----------+-----+ \n     //|     hello|    6| \n     //|     sqoop|    1| \n     //|     flink|    1| \n     //|    pulsar|    1| \n     //|     doris|    1| \n     //|clickhouse|    1| \n     //|     spark|    1| \n     //+----------+-----+ \n    result . show ( ) \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #   SparkSQL的数据结构之间转换(掌握) \n \n \n RDD和DF和DS区别和练习 \n RDD通过scheme转化为df，df增加泛型转化ds \n df是在运行的时候检查类型的，rdd和dataset是在编译时候进行类型检查 \n dataset在spark2.0之后基本dataframe统一，dataSet[ROW]=dataframe \n 电影评分案例(掌握) \n \n \n 电影数据集统计需求分析 \n \n \n 需求：对电影评分数据进行统计分析 \n \n 获取Top10电影（电影评分平均值最高，并且每个电影被评分的次数大于2000) \n \n \n \n 数据集的认知： \n \n \n \n \n \n 步骤 \n \n 1-首先读取数据集 \n 2-数据集的解析，使用RDD转DF(case class) \n 3-SQL操作的实现 \n 4-使用DSL实现 \n 5-将结果存入到csv结果文件中 \n 6-将结果数据存入到MySQL \n \n \n \n 代码 \n \n \n package   cn . itcast . sparksql . moviesPro \n\n\n import   java . util . Properties\n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  Row ,  SaveMode ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n\n /**\n * DESC:\n * 1-首先读取数据集\n * 2-数据集的解析，使用RDD转DF(case class)\n * 3-SQL操作的实现\n * 4-使用DSL实现\n * 5-将结果存入到csv结果文件中\n * 6-将结果数据存入到MySQL中\n */ \n case   class  MoviesRatings ( userId :   String ,  moviesId :   String ,  ratings :   Double ,  timestamp :   String ) \n\n object  _01MoviesRatingLoader  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     val  spark :  SparkSession  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) \n         . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n         . setMaster ( "local[*]" ) \n         . set ( "spark.sql.shuffle.partitions" ,   "4" ) //默认200 \n       val  spark :  SparkSession  =  SparkSession . builder ( ) . config ( conf ) . getOrCreate ( ) \n      spark\n     } \n     import   spark . implicits . _\n     //1-首先读取数据集 \n     val  fileRDD :  RDD [ String ]   =  spark . sparkContext . textFile ( "data/baseinput/ml-1m/ratings.dat" ) \n     //2-数据集的解析，使用RDD转DF(case class) \n     val  moviesDF :  DataFrame  =  fileRDD\n       . filter ( line  =>  line  !=   null   &&  line . trim . split ( "::" ) . length  ==   4 ) \n       . mapPartitions ( iter  =>   { \n        iter . map ( line  =>   { \n           val  arr :  Array [ String ]   =  line . split ( "::" ) \n          MoviesRatings ( arr ( 0 ) ,  arr ( 1 ) ,  arr ( 2 ) . toDouble ,  arr ( 3 ) ) \n         } ) \n       } ) . toDF\n    moviesDF . show ( 3 ,   false ) \n     //+------+--------+-------+---------+ \n     //|userId|moviesId|ratings|timestamp| \n     //+------+--------+-------+---------+ \n     //|1     |1193    |5.0    |978300760| \n     //|1     |661     |3.0    |978302109| \n     //|1     |914     |3.0    |978301968| \n     //+------+--------+-------+---------+ \n    moviesDF . printSchema ( ) \n     //root \n     // |-- userId: string (nullable = true) \n     // |-- moviesId: string (nullable = true) \n     // |-- ratings: double (nullable = false) \n     // |-- timestamp: string (nullable = true) \n     //3-SQL操作的实现 \n    moviesDF . createOrReplaceTempView ( "movies_table" ) \n     //需求：获取Top10电影（电影评分平均值最高，并且每个电影被评分的次数大于2000) \n     val  sql  = \n       """\n        |select moviesId,round(avg(ratings),2) as avg_rating,count(moviesId) as rnt_rating\n        |from movies_table\n        |group by moviesId\n        |having rnt_rating>2000\n        |order by avg_rating desc,rnt_rating desc\n        |limit 10\n        |""" . stripMargin\n    println ( "sql funtions way is:..........." ) \n     //spark.sql(sql).show() \n     //4-使用DSL实现 \n     val  resultDF :  Dataset [ Row ]   =  moviesDF\n       . select ( "moviesId" ,   "ratings" ) \n       . groupBy ( "moviesId" ) \n       . agg ( \n        round ( avg ( "ratings" ) ,   2 ) . as ( "avg_rating" ) , \n        count ( "moviesId" ) . as ( "rnt_rating" ) \n       ) \n       . filter ( \'rnt_rating   >   2000 ) \n       //.filter($"rnt_rating" >2000) \n       //.filter(col("rnt_rating") >2000) \n       //.filter(column("rnt_rating") >2000) \n       . orderBy ( $ "avg_rating" . desc ,   \'rnt_rating . desc ) \n       . limit ( 10 ) \n    println ( "dsl funtions way is:..........." ) \n    resultDF . show ( ) \n     //5-将结果存入到csv结果文件中 \n     //+--------+----------+----------+ \n     //|moviesId|avg_rating|rnt_rating| \n     //+--------+----------+----------+ \n     //|     318|      4.55|      2227| \n     //resultDF \n     //  .coalesce(1) \n     //  .write \n     //  .mode(SaveMode.Overwrite) \n     //  .csv("data/baseoutput/output-2/") \n     // Thread.sleep(100 * 1000) \n     //6-将结果数据存入到MySQL中 \n     //创建数据库和表 \n     /*    CREATE DATABASE bigdata CHARACTER SET utf8;\n        CREATE TABLE `tb_top10_movies` (\n          `movieId` INT(11) NOT NULL,\n          `avg_rating` FLOAT(10) DEFAULT NULL,\n          `cnt_rating` INT(11) DEFAULT NULL,\n          PRIMARY KEY (`movieId`)\n        ) ENGINE=INNODB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;\n        SELECT * FROM tb_top10_movies*/ \n     //写入url，方法1 \n     //val prop = new Properties() \n     //prop.setProperty("driver", "com.mysql.jdbc.Driver") \n     //prop.setProperty("user", "root") \n     //prop.setProperty("password", "root") \n     //resultDF \n     //  .coalesce(1) \n     //  .write \n     //  .mode(SaveMode.Overwrite) \n     //  .jdbc("jdbc:mysql://localhost:3306/bigdata", "tb_top10_movie", prop) \n     //写法2,https://spark.apache.org/docs/3.1.1/sql-data-sources-jdbc.html \n     // Saving data to a JDBC source \n    resultDF . write\n       . format ( "jdbc" ) \n       . mode ( SaveMode . Overwrite ) \n       . option ( "url" ,   "jdbc:mysql://localhost:3306/bigdata" ) \n       . option ( "dbtable" ,   "tb_top10_movie" ) \n       . option ( "user" ,   "root" ) \n       . option ( "password" ,   "root" ) \n       . save ( ) \n    println ( "data write finished!" ) \n     //读取数据方法1 \n     //val jdbcDF = spark.read \n     //  .format("jdbc") \n     //  .option("url", "jdbc:mysql://localhost:3306/bigdata") \n     //  .option("dbtable", "tb_top10_movie") \n     //  .option("user", "root") \n     //  .option("password", "root") \n     //  .load() \n    println ( "data reader finished!" ) \n     //读取方法2 \n     val  connectionProperties  =   new  Properties ( ) \n    connectionProperties . put ( "user" ,   "root" ) \n    connectionProperties . put ( "password" ,   "root" ) \n     val  jdbcDF2  =  spark . read\n       . jdbc ( "jdbc:mysql://localhost:3306/bigdata" ,   "tb_top10_movie" ,  connectionProperties ) \n    jdbcDF2 . show ( ) \n    \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 \n 总结： \n \n \n 寻找多种数据源官网 \n Spark SQL整合Hive \n SparkSQL的和Hive本地集成(为了测试) \n \n \n SparkSQL和Hive本地测试步骤 \n \n \n 0-数据 \n \n \n 1,zhangsan,30 \n 2,lisi,40 \n 3,wangwu,50 \n \n \n 1-引入maven的依赖，spark_hive \n \n \n \n \n < dependency > \n           < groupId > org.apache.spark </ groupId > \n           < artifactId > spark-hive_2.11 </ artifactId > \n           < version > 2.4.5 </ version > \n  </ dependency > \n \n 1 2 3 4 5 \n \n 2-在sparkconf下面引用ennableHiveSupprt支持Hive \n \n \n 3-直接写hive的语句，使用spark.sql(hivesql) \n \n \n 4-打印结果 \n \n \n 代码 \n \n \n package   cn . itcast . sparksql . toHive \n\n import   org . apache . spark . sql . SparkSession\n\n /**\n * DESC:\n *\n */ \n object  _01SparkToHive  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //现在我们使用SparkSession \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . enableHiveSupport ( ) //to a persistent Hive metastore \n       . getOrCreate ( ) \n     //hive默认的元数据信息在derby中存储，后面配置了mysql \n\n    spark . sql ( "show databases" ) . show ( ) \n    spark . sql ( "create table if not exists student2(id int,name String,age int) row format delimited fields terminated by \',\'" ) \n    spark . sql ( "load data local inpath \'data/baseinput/sql/hive/students.csv\' overwrite into table student2" ) \n    spark . sql ( "select * from student2 where age >30" ) . show ( ) \n\n\n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #  Spark整合集群环境的Hive \n \n \n SParkOnHive \n \n Hive---HiveOnMR \n Shark----HiveOnSpark---大部分用的都是Hive \n SparkSQL----独立实现的一套引擎 \n SparkSQL+HIve整合---- SparkOnHive --除了使用Hive元数据其他均使用SparkSQL的东西 \n \n \n \n 如何搭建SparkOnHive环境？ \n \n \n 1-首先让Spark知道Hive元数据的信息在哪里，需要将hive-site.xml放入saprk的conf目录下 \n \n \n \n \n \n \n \n \n 2-需要将hive-site.xml分发到其他两台上 \n \n \n 3-需要将mysql的jar包放在spark的jars目录下 \n \n \n 启动metastore \n \n \n \n \n nohup  /export/server/hive/bin/hive  --service  metastore  2 > &1   >>  /var/log.log  & \n \n 1 \n \n 4-通过spark-sql或spark-shell测试是否整合成功 \n \n \n \n \n \n \n \n \n 4-通过IDEA远程连接集群环境的Hive进行相关操作 \n \n \n \n \n \n \n \n \n 代码 \n \n \n package   cn . itcast . sparkbase \n\n import   org . apache . spark . sql . SparkSession\n\n /**\n * DESC:\n */ \n object  _02SparkToHive  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       //在2.0.0之前hive.sql.warehouse.dir，2.0之后spark.sql.warehouse.dir \n       //warehouse的hive数据存放地址 \n       . config ( "spark.sql.warehouse.dir" ,   "hdfs://node1:8020/user/hive/warehouse" ) \n       //metastore服务地址，thrift \n       . config ( "hive.metastore.uris" ,   "thrift://node3:9083" ) \n       . enableHiveSupport ( )   //to a persistent Hive metastore \n       . getOrCreate ( ) \n\n    spark . sql ( "show databases" ) . show ( ) \n    spark . sql ( "use sparkhive3" ) \n    spark . sql ( "create table if not exists student2(id int,name String,age int) row format delimited fields terminated by \',\'" ) \n    spark . sql ( "show tables" ) . show ( ) \n    spark . sql ( "load data local inpath \'data/baseinput/sql/hive/students.csv\' overwrite into table student2" ) \n    spark . sql ( \n     """\n      |select *\n      |from student2\n      |where age >30\n      |""" . stripMargin ) . show ( ) \n     //+---+------+---+ \n     //| id|  name|age| \n     //+---+------+---+ \n     //|  2|  lisi| 40| \n     //|  3|wangwu| 50| \n     //+---+------+---+ \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 \n 两种启动方式 \n \n \n \n \n \n Spark的CLI方式 \n \n \n \n \n \n Spark的Beeline方式，thrift \n \n \n Spark的ThriftServer服务(类似于Hive的HiveServer2)，在通过Beeline连接执行SQL\n \n 1 \n \n \n SPARK_HOME = /export/server/spark\n $SPARK_HOME /sbin/start-thriftserver.sh  \\ \n --hiveconf   hive.server2.thrift.port = 10001   \\ \\ 同时连hive可换一个port\n --hiveconf   hive.server2.thrift.bind.host = node3  \\ \n --master  local [ 2 ] \n \n 1 2 3 4 5 \n \n \n \n \n 可以在其他机器上访问sparkonhive \n \n \n \n \n \n 可以通过thrift访问sparkonhive的数据 \n SparkSQL的UDF函数(必须掌握) \n \n \n UDF：一对一，Spark使用最多 \n \n \n UDAF：多对一，Spark基本实现 \n \n \n UDTF：一对多，Hive实现了UDTF，Spark没有实现 \n \n \n \n \n \n \n \n \n 编程模式sparksession使用UDF 需要先注册 \n spark.udf.register() 方式 \n \n 1.通过匿名函数注册udf \n \n //注册udf, 返回字符串长度 \nspark . udf . register ( "strLen" ,   ( str :   String )   =>  str . length ( ) ) \n //spark sql中使用udf \nspark . sql ( "select name,strLen(name) as name_len from user" ) . show ( false ) \n \n 1 2 3 4 \n 2.通过实名函数注册udf \n \n //定义实名函数 \n def  getStrLen ( str :   String ) :   Int   =   { \n  str . length\n } \n\n //注册udf,要在实名函数后面加 _(注意前面有个空格) \nspark . udf . register ( "strLen" ,  getStrLen _ ) \n //spark sql中使用udf \nspark . sql ( "select name,strLen(name) as name_len from user" ) . show ( false ) \n \n 1 2 3 4 5 6 7 8 9 \n SparkSQL中使用UDF （如果是打包单个类注意指定META-INF/MANIFEST.MF在src下，不要放在resouces,不然会找不到主类，可带一个空main函数类，注册时就会报类路径错误） \n 方式一：在启动spark-sql时通过--jars指定 \n \n cd   $SPARK_HOME /bin\nspark-sql  --jars  /home/hadoop/lib/udf.jar\nCREATE TEMPORARY FUNCTION hello AS  \'com.luogankun.udf.HelloUDF\' ; \n select  hello ( url )  from page_views limit  1 ; \n \n 1 2 3 4 \n 方式二： \n \n 1）需要先将udf.jar的路径配置到spark-env.sh的SPARK_CLASSPATH中，形如： \n export   SPARK_CLASSPATH = $SPARK_CLASSPATH :/home/hadoop/software/mysql-connector-java-5.1.27-bin.jar:/home/hadoop/lib/udf.jar\n \n 1 2）再启动spark-sql，直接CREATE TEMPORARY FUNCTION即可； \n cd   $SPARK_HOME /bin\nspark-sql\nCREATE TEMPORARY FUNCTION hello AS  \'com.luogankun.udf.HelloUDF\' ; \n select  hello ( url )  from page_views limit  1 ; \n \n 1 2 3 4 \n 方式三：Thrift JDBC Server中使用UDF \n \n 在beeline命令行中执行： \n create  function  base_analizer as  \'com.zhengkw.udf.BaseFieldUDF\'  using jar  \'hdfs://hadoop102:9000/user/hive/jars/hivefunction-1.0-SNAPSHOT.jar\' ; //hdfs路径\n\n add  jar /home/hadoop/lib/udf.jar ; //本地机器路径\nCREATE TEMPORARY FUNCTION hello AS  \'com.luogankun.udf.HelloUDF\' ; \n select  hello ( url )  from page_views limit  1 ; \n\n \n 1 2 3 4 5 6 \n spark/hive共用自定义函数(见hive udf) \n \n /**\n基于DataFrame(或者DataSet) 的Java(或Python、Scale) 可以轻松的定义注册UDF，但是想在SQL(SparkSQL、Hive) 中自定义或者想共用就遇到困难。这时，可以先按照一定规约自定义函数，再向Spark(或Hive)注册为永久函数，实现在Spark和Hive共享UDF的目的。\n**/ \n \n 1 2 3 \n UDF函数案例1 \n \n package   cn . itcast . sparksql . udf \n\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . api . java . UDF1\n import   org . apache . spark . sql . types . StringType\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n\n /**\n * DESC:\n * 1-申请资源\n * 2-读取数据\n * 3-执行转化为大写，适合于UDF函数\n */ \n case   class  Smaller ( line :   String ) \n\n object  _01wordsToBigger  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-申请资源 \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     //2-读取数据 \n     val  fileRDD :  RDD [ String ]   =  spark . sparkContext . textFile ( "data\\\\baseinput\\\\sql\\\\udf\\\\udf.txt" ) \n     //3-执行转化为大写，适合于UDF函数 \n     val  wordsDF :  DataFrame  =  fileRDD . map ( x  =>  Smaller ( x ) ) . toDF\n    wordsDF . show ( ) \n    wordsDF . printSchema ( )   // |-- line: string (nullable = true) \n     //user-defined function \n     //spark.udf.register("wordToBigger", new UDF1[String, String] { \n     //  override def call(t1: String): String = { \n     //    t1.toUpperCase() \n     //  } \n     //}, StringType) \n    spark . udf . register ( "wordToBigger" , ( line : String ) => { \n      line . toUpperCase ( ) \n     } ) \n     //SQL \n    wordsDF . createOrReplaceTempView ( "word_view" ) \n     val  result1 :  DataFrame  =  spark . sql ( "select line,wordToBigger(line) as bigger from word_view" ) \n    result1 . show ( ) \n     //DSL \n     val  result2 :  DataFrame  =  wordsDF . select ( \'line , \n      callUDF ( "wordToBigger" ,   \'line ) . as ( "bigger" ) ) \n    result2 . show ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \n 更多参数 \n \n package   cn . itcast . sparksql . udf \n\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . api . java . UDF1\n import   org . apache . spark . sql . types . StringType\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n\n /**\n * DESC:\n * 1-申请资源\n * 2-读取数据\n * 3-执行转化为大写，适合于UDF函数\n */ \n\n object  _02udfDemo  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-申请资源 \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n\n     val  df  =  Seq ( ( "id1" ,   1 ) ,   ( "id2" ,   4 ) ,   ( "id3" ,   5 ) ) . toDF ( "id" ,   "value" ) \n    spark . udf . register ( "simpleUDF" ,   ( n :   Int )   =>  n  *  n ) \n    df . select ( $ "id" ,  callUDF ( "simpleUDF" ,  $ "value" ) . as ( "pow(x,2)" ) ) . show ( ) \n     val  df1  =  Seq ( ( "id1" ,   1 , 6 ) ,   ( "id2" ,   4 , 7 ) ,   ( "id3" ,   5 , 8 ) ) . toDF ( "id" ,   "value1" , "value2" ) \n    spark . udf . register ( "simpleUDF2" , ( v1 : Int , v2 : Int ) => { \n      v1 * v2\n     } ) \n    df1 . createOrReplaceTempView ( "table_df" ) \n    spark . sql ( \n       """\n        |select id,simpleUDF2(value1,value2) as simple\n        |from table_df\n        |""" . stripMargin ) . show ( ) \n\n    df1 . select ( $ "id" , \n      callUDF ( "simpleUDF2" , \'value1,\' value2 ) . as ( "bigger" ) ) . show ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 \n UDAF了解 \n \n \n package   cn . itcast . sparksql . udf \n import   org . apache . spark . sql . { DataFrame ,  Row ,  SparkSession } \n import   org . apache . spark . sql . expressions . { MutableAggregationBuffer ,  UserDefinedAggregateFunction } \n import   org . apache . spark . sql . types . _\n\n class  SparkFunctionUDAF  extends  UserDefinedAggregateFunction { \n   //输入的数据类型的schema \n   override   def  inputSchema :  StructType  =   { \n    StructType ( StructField ( "input" , LongType ) :: Nil ) \n   } \n   //缓冲区数据类型schema，说白了就是转换之后的数据的schema \n   //求解平均值“总金额，总人数” \n   override   def  bufferSchema :  StructType  =   { \n    StructType ( StructField ( "sum" , LongType ) :: StructField ( "total" , LongType ) :: Nil ) \n   } \n   //返回值的数据类型---总金额/总人数 可能会有小数，返回doubleType \n   override   def  dataType :  DataType  =   { \n    DoubleType\n   } \n   //确定是否相同的输入会有相同的输出 \n   override   def  deterministic :   Boolean   =   { \n     true \n   } \n   //初始化内部数据结构 \n   override   def  initialize ( buffer :  MutableAggregationBuffer ) :   Unit   =   { \n    buffer ( 0 )   =   0L    //总金额都累加到Buffer0中，初始值为0 \n    buffer ( 1 )   =   0L    //总人数都累加到buffer1参数中 \n   } \n   //更新数据内部结构 \n   override   def  update ( buffer :  MutableAggregationBuffer ,  input :  Row ) :   Unit   =   { \n     //所有的金额相加 \n    buffer ( 0 )   =  buffer . getLong ( 0 )   +  input . getLong ( 0 ) \n     //一共有多少条数据 \n    buffer ( 1 )   =  buffer . getLong ( 1 )   +   1   //能否累加10？--不能，+！为了求解人数 \n   } \n   //来自不同分区的数据进行合并 \n   override   def  merge ( buffer1 :  MutableAggregationBuffer ,  buffer2 :  Row ) :   Unit   =   { \n    buffer1 ( 0 )   = buffer1 . getLong ( 0 )   +  buffer2 . getLong ( 0 ) \n    buffer1 ( 1 )   =  buffer1 . getLong ( 1 )   +  buffer2 . getLong ( 1 ) \n   } \n   //计算输出数据值 \n   override   def  evaluate ( buffer :  Row ) :   Any   =   { \n    buffer . getLong ( 0 ) . toDouble  /  buffer . getLong ( 1 ) \n   } \n } \n\n object  SparkFunctionUDAF  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //获取sparkSession \n     val  sparkSession :  SparkSession  =  SparkSession . builder ( ) . appName ( "sparkUDAF" ) . master ( "local[2]" ) . getOrCreate ( ) \n     //通过sparkSession读取json文件得到DataFrame \n     val  employeeDF :  DataFrame  =  sparkSession . read . json ( "data\\\\baseinput\\\\sql\\\\udf\\\\udaf.txt" ) \n     //通过DataFrame创建临时表 \n    employeeDF . createOrReplaceTempView ( "employee_table" ) \n     //注册我们的自定义UDAF函数 \n    sparkSession . udf . register ( "avgSal" , new  SparkFunctionUDAF ) \n     //调用我们的自定义UDAF函数 \n    sparkSession . sql ( "select avgSal(salary) from employee_table" ) . show ( ) \n    sparkSession . close ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 #  SparkSQL函数 \n \n \n Hive中的开窗函数 \n \n \n 聚合类开窗函数 \n \n count() over(order by partition by) \n \n \n \n 排序类开窗函数 \n \n row_number() over(partition by order by)   12345 \n rank() over(partition by order by)     446 \n dense_rank() over(partition by order by)   445 \n \n \n \n 代码 \n \n \n package   cn . itcast . sparksql . func \n\n import   org . apache . spark . sql . SparkSession\n\n /**\n * DESC:\n */ \n case   class  Score ( name :   String ,  clazz :   Int ,  score :   Int ) \n\n object  _01class  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-申请资源 \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     val  scoreDF  =  spark . sparkContext . makeRDD ( Array ( \n      Score ( "a1" ,   1 ,   80 ) , \n      Score ( "a2" ,   1 ,   78 ) , \n      Score ( "a3" ,   1 ,   95 ) , \n      Score ( "a4" ,   2 ,   74 ) , \n      Score ( "a5" ,   2 ,   92 ) , \n      Score ( "a6" ,   3 ,   99 ) , \n      Score ( "a7" ,   3 ,   99 ) , \n      Score ( "a8" ,   3 ,   45 ) , \n      Score ( "a9" ,   3 ,   55 ) , \n      Score ( "a10" ,   3 ,   78 ) , \n      Score ( "a11" ,   3 ,   100 ) ) \n     ) . toDF ( "name" ,   "class" ,   "score" ) \n    scoreDF . createOrReplaceTempView ( "scores" ) \n    scoreDF . printSchema ( ) \n     //聚合开窗函数 \n    spark . sql ( "select count(name) from scores" ) . show ( ) \n    spark . sql ( "select name,class,score,count(name) over() ccc from scores" ) . show ( ) \n    spark . sql ( "select name,class,score,count(name) over(partition by class) ccc from scores" ) . show ( ) \n     //排序开窗函数 123 \n    spark . sql ( "select name,class,score,row_number() over(order by class) sss from scores" ) . show ( ) \n    spark . sql ( "select name,class,score,row_number() over(partition by class order by score) sss from scores" ) . show ( ) \n     //446 \n    spark . sql ( "select name,class,score,rank() over(order by class) sss from scores" ) . show ( ) \n    spark . sql ( "select name,class,score,rank() over(partition by class order by score) sss from scores" ) . show ( ) \n     //445 \n    spark . sql ( "select name,class,score,dense_rank() over(order by class) sss from scores" ) . show ( ) \n    spark . sql ( "select name,class,score,dense_rank() over(partition by class order by score) sss from scores" ) . show ( ) \n     //ntile \n    spark . sql ( "select name,class,score,ntile(6) over(order by class) sss from scores" ) . show ( ) \n    spark . sql ( "select name,class,score,ntile(6) over(partition by class order by score) sss from scores" ) . show ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #  SparkSQL底层如何执行解析成RDD \n \n \n \n \n \n 步骤 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 如何查看逻辑计划和物理计划 \n \n \n （1）通过WebUI查看 \n \n \n \n \n \n （2）通过spark-shell交互式命令行，启动4040端口 \n \n \n \n \n \n \n \n \n 面试时候回答的SparkSQL底层解析原理？ \n SparkStreaming引入--RDD--DStream \n 流数据的处理模式 \n \n \n \n \n \n 两种处理方式 \n \n \n \n \n \n SparkStreaming官网 \n SparkStreaming数据结构 \n SparkStreaming原理架构 \n SparkStreaming原理 \n 基础原理 \n SparkStreaming原理深入 \n \n 整个SparkStreaming涉及两个时间\n \n Receiver接收器需要接受数据的时间，一般设置200ms左右，官网建议我们不能低于50ms \n \n \n \n https://spark.apache.org/docs/3.1.1/configuration.html \n SparkStreaming处理还有时间，一般程序中设置5s \n DStream两种算子 \n \n 底层RDD@Time时间序列构成 \n RDD分为Transormation算子和Action算子，那个DStream如何划分呢？ \n 答案：Transormation和OutPutOpration操作 \n \n \n \n \n \n \n 问题：对于SparkSTreaming的DStream的操作支持的算子并没有那么多，比如排序算子 \n SparkStreaming解决方案利用transform的方法将DStream转化为RDD，因为RDD的transform很多的 \n SparkStreaming初体验-无状态 \n \n \n 需求：数据按照每5s处理一次，输入数据wordcount \n \n \n 数据源：nc -lk 9999/9998 模拟socket数据源 \n \n \n 步骤： \n \n 1-准备上下文环境 \n 2-读取数据 \n 3-flatMap \n 4-map \n 5-reduceBykey \n 6-start \n 7-awaitTermination \n \n \n \n 代码： \n \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n *1-准备上下文环境\n *2-读取数据\n *3-flatMap\n *4-map\n *5-reduceBykey\n *6-start\n *7-awaitTermination\n */ \n object  _01baseStreaming  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1-准备上下文环境 \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n     //batchDuration the time interval at which streaming data will be divided into batches \n     val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n     // 2-读取数据--从socket读数据，需要在linux或windows中安装nc，如何安装nc；yum install -y nc   执行nc -lk 9999 \n     val  receiveRDD :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n     // 3-flatMap \n     val  flatRDD :  DStream [ String ]   =  receiveRDD . flatMap ( _ . split ( "\\\\s+" ) ) \n     // 4-map \n     val  mapRDD :  DStream [ ( String ,   Int ) ]   =  flatRDD . map ( x  =>   ( x ,   1 ) ) \n     // 5-reduceBykey \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapRDD . reduceByKey ( ( a :   Int ,  b :   Int )   =>  a  +  b ) \n    resultRDD . print ( ) \n     // 6-start \n     //Start the execution of the streams. \n    ssc . start ( ) \n     // 7-awaitTermination--Wait for the execution to stop \n    ssc . awaitTermination ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 \n \n 截图 \n 状态计算 \n updateStateByKey \n \n \n 首先实现有状态的统计，然后我们进行transform排序 \n \n \n 代码： \n \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-准备上下文环境\n * 2-读取数据\n * 3-flatMap\n * 4-map\n * 5-reduceBykey\n * 6-start\n * 7-awaitTermination\n */ \n object  _02baseStreaming  { \n   /**\n   * @param currentValue 当前的值\n   * @param historyValue 历史的值\n   * @return Option-none some\n   */ \n   def  updateFunc ( currentValue :  Seq [ Int ] ,  historyValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sumV :   Int   =  currentValue . sum  +  historyValue . getOrElse ( 0 ) //如果有值就给值否则给0 \n     //Some(sumV) \n    Option ( sumV ) \n   } \n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1-准备上下文环境 \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n     //batchDuration the time interval at which streaming data will be divided into batches \n     val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n    ssc . checkpoint ( "data/baseoutput/output-4" ) \n     // 2-读取数据--从socket读数据，需要在linux或windows中安装nc，如何安装nc；yum install -y nc   执行nc -lk 9999 \n     val  receiveRDD :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n     // 3-flatMap \n     val  flatRDD :  DStream [ String ]   =  receiveRDD . flatMap ( _ . split ( "\\\\s+" ) ) \n     // 4-map \n     val  mapRDD :  DStream [ ( String ,   Int ) ]   =  flatRDD . map ( x  =>   ( x ,   1 ) ) \n     // 5-reduceBykey \n       //在Scala中，它和Java一样也是拥有方法和函数。Scala的方法是类的一部分，而函数是一个对象可以赋值给一个变量。换句话来说，在类中定义的函数即是方法。 \n\n //Scala 中可以使用 def语句和val 语句定义函数，而定义方法只能使用def 语句。 \n       //需要函数自动转换，不会自动转换的可以通过方法名+空格+_转化成函数 \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapRDD . updateStateByKey ( updateFunc ) \n     //增加排序的方法--tranform中使用rdd \n     val  sortResultDS :  DStream [ ( String ,   Int ) ]   =  resultRDD . transform ( rdd  =>   { \n      rdd . sortBy ( _ . _2 ,   false ) \n     } ) \n    sortResultDS . print ( ) \n      \n       // 启动新的线程，希望在特殊的场合关闭SparkStreaming \n     new  Thread ( new  Runnable  { \n       override   def  run ( ) :   Unit   =   { \n\n         while   (   true   )   { \n           try   { \n            Thread . sleep ( 5000 ) \n           }   catch   { \n             case  ex  :  Exception  =>  println ( ex ) \n           } \n\n           // 监控HDFS文件的变化 \n           val  fs :  FileSystem  =  FileSystem . get ( new  URI ( "hdfs://node1:8020" ) ,   new  Configuration ( ) ,   "root" ) \n\n           val  state :  StreamingContextState  =  ssc . getState ( ) \n           // 如果环境对象处于活动状态，可以进行关闭操作 \n           if   (  state  ==  StreamingContextState . ACTIVE  )   { \n\n             // 判断路径是否存在 \n             val  flg :   Boolean   =  fs . exists ( new  Path ( "hdfs://node1:8020/spark/stopSparkHistory/" + appName + "@" + ssc . sparkContext . getConf . getAppId ) ) \n             if   (  flg  )   { \n               // 关闭采集器和Driver:优雅的关闭 \n              ssc . stop ( true ,   true ) \n              System . exit ( 0 ) \n             } \n\n           } \n         } \n\n       } \n     } ) . start ( ) \n     // 6-start \n     //Start the execution of the streams. \n    ssc . start ( ) \n      \n     \n     // 7-awaitTermination--Wait for the execution to stop调用awaitTermination()，driver将阻塞在这里，直到流式应用意外退出。 \n    ssc . awaitTermination ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 \n 双流合并 \n \n 有两个流的合并union \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-准备上下文环境\n * 2-读取数据\n * 3-flatMap\n * 4-map\n * 5-reduceBykey\n * 6-start\n * 7-awaitTermination\n */ \n object  _06twoSourceStreaming  { \n\n   def  updateFunc ( curentValue :  Seq [ Int ] ,  histouryValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  curentValue . sum  +  histouryValue . getOrElse ( 0 ) \n    Option ( sum ) \n   } \n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1-准备上下文环境 \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[3]" ) \n     //batchDuration the time interval at which streaming data will be divided into batches \n     val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n    ssc . checkpoint ( "data/baseoutput/output-6" ) \n     // 2-读取数据--从socket读数据，需要在linux或windows中安装nc，如何安装nc；yum install -y nc   执行nc -lk 9999 \n     val  receiveRDD1 :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n     val  receiveRDD2 :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9998 ) \n     val  receiveRDD :  DStream [ String ]   =  receiveRDD1 . union ( receiveRDD2 ) \n     // 3-flatMap \n     val  flatRDD :  DStream [ String ]   =  receiveRDD . flatMap ( _ . split ( "\\\\s+" ) ) \n     // 4-map \n     val  mapRDD :  DStream [ ( String ,   Int ) ]   =  flatRDD . map ( x  =>   ( x ,   1 ) ) \n     // 5-reduceBykey \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapRDD . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n     // 6-start \n     //Start the execution of the streams. \n    ssc . start ( ) \n     // 7-awaitTermination--Wait for the execution to stop \n    ssc . awaitTermination ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 #  mapWithState \n \n \n updateStateByKey的所有计算结果经过计算之后都会返回Driver端 \n \n \n mapWithState只会返回driver端更新的数据，而不是全部数据 \n \n \n \n \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . commons . lang3 . StringUtils\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { MapWithStateDStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  State ,  StateSpec ,  StreamingContext } \n\n /**\n * DESC:\n * 1-首先加载StreamingContext上下文对象，获取资源，内部调用sparkconf\n * 2-读取外部数据源，如socket数据源\n * 3-flatMap\n * 4-map\n * 5-mapwithstate\n * 6-print\n * 7-ssc.start\n * 8-ssc.awaitTermination等待程序异常退出\n */ \n object  _04mapWithState  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-首先加载StreamingContext上下文对象，获取资源，内部调用sparkconf \n     val  ssc :  StreamingContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc\n     } \n    ssc . checkpoint ( "data/baseoutput/output-5" ) \n     //2-读取外部数据源，如socket数据源 \n     val  receiveStream :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n     //3-flatMap \n     //4-map \n     //5-mapwithstate \n     val  result :  MapWithStateDStream [ String ,   Int ,   Int ,   Any ]   =  receiveStream\n       . filter ( line  =>  StringUtils . isNotBlank ( line ) ) \n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . mapWithState ( StateSpec . function ( mappingFunction ) ) \n     //6-print \n    result . print ( ) \n     //7-ssc.start \n    ssc . start ( ) \n     //8-ssc.awaitTermination等待程序异常退出 \n    ssc . awaitTermination ( ) \n   } \n\n   /**\n   * word-统计单词\n   * option-\n   * state-\n   */ \n   val  mappingFunction  =   ( word :   String ,  option :  Option [ Int ] ,  state :  State [ Int ] )   =>   { \n     //1-首先查看状态的时间是否超时 \n     if   ( state . isTimingOut ( ) )   { \n      println ( word  +   "is time out.." ) \n       //2-没有超时状态 \n     }   else   { \n       //3-option历史的值，结合当前的状态的值累加 \n       val  sum :   Int   =  option . getOrElse ( 0 )   +  state . getOption ( ) . getOrElse ( 0 ) \n       val  keyFreq :   ( String ,   Int )   =   ( word ,  sum ) \n       //4-state的update的方法执行 \n      state . update ( sum ) \n      keyFreq\n     } \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 #  窗口计算 \n \n \n \n \n \n 窗口长度:被包含在窗口中的数据需要被处理 \n \n \n 窗口滑动时间间隔：窗口每过多久滑动一次 \n \n \n 数据处理时间5s处理一次，streamigcontect(second(5)) \n \n \n \n \n \n 切分批次：没隔多久处理一次，一般设置5s \n \n \n 窗口长度：被包括在窗口中的数据需要被计算 \n \n \n 窗口滑动时间间隔；窗口每隔多久计算一次 \n \n \n 当窗口的长度=窗口的滑动时间间隔，不会造成数据丢失或重复 \n \n \n 当窗口的长度<滑动的时间间隔，会造成数据丢失 \n \n \n 当窗口的长度>滑动时间间隔，造成数据重复， \n （ 重复得看最后的数据怎么使用，如果说是每5min统计最近1小时的数据，只要这一小时的数据，没有重复可言，如果是要保留数据最后累计，就会有重复累计问题。 ） \n \n \n 上述案例下，窗口的长度设置为17s，不能随便设置 \n \n \n 一般设置，窗口的滑动时间间隔和窗口的长度必须是数据处理时间的整数倍，否则rdd需要被分为两部分，违反了rdd的不可变 \n \n \n 案例： \n \n \n 每隔10s统计一下10s的数据 \n \n \n \n \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-准备上下文环境\n * 2-读取数据\n * 3-flatMap\n * 4-map\n * 5-reduceBykey\n * 6-start\n * 7-awaitTermination\n */ \n object  _07windowsOpration  { \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1-准备上下文环境 \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[3]" ) \n     //batchDuration the time interval at which streaming data will be divided into batches \n     val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n    ssc . checkpoint ( "data/baseoutput/output-6" ) \n     // 2-读取数据--从socket读数据，需要在linux或windows中安装nc，如何安装nc；yum install -y nc   执行nc -lk 9999 \n     val  receiveRDD :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n\n     // 3-flatMap \n     val  flatRDD :  DStream [ String ]   =  receiveRDD . flatMap ( _ . split ( "\\\\s+" ) ) \n     // 4-map \n     val  mapRDD :  DStream [ ( String ,   Int ) ]   =  flatRDD . map ( x  =>   ( x ,   1 ) ) \n     // 5-reduceBykey \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapRDD . reduceByKeyAndWindow ( \n       ( a :   Int ,  b :   Int )   =>  a  +  b , \n      Seconds ( 10 ) , \n      Seconds ( 10 ) ) \n    resultRDD . print ( ) \n     // 6-start \n     //Start the execution of the streams. \n    ssc . start ( ) \n     // 7-awaitTermination--Wait for the execution to stop \n    ssc . awaitTermination ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \n \n 换种API实现 \n \n \n \n \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-准备上下文环境\n * 2-读取数据\n * 3-flatMap\n * 4-map\n * 5-reduceBykey\n * 6-start\n * 7-awaitTermination\n */ \n object  _08windowsOpration  { \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1-准备上下文环境 \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[3]" ) \n     //batchDuration the time interval at which streaming data will be divided into batches \n     val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n    ssc . checkpoint ( "data/baseoutput/output-6" ) \n     // 2-读取数据--从socket读数据，需要在linux或windows中安装nc，如何安装nc；yum install -y nc   执行nc -lk 9999 \n     val  receiveRDD :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n\n     // 3-flatMap \n     val  flatRDD :  DStream [ String ]   =  receiveRDD . flatMap ( _ . split ( "\\\\s+" ) ) \n     // 4-map \n     val  mapRDD :  DStream [ ( String ,   Int ) ]   =  flatRDD . map ( x  =>   ( x ,   1 ) ) \n     // 5-reduceBykey \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapRDD . window ( Seconds ( 10 ) ,  Seconds ( 10 ) ) \n     // 6-tranform \n     val  resultRDD1 :  DStream [ ( String ,   Int ) ]   =  resultRDD . transform ( rdd  =>   { \n      rdd . reduceByKey ( _  +  _ ) \n     } ) \n    resultRDD1 . print ( ) \n     // 6-start \n     //Start the execution of the streams. \n    ssc . start ( ) \n     // 7-awaitTermination--Wait for the execution to stop \n    ssc . awaitTermination ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \n \n \n \n \n 增加排序的功能 \n \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . streaming . dstream . { DStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-准备上下文环境\n * 2-读取数据\n * 3-flatMap\n * 4-map\n * 5-reduceBykey\n * 6-start\n * 7-awaitTermination\n */ \n object  _08windowsOpration  { \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1-准备上下文环境 \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[3]" ) \n     //batchDuration the time interval at which streaming data will be divided into batches \n     val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n    ssc . checkpoint ( "data/baseoutput/output-6" ) \n     // 2-读取数据--从socket读数据，需要在linux或windows中安装nc，如何安装nc；yum install -y nc   执行nc -lk 9999 \n     val  receiveRDD :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n\n     // 3-flatMap \n     val  flatRDD :  DStream [ String ]   =  receiveRDD . flatMap ( _ . split ( "\\\\s+" ) ) \n     // 4-map \n     val  mapRDD :  DStream [ ( String ,   Int ) ]   =  flatRDD . map ( x  =>   ( x ,   1 ) ) \n     // 5-reduceBykey \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapRDD . window ( Seconds ( 10 ) ,  Seconds ( 5 ) ) \n     // 6-tranform \n     val  resultRDD1 :  DStream [ ( String ,   Int ) ]   =  resultRDD . transform ( rdd  =>   { \n       val  reduceRDD :  RDD [ ( String ,   Int ) ]   =  rdd . reduceByKey ( _  +  _ ) \n      reduceRDD . sortBy ( _ . _2 ,   false ) \n     } ) \n    resultRDD1 . print ( ) \n     // 6-start \n     //Start the execution of the streams. \n    ssc . start ( ) \n     // 7-awaitTermination--Wait for the execution to stop \n    ssc . awaitTermination ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #  SparkStreaming和SparkSQL整合 \n SparkStreaming和基本数据源(监控HDFS文件夹下内容同格式文件) \n \n 基础数据源 \n \n package   cn . itcast . sparkstreaming . filesource \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . DStream\n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-申请资源\n * 2-读取数据源，设置文件夹\n * 3-通过flatMap，map，updateStateByKey统计\n * 4-ssc.start\n * 5-ssc.awitTermination\n * 6-ssc.stop\n */ \n object  _01textFileSource  { \n\n   def  updateFunc ( currenValue :  Seq [ Int ] ,  historyValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  currenValue . sum  +  historyValue . getOrElse ( 0 ) \n    Some ( sum ) \n   } \n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1-申请资源 \n     val  ssc :  StreamingContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc\n     } \n    ssc . checkpoint ( "data/baseoutput/output-7" ) \n     // 2-读取数据源，设置文件夹 \n     val  rddDS :  DStream [ String ]   =  ssc . textFileStream ( "hdfs://node1.itcast.cn:8020/wordcount/trans/" ) \n     // 3-通过flatMap，map，updateStateByKey统计 \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  rddDS\n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n     // 4-ssc.start \n    ssc . start ( ) \n     // 5-ssc.awitTermination \n    ssc . awaitTermination ( ) \n     // 6-ssc.stop \n    ssc . stop ( true , true ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \n \n \n \n \n 通过SparkSQL整合SparkSTreaming \n \n \n package   cn . itcast . sparkstreaming . filesource \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . streaming . dstream . DStream\n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-申请资源\n * 2-读取数据源，设置文件夹\n * 3-通过flatMap，map，updateStateByKey统计\n * 4-ssc.start\n * 5-ssc.awitTermination\n * 6-ssc.stop\n */ \n object  _03textFileSourceSparkSQL  { \n\n   def  updateFunc ( currenValue :  Seq [ Int ] ,  historyValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  currenValue . sum  +  historyValue . getOrElse ( 0 ) \n    Some ( sum ) \n   } \n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1-申请资源 \n     val  ssc :  StreamingContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc\n     } \n    ssc . checkpoint ( "data/baseoutput/output-7" ) \n     // 2-读取数据源，设置文件夹 \n     val  rddDS :  DStream [ String ]   =  ssc . textFileStream ( "hdfs://node1.itcast.cn:8020/wordcount/trans/" ) \n     // 3-通过flatMap，map，updateStateByKey统计 \n     val  resultRDD :  DStream [ String ]   =  rddDS . flatMap ( _ . split ( "\\\\s+" ) ) \n\n     val  resultValue :  DStream [ ( String ,   Int ) ]   =  resultRDD . map ( ( _ ,   1 ) ) . updateStateByKey ( updateFunc ) \n\n    resultValue . foreachRDD ( rdd  =>   { \n       // Get the singleton instance of SparkSession \n       val  spark  =  SparkSession . builder . config ( rdd . sparkContext . getConf ) . getOrCreate ( ) \n       import   spark . implicits . _\n       val  df :  DataFrame  =  rdd . toDF ( "word" ,   "count" ) \n       //dsl \n       //sql \n      df . createOrReplaceTempView ( "table_view" ) \n       val  result :  DataFrame  =  spark . sql ( \n         """\n          |select *\n          |from table_view\n          |""" . stripMargin ) \n      result . show ( ) \n     } ) \n\n     // 4-ssc.start \n    ssc . start ( ) \n     // 5-ssc.awitTermination \n    ssc . awaitTermination ( ) \n     // 6-ssc.stop \n    ssc . stop ( true ,   true ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 #  SparkStreaming和Kafka整合 \n kafka原理 \n \n Kafka \n 什么是消息：应用之间传输的数据 \n 什么是消息队列：应用之间保障消息传递的准确性 \n 消息队列分类：点对点，发布订阅者方式 \n 消息队列应用：限流消峰，应用解耦 \n Kafka：Scala+Java混编，LinkedIn(ActiveMQ) \n Kafka：生产者，消费者，Broker \n 当前的虚拟机的Kafka的版本是1,1.0大家之前学习的是2,4,1，高版本命令更多使用的是boostrap-server替代zk \n \n \n Kafka安装 \n \n 脚本： \n当前版本是1.1.0 大家使用的是2.4.1 \n1-kafka启动  \n nohup  bin/kafka-server-start.sh config/server.properties   & \n2-查看kafka内部的topic的内容 \n/export/server/kafka/bin/kafka-topics.sh  --list   --zookeeper  node1:2181\n #3-描述topic \n/export/server/kafka/bin/kafka-topics.sh  --describe   --zookeeper  node1:2181  --topic  spark_kafka\n创建topic \n/export/server/kafka/bin/kafka-topics.sh  --create   --zookeeper  node1:2181 --replication-factor  1   --partitions   3   --topic  spark_kafka\n #描述topic \n/export/server/kafka/bin/kafka-topics.sh  --describe   --zookeeper  node1:2181  --topic  spark_kafka\n删除opic \n/export/server/kafka/bin/kafka-topics.sh  --zookeeper  node1:2181  --delete   --topic   test \n\n #生产者和消费者 \n/export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092  --topic  spark_kafka\n/export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092  --topic  spark_kafka --from-beginning \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #  整合两种方式区别 \n \n 实现SparkStreaming和kafka整合 \n 两个版本：0.8级0.10以上，目前企业中使用的是0.10版本以上 \n 两种消费方式：\n \n Receiver：启动多个Receiver接收器线程拉取kafka数据过来合并，使用WAL和checkpount等\n \n 问题1：需要开启多个Receiver线程拉取数据，需要合并数据源 \n 问题2：使用的HighLevelAPI对接的是offset维护在ZK中 \n 问题3：需要大量的WAL或checkpoint影响效率 \n \n \n Direct：直接采用直接连接kafka的topic的partition数据，获取区中offset \n \n 解决1：直接使用spark的rdd的一个分区对接的是kafka的一个partition \n 解决2：使用的是lowlevel api将offset在kafka的topic中存放的，还可以手动设置 \n 解决3：没有提供WAL的方式 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 几个特点： \n \n \n \n \n \n \n \n \n \n kafka+sparkstreaming消费者消费数据 \n SparkStreaming010整合Kafka \n \n \n 010以上kafka版本，使用Directly的方式 \n \n \n 需求：【自动提交偏移量】使用sparkstreaming提供的API读取kafka中的数据，使用该数据完成wordcount \n \n \n 步骤： \n \n 1-导入有kafka和spark整合的Jar包 \n \n \n \n < dependency > \n\t < groupId > org.apache.spark </ groupId > \n\t < artifactId > spark-streaming-kafka-0-10_2.11 </ artifactId > \n\t < version > 2.4.5 </ version > \n </ dependency > \n \n 1 2 3 4 5 \n \n 2-调用streamingCOntext \n \n \n 3-KafkaUtils.creatDriectlyStream的方法直接连接Kafka集群的分区 \n \n \n \n \n \n \n \n \n 在大多数情况下使用此功能，它将在所有执行程序之间一致地分配分区。 \n \n \n LocationStrategies 的几种方式。 \n \n \n 1.  LocationStrategies . PreferBrokers ( ) \n \n 1 仅仅在你 spark 的 executor 在相同的节点上，优先分配到存在  kafka  broker 的机器上； \n    2.  LocationStrategies . PreferConsistent ( ) ; \n \n 1 大多数情况下使用，一致性的方式分配分区所有 executor 上。（主要是为了分布均匀） \n    3.  LocationStrategies . PreferFixed ( hostMap :  collection . Map [ TopicPartition ,   String ] ) \n   4.  LocationStrategies . PreferFixed ( hostMap :  ju . Map [ TopicPartition ,   String ] ) \n \n 1 2 如果你的负载不均衡，可以通过这两种方式来手动指定分配方式，其他没有在 map 中指定的，均采用 preferConsistent() 的方式分配； \n \n \n \n \n \n \n \n \n \n \n \n 4-获取record记录中的value的值 \n \n \n 5-根据value进行累加求和wordcount \n \n \n 6-ssc.statrt \n \n \n 7-ssc.awaitTermination \n \n \n 8-ssc.stop(true,true) \n \n \n 代码 \n \n \n package   cn . itcast . sparkstreaming . kafka \n\n import   org . apache . kafka . clients . consumer . ConsumerRecord\n import   org . apache . kafka . common . serialization . StringDeserializer\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  InputDStream } \n import   org . apache . spark . streaming . kafka010 . { ConsumerStrategies ,  KafkaUtils ,  LocationStrategies } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-导入有kafka和spark整合的Jar包\n * 2-调用streamingCOntext\n * 3-KafkaUtils.creatDriectlyStream的方法直接连接Kafka集群的分区\n * 4-获取record记录中的value的值\n * 5-根据value进行累加求和wordcount\n * 6-ssc.statrt\n * 7-ssc.awaitTermination\n * 8-ssc.stop(true,true)\n */ \n object  _01SparkStreamingKafkaAuto  { \n   def  updateFunc ( curentValue :  Seq [ Int ] ,  histouryValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  curentValue . sum  +  histouryValue . getOrElse ( 0 ) \n    Option ( sum ) \n   } \n\n   val  kafkaParams  =  Map [ String ,  Object ] ( \n     "bootstrap.servers"   ->   "node1:9092" , \n     "key.deserializer"   ->  classOf [ StringDeserializer ] , \n     "value.deserializer"   ->  classOf [ StringDeserializer ] , \n     "group.id"   ->   "spark_group" , \n     //offset的偏移量自动设置为最新偏移量，有几种设置偏移量的方法 \n     // //这里的auto.offset.reset代表的是自动重置offset为latest就表示的是最新的偏移量，如果没有偏移从最新的位置开始 \n     "auto.offset.reset"   ->   "latest" , \n     //是否自动提交，这里设置为自动提交，提交到kafka指导的__consumeroffset中，由kafka自己维护，如果设置为false可以使用ckeckpoint或者是将offset存入mysql \n     // //这里如果是false手动提交，默认由SparkStreaming提交到checkpoint中，在这里也可以根据用户或程序员将offset偏移量提交到mysql或redis中 \n     "enable.auto.commit"   ->   ( true :  java . lang . Boolean ) , \n     //自动设置提交的时间 \n     "auto.commit.interval.ms"   ->   "1000" \n   ) \n\n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-导入有kafka和spark整合的Jar包 \n     //2-调用streamingCOntext \n     val  ssc :  StreamingContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc\n     } \n    ssc . checkpoint ( "data/baseoutput/cck1" ) \n     //3-KafkaUtils.creatDriectlyStream的方法直接连接Kafka集群的分区 \n     //ssc: StreamingContext, \n     //locationStrategy: LocationStrategy, \n     //consumerStrategy: ConsumerStrategy[K, V] \n     val  streamRDD :  InputDStream [ ConsumerRecord [ String ,   String ] ]   =  KafkaUtils . createDirectStream [ String ,   String ] ( ssc , \n      LocationStrategies . PreferConsistent , \n      ConsumerStrategies . Subscribe [ String ,   String ] ( Array ( "spark_kafka" ) ,  kafkaParams ) ) \n     //4-获取record记录中的value的值 \n     val  mapValue :  DStream [ String ]   =  streamRDD . map ( _ . value ( ) ) \n     //5-根据value进行累加求和wordcount \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapValue\n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n     //6-ssc.statrt \n    ssc . start ( ) \n     //7-ssc.awaitTermination \n    ssc . awaitTermination ( ) \n     //8-ssc.stop(true,true) \n    ssc . stop ( true ,   true ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 #  手动提交偏移量 \n ​\t （实现恰好一次，但还是存在断网没有及时提交数据） \n \n \n https://spark.apache.org/docs/3.1.1/streaming-kafka-0-10-integration.html \n \n \n \n \n \n `代码（手动提交的偏移量保存在checkpoint中，checkpoint设置为hdfs，这种方式会导致hdfs小文件过多） \n \n \n package   cn . itcast . sparkstreaming . kafka \n\n import   org . apache . kafka . clients . consumer . ConsumerRecord\n import   org . apache . kafka . common . serialization . StringDeserializer\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  InputDStream } \n import   org . apache . spark . streaming . kafka010 . { CanCommitOffsets ,  ConsumerStrategies ,  HasOffsetRanges ,  KafkaUtils ,  LocationStrategies ,  OffsetRange } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-导入有kafka和spark整合的Jar包\n * 2-调用streamingCOntext\n * 3-KafkaUtils.creatDriectlyStream的方法直接连接Kafka集群的分区\n * 4-获取record记录中的value的值\n * 5-根据value进行累加求和wordcount\n * 6-ssc.statrt\n * 7-ssc.awaitTermination\n * 8-ssc.stop(true,true)\n */ \n object  _02SparkStreamingKafkaByPass  { \n   def  updateFunc ( curentValue :  Seq [ Int ] ,  histouryValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  curentValue . sum  +  histouryValue . getOrElse ( 0 ) \n    Option ( sum ) \n   } \n\n   val  kafkaParams  =  Map [ String ,  Object ] ( \n     "bootstrap.servers"   ->   "node1:9092" , \n     "key.deserializer"   ->  classOf [ StringDeserializer ] , \n     "value.deserializer"   ->  classOf [ StringDeserializer ] , \n     "group.id"   ->   "spark_group" , \n     //offset的偏移量自动设置为最新偏移量，有几种设置偏移量的方法 \n     //这里的auto.offset.reset代表的是自动重置offset为latest就表示的是最新的偏移量，如果没有偏移从最新的位置开始 \n     "auto.offset.reset"   ->   "latest" , \n     //这里如果是false手动提交，默认由SparkStreaming提交到checkpoint中，在这里也可以根据用户或程序员将offset偏移量提交到mysql或redis中 \n     "enable.auto.commit"   ->   ( false :  java . lang . Boolean ) \n   ) \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-导入有kafka和spark整合的Jar包 \n     //2-调用streamingCOntext \n     val  ssc :  StreamingContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc\n     } \n    ssc . checkpoint ( "data/baseoutput/cck2" ) \n     //3-KafkaUtils.creatDriectlyStream的方法直接连接Kafka集群的分区 \n     //ssc: StreamingContext, \n     //locationStrategy: LocationStrategy, \n     //consumerStrategy: ConsumerStrategy[K, V] \n     val  streamRDD :  InputDStream [ ConsumerRecord [ String ,   String ] ]   =  KafkaUtils . createDirectStream [ String ,   String ] ( ssc , \n      LocationStrategies . PreferConsistent , \n      ConsumerStrategies . Subscribe [ String ,   String ] ( Array ( "spark_kafka" ) ,  kafkaParams ) ) \n     //实现获取offset然后手动提交offset \n    streamRDD . foreachRDD ( f  =>   { \n       if   ( f . count ( )   >   0 )   { \n         //这里仅仅是为了打印 \n        println ( "rdd is:" ,  f ) \n        f . foreach ( record  =>   { \n          println ( "record result is:" ,  record ) \n           val  value :   String   =  record . value ( ) \n          println ( "value is:" ,  value ) \n         } ) \n       }   //end id \n       //获取offset \n       val  offsetRanges :  Array [ OffsetRange ]   =  f . asInstanceOf [ HasOffsetRanges ] . offsetRanges\n       //这里仅仅是为了打印 \n       for ( offsetRange  <-  offsetRanges ) { \n        println ( s "topic: ${ offsetRange . topic }  partition: ${ offsetRange . partition }  fromoffset: ${ offsetRange . fromOffset }  endoffset: ${ offsetRange . untilOffset } " ) \n       } //end for \n       //提交offset,手动的方式默认提交到checkpoint的目录中 \n      streamRDD . asInstanceOf [ CanCommitOffsets ] . commitAsync ( offsetRanges ) \n     } ) \n\n\n     //4-获取record记录中的value的值 \n     val  mapValue :  DStream [ String ]   =  streamRDD . map ( _ . value ( ) ) \n     //5-根据value进行累加求和wordcount \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapValue\n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n     //6-ssc.statrt \n    ssc . start ( ) \n     //7-ssc.awaitTermination \n    ssc . awaitTermination ( ) \n     //8-ssc.stop(true,true) \n    ssc . stop ( true ,   true ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 \n \n \n \n \n \n \n \n 代码模板抽取 \n \n package   cn . itcast . sparkstreaming . kafka \n\n import   org . apache . kafka . clients . consumer . ConsumerRecord\n import   org . apache . kafka . common . serialization . StringDeserializer\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  InputDStream } \n import   org . apache . spark . streaming . kafka010 . _\n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-导入有kafka和spark整合的Jar包\n * 2-调用streamingCOntext\n * 3-KafkaUtils.creatDriectlyStream的方法直接连接Kafka集群的分区\n * 4-获取record记录中的value的值\n * 5-根据value进行累加求和wordcount\n * 6-ssc.statrt\n * 7-ssc.awaitTermination\n * 8-ssc.stop(true,true)\n */ \n object  _03SparkStreamingKafkaModel  { \n   def  updateFunc ( curentValue :  Seq [ Int ] ,  histouryValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  curentValue . sum  +  histouryValue . getOrElse ( 0 ) \n    Option ( sum ) \n   } \n\n   val  kafkaParams  =  Map [ String ,  Object ] ( \n     "bootstrap.servers"   ->   "node1:9092" , \n     "key.deserializer"   ->  classOf [ StringDeserializer ] , \n     "value.deserializer"   ->  classOf [ StringDeserializer ] , \n     "group.id"   ->   "spark_group" , \n     //offset的偏移量自动设置为最新偏移量，有几种设置偏移量的方法 \n     //这里的auto.offset.reset代表的是自动重置offset为latest就表示的是最新的偏移量，如果没有偏移从最新的位置开始 \n     "auto.offset.reset"   ->   "latest" , \n     //这里如果是false手动提交，默认由SparkStreaming提交到checkpoint中，在这里也可以根据用户或程序员将offset偏移量提交到mysql或redis中 \n     "enable.auto.commit"   ->   ( false :  java . lang . Boolean ) \n   ) \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-导入有kafka和spark整合的Jar包 \n     //2-调用streamingCOntext \n     val  ssc :  StreamingContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc\n     } \n    ssc . checkpoint ( "data/baseoutput/cck3" ) \n    compute ( ssc ) \n     //6-ssc.statrt \n    ssc . start ( ) \n     //7-ssc.awaitTermination \n    ssc . awaitTermination ( ) \n     //8-ssc.stop(true,true) \n    ssc . stop ( true ,   true ) \n   } \n\n\n   def  compute ( ssc :  StreamingContext ) :   Unit   =   { \n     //3-KafkaUtils.creatDriectlyStream的方法直接连接Kafka集群的分区 \n     //ssc: StreamingContext, \n     //locationStrategy: LocationStrategy, \n     //consumerStrategy: ConsumerStrategy[K, V] \n     val  streamRDD :  InputDStream [ ConsumerRecord [ String ,   String ] ]   =  KafkaUtils . createDirectStream [ String ,   String ] ( ssc , \n      LocationStrategies . PreferConsistent , \n      ConsumerStrategies . Subscribe [ String ,   String ] ( Array ( "spark_kafka" ) ,  kafkaParams ) ) \n     //实现获取offset然后手动提交offset \n    streamRDD . foreachRDD ( f  =>   { \n       if   ( f . count ( )   >   0 )   { \n         //这里仅仅是为了打印 \n        println ( "rdd is:" ,  f ) \n        f . foreach ( record  =>   { \n          println ( "record result is:" ,  record ) \n           val  value :   String   =  record . value ( ) \n          println ( "value is:" ,  value ) \n         } ) \n       }   //end id \n       //获取offset \n       val  offsetRanges :  Array [ OffsetRange ]   =  f . asInstanceOf [ HasOffsetRanges ] . offsetRanges\n       //这里仅仅是为了打印 \n       for   ( offsetRange  <-  offsetRanges )   { \n        println ( s "topic: ${ offsetRange . topic }  partition: ${ offsetRange . partition }  fromoffset: ${ offsetRange . fromOffset }  endoffset: ${ offsetRange . untilOffset } " ) \n       }   //end for \n       //提交offset,手动的方式默认提交到checkpoint的目录中 \n      streamRDD . asInstanceOf [ CanCommitOffsets ] . commitAsync ( offsetRanges ) \n     } ) \n     //4-获取record记录中的value的值 \n     val  mapValue :  DStream [ String ]   =  streamRDD . map ( _ . value ( ) ) \n     //5-根据value进行累加求和wordcount \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapValue\n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 #  Checkpoint 恢复 \n \n \n \n \n \n \n \n \n \n 当Streaming Application再次运行时，从Checkpoint检查点目录恢复时，有时有问题， 比如修改程序，再次从运行时 ，可能 出现类型转换异常 ，如下所示： \n \n ERROR Utils: Exception encountered   java.lang.ClassCastException: cannot assign instance of  cn.itcast.spark.ckpt.StreamingCkptState$$anonfun$streamingProcess$1 to field  org.apache.spark.streaming.dstream.ForEachDStream.org$apache$spark$streaming$dstream$ForEachDStream$$foreachFunc  of type scala.Function2 in instance of  org.apache.spark.streaming.dstream.ForEachDStream     at  java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2133)     at  java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1305)     at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2024) \n \n 原因在于修改DStream转换操作，在检查点目录中存储的数据没有此类的相关代码，所以保存ClassCastException异常。 \n 此时无法从检查点读取偏移量信息和转态信息，所以实际开发人员：SparkStreaming中Checkpoint功能，属于鸡肋，食之无味，弃之可惜。解决方案： \n l  1）、针对状态信息：当应用启动时，从外部存储系统读取最新状态，比如从MySQL表读取，或者从Redis读取； \n l  2）、针对偏移量数据：自己管理偏移量，将偏移量存储到MySQL表、Zookeeper、HBase或Redis； \n \n \n \n 代码 \n \n \n package   cn . itcast . sparkstreaming . kafka \n\n import   org . apache . kafka . clients . consumer . ConsumerRecord\n import   org . apache . kafka . common . serialization . StringDeserializer\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  InputDStream } \n import   org . apache . spark . streaming . kafka010 . _\n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-导入有kafka和spark整合的Jar包\n * 2-调用streamingCOntext\n * 3-KafkaUtils.creatDriectlyStream的方法直接连接Kafka集群的分区\n * 4-获取record记录中的value的值\n * 5-根据value进行累加求和wordcount\n * 6-ssc.statrt\n * 7-ssc.awaitTermination\n * 8-ssc.stop(true,true)\n */ \n object  _04getActiveOrCreate  { \n   def  updateFunc ( curentValue :  Seq [ Int ] ,  histouryValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  curentValue . sum  +  histouryValue . getOrElse ( 0 ) \n    Option ( sum ) \n   } \n\n   val  kafkaParams  =  Map [ String ,  Object ] ( \n     "bootstrap.servers"   ->   "node1:9092" , \n     "key.deserializer"   ->  classOf [ StringDeserializer ] , \n     "value.deserializer"   ->  classOf [ StringDeserializer ] , \n     "group.id"   ->   "spark_group" , \n     //offset的偏移量自动设置为最新偏移量，有几种设置偏移量的方法 \n     //这里的auto.offset.reset代表的是自动重置offset为latest就表示的是最新的偏移量，如果没有偏移从最新的位置开始 \n     "auto.offset.reset"   ->   "latest" , \n     //这里如果是false手动提交，默认由SparkStreaming提交到checkpoint中，在这里也可以根据用户或程序员将offset偏移量提交到mysql或redis中 \n     "enable.auto.commit"   ->   ( false :  java . lang . Boolean ) \n   ) \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-导入有kafka和spark整合的Jar包 \n     val  CHECKPOINT  =   "data/baseoutput/cck5" \n     //2-调用streamingCOntext \n     val  ssc :  StreamingContext  =  StreamingContext . getActiveOrCreate ( CHECKPOINT ,   ( )   =>   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc . checkpoint ( CHECKPOINT ) \n      compute ( ssc ) \n      ssc\n     } ) \n     //6-ssc.statrt \n    ssc . start ( ) \n     //7-ssc.awaitTermination \n    ssc . awaitTermination ( ) \n     //8-ssc.stop(true,true) \n    ssc . stop ( true ,   true ) \n   } \n\n\n   def  compute ( ssc :  StreamingContext ) :   Unit   =   { \n     //3-KafkaUtils.creatDriectlyStream的方法直接连接Kafka集群的分区 \n     //ssc: StreamingContext, \n     //locationStrategy: LocationStrategy, \n     //consumerStrategy: ConsumerStrategy[K, V] \n     val  streamRDD :  InputDStream [ ConsumerRecord [ String ,   String ] ]   =  KafkaUtils . createDirectStream [ String ,   String ] ( ssc , \n      LocationStrategies . PreferConsistent , \n      ConsumerStrategies . Subscribe [ String ,   String ] ( Array ( "spark_kafka" ) ,  kafkaParams ) ) \n     //实现获取offset然后手动提交offset \n    streamRDD . foreachRDD ( f  =>   { \n       if   ( f . count ( )   >   0 )   { \n         //这里仅仅是为了打印 \n        println ( "rdd is:" ,  f ) \n        f . foreach ( record  =>   { \n          println ( "record result is:" ,  record ) \n           val  value :   String   =  record . value ( ) \n          println ( "value is:" ,  value ) \n         } ) \n       }   //end id \n       //获取offset \n       val  offsetRanges :  Array [ OffsetRange ]   =  f . asInstanceOf [ HasOffsetRanges ] . offsetRanges\n       //这里仅仅是为了打印 \n       for   ( offsetRange  <-  offsetRanges )   { \n        println ( s "topic: ${ offsetRange . topic }  partition: ${ offsetRange . partition }  fromoffset: ${ offsetRange . fromOffset }  endoffset: ${ offsetRange . untilOffset } " ) \n       }   //end for \n       //提交offset,手动的方式默认提交到checkpoint的目录中 \n      streamRDD . asInstanceOf [ CanCommitOffsets ] . commitAsync ( offsetRanges ) \n     } ) \n     //4-获取record记录中的value的值 \n     val  mapValue :  DStream [ String ]   =  streamRDD . map ( _ . value ( ) ) \n     //5-根据value进行累加求和wordcount \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapValue\n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 偏移量存储在MySQL中 \n \n \n 实际工作中，将offset存储在mysql或redis中，可以监控redis或mysql中offset的变化，从而更好管理offset \n \n \n \n \n \n 需要使用MySQL的驱动包，DriverManager结合 \n \n \n     CREATE TABLE `t_offset` (\n      `topic` varchar(255) NOT NULL,\n      `partition` int(11) NOT NULL,\n      `groupid` varchar(255) NOT NULL,\n      `offset` bigint(20) DEFAULT NULL,\n      PRIMARY KEY (`topic`,`partition`,`groupid`)\n    ) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n \n 1 2 3 4 5 6 7 \n \n 查询topic的信息 \n \n \n \n \n \n 查看官网对于TopicPerition的讲解 \n \n \n \n \n \n \n \n \n 1-MySQL的偏移量查询或保存的工具类提供 \n \n \n package   cn . itcast . sparkstreaming . kafka . toMySQL \n\n import   java . sql . { DriverManager ,  ResultSet } \n\n import   org . apache . kafka . common . TopicPartition\n import   org . apache . spark . streaming . kafka010 . OffsetRange\n\n import   scala . collection . mutable \n\n object  OffsetUtil  { \n\n     //从数据库读取偏移量 \n     def  getOffsetMap ( groupid :   String ,  topic :   String )   =   { \n       val  connection  =  DriverManager . getConnection ( "jdbc:mysql://localhost:3306/bigdata?characterEncoding=UTF-8" ,   "root" ,   "root" ) \n       val  pstmt  =  connection . prepareStatement ( "select * from t_offset where groupid=? and topic=?" ) \n      pstmt . setString ( 1 ,  groupid ) \n      pstmt . setString ( 2 ,  topic ) \n       val  rs :  ResultSet  =  pstmt . executeQuery ( ) \n       val  offsetMap  =  mutable . Map [ TopicPartition ,   Long ] ( ) \n       while   ( rs . next ( ) )   { \n        offsetMap  +=   new  TopicPartition ( rs . getString ( "topic" ) ,  rs . getInt ( "partition" ) )   ->  rs . getLong ( "offset" ) \n       } \n      rs . close ( ) \n      pstmt . close ( ) \n      connection . close ( ) \n      offsetMap\n     } \n\n     //将偏移量保存到数据库 \n     def  saveOffsetRanges ( groupid :   String ,  offsetRange :  Array [ OffsetRange ] )   =   { \n       val  connection  =  DriverManager . getConnection ( "jdbc:mysql://localhost:3306/bigdata?characterEncoding=UTF-8" ,   "root" ,   "root" ) \n       //replace into表示之前有就替换,没有就插入 \n       val  pstmt  =  connection . prepareStatement ( "replace into t_offset (`topic`, `partition`, `groupid`, `offset`) values(?,?,?,?)" ) \n       for   ( o  <-  offsetRange )   { \n        pstmt . setString ( 1 ,  o . topic ) \n        pstmt . setInt ( 2 ,  o . partition ) \n        pstmt . setString ( 3 ,  groupid ) \n        pstmt . setLong ( 4 ,  o . untilOffset ) \n        pstmt . executeUpdate ( ) \n       } \n      pstmt . close ( ) \n      connection . close ( ) \n     } \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \n \n 2-使用MySQL偏移量的工具类给出偏移量 \n \n \n 分析一下，什么时候需要从mysql中读取偏移量 \n \n 首先，MysQL中是有offset的记录，才可以读取，判断Map的数是否有空 \n 接着，如果Mysql没有记录，拉取最新的偏移量直接消费即可 \n 接着，如果MySQL存放了记录，直接读取，直接在subscribe增加第三个参数 \n \n \n \n \n 分析一下，什么时候应该将偏移量写入Mysql \n \n 每次有新的数据消费需要更新MySQL的Offset(utilOffset) \n \n \n \n \n 代码： \n \n \n package   cn . itcast . sparkstreaming . kafka . toMySQL \n\n import   org . apache . kafka . clients . consumer . ConsumerRecord\n import   org . apache . kafka . common . TopicPartition\n import   org . apache . kafka . common . serialization . StringDeserializer\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  InputDStream } \n import   org . apache . spark . streaming . kafka010 . _\n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n import   scala . collection . mutable \n\n /**\n * DESC:\n * 1-导入有kafka和spark整合的Jar包\n * 2-调用streamingCOntext\n * 3-KafkaUtils.creatDriectlyStream的方法直接连接Kafka集群的分区\n * 4-获取record记录中的value的值\n * 5-根据value进行累加求和wordcount\n * 6-ssc.statrt\n * 7-ssc.awaitTermination\n * 8-ssc.stop(true,true)\n */ \n object  _01getActiveOrCreate  { \n   def  updateFunc ( curentValue :  Seq [ Int ] ,  histouryValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  curentValue . sum  +  histouryValue . getOrElse ( 0 ) \n    Option ( sum ) \n   } \n\n   val  kafkaParams  =  Map [ String ,  Object ] ( \n     "bootstrap.servers"   ->   "node1:9092" , \n     "key.deserializer"   ->  classOf [ StringDeserializer ] , \n     "value.deserializer"   ->  classOf [ StringDeserializer ] , \n     "group.id"   ->   "spark_group" , \n     //offset的偏移量自动设置为最新偏移量，有几种设置偏移量的方法 \n     //这里的auto.offset.reset代表的是自动重置offset为latest就表示的是最新的偏移量，如果没有偏移从最新的位置开始 \n     "auto.offset.reset"   ->   "latest" , \n     //这里如果是false手动提交，默认由SparkStreaming提交到checkpoint中，在这里也可以根据用户或程序员将offset偏移量提交到mysql或redis中 \n     "enable.auto.commit"   ->   ( false :  java . lang . Boolean ) \n   ) \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-导入有kafka和spark整合的Jar包 \n     val  CHECKPOINT  =   "data/baseoutput/cck6" \n     //2-调用streamingCOntext \n     val  ssc :  StreamingContext  =  StreamingContext . getActiveOrCreate ( CHECKPOINT ,   ( )   =>   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc . checkpoint ( CHECKPOINT ) \n      compute ( ssc ) \n      ssc\n     } ) \n     //6-ssc.statrt \n    ssc . start ( ) \n     //7-ssc.awaitTermination \n    ssc . awaitTermination ( ) \n     //8-ssc.stop(true,true) \n    ssc . stop ( true ,   true ) \n   } \n\n\n   def  compute ( ssc :  StreamingContext ) :   Unit   =   { \n     //1-首先获取偏移量 \n     val  offsetMap :  mutable . Map [ TopicPartition ,   Long ]   =  OffsetUtil . getOffsetMap ( "spark_group" ,   "spark_kafka" ) \n     var  streamRDD :  InputDStream [ ConsumerRecord [ String ,   String ] ]   =   null \n     if   ( offsetMap . size  >   0 )   { \n      println ( "如果MySQL中有记录offset,则应该从该offset处开始消费" ) \n       //3-KafkaUtils.creatDriectlyStream的方法直接连接Kafka集群的分区 \n      streamRDD  =  KafkaUtils . createDirectStream [ String ,   String ] ( ssc , \n        LocationStrategies . PreferConsistent , \n        ConsumerStrategies . Subscribe [ String ,   String ] ( Array ( "spark_kafka" ) ,  kafkaParams ,  offsetMap ) ) \n     }   else   { \n      println ( "如果MySQL中没有记录offset,则直接连接,从latest开始消费" ) \n       //3-KafkaUtils.creatDriectlyStream的方法直接连接Kafka集群的分区 \n      streamRDD  =  KafkaUtils . createDirectStream [ String ,   String ] ( ssc , \n        LocationStrategies . PreferConsistent , \n        ConsumerStrategies . Subscribe [ String ,   String ] ( Array ( "spark_kafka" ) ,  kafkaParams ) ) \n     } \n     //实现获取offset然后手动提交offset \n    streamRDD . foreachRDD ( f  =>   { \n       if   ( f . count ( )   >   0 )   { \n         //这里仅仅是为了打印 \n         //println("rdd is:", f) \n        f . foreach ( record  =>   { \n          println ( "record result is:" ,  record ) \n           val  value :   String   =  record . value ( ) \n           //println("value is:", value) \n         } ) \n       }   //end id \n       //获取offset \n       val  offsetRanges :  Array [ OffsetRange ]   =  f . asInstanceOf [ HasOffsetRanges ] . offsetRanges\n       //这里仅仅是为了打印 \n       for   ( offsetRange  <-  offsetRanges )   { \n        println ( s "topic: ${ offsetRange . topic }  partition: ${ offsetRange . partition }  fromoffset: ${ offsetRange . fromOffset }  endoffset: ${ offsetRange . untilOffset } " ) \n       }   //end for \n       //提交offset,手动的方式默认提交到checkpoint的目录中 \n       //streamRDD.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges) \n       //手动保存在MySQL中 \n      OffsetUtil . saveOffsetRanges ( "spark_group" , offsetRanges ) \n     } ) \n     //4-获取record记录中的value的值 \n     val  mapValue :  DStream [ String ]   =  streamRDD . map ( _ . value ( ) ) \n     //5-根据value进行累加求和wordcount \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapValue\n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 \n \n \n \n \n \n \n \n 偏移量也可以保存至Zookeeper上或者Redis中，原因如下： \n l  1）、保存Zookeeper上：方便使用Kafka 监控工具管理Kafka 各个Topic被消费信息； \n l  2）、保存Redis上：从Redis读取数据和保存数据很快，基于内存数据库； \n 代码可以进一步优化，提高性能： 由于每批次数据结果RDD输出以后，都需要向MySQL数据库表更新偏移量数据，频繁连接数据库，建议构建数据库连接池，每次从池子中获取连接。 \n StructuredStreamig引入 \n StructuredStreamig定义 \n \n StructuredStreaming 结构化流 --处理实时数据(流式数据) \n 流式处理的方式 ：（1）原生的处理方式：来一条数据处理一条 结构化流和Flink（2） 微批次处理方式 ：SParkStreaming \n 底层数据结构 ：底层数据结构类似SparkSQL，使用DataFrame和DataSet \n StructuredStreaming与sparkstreaming区别 \n SparkStreaming问题四点 \n \n 1-SparkStreaing的时间是基于ProcessingTime处理时间，而不是EventTime事件时间 \n 2-SparkStreaming编程API偏底层，本质上就是要去构造RDD的DAG执行图 \n 3-SparkStreaming无法实现端到端的一致性, DStream 只能保证自己的一致性语义是 exactly-once 的 ，而 input 接入 Spark Streaming 和 Spark Straming 输出到外部存储的语义往往需要用户自己来保证； \n 4-SparkStreaming无法实现流批统一,有时候确实需要将的流处理逻辑运行到批数据上面,转化成RDD需要一定的工作量 \n \n StructuredStreaming应用场景 \n \n 结构化流解决SparkStreaing的问题 \n 1-结构化流是基于EventTime事件时间处理 \n 2-使用DataFrame和DataSet的API \n 3-结构化流实现从source到sink的端到端的一致性exactly-once \n 4-结构化流实现批处理和流式处理的统一 \n 结构化流使用两种API\n \n ProcesstimgTime--微批次处理场景 \n CotinuceTime-实时的场景 \n 结构化流编程模型 \n 入门案例 \n StructuredStreaming第一个案例-Socket \n \n \n 需求：实现wordcount案例 \n \n \n \n \n \n 分析： \n \n \n \n \n \n l 第一行表示从socket不断接收数据， \n l 第二行是时间轴，表示每隔1秒进行一次数据处理， \n l 第三行可以看成是之前提到的“unbound table"， \n l 第四行为最终的wordCounts是结果集。 \n \n \n 代码： \n \n \n package   cn . iitcast . structedstreaming \n\n import   java . util . concurrent . TimeUnit\n\n import   org . apache . commons . lang3 . StringUtils\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  Row ,  SparkSession } \n\n /**\n * DESC:\n * 1-首先准备sparksession，底层是sparksql的引擎\n * 2-读取socket的流式数据\n * 3-wordcount统计\n * 4-输出到控制台\n */ \n object  _01socketSource  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-首先准备sparksession，底层是sparksql的引擎 \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       //.set("spark.sql.shuffle.partitions","4") \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" , "4" ) \n       . getOrCreate ( ) \n     //spark.sparkContext.setLogLevel("WARN") \n     import   spark . implicits . _\n     //2-读取socket的流式数据 \n     val  streamData :  DataFrame  =  spark . readStream\n       . format ( "socket" ) \n       . option ( "host" ,   "node1" ) \n       . option ( "port" ,   9999 ) \n       . load ( ) \n    streamData . printSchema ( )   // |-- value: string (nullable = true) \n     //3-wordcount统计 \n     val  result :  Dataset [ Row ]   =  streamData\n       . as [ String ] \n       . filter ( StringUtils . isNoneBlank ( _ ) ) \n       . flatMap ( x  =>  x . split ( "\\\\s+" ) ) \n       . groupBy ( "value" ) \n       . count ( ) \n       . orderBy ( \'count . desc ) \n     //value.printSchema() \n     //4-输出到控制台 \n     val  query :  StreamingQuery  =  result . writeStream\n       . format ( "console" ) \n       //.outputMode("complete") \n       . outputMode ( OutputMode . Complete ( ) ) \n       . option ( "numRows" ,   5 ) \n       . option ( "truncate" ,   "false" ) \n       //.trigger(Trigger.ProcessingTime(10)) \n       . trigger ( Trigger . ProcessingTime ( 0 ) ) \n       . start ( )   //Starts the execution of the streaming query, \n\n    query . awaitTermination ( ) \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \n \n 总结： \n \n \n 一定是spark.sql.shuffle.partition分区个数默认为200需要设置小一些 \n StructuredStreaming文本数据源 \n \n \n 需求：通过文件夹不断放入文件然后 统计年龄小于25岁的人群的爱好排行榜。 \n \n \n 分析： \n \n \n 1-指定SparkSession \n \n \n 2-读取文本数据源 \n \n \n 3-执行统计 \n \n \n 4-输出到控制台 \n \n \n 5-query.awaitTermination \n \n \n 6-query.stop \n \n \n 代码： \n \n \n package   cn . iitcast . structedstreaming . filesource \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . types . { DataTypes ,  StructField ,  StructType } \n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n\n /**\n * DESC:\n */ \n object  _01FileSource  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-指定SparkSession \n     //1-首先准备sparksession，底层是sparksql的引擎 \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       . getOrCreate ( ) \n     //2-读取文本数据源---将不同名称的文件放入到文件夹中统计 \n     val  schema :  StructType  =  StructType ( \n      StructField ( "name" ,  DataTypes . StringType ,   true )   :: \n        StructField ( "age" ,  DataTypes . IntegerType ,   true )   :: \n        StructField ( "hobby" ,  DataTypes . StringType ,   true )   ::  Nil\n     ) \n       //思考：批处理如何读取数据设定查询条件？ \n     val  streamDF :  DataFrame  =  spark . readStream\n       . format ( "csv" ) \n       . option ( "sep" ,   ";" ) \n       . option ( "header" ,   "false" ) \n       . schema ( schema ) \n       . load ( "data/baseinput/struct/" ) \n    streamDF . printSchema ( ) \n     import   spark . implicits . _\n     //3-执行统计-统计年龄小于25岁的人群的爱好排行榜。 \n     val  result :  DataFrame  =  streamDF\n       . filter ( \'age   <   25 ) \n       . groupBy ( \'hobby ) \n       . count ( ) \n     //4-输出到控制台 \n     val  query :  StreamingQuery  =  result . writeStream\n       . format ( "console" ) \n       . outputMode ( OutputMode . Complete ( ) ) \n       . option ( "numRows" ,   10 ) \n       . option ( "truncate" ,   "false" ) \n       . trigger ( Trigger . ProcessingTime ( 0 ) ) \n       . start ( ) \n     //5-query.awaitTermination \n    query . awaitTermination ( ) \n     //6-query.stop \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 \n \n 总结： \n \n \n 读取文本数据源的参考 \n StructuredStreaming的Kakfa整合 \n \n \n 需求：通过StructuredStreaming结合Kakfa消费Kafka中的数据实现单词统计计数 \n \n \n 步骤： \n \n 1-准备上下文环境 \n 2-读取Kafka的数据 \n 3-将Kafka的数据转化，实现单词统计技术 \n 4-将得到结果写入控制台 \n 5.query.awaitTermination \n 6-query.stop \n \n \n \n 代码： \n \n \n package   cn . iitcast . structedstreaming . kafka \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  Row ,  SparkSession } \n\n /**\n * DESC:\n * * 1-准备上下文环境\n * * 2-读取Kafka的数据\n * * 3-将Kafka的数据转化，实现单词统计技术\n * * 4-将得到结果写入控制台\n * * 5.query.awaitTermination\n * * 6-query.stop\n */ \n object  _01KafkaSourceWordcount  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-准备上下文环境 \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       . getOrCreate ( ) \n     //spark.sparkContext.setLogLevel("WARN") \n     import   spark . implicits . _\n     //2-读取Kafka的数据 \n     val  streamDF :  DataFrame  =  spark . readStream\n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "subscribe" ,   "wordstopic" ) \n       . load ( ) \n     //streamDF.printSchema() \n     //root \n     // |-- key: binary (nullable = true) \n     // |-- value: binary (nullable = true) \n     // |-- topic: string (nullable = true) \n     // |-- partition: integer (nullable = true) \n     // |-- offset: long (nullable = true) \n     // |-- timestamp: timestamp (nullable = true) \n     // |-- timestampType: integer (nullable = true) \n     //3-将Kafka的数据转化，实现单词统计技术 \n     val  result :  Dataset [ Row ]   =  streamDF\n       . selectExpr ( "cast (value as string)" )   //因为kafka得到的数据是binary类型的数据需要使用cast转换 \n       . as [ String ] \n       . flatMap ( x  =>  x . split ( "\\\\s+" ) )   // |-- value: string (nullable = true) \n       . groupBy ( $ "value" ) \n       . count ( ) \n       . orderBy ( \'count . desc ) \n     //.groupBy("value") \n     //4-将得到结果写入控制台 \n     val  query :  StreamingQuery  =  result\n       . writeStream\n       . format ( "console" ) \n       . outputMode ( OutputMode . Complete ( ) ) \n       . trigger ( Trigger . ProcessingTime ( 0 ) ) \n       . option ( "numRows" ,   10 ) \n       . option ( "truncate" ,   false ) \n       . start ( ) \n     //5.query.awaitTermination \n    query . awaitTermination ( ) \n     //6-query.stop \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \n \n 总结： \n \n \n Kafka整合代码需要掌握 \n \n \n \n \n \n \n \n \n \n \n \n 数据结果写入kafka \n \n \n \n \n \n // Write key-value data from a DataFrame to a specific Kafka topic specified in an option \n val  ds  =  df\n   . selectExpr ( "CAST(key AS STRING)" ,   "CAST(value AS STRING)" ) \n   . writeStream\n   . format ( "kafka" ) \n   . option ( "kafka.bootstrap.servers" ,   "host1:port1,host2:port2" ) \n   . option ( "topic" ,   "topic1" ) \n   . start ( ) \n \n 1 2 3 4 5 6 7 8 #  运营商基站数据ETL \n \n \n 案例 \n \n \n \n \n \n 准备Kafka的topic \n实时ETL案例1 \n #查看topic信息 \n/export/server/kafka/bin/kafka-topics.sh  --list   --zookeeper  node1:2181\n #删除topic \n/export/server/kafka/bin/kafka-topics.sh  --delete   --zookeeper  node1:2181  --topic  stationTopic\n/export/server/kafka/bin/kafka-topics.sh  --delete   --zookeeper  node1:2181  --topic  etlTopic\n\n #创建topic \n/export/server/kafka/bin/kafka-topics.sh  --create   --zookeeper  node1:2181 --replication-factor  1   --partitions   3   --topic  stationTopic\n/export/server/kafka/bin/kafka-topics.sh  --create   --zookeeper  node1:2181 --replication-factor  1   --partitions   3   --topic  etlTopic\n\n #模拟生产者 \n/export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092  --topic  stationTopic\n/export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092  --topic  etlTopic\n\n #模拟消费者----------stationTopic \n/export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092  --topic  stationTopic --from-beginning\n/export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092  --topic  etlTopic --from-beginning\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \n \n Mock模拟一个Kafka的Producer生产Station数据 \n \n \n \n \n \n package   cn . iitcast . structedstreaming . kafka \n\n import   java . util . Properties\n\n import   org . apache . kafka . clients . producer . { KafkaProducer ,  ProducerRecord } \n import   org . apache . kafka . common . serialization . StringSerializer\n\n import   scala . util . Random\n\n /**\n * 模拟产生基站日志数据，实时发送Kafka Topic中，数据字段信息：\n * 基站标识符ID, 主叫号码, 被叫号码, 通话状态, 通话时间，通话时长\n */ \n object  MockStationLog  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 发送Kafka Topic \n     val  props  =   new  Properties ( ) \n    props . put ( "bootstrap.servers" ,   "node1:9092" ) \n    props . put ( "acks" ,   "1" ) \n    props . put ( "retries" ,   "3" ) \n    props . put ( "key.serializer" ,  classOf [ StringSerializer ] . getName ) \n    props . put ( "value.serializer" ,  classOf [ StringSerializer ] . getName ) \n     val  producer  =   new  KafkaProducer [ String ,   String ] ( props ) \n\n     val  random  =   new  Random ( ) \n     val  allStatus  =  Array ( \n       "fail" ,   "busy" ,   "barring" ,   "success" ,   "success" ,   "success" , \n       "success" ,   "success" ,   "success" ,   "success" ,   "success" ,   "success" \n     ) \n\n     while   ( true )   { \n       val  callOut :   String   =   "1860000%04d" . format ( random . nextInt ( 10000 ) ) \n       val  callIn :   String   =   "1890000%04d" . format ( random . nextInt ( 10000 ) ) \n       val  callStatus :   String   =  allStatus ( random . nextInt ( allStatus . length ) ) \n       val  callDuration  =   if   ( "success" . equals ( callStatus ) )   ( 1   +  random . nextInt ( 10 ) )   *   1000L   else   0L \n\n       // 随机产生一条基站日志数据 \n       val  stationLog :  StationLog  =  StationLog ( \n         "station_"   +  random . nextInt ( 10 ) , \n        callOut , \n        callIn , \n        callStatus , \n        System . currentTimeMillis ( ) , \n        callDuration\n       ) \n      println ( stationLog . toString ) \n      Thread . sleep ( 100   +  random . nextInt ( 100 ) ) \n\n       val  record  =   new  ProducerRecord [ String ,   String ] ( "stationTopic" ,  stationLog . toString ) \n      producer . send ( record ) \n     } \n\n    producer . close ( )   // 关闭连接 \n   } \n\n   /**\n   * 基站通话日志数据\n   */ \n   case   class  StationLog ( \n                         stationId :   String ,   //基站标识符ID \n                         callOut :   String ,   //主叫号码 \n                         callIn :   String ,   //被叫号码 \n                         callStatus :   String ,   //通话状态 \n                         callTime :   Long ,   //通话时间 \n                         duration :   Long   //通话时长 \n                        )   { \n     override   def  toString :   String   =   { \n       s " $ stationId , $ callOut , $ callIn , $ callStatus , $ callTime , $ duration " \n     } \n   } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 \n \n StationTopic开启消费者消费Producer产生的数据 \n \n \n 接着通过结构化流获取staiontopic的kafka数据，进行过滤 \n \n \n 步骤 \n \n 1-首先将Kafka两个Topic构建好 \n 2-启动模拟程序模拟生产者生产station数据到staiontopic中 \n 3-启动StructuredStreaming消费staiontopic的数据 \n 4-进行etl的操作---------过滤下标三个字段(callStatus=‘success’) \n 5-将过滤出来的数据写入到etltopic中 \n 6-查看etltopic的消费者查看结果是否正确 \n \n 代码 \n package   cn . iitcast . structedstreaming . kafka \n\n import   org . apache . commons . lang3 . StringUtils\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . streaming . StreamingQuery\n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  SparkSession } \n\n /**\n * DESC:\n */ \n object  _02StationDataProcess  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-首先将Kafka两个Topic构建好 \n     //1-准备上下文环境 \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     //2-启动模拟程序模拟生产者生产station数据到staiontopic中 \n     //3-启动StructuredStreaming消费staiontopic的数据 \n     val  streamDF :  DataFrame  =  spark . readStream\n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "subscribe" ,   "stationTopic" ) \n       . load ( ) \n     //4-进行etl的操作---------过滤下标三个字段(callStatus=‘success’) \n     val  result :  Dataset [ String ]   =  streamDF\n       . selectExpr ( "cast (value as string)" ) \n       . as [ String ] \n       . filter ( line  =>  StringUtils . isNotBlank ( line )   &&   "success" . equals ( line . trim . split ( "," ) ( 3 ) ) ) \n     //5-将过滤出来的数据写入到etltopic中 \n     val  query :  StreamingQuery  =  result . writeStream\n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "topic" ,   "etlTopic" ) \n       . option ( "checkpointLocation" , "data/baseoutput/checkpoint-2" ) \n       . start ( ) \n     //6-查看etltopic的消费者查看结果是否正确 \n    query . awaitTermination ( ) \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 结果 \n 物联网案例 \n \n \n IOT \n \n \n 基本配置 \n物联网设备案例 \n #查看topic信息 \n/export/server/kafka/bin/kafka-topics.sh  --list   --zookeeper  node1:2181\n #删除topic \n/export/server/kafka/bin/kafka-topics.sh  --delete   --zookeeper  node1:2181  --topic  iotTopic\n\n #创建topic \n/export/server/kafka/bin/kafka-topics.sh  --create   --zookeeper  node1:2181 --replication-factor  1   --partitions   3   --topic  iotTopic\n\n #模拟生产者 \n/export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092  --topic  iotTopic\n #模拟消费者 \n/export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092  --topic  iotTopic --from-beginning\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 \n \n 对物联网设备状态信号数据，实时统计分析: \n 1）、信号强度大于30的设备； \n 2）、各种设备类型的数量； \n 3）、各种设备类型的平均信号强度； \n \n \n 梳理流程： \n \n \n 1-首先准备生产者生产IOT的设备的数据，IOT可以开启topic的消费者查看是否成功消费 \n \n \n 2-使用StructuredStreaming读取Kafka的数据 \n \n \n 3-处理IOTTopic的数据--DSL&SQL \n \n \n 4-将数据结果写入控制台 \n \n \n 5-query.awaitTermination \n \n \n 6-query.stop \n \n \n 代码： \n \n \n SQL的实现 \n \n \n package   cn . iitcast . structedstreaming . iot \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . types . DoubleType\n\n /**\n * DESC:\n * 1-首先准备生产者生产IOT的设备的数据，IOT可以开启topic的消费者查看是否成功消费\n * 2-使用StructuredStreaming读取Kafka的数据\n * 3-处理IOTTopic的数据--DSL&SQL\n * 4-将数据结果写入控制台\n * 5-query.awaitTermination\n * 6-query.stop\n */ \n object  _01IOTStreamProcess  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-首先准备生产者生产IOT的设备的数据，IOT可以开启topic的消费者查看是否成功消费 \n     //2-使用StructuredStreaming读取Kafka的数据 \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     val  streamDF :  DataFrame  =  spark . readStream\n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "subscribe" ,   "iotTopic" ) \n       . load ( ) \n     //3-处理IOTTopic的数据--DSL&SQL \n     val  parseJsonData :  DataFrame  =  streamDF\n       . selectExpr ( "cast (value as string)" ) \n       . as [ String ] \n       //{"device":"device_10","deviceType":"db","signal":74.0,"time":1617612014757} \n       . select ( \n        get_json_object ( $ "value" ,   "$.device" ) . as ( "device" ) , \n        get_json_object ( $ "value" ,   "$.deviceType" ) . as ( "deviceType" ) , \n        get_json_object ( $ "value" ,   "$.signal" ) . cast ( DoubleType ) . as ( "signal" ) , \n        get_json_object ( $ "value" ,   "$.time" ) . as ( "time" ) \n       ) \n     //SQL:signal > 30 所有数据，按照设备类型 分组，统计数量、平均信号强度 \n    parseJsonData . createOrReplaceTempView ( "table_view" ) \n     val  sql  = \n       """\n        |select round(avg(signal),2) as avg_signal,count(deviceType) as device_counts\n        |from table_view\n        |where signal >30\n        |group by deviceType\n        |""" . stripMargin\n     val  result :  DataFrame  =  spark . sql ( sql ) \n     //4-将数据结果写入控制台 \n     val  query :  StreamingQuery  =  result . writeStream\n       . format ( "console" ) \n       . outputMode ( OutputMode . Complete ( ) ) \n       . option ( "numRows" ,   10 ) \n       . option ( "truncate" ,   false ) \n       . trigger ( Trigger . ProcessingTime ( 0 ) ) \n       . start ( ) \n     //5-query.awaitTermination \n    query . awaitTermination ( ) \n     //6-query.stop \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 \n 方法2：使用DSL方式 \n \n package   cn . iitcast . structedstreaming . iot \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . functions . _\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . types . DoubleType\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n\n /**\n * DESC:\n * 1-首先准备生产者生产IOT的设备的数据，IOT可以开启topic的消费者查看是否成功消费\n * 2-使用StructuredStreaming读取Kafka的数据\n * 3-处理IOTTopic的数据--DSL&SQL\n * 4-将数据结果写入控制台\n * 5-query.awaitTermination\n * 6-query.stop\n */ \n object  _02IOTStreamProcessDSL  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-首先准备生产者生产IOT的设备的数据，IOT可以开启topic的消费者查看是否成功消费 \n     //2-使用StructuredStreaming读取Kafka的数据 \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     val  streamDF :  DataFrame  =  spark . readStream\n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "subscribe" ,   "iotTopic" ) \n       . load ( ) \n     //3-处理IOTTopic的数据--DSL&SQL \n     val  parseJsonData :  DataFrame  =  streamDF\n       . selectExpr ( "cast (value as string)" ) \n       . as [ String ] \n       //{"device":"device_10","deviceType":"db","signal":74.0,"time":1617612014757} \n       . select ( \n        get_json_object ( $ "value" ,   "$.device" ) . as ( "device" ) , \n        get_json_object ( $ "value" ,   "$.deviceType" ) . as ( "deviceType" ) , \n        get_json_object ( $ "value" ,   "$.signal" ) . cast ( DoubleType ) . as ( "signal" ) , \n        get_json_object ( $ "value" ,   "$.time" ) . as ( "time" ) \n       ) \n     //SQL:signal > 30 所有数据，按照设备类型 分组，统计数量、平均信号强度 \n    parseJsonData . createOrReplaceTempView ( "table_view" ) \n     /* val sql =\n       """\n         |select round(avg(signal),2) as avg_signal,count(deviceType) as device_counts\n         |from table_view\n         |where signal >30\n         |group by deviceType\n         |""".stripMargin\n     val result: DataFrame = spark.sql(sql)*/ \n     val  result :  DataFrame  =  parseJsonData\n       //.select("signal", "deviceType") \n       . filter ( \'signal   >   30 ) \n       . groupBy ( "deviceType" ) \n       . agg ( \n        round ( avg ( "signal" ) ,   2 ) . as ( "avg_signal" ) , \n        count ( "deviceType" ) . as ( "device_counts" ) \n       ) \n\n     //4-将数据结果写入控制台 \n     val  query :  StreamingQuery  =  result . writeStream\n       . format ( "console" ) \n       . outputMode ( OutputMode . Complete ( ) ) \n       . option ( "numRows" ,   10 ) \n       . option ( "truncate" ,   false ) \n       . trigger ( Trigger . ProcessingTime ( 0 ) ) \n       . start ( ) \n     //5-query.awaitTermination \n    query . awaitTermination ( ) \n     //6-query.stop \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 \n \n 总结： \n \n \n 解析Json的方式可以采用SparkSQL的getJsonObject的方式 \n \n \n 更复杂的Json格式解析 \n \n \n 对于重复数据去重操作 \n \n 对于指定的字段或多个字段进行去重操作 \n StructureedStreaming的Foreach及ForeachBatch \n \n 结构化流写入MySQL中 \n \n \n Foreach \n \n \n 了解foreach \n \n \n 1-驱动包导入 \n \n \n   url="jdbc:mysql://localhost:3306/database_name"\n  驱动：com.mysql.jdbc.Driver\n  驱动包Maven:\n   \x3c!-- MySQL Client 5.1.38依赖 --\x3e \n   < dependency > \n  \t < groupId > mysql </ groupId > \n  \t < artifactId > mysql-connector-java </ artifactId > \n  \t < version > 5.1.38 </ version > \n   </ dependency > \n \n 1 2 3 4 5 6 7 8 9 \n 2-在MySQL中创建表 \n \n CREATE   TABLE   ` t_word `   ( \n     ` id `   int ( 11 )   NOT   NULL   AUTO_INCREMENT , \n     ` word `   varchar ( 255 )   NOT   NULL , \n     ` count `   int ( 11 )   DEFAULT   NULL , \n     PRIMARY   KEY   ( ` id ` ) , \n     UNIQUE   KEY   ` word `   ( ` word ` ) \n   )   ENGINE = InnoDB   AUTO_INCREMENT = 26   DEFAULT   CHARSET = utf8 ; \n\n \n 1 2 3 4 5 6 7 8 \n 2-JDBC的驱动类 \n \n //注意这里不要加format(“console”) \n   class  JDBCSink ( url :   String ,  username :   String ,  password :   String ) \n     extends  ForeachWriter [ Row ]   with  Serializable  { \n    var  connection :  Connection  =  _  //_表示占位符,后面会给变量赋值 \n     var  preparedStatement :  PreparedStatement  =  _\n\n     //开启连接 \n     override   def   open ( partitionId :   Long ,  version :   Long ) :   Boolean   =   { \n      connection  =  DriverManager . getConnection ( url ,  username ,  password ) \n       true \n     } \n\n     /*\n    CREATE TABLE `t_word` (\n        `id` int(11) NOT NULL AUTO_INCREMENT,\n        `word` varchar(255) NOT NULL,\n        `count` int(11) DEFAULT NULL,\n        PRIMARY KEY (`id`),\n        UNIQUE KEY `word` (`word`)\n      ) ENGINE=InnoDB AUTO_INCREMENT=26 DEFAULT CHARSET=utf8;\n     */ \n     //replace INTO `bigdata`.`t_word` (`id`, `word`, `count`) VALUES (NULL, NULL, NULL); \n     //处理数据--存到MySQL \n     override   def  process ( row :  Row ) :   Unit   =   { \n       val  word :   String   =  row . get ( 0 ) . toString\n       val  count :   String   =  row . get ( 1 ) . toString  //这可能需要转换为int，row.getLong() \n      println ( word  +   ":"   +  count ) \n       //REPLACE INTO:表示如果表中没有数据则插入,如果有数据则替换 \n       //注意:REPLACE INTO要求表有主键或唯一索引 \n       val  sql  =   "REPLACE INTO `t_word` (`id`, `word`, `count`) VALUES (NULL, ?, ?);" \n      preparedStatement  =  connection . prepareStatement ( sql ) \n      preparedStatement . setString ( 1 ,  word ) \n      preparedStatement . setInt ( 2 ,  Integer . parseInt ( count ) ) \n      preparedStatement . executeUpdate ( ) \n     } \n\n     //关闭资源 \n     override   def  close ( errorOrNull :  Throwable ) :   Unit   =   { \n       if   ( connection  !=   null )   { \n        connection . close ( ) \n       } \n       if   ( preparedStatement  !=   null )   { \n        preparedStatement . close ( ) \n       } \n     } \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \n 演示如何使用这个工具 \n \n package   cn . iitcast . structedstreaming . toMySQL \n\n import   java . sql . { Connection ,  DriverManager ,  PreparedStatement } \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . { ForeachWriter ,  Row ,  SparkSession } \n\n /**\n * DESC:\n * 1-首先申请资源\n * 2-通过Spark读取Socket的数据，实现wordcount\n * 3-统计wordcountt\n * 4-将wordcount写入MySQL\n * 5-query.awaitTermination\n * 6-query.stop\n */ \n object  _01ForeachWayToMySQL  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1-首先申请资源 \n     //1-首先准备sparksession，底层是sparksql的引擎 \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       . getOrCreate ( ) \n     //spark.sparkContext.setLogLevel("WARN") \n     import   spark . implicits . _\n     // 2-通过Spark读取Socket的数据，实现wordcount \n     val  lines  =  spark . readStream\n       . format ( "socket" ) \n       . option ( "host" ,   "node1" ) \n       . option ( "port" ,   9999 ) \n       . load ( ) \n      \n      \n     // 3-统计wordcount \n     // Split the lines into words \n     val  words  =  lines\n       . as [ String ] \n       . flatMap ( _ . split ( "\\\\s+" ) ) \n     // Generate running word count \n     val  wordCounts  =  words\n       . groupBy ( "value" ) \n       . count ( ) \n     val  jDBCSink  =   new  JDBCSink ( "jdbc:mysql://localhost:3306/bigdata?characterEncoding=UTF-8" ,   "root" ,   "root" ) \n     // 4-将wordcount写入MySQL \n     val  query :  StreamingQuery  =  wordCounts\n       . writeStream\n       . foreach ( jDBCSink ) \n       . outputMode ( OutputMode . Complete ( ) ) \n       . trigger ( Trigger . ProcessingTime ( 0 ) ) \n       . start ( ) \n     // 5-query.awaitTermination \n    query . awaitTermination ( ) \n     // 6-query.stop \n    query . stop ( ) \n   } \n\n   //注意这里不要加format(“console”) \n   class  JDBCSink ( url :   String ,  username :   String ,  password :   String ) \n     extends  ForeachWriter [ Row ]   with  Serializable  { \n     var  connection :  Connection  =  _  //_表示占位符,后面会给变量赋值 \n     var  preparedStatement :  PreparedStatement  =  _\n\n     //开启连接 \n     override   def   open ( partitionId :   Long ,  version :   Long ) :   Boolean   =   { \n      connection  =  DriverManager . getConnection ( url ,  username ,  password ) \n       true \n     } \n\n     /*\n    CREATE TABLE `t_word` (\n        `id` int(11) NOT NULL AUTO_INCREMENT,\n        `word` varchar(255) NOT NULL,\n        `count` int(11) DEFAULT NULL,\n        PRIMARY KEY (`id`),\n        UNIQUE KEY `word` (`word`)\n      ) ENGINE=InnoDB AUTO_INCREMENT=26 DEFAULT CHARSET=utf8;\n     */ \n     //replace INTO `bigdata`.`t_word` (`id`, `word`, `count`) VALUES (NULL, NULL, NULL); \n     //处理数据--存到MySQL \n     override   def  process ( row :  Row ) :   Unit   =   { \n       val  word :   String   =  row . get ( 0 ) . toString\n       val  count :   String   =  row . get ( 1 ) . toString  //这可能需要转换为int，row.getLong() \n      println ( word  +   ":"   +  count ) \n       //REPLACE INTO:表示如果表中没有数据则插入,如果有数据则替换 \n       //注意:REPLACE INTO要求表有主键或唯一索引 \n       val  sql  =   "REPLACE INTO `t_word` (`id`, `word`, `count`) VALUES (NULL, ?, ?);" \n      preparedStatement  =  connection . prepareStatement ( sql ) \n      preparedStatement . setString ( 1 ,  word ) \n      preparedStatement . setInt ( 2 ,  Integer . parseInt ( count ) ) \n      preparedStatement . executeUpdate ( ) \n     } \n\n     //关闭资源 \n     override   def  close ( errorOrNull :  Throwable ) :   Unit   =   { \n       if   ( connection  !=   null )   { \n        connection . close ( ) \n       } \n       if   ( preparedStatement  !=   null )   { \n        preparedStatement . close ( ) \n       } \n     } \n   } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 \n 上述的方法就是集成ForeachWriter的类实现其中open，process和close方法，通过foreach传入对应参数即可 \n \n ForeachBatch \n    package   cn . iitcast . structedstreaming . toMySQL \n  \n   import   java . sql . { Connection ,  DriverManager ,  PreparedStatement } \n  \n   import   org . apache . spark . SparkConf\n   import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n   import   org . apache . spark . sql . { DataFrame ,  ForeachWriter ,  Row ,  SaveMode ,  SparkSession } \n  \n   /**\n   * DESC:\n   * 1-首先申请资源\n   * 2-通过Spark读取Socket的数据，实现wordcount\n   * 3-统计wordcountt\n   * 4-将wordcount写入MySQL\n   * 5-query.awaitTermination\n   * 6-query.stop\n   */ \n   object  _02ForeeachBatchWayToMySQL  { \n     def  main ( args :  Array [ String ] ) :   Unit   =   { \n       // 1-首先申请资源 \n       //1-首先准备sparksession，底层是sparksql的引擎 \n       val  conf :  SparkConf  =   new  SparkConf ( ) \n         . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n         . setMaster ( "local[*]" ) \n       val  spark :  SparkSession  =  SparkSession\n         . builder ( ) \n         . config ( conf ) \n         . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n         . getOrCreate ( ) \n       //spark.sparkContext.setLogLevel("WARN") \n       import   spark . implicits . _\n       // 2-通过Spark读取Socket的数据，实现wordcount \n       val  lines  =  spark . readStream\n         . format ( "socket" ) \n         . option ( "host" ,   "node1" ) \n         . option ( "port" ,   9999 ) \n         . load ( ) \n        \n        \n       // 3-统计wordcount \n       // Split the lines into words \n       val  words  =  lines\n         . as [ String ] \n         . flatMap ( _ . split ( "\\\\s+" ) ) \n       // Generate running word count \n       val  wordCounts  =  words\n         . groupBy ( "value" ) \n         . count ( ) \n       // 4-将结果写入到MySQL中 \n       val  query :  StreamingQuery  =  wordCounts\n         . writeStream\n         . outputMode ( OutputMode . Complete ( ) ) \n         . foreachBatch ( ( data :  DataFrame ,  batchID :   Long )   =>   { \n          println ( "BatchId is:" ,  batchID ) \n          data\n             . coalesce ( 1 ) \n             . write\n             . mode ( SaveMode . Overwrite ) \n             . format ( "jdbc" ) \n             . option ( "driver" ,   "com.mysql.jdbc.Driver" ) \n             . option ( "url" ,   "jdbc:mysql://localhost:3306/bigdata?characterEncoding=UTF-8" ) \n             . option ( "user" ,   "root" ) \n             . option ( "password" ,   "root" ) \n             . option ( "dbtable" ,   "bigdata.tb_word_count2" ) \n             . save ( ) \n         } ) . start ( ) \n  \n       // 5-query.awaitTermination \n      query . awaitTermination ( ) \n       // 6-query.stop \n      query . stop ( ) \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 \n \n 需要注意的是foreach需要传入的是数据集本身，batchId \n \n \n 默认情况下，foreachBatch仅提供至少一次写保证。 但是，可以使用提供给该函数的batchId作为重复数据删除输出并获得一次性保证的方法。 \n foreachBatch不适用于连续处理模式，因为它从根本上依赖于流式查询的微批量执行。 如果以连续模式写入数据，请改用foreach。 \n 背压处理 \n 在结构化流中自动实现反压机制，控制kafka数据接入 \n spark.streaming积压 \n 事件时间--水印机制 \n \n \n \n \n \n SparkStreaming框架仅仅支持处理时间ProcessTime， \n StructuredStreaming支持事件时间和处理时间， \n Flink框架支持三种时间数据操作， \n \n \n 基于事件时间处理 \n \n \n \n \n \n \n \n \n \n \n \n 让Spark SQL引擎****自动追踪数据中当前事件时间EventTime，依据规则清除旧的状态数据****。 \n \n \n 图中的processing time是触发计算（多个未销毁的窗口）和水位线更新的时间，但不一定窗口会销毁，窗口销毁的时间是wartermark>=窗口的结束时间。 \n \n \n 加了水印之后就可以查到之前的数据，至于查多久，关注即将销毁的窗口数据（水位线在开始时间较小的那个窗口内，从这个窗口之后的聚合数据都还保存，只不过输出模式只输出有更新的key），销毁之后之前的窗口就不会再更新，意味着那个窗口的累计数据已经确定了。 \n \n \n \n 获取上一个窗口水印--\x3e计算该事件时间分布在哪个窗口(滑动窗口会有两个)，判断该事件窗口是否还没被销毁，没被销毁就聚合更新--\x3e有更新的窗口数据，等待按照update模式触发时间拉取输出 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n 代码 \n \n \n package   cn . iitcast . structedstreaming . kafka \n\n import   java . sql . Timestamp\n\n import   org . apache . commons . lang3 . StringUtils\n import   org . apache . spark . SparkContext\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n\n /**\n * 基于Structured Streaming 读取TCP Socket读取数据，事件时间窗口统计词频，将结果打印到控制台\n * 每5秒钟统计最近10秒内的数据（词频：WordCount)，设置水位Watermark时间为10秒\n * 2019-10-10 12:00:07,dog\n * 2019-10-10 12:00:08,owl\n *\n * 2019-10-10 12:00:14,dog\n * 2019-10-10 12:00:09,cat\n *\n * 2019-10-10 12:00:15,cat\n * 2019-10-10 12:00:08,dog\n * 2019-10-10 12:00:13,owl\n * 2019-10-10 12:00:21,owl\n *\n * 2019-10-10 12:00:04,donkey  --丢失\n * 2019-10-10 12:00:17,owl     --不丢失\n */ \n object  StructuredWindow  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1. 构建SparkSession实例对象，传递sparkConf参数 \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . config ( "spark.sql.shuffle.partitions" ,   "3" ) \n       . getOrCreate ( ) \n     val  sc :  SparkContext  =  spark . sparkContext\n    sc . setLogLevel ( "WARN" ) \n     import   org . apache . spark . sql . functions . _\n     import   spark . implicits . _\n\n     // 2. 使用SparkSession从TCP Socket读取流式数据 \n     val  inputStreamDF :  DataFrame  =  spark . readStream\n       . format ( "socket" ) \n       . option ( "host" ,   "node1" ) \n       . option ( "port" ,   9999 ) \n       . load ( ) \n\n     // 3. 针对获取流式DStream进行词频统计 \n     val  resultStreamDF  =  inputStreamDF\n       . as [ String ] \n       . filter ( StringUtils . isNotBlank ( _ ) ) \n       // 将每行数据进行分割单词: 2019-10-12 09:00:02,cat dog \n       . flatMap ( line  =>   { \n         val  arr  =  line . trim . split ( "," ) \n         val  timestampStr :   String   =  arr ( 0 ) \n         val  wordsStr :   String   =  arr ( 1 ) \n        wordsStr\n           . split ( "\\\\s+" ) \n           //(时间戳,单词) \n           . map ( ( Timestamp . valueOf ( timestampStr ) ,  _ ) ) \n       } ) \n       // 设置列的名称 \n       . toDF ( "timestamp" ,   "word" ) \n       // TODO：设置水位Watermark \n       . withWatermark ( "timestamp" ,   "10 seconds" ) \n       //依据事件时间生成窗口 \n       // TODO：设置基于事件时间（event time）窗口 -> time, 每5秒统计最近10秒内数据 \n       . groupBy ( \n           //structed streaming window 滑动步长不填默认窗口步长使用窗口长度，spark streaming 不填默认是使用 ssc 读取源文件每个batch 的步长，两者不一样。 \n        window ( $ "timestamp" ,   "10 seconds" ,   "5 seconds" ) , \n        $ "word" \n       ) . count ( ) \n       // 按照窗口字段降序排序 \n       //.orderBy($"window") \n\n     /*\n        root\n         |-- window: struct (nullable = true)\n         |    |-- start: timestamp (nullable = true)\n         |    |-- end: timestamp (nullable = true)\n         |-- word: string (nullable = true)\n         |-- count: long (nullable = false)\n     */ \n     //resultStreamDF.printSchema() \n\n     // 4. 将计算的结果输出，打印到控制台 \n     val  query :  StreamingQuery  =  resultStreamDF . writeStream\n       . outputMode ( OutputMode . Update ( ) ) \n       . format ( "console" ) \n       . option ( "numRows" ,   "100" ) \n       . option ( "truncate" ,   "false" ) \n       . trigger ( Trigger . ProcessingTime ( "5 seconds" ) ) \n       . start ( ) \n    query . awaitTermination ( ) \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 \n StructuredStreaming的Continue的连续处理机制 \n \n \n 连续处理（Continuous Processing）是Spark 2.3中引入的一种新的实验性流执行模式，可实现毫秒级端到端延迟(类似Storm、Flink)，但只能保证At-Least-Once 最少一次：即不会丢失数据，但可能会有重复结果。 \n 默认的微批处理（micro-batch processing）引擎，可以实现Exactly-Once 精确一次保证，但最多可实现100ms的延迟。 \n \n \n 结构化流提供了处于试验阶段的Continue的连续处理机制 \n \n \n 需要在writeStream.trigger(Trigger.ContinueTime()) \n \n \n import   org . apache . commons . lang3 . StringUtils\n import   org . apache . spark . SparkContext\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  SparkSession } \n\n /**\n * 从Spark 2.3版本开始，StructuredStreaming结构化流中添加新流式数据处理方式：Continuous processing\n * 持续流数据处理：当数据一产生就立即处理，类似Storm、Flink框架，延迟性达到100ms以下，目前属于实验开发阶段\n */ \n object  StructuredContinuous  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1. 构建SparkSession会话实例对象，设置属性信息 \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . config ( "spark.sql.shuffle.partitions" ,   "3" ) \n       . getOrCreate ( ) \n     val  sc :  SparkContext  =  spark . sparkContext\n    sc . setLogLevel ( "WARN" ) \n     import   org . apache . spark . sql . functions . _\n     import   spark . implicits . _\n\n     // 1. 从KAFKA读取数据 \n     val  kafkaStreamDF :  DataFrame  =  spark . readStream\n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "subscribe" ,   "stationTopic" ) \n       . load ( ) \n\n     // 2. 对基站日志数据进行ETL操作 \n     // station_0,18600004405,18900009049,success,1589711564033,9000 \n     val  etlStreamDF :  Dataset [ String ]   =  kafkaStreamDF\n       // 获取value字段的值，转换为String类型 \n       . selectExpr ( "CAST(value AS STRING)" ) \n       // 转换为Dataset类型 \n       . as [ String ] \n       // 过滤数据：通话状态为success \n       . filter ( log  =>  StringUtils . isNotBlank ( log )   &&   "success" . equals ( log . trim . split ( "," ) ( 3 ) ) ) \n\n     // 3. 针对流式应用来说，输出的是流 \n     val  query :  StreamingQuery  =  etlStreamDF . writeStream\n       . outputMode ( OutputMode . Append ( ) ) \n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "topic" ,   "etlTopic" ) \n       . option ( "checkpointLocation" ,   "./ckp"   +  System . currentTimeMillis ( ) ) \n       // TODO: 设置持续流处理 Continuous Processing, 指定CKPT时间间隔 \n       //the continuous processing engine will records the progress of the query every second \n       //持续流处理引擎，将每1秒中记录当前查询Query进度状态 \n       . trigger ( Trigger . Continuous ( "1 second" ) ) \n       . start ( ) \n    query . awaitTermination ( ) \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 \n 注意其中有很多限制，使用的时候需要注意 \n \n'},{frontmatter:{layout:"Tags",title:"Tags"},regularPath:"/tag/",key:"v-b1564aac",path:"/tag/",content:""},{frontmatter:{layout:"FrontmatterKey",title:"Categories"},regularPath:"/categories/",key:"v-ef9325c4",path:"/categories/",content:""},{frontmatter:{layout:"TimeLines",title:"Timeline"},regularPath:"/timeline/",key:"v-6319eb4e",path:"/timeline/",content:""},{frontmatter:{layout:"Tag",title:"markdown Tags"},regularPath:"/tag/markdown/",key:"v-3ae5b494",path:"/tag/markdown/",content:""},{frontmatter:{layout:"Tag",title:"推荐 Tags"},regularPath:"/tag/%E6%8E%A8%E8%8D%90/",key:"v-584666fc",path:"/tag/推荐/",content:""},{frontmatter:{layout:"Tag",title:"资源分享 Tags"},regularPath:"/tag/%E8%B5%84%E6%BA%90%E5%88%86%E4%BA%AB/",key:"v-23a8b635",path:"/tag/资源分享/",content:""},{frontmatter:{layout:"Tag",title:"消息队列 Tags"},regularPath:"/tag/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/",key:"v-c481210e",path:"/tag/消息队列/",content:""},{frontmatter:{layout:"Tag",title:"生产者-消费者 Tags"},regularPath:"/tag/%E7%94%9F%E4%BA%A7%E8%80%85-%E6%B6%88%E8%B4%B9%E8%80%85/",key:"v-598aa1ae",path:"/tag/生产者-消费者/",content:""},{frontmatter:{layout:"Tag",title:"云原生 Tags"},regularPath:"/tag/%E4%BA%91%E5%8E%9F%E7%94%9F/",key:"v-6b29cdd0",path:"/tag/云原生/",content:""},{frontmatter:{layout:"Tag",title:"容器技术 Tags"},regularPath:"/tag/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/",key:"v-7ed06156",path:"/tag/容器技术/",content:""},{frontmatter:{layout:"Tag",title:"远程过程调用 Tags"},regularPath:"/tag/%E8%BF%9C%E7%A8%8B%E8%BF%87%E7%A8%8B%E8%B0%83%E7%94%A8/",key:"v-1f026d94",path:"/tag/远程过程调用/",content:""},{frontmatter:{layout:"Tag",title:"索引 Tags"},regularPath:"/tag/%E7%B4%A2%E5%BC%95/",key:"v-036115ab",path:"/tag/索引/",content:""},{frontmatter:{layout:"Tag",title:"空间索引 Tags"},regularPath:"/tag/%E7%A9%BA%E9%97%B4%E7%B4%A2%E5%BC%95/",key:"v-6dd70f48",path:"/tag/空间索引/",content:""},{frontmatter:{layout:"Tag",title:"附近的人 Tags"},regularPath:"/tag/%E9%99%84%E8%BF%91%E7%9A%84%E4%BA%BA/",key:"v-5f2b2f45",path:"/tag/附近的人/",content:""},{frontmatter:{layout:"Tag",title:"k8s Tags"},regularPath:"/tag/k8s/",key:"v-32360c9a",path:"/tag/k8s/",content:""},{frontmatter:{layout:"Tag",title:"Flink Tags"},regularPath:"/tag/Flink/",key:"v-f57983ce",path:"/tag/Flink/",content:""},{frontmatter:{layout:"Tag",title:"beam Tags"},regularPath:"/tag/beam/",key:"v-1560bc14",path:"/tag/beam/",content:""},{frontmatter:{layout:"Tag",title:"k8s部署 Tags"},regularPath:"/tag/k8s%E9%83%A8%E7%BD%B2/",key:"v-53d8f7ba",path:"/tag/k8s部署/",content:""},{frontmatter:{layout:"Tag",title:"离线同步数据 Tags"},regularPath:"/tag/%E7%A6%BB%E7%BA%BF%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE/",key:"v-b6d24872",path:"/tag/离线同步数据/",content:""},{frontmatter:{layout:"Tag",title:"近实时 Tags"},regularPath:"/tag/%E8%BF%91%E5%AE%9E%E6%97%B6/",key:"v-70e3a6de",path:"/tag/近实时/",content:""},{frontmatter:{layout:"Tag",title:"linux命令 Tags"},regularPath:"/tag/linux%E5%91%BD%E4%BB%A4/",key:"v-70980f1d",path:"/tag/linux命令/",content:""},{frontmatter:{layout:"Tag",title:"CICD Tags"},regularPath:"/tag/CICD/",key:"v-18e4cd44",path:"/tag/CICD/",content:""},{frontmatter:{layout:"Tag",title:"持续集成部署 Tags"},regularPath:"/tag/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E9%83%A8%E7%BD%B2/",key:"v-9f88a2c0",path:"/tag/持续集成部署/",content:""},{frontmatter:{layout:"Tag",title:"pipeline Tags"},regularPath:"/tag/pipeline/",key:"v-6ac82e63",path:"/tag/pipeline/",content:""},{frontmatter:{layout:"Tag",title:"redis Tags"},regularPath:"/tag/redis/",key:"v-60190584",path:"/tag/redis/",content:""},{frontmatter:{layout:"Tag",title:"一致性hash Tags"},regularPath:"/tag/%E4%B8%80%E8%87%B4%E6%80%A7hash/",key:"v-7e42d028",path:"/tag/一致性hash/",content:""},{frontmatter:{layout:"Tag",title:"水平扩容 Tags"},regularPath:"/tag/%E6%B0%B4%E5%B9%B3%E6%89%A9%E5%AE%B9/",key:"v-257df835",path:"/tag/水平扩容/",content:""},{frontmatter:{layout:"Tag",title:"黑名单过滤 Tags"},regularPath:"/tag/%E9%BB%91%E5%90%8D%E5%8D%95%E8%BF%87%E6%BB%A4/",key:"v-38e037b7",path:"/tag/黑名单过滤/",content:""},{frontmatter:{layout:"Tag",title:"缓存穿透优化 Tags"},regularPath:"/tag/%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E4%BC%98%E5%8C%96/",key:"v-2ad4af63",path:"/tag/缓存穿透优化/",content:""},{frontmatter:{layout:"Tag",title:"分布式 Tags"},regularPath:"/tag/%E5%88%86%E5%B8%83%E5%BC%8F/",key:"v-08174efe",path:"/tag/分布式/",content:""},{frontmatter:{layout:"Tag",title:"一致性 Tags"},regularPath:"/tag/%E4%B8%80%E8%87%B4%E6%80%A7/",key:"v-35aa74ba",path:"/tag/一致性/",content:""},{frontmatter:{layout:"Tag",title:"增量更新 Tags"},regularPath:"/tag/%E5%A2%9E%E9%87%8F%E6%9B%B4%E6%96%B0/",key:"v-1aaea625",path:"/tag/增量更新/",content:""},{frontmatter:{layout:"Tag",title:"bisdiff/bispatch Tags"},regularPath:"/tag/bisdiff/bispatch/",key:"v-2370020b",path:"/tag/bisdiff/bispatch/",content:""},{frontmatter:{layout:"Tag",title:"序列化 Tags"},regularPath:"/tag/%E5%BA%8F%E5%88%97%E5%8C%96/",key:"v-84b6fb3e",path:"/tag/序列化/",content:""},{frontmatter:{layout:"Tag",title:"缓存 Tags"},regularPath:"/tag/%E7%BC%93%E5%AD%98/",key:"v-d5dea5f8",path:"/tag/缓存/",content:""},{frontmatter:{layout:"Tag",title:"一级缓存 Tags"},regularPath:"/tag/%E4%B8%80%E7%BA%A7%E7%BC%93%E5%AD%98/",key:"v-812754e8",path:"/tag/一级缓存/",content:""},{frontmatter:{layout:"Tag",title:"分布式id生成 Tags"},regularPath:"/tag/%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90/",key:"v-2a06d83f",path:"/tag/分布式id生成/",content:""},{frontmatter:{layout:"Tag",title:"kafka的高性能原理 Tags"},regularPath:"/tag/kafka%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD%E5%8E%9F%E7%90%86/",key:"v-5b26dd53",path:"/tag/kafka的高性能原理/",content:""},{frontmatter:{layout:"Tag",title:"服务器小文件传输 Tags"},regularPath:"/tag/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93/",key:"v-0dd50f9b",path:"/tag/服务器小文件传输/",content:""},{frontmatter:{layout:"Tag",title:"线程池设计 Tags"},regularPath:"/tag/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%AE%BE%E8%AE%A1/",key:"v-19522da4",path:"/tag/线程池设计/",content:""},{frontmatter:{layout:"Tag",title:"spring Tags"},regularPath:"/tag/spring/",key:"v-76ecf1d8",path:"/tag/spring/",content:""},{frontmatter:{layout:"Tag",title:"网络IO Tags"},regularPath:"/tag/%E7%BD%91%E7%BB%9CIO/",key:"v-7ed250eb",path:"/tag/网络IO/",content:""},{frontmatter:{layout:"Tag",title:"列式存储 Tags"},regularPath:"/tag/%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8/",key:"v-6367c596",path:"/tag/列式存储/",content:""},{frontmatter:{layout:"Tag",title:"搜索 Tags"},regularPath:"/tag/%E6%90%9C%E7%B4%A2/",key:"v-771b1f98",path:"/tag/搜索/",content:""},{frontmatter:{layout:"Tag",title:"倒排索引 Tags"},regularPath:"/tag/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/",key:"v-1fdf3cdf",path:"/tag/倒排索引/",content:""},{frontmatter:{layout:"Tag",title:"流批一体编程框架 Tags"},regularPath:"/tag/%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93%E7%BC%96%E7%A8%8B%E6%A1%86%E6%9E%B6/",key:"v-402a1f0e",path:"/tag/流批一体编程框架/",content:""},{frontmatter:{layout:"Tag",title:"实时计算 Tags"},regularPath:"/tag/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/",key:"v-134dedee",path:"/tag/实时计算/",content:""},{frontmatter:{layout:"Tag",title:"流批一体 Tags"},regularPath:"/tag/%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93/",key:"v-099ad802",path:"/tag/流批一体/",content:""},{frontmatter:{layout:"Tag",title:"离线计算 Tags"},regularPath:"/tag/%E7%A6%BB%E7%BA%BF%E8%AE%A1%E7%AE%97/",key:"v-dd643e04",path:"/tag/离线计算/",content:""},{frontmatter:{layout:"Tag",title:"sparksql Tags"},regularPath:"/tag/sparksql/",key:"v-093485d8",path:"/tag/sparksql/",content:""},{frontmatter:{layout:"Category",title:"tool Categories"},regularPath:"/categories/tool/",key:"v-638ddf39",path:"/categories/tool/",content:""},{frontmatter:{layout:"Category",title:"软件资源 Categories"},regularPath:"/categories/%E8%BD%AF%E4%BB%B6%E8%B5%84%E6%BA%90/",key:"v-b571f312",path:"/categories/软件资源/",content:""},{frontmatter:{layout:"Category",title:"随笔 Categories"},regularPath:"/categories/%E9%9A%8F%E7%AC%94/",key:"v-2bf76980",path:"/categories/随笔/",content:""},{frontmatter:{layout:"Category",title:"中间件 Categories"},regularPath:"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/",key:"v-ab31fcde",path:"/categories/中间件/",content:""},{frontmatter:{layout:"Category",title:"消息队列 Categories"},regularPath:"/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/",key:"v-c77fd4f6",path:"/categories/消息队列/",content:""},{frontmatter:{layout:"Category",title:"云原生 Categories"},regularPath:"/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/",key:"v-6f5d94e8",path:"/categories/云原生/",content:""},{frontmatter:{layout:"Category",title:"其他 Categories"},regularPath:"/categories/%E5%85%B6%E4%BB%96/",key:"v-7fc479ec",path:"/categories/其他/",content:""},{frontmatter:{layout:"Category",title:"算法 Categories"},regularPath:"/categories/%E7%AE%97%E6%B3%95/",key:"v-f37f30be",path:"/categories/算法/",content:""},{frontmatter:{layout:"Category",title:"数据同步 Categories"},regularPath:"/categories/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/",key:"v-5db6d2ed",path:"/categories/数据同步/",content:""},{frontmatter:{layout:"Category",title:"CICD Categories"},regularPath:"/categories/CICD/",key:"v-60c96f6a",path:"/categories/CICD/",content:""},{frontmatter:{layout:"Category",title:"存储引擎 Categories"},regularPath:"/categories/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/",key:"v-2dd62492",path:"/categories/存储引擎/",content:""},{frontmatter:{layout:"Category",title:"nosql Categories"},regularPath:"/categories/nosql/",key:"v-03f2e000",path:"/categories/nosql/",content:""},{frontmatter:{layout:"Category",title:"计算引擎 Categories"},regularPath:"/categories/%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/",key:"v-c1e9d86a",path:"/categories/计算引擎/",content:""}],themeConfig:{repo:"GordonChanFZ/gordonchanfz.github.io/",docsDir:"docs",docsBranch:"main",editLinks:!1,subSidebar:"auto",codeTheme:"solarizedlight",activeHeaderLinks:!1,nav:[{text:"首页",link:"/",icon:"reco-home"},{text:"计算引擎",items:[{text:"flink",link:"/计算引擎/flink.md"},{text:"spark",link:"/计算引擎/spark.md"}]},{text:"存储引擎",items:[{text:"ElasticSearch",link:"/存储引擎/ElasticSearch.md"},{text:"hbase",link:"/存储引擎/hbase.md"},{text:"redis",link:"/存储引擎/redis.md"}]},{text:"云原生",items:[{text:"Docker",link:"/云原生/Docker.md"}]},{text:"中间件",items:[{text:"kafka",link:"/中间件/kafka.md"},{text:"Sqoop基本使用",link:"/中间件/Sqoop基本使用.md"}]},{text:"工具",link:"/tool/emoji/我的常用emoji"},{text:"关于我",link:"/aboutme",icon:"reco-account"}],sidebar:{"/sql/":[{title:"Mysql",collapsable:!1,sidebarDepth:2,children:["mysql/MySQL的存储引擎有哪些？以及它们的对比和使用场景","mysql/MySQL索引","mysql/MySQL 数据库要用B+树存储索引？","mysql/MySQL的事务、隔离级别和MVCC原理","mysql/MySQL的锁机制","mysql/MySQL主从复制和分表分库原理","mysql/MySQL的慢查询和数据库优化","mysql/sql编程题","mysql/开窗函数"]},{title:"Redis",collapsable:!1,sidebarDepth:2,children:["redis/Redis底层数据结构","redis/Redis的发布订阅模式","redis/Redis面试题"]},{title:"ElasticSearch",collapsable:!1,sidebarDepth:2,children:["es/ElasticSearch技术方案——使用场景","es/ElasticSearch实现站内搜索","es/ES重点"]}],"/java/":[{title:"Java 基础",collapsable:!1,sidebarDepth:2,children:["basics/面向对象","basics/枚举","basics/常用类","basics/异常处理","basics/网络编程","basics/Java8新特性","basics/Java基础常见面试题"]},{title:"Java 集合",collapsable:!1,sidebarDepth:2,children:["collection/HashMap底层原理分析","collection/ConcurrentHashMap底层原理分析","collection/HashSet","collection/集合常见面试题"]},{title:"Java IO",collapsable:!1,sidebarDepth:2,children:["io/IO基础.md"]},{title:"Java 反射",collapsable:!1,sidebarDepth:2,children:["reflect/反射基础","reflect/反射总结","reflect/反射常见面试题","reflect/详解动态代理"]},{title:"Java 并发编程",collapsable:!1,sidebarDepth:2,children:["thread/多线程基础笔记","thread/多线程基础常见问题","thread/Java解决线程安全问题","thread/synchronized 总结","thread/volatile总结","thread/AQS原理和LOCK锁原理分析","thread/原子操作 与 Atomic原子类总结","thread/深入解析ThreadLocal","thread/死锁总结","thread/线程池总结"]},{title:"JVM",collapsable:!1,sidebarDepth:2,children:["jvm/JVM总结","jvm/JVM调试排错"]}],"/java相关框架/":[{title:"Spring",collapsable:!1,sidebarDepth:2,children:["spring family/Spring/Spring总结.md","spring family/Spring/Spring常见面试题.md"]},{title:"SpringMVC",collapsable:!1,sidebarDepth:2,children:["spring family/SpringMVC/SpringMVC.md"]},{title:"SpringBoot",collapsable:!1,sidebarDepth:2,children:["spring family/SpringBoot/SpringBoot自动装配和Starter原理.md","spring family/SpringBoot/SpringBoot常见面试题.md","spring family/SpringBoot/SpringBoot 配置获取request中body的json格式参数.md","spring family/SpringBoot/修改SpringBoot默认的启动图案.md","spring family/SpringBoot/SpringBoot 配置获取request中body的json格式参数.md","spring family/SpringBoot/SpringBoot 配置返回前端时间戳和接收时间戳自动转为LocalDateTime（前后端统一使用时间戳交互）.md"]},{title:"JWT",collapsable:!1,sidebarDepth:2,children:["JWT相关.md"]},{title:"Netty",collapsable:!1,sidebarDepth:2,children:["Netty/Netty总结.md","Netty/《Netty权威指南2》笔记.md","Netty/Netty面试题.md","Netty/WebSocket协议.md"]}],"/大数据/":[{title:"Hadoop",collapsable:!1,sidebarDepth:2,children:["Hadoop/Hadoop概述","Hadoop/HDFS总结","Hadoop/MapReduce总结","Hadoop/Yarn总结"]},{title:"Hive",collapsable:!1,sidebarDepth:2,children:["Hive/Hive概述","Hive/Hive的DDL和DML语句","Hive/Hive的分区表和分桶表","Hive/Hive的常用函数和压缩存储","Hive/Hive实战"]},{title:"HBase",collapsable:!1,sidebarDepth:2,children:["HBase/HBase"]},{title:"Flink",collapsable:!1,sidebarDepth:2,children:["Flink/flink"]}],"/算法与数据结构/":[{title:"数据结构",collapsable:!1,sidebarDepth:2,children:["数据结构/线性表","数据结构/二分查找法","数据结构/稀疏数组","数据结构/位运算总结","数据结构/栈和队列","数据结构/两个栈实现队列和两个队列实现栈","数据结构/树"]},{title:"算法技巧",collapsable:!1,sidebarDepth:2,children:["算法技巧/01数组","算法技巧/02链表","算法技巧/03队列和栈","算法技巧/04字符串","算法技巧/05二叉树","算法技巧/06DFS","算法技巧/07回溯","算法技巧/08贪心","算法技巧/09动态规划","算法技巧/10数据结构设计","算法技巧/11BFS","算法技巧/12其他"]},{title:"codetop",collapsable:!1,sidebarDepth:2,children:["codetop/codetop(21-40)"]},{title:"《剑指offer》",collapsable:!1,sidebarDepth:2,children:["《剑指offer》总结"]}],"/架构/":[{title:"微服务",collapsable:!1,sidebarDepth:2,children:["微服务/微服务架构 总览"]},{title:"分布式ID",collapsable:!1,sidebarDepth:2,children:["分布式ID/分布式ID方案","分布式ID/Redis实现分布式ID","分布式ID/ZooKeeper实现分布式ID"]},{title:"分布式锁",collapsable:!1,sidebarDepth:2,children:["分布式锁/分布式锁"]},{title:"分布式事务",collapsable:!1,sidebarDepth:2,children:["分布式事务/分布式事务方案"]},{title:"OAuth2.0",collapsable:!1,sidebarDepth:2,children:["OAuth2.0/理解OAuth2.0"]},{title:"云原生",collapsable:!1,sidebarDepth:2,children:["云原生/云原生架构和K8S"]},{title:"其他",collapsable:!1,sidebarDepth:2,children:["高可用方案","Gateway实现动态路由和灰度发布"]}]},type:"blog",blogConfig:{socialLinks:[{icon:"reco-github",link:"https://gordonchanfz.github.io/"}]},logo:"/logo.png",search:!0,searchMaxSuggestions:10,lastUpdated:"Last Updated",author:"Gordon",authorAvatar:"/avatar.jpeg",record:"xxxxxx号-1",startYear:"2018"},locales:{"/":{lang:"zh-CN",title:"个人博客",path:"/"}}};var Me={data:()=>({username:"",password:""}),methods:{login(){if(this.username&&this.password){var n={user1:"password1",user2:"password2"},e=!1;for(var t in n){var r=t,a=n[t];if(this.username===r&&this.password===a){e=!0;break}}if(e){const n=JSON.stringify({name:this.username,time:(new Date).getTime()});window.localStorage.setItem("user_auth_xxxxxxxxxxxx",n),this.$emit("close",!0)}else this.$dlg.alert("抱歉，账号密码不对",{messageType:"warning"})}else this.$dlg.alert("请输入账号密码",{messageType:"warning"})}}},je=(t(275),Object(Ce.a)(Me,(function(){var n=this,e=n._self._c;return e("div",{staticClass:"login-form"},[e("div",{staticClass:"form-header"},[n._v("账号")]),n._v(" "),e("div",[e("input",{directives:[{name:"model",rawName:"v-model",value:n.username,expression:"username"}],staticClass:"form-control",attrs:{type:"text"},domProps:{value:n.username},on:{input:function(e){e.target.composing||(n.username=e.target.value)}}})]),n._v(" "),e("div",{staticClass:"form-header"},[n._v("密码")]),n._v(" "),e("div",[e("input",{directives:[{name:"model",rawName:"v-model",value:n.password,expression:"password"}],staticClass:"form-control",attrs:{type:"password"},domProps:{value:n.password},on:{input:function(e){e.target.composing||(n.password=e.target.value)}}})]),n._v(" "),e("div",{staticClass:"btn-row"},[e("button",{staticClass:"btn",on:{click:n.login}},[n._v("\n      登录\n    ")])])])}),[],!1,null,null,null).exports),Le=(t(17),t(28)),Ne={computed:{$recoPosts(){let n=this.$site.pages;return n=Object(Le.a)(n,!1),Object(Le.c)(n),n},$recoPostsForTimeline(){let n=this.$recoPosts;const e={},t=[];n=Object(Le.a)(n,!0),this.pages=0==n.length?[]:n;for(let t=0,r=n.length;t<r;t++){const r=n[t],a=$e(r.frontmatter.date,"year");e[a]?e[a].push(r):e[a]=[r]}for(const n in e){const r=e[n];Object(Le.b)(r),t.unshift({year:n,data:r})}return t},$categoriesList(){return this.$categories.list.map(n=>(n.pages=n.pages.filter(n=>!1!==n.frontmatter.publish),n))},$tagesList(){return this.$tags.list.map(n=>(n.pages=n.pages.filter(n=>!1!==n.frontmatter.publish),n))},$showSubSideBar(){const{$themeConfig:{subSidebar:n,sidebar:e},$frontmatter:{subSidebar:t,sidebar:r}}=this,a=this.$page.headers||[];return!([t,r].indexOf(!1)>-1)&&([t,r].indexOf("auto")>-1&&a.length>0||[n,e].indexOf("auto")>-1&&a.length>0)}}};function $e(n,e){n=function(n){var e=new Date(n).toJSON();return new Date(+new Date(e)+288e5).toISOString().replace(/T/g," ").replace(/\.[\d]{3}Z/,"").replace(/-/g,"/")}(n);const t=new Date(n),r=t.getFullYear(),a=t.getMonth()+1,o=t.getDate();return"year"==e?r:`${a}-${o}`}var Ue={all:"全部",article:"文章",tag:"标签",category:"分类",friendLink:"友情链接",timeLine:"时间轴",timeLineMsg:"昨日重现！"},ze={all:"全部",article:"文章",tag:"標簽",category:"分類",friendLink:"友情鏈接",timeLine:"時間軸",timeLineMsg:"昨日重現！"},He={all:"All",article:"Articles",tag:"Tags",category:"Categories",friendLink:"Friend Links",timeLine:"TimeLine",timeLineMsg:"Yesterday Once More!"},qe={all:"全部",article:"文章",tag:"ラベル",category:"分類",friendLink:"友情リンク",timeLine:"タイムライン",timeLineMsg:"昨日また！"},Ve={all:"전체",article:"글",tag:"태그",category:"분류",friendLink:"링크 참조",timeLine:"타임 라인",timeLineMsg:"어제 또!"},Ke={all:"Todas",article:"Artículos",tag:"Etiquetas",category:"Categorías",friendLink:"Páginas amigas",timeLine:"Cronología",timeLineMsg:"¡Ayer otra vez!"},We={computed:{$recoLocales(){const n=this.$themeLocaleConfig.recoLocales||{};return/^zh\-(CN|SG)$/.test(this.$lang)?{...Ue,...n}:/^zh\-(HK|MO|TW)$/.test(this.$lang)?{...ze,...n}:/^ja\-JP$/.test(this.$lang)?{...qe,...n}:/^ko\-KR$/.test(this.$lang)?{...Ve,...n}:/^es(\-[A-Z]+)?$/.test(this.$lang)?{...Ke,...n}:{...He,...n}}}},Ge=t(42);t(276);r.b.component("tongji",()=>Promise.all([t.e(0),t.e(33)]).then(t.bind(null,1564))),r.b.component("Badge",()=>Promise.all([t.e(0),t.e(35)]).then(t.bind(null,1607)));var Je={name:"BackToTop",data:()=>({visible:!1,customStyle:{right:"1rem",bottom:"6rem",width:"2.5rem",height:"2.5rem","border-radius":".25rem","line-height":"2.5rem"},visibilityHeight:400}),mounted(){window.addEventListener("scroll",this.throttle(this.handleScroll,500))},beforeDestroy(){window.removeEventListener("scroll",this.throttle(this.handleScroll,500))},methods:{handleScroll(){this.visible=window.pageYOffset>this.visibilityHeight},backToTop(){window.scrollTo(0,0)},throttle(n,e){let t=null,r=Date.now();return function(){const a=Date.now(),o=e-(a-r),i=this,s=arguments;clearTimeout(t),o<=0?(n.apply(i,s),r=Date.now()):t=setTimeout(n,o)}}}},Ye=(t(277),Object(Ce.a)(Je,(function(){var n=this._self._c;return n("transition",{attrs:{name:"fade"}},[n("div",{directives:[{name:"show",rawName:"v-show",value:this.visible,expression:"visible"}],staticClass:"back-to-ceiling",style:this.customStyle,on:{click:this.backToTop}},[n("svg",{staticClass:"icon",attrs:{t:"1574745035067",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"5404"}},[n("path",{attrs:{d:"M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z","p-id":"5405"}}),n("path",{attrs:{d:"M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z","p-id":"5406"}})])])])}),[],!1,null,"c6073ba8",null).exports);t(278),t(284);const Qe={prev:"上一页",next:"下一页",go:"前往",jump:"跳转至"},Xe={prev:"上壹頁",next:"下壹頁",go:"前往",jump:"跳轉至"},Ze={prev:"Prev",next:"Next",go:"Go",jump:"Jump To"},nt={prev:"前のページ",next:"次のページ",go:"へ",jump:"ジャンプ"},et={prev:"이전 페이지",next:"다음 페이지",go:"행",jump:"건너뛰기"};var tt={data:()=>({changePage:"",pageSize:10}),props:{total:{type:Number,default:10},perPage:{type:Number,default:10},currentPage:{type:Number,default:1}},computed:{pages(){return Math.ceil(this.total/this.pageSize)},efont:function(){return this.pages>7},indexes:function(){var n=1,e=this.pages,t=[];for(this.pages>=7&&(this.currentPage>5&&this.currentPage<this.pages-4?(n=Number(this.currentPage)-3,e=Number(this.currentPage)+3):this.currentPage<=5?(n=1,e=7):(e=this.pages,n=this.pages-6));n<=e;)t.push(n),n++;return t},pagationLocales(){return function(n){const{$lang:e,$recoLocales:{pagation:t}={}}=n;return t||(/^zh\-(CN|SG)$/.test(e)?Qe:/^zh\-(HK|MO|TW)$/.test(e)?Xe:/^ja\-JP$/.test(e)?nt:/^ko\-KR$/.test(e)?et:Ze)}(this)},showStartFakePageNum:function(){return this.efont&&!this.indexes.includes(1)},showLastFakePageNum:function(){return this.efont&&!this.indexes.includes(this.pages)}},methods:{goPrev(){let n=this.currentPage;this.currentPage>1&&this.emit(--n)},goNext(){let n=this.currentPage;n<this.pages&&this.emit(++n)},jumpPage:function(n){const e=parseInt(n);e<=this.pages&&e>0?this.emit(e):alert(`请输入大于0，并且小于等于${this.pages}的页码！`)},emit(n){this.$emit("getCurrentPage",n)}}},rt=(t(285),Object(Ce.a)(tt,(function(){var n=this,e=n._self._c;return e("div",{directives:[{name:"show",rawName:"v-show",value:n.pages>1,expression:"pages > 1"}],staticClass:"pagation"},[e("div",{staticClass:"pagation-list"},[e("span",{directives:[{name:"show",rawName:"v-show",value:n.currentPage>1,expression:"currentPage > 1"}],staticClass:"jump",attrs:{unselectable:"on"},on:{click:n.goPrev}},[n._v(n._s(n.pagationLocales.prev))]),n._v(" "),n.showStartFakePageNum?e("span",{staticClass:"jump",on:{click:function(e){return n.jumpPage(1)}}},[n._v("1")]):n._e(),n._v(" "),n.showStartFakePageNum&&n.indexes[0]>2?e("span",{staticClass:"ellipsis"},[n._v("...")]):n._e(),n._v(" "),n._l(n.indexes,(function(t){return e("span",{key:t,staticClass:"jump",class:{bgprimary:n.currentPage==t},on:{click:function(e){return n.jumpPage(t)}}},[n._v(n._s(t))])})),n._v(" "),n.showLastFakePageNum&&n.pages-n.indexes.at(-1)>1?e("span",{staticClass:"ellipsis"},[n._v("...")]):n._e(),n._v(" "),n.showLastFakePageNum?e("span",{staticClass:"jump",on:{click:function(e){return n.jumpPage(n.pages)}}},[n._v(n._s(n.pages))]):n._e(),n._v(" "),n.currentPage<n.pages?e("span",{staticClass:"jump",on:{click:n.goNext}},[n._v(n._s(n.pagationLocales.next))]):n._e(),n._v(" "),e("span",{staticClass:"jumppoint"},[n._v(n._s(n.pagationLocales.jump))]),n._v(" "),e("span",{staticClass:"jumpinp"},[e("input",{directives:[{name:"model",rawName:"v-model",value:n.changePage,expression:"changePage"}],attrs:{type:"text"},domProps:{value:n.changePage},on:{input:function(e){e.target.composing||(n.changePage=e.target.value)}}})]),n._v(" "),e("span",{staticClass:"jump gobtn",on:{click:function(e){return n.jumpPage(n.changePage)}}},[n._v(n._s(n.pagationLocales.go))])],2)])}),[],!1,null,"22b6649c",null).exports),at={name:"Valine",props:{options:{type:Object,default:()=>({})}},mounted:function(){this.initValine()},methods:{initValine(){new(t(286))({el:"#valine",placeholder:"just go go",notify:!1,verify:!1,avatar:"retro",visitor:!0,recordIP:!1,path:window.location.pathname,...this.options})}},watch:{$route(n,e){n.path!==e.path&&setTimeout(()=>{this.initValine()},300)}}},ot=(t(287),Object(Ce.a)(at,(function(){this._self._c;return this._m(0)}),[function(){var n=this._self._c;return n("div",{staticClass:"valine-wrapper"},[n("div",{attrs:{id:"valine"}})])}],!1,null,null,null).exports),it=t(2);function st(n){return Object(it.a)((function(e,t){void 0===e.inject&&(e.inject={}),Array.isArray(e.inject)||(e.inject[t]=n||t)}))}function lt(n){return"function"!=typeof n||!n.managed&&!n.managedReactive}function ct(n){var e=function(){var t=this,r="function"==typeof n?n.call(this):n;for(var a in(r=Object.create(r||null))[pt]=Object.create(this[pt]||{}),e.managed)r[e.managed[a]]=this[a];var o=function(n){r[e.managedReactive[n]]=i[n],Object.defineProperty(r[pt],e.managedReactive[n],{enumerable:!0,configurable:!0,get:function(){return t[n]}})},i=this;for(var a in e.managedReactive)o(a);return r};return e.managed={},e.managedReactive={},e}var pt="__reactiveInject__";function ut(n){Array.isArray(n.inject)||(n.inject=n.inject||{},n.inject[pt]={from:pt,default:{}})}var dt="undefined"!=typeof Reflect&&void 0!==Reflect.getMetadata;function mt(n,e,t){if(dt&&!Array.isArray(n)&&"function"!=typeof n&&!n.hasOwnProperty("type")&&void 0===n.type){var r=Reflect.getMetadata("design:type",e,t);r!==Object&&(n.type=r)}}function gt(n){return void 0===n&&(n={}),function(e,t){mt(n,e,t),Object(it.a)((function(e,t){(e.props||(e.props={}))[t]=n}))(e,t)}}function ft(n,e){void 0===e&&(e={});var t=e.deep,r=void 0!==t&&t,a=e.immediate,o=void 0!==a&&a;return Object(it.a)((function(e,t){"object"!=typeof e.watch&&(e.watch=Object.create(null));var a=e.watch;"object"!=typeof a[n]||Array.isArray(a[n])?void 0===a[n]&&(a[n]=[]):a[n]=[a[n]],a[n].push({handler:t,deep:r,immediate:o})}))}var ht=t(21);const vt=n=>Object(ht.stringify)(n),bt=(n,e)=>`${n}${Object(ht.stringify)(e,{addQueryPrefix:!0})}`,kt=(n,e)=>`${n.replace(/\/$/,"")}/${e.replace(/^\//,"")}`;var yt=t(134),St=t.n(yt);const xt=n=>St()(n,"YYYY-MM-DD HH:mm:ss"),wt=n=>(n.split("#")[0]||"").split("?")[0]||"",Et=n=>Object(ht.parse)(n,{ignoreQueryPrefix:!0})
/*!
 * vue-i18n v8.28.2 
 * (c) 2022 kazuya kawaguchi
 * Released under the MIT License.
 */;var Dt=["compactDisplay","currency","currencyDisplay","currencySign","localeMatcher","notation","numberingSystem","signDisplay","style","unit","unitDisplay","useGrouping","minimumIntegerDigits","minimumFractionDigits","maximumFractionDigits","minimumSignificantDigits","maximumSignificantDigits"],Ct=["dateStyle","timeStyle","calendar","localeMatcher","hour12","hourCycle","timeZone","formatMatcher","weekday","era","year","month","day","hour","minute","second","timeZoneName"];function It(n,e){"undefined"!=typeof console&&(console.warn("[vue-i18n] "+n),e&&console.warn(e.stack))}var Tt=Array.isArray;function Ot(n){return null!==n&&"object"==typeof n}function At(n){return"string"==typeof n}var _t=Object.prototype.toString;function Rt(n){return"[object Object]"===_t.call(n)}function Pt(n){return null==n}function Ft(n){return"function"==typeof n}function Bt(){for(var n=[],e=arguments.length;e--;)n[e]=arguments[e];var t=null,r=null;return 1===n.length?Ot(n[0])||Tt(n[0])?r=n[0]:"string"==typeof n[0]&&(t=n[0]):2===n.length&&("string"==typeof n[0]&&(t=n[0]),(Ot(n[1])||Tt(n[1]))&&(r=n[1])),{locale:t,params:r}}function Mt(n){return JSON.parse(JSON.stringify(n))}function jt(n,e){return!!~n.indexOf(e)}var Lt=Object.prototype.hasOwnProperty;function Nt(n,e){return Lt.call(n,e)}function $t(n){for(var e=arguments,t=Object(n),r=1;r<arguments.length;r++){var a=e[r];if(null!=a){var o=void 0;for(o in a)Nt(a,o)&&(Ot(a[o])?t[o]=$t(t[o],a[o]):t[o]=a[o])}}return t}function Ut(n,e){if(n===e)return!0;var t=Ot(n),r=Ot(e);if(!t||!r)return!t&&!r&&String(n)===String(e);try{var a=Tt(n),o=Tt(e);if(a&&o)return n.length===e.length&&n.every((function(n,t){return Ut(n,e[t])}));if(a||o)return!1;var i=Object.keys(n),s=Object.keys(e);return i.length===s.length&&i.every((function(t){return Ut(n[t],e[t])}))}catch(n){return!1}}function zt(n){return null!=n&&Object.keys(n).forEach((function(e){"string"==typeof n[e]&&(n[e]=n[e].replace(/</g,"&lt;").replace(/>/g,"&gt;").replace(/"/g,"&quot;").replace(/'/g,"&apos;"))})),n}var Ht={name:"i18n",functional:!0,props:{tag:{type:[String,Boolean,Object],default:"span"},path:{type:String,required:!0},locale:{type:String},places:{type:[Array,Object]}},render:function(n,e){var t=e.data,r=e.parent,a=e.props,o=e.slots,i=r.$i18n;if(i){var s=a.path,l=a.locale,c=a.places,p=o(),u=i.i(s,l,function(n){var e;for(e in n)if("default"!==e)return!1;return Boolean(e)}(p)||c?function(n,e){var t=e?function(n){0;return Array.isArray(n)?n.reduce(Vt,{}):Object.assign({},n)}(e):{};if(!n)return t;var r=(n=n.filter((function(n){return n.tag||""!==n.text.trim()}))).every(Kt);0;return n.reduce(r?qt:Vt,t)}(p.default,c):p),d=a.tag&&!0!==a.tag||!1===a.tag?a.tag:"span";return d?n(d,t,u):u}}};function qt(n,e){return e.data&&e.data.attrs&&e.data.attrs.place&&(n[e.data.attrs.place]=e),n}function Vt(n,e,t){return n[t]=e,n}function Kt(n){return Boolean(n.data&&n.data.attrs&&n.data.attrs.place)}var Wt,Gt={name:"i18n-n",functional:!0,props:{tag:{type:[String,Boolean,Object],default:"span"},value:{type:Number,required:!0},format:{type:[String,Object]},locale:{type:String}},render:function(n,e){var t=e.props,r=e.parent,a=e.data,o=r.$i18n;if(!o)return null;var i=null,s=null;At(t.format)?i=t.format:Ot(t.format)&&(t.format.key&&(i=t.format.key),s=Object.keys(t.format).reduce((function(n,e){var r;return jt(Dt,e)?Object.assign({},n,((r={})[e]=t.format[e],r)):n}),null));var l=t.locale||o.locale,c=o._ntp(t.value,l,i,s),p=c.map((function(n,e){var t,r=a.scopedSlots&&a.scopedSlots[n.type];return r?r(((t={})[n.type]=n.value,t.index=e,t.parts=c,t)):n.value})),u=t.tag&&!0!==t.tag||!1===t.tag?t.tag:"span";return u?n(u,{attrs:a.attrs,class:a.class,staticClass:a.staticClass},p):p}};function Jt(n,e,t){Xt(n,t)&&Zt(n,e,t)}function Yt(n,e,t,r){if(Xt(n,t)){var a=t.context.$i18n;(function(n,e){var t=e.context;return n._locale===t.$i18n.locale})(n,t)&&Ut(e.value,e.oldValue)&&Ut(n._localeMessage,a.getLocaleMessage(a.locale))||Zt(n,e,t)}}function Qt(n,e,t,r){if(t.context){var a=t.context.$i18n||{};e.modifiers.preserve||a.preserveDirectiveContent||(n.textContent=""),n._vt=void 0,delete n._vt,n._locale=void 0,delete n._locale,n._localeMessage=void 0,delete n._localeMessage}else It("Vue instance does not exists in VNode context")}function Xt(n,e){var t=e.context;return t?!!t.$i18n||(It("VueI18n instance does not exists in Vue instance"),!1):(It("Vue instance does not exists in VNode context"),!1)}function Zt(n,e,t){var r,a,o=function(n){var e,t,r,a;At(n)?e=n:Rt(n)&&(e=n.path,t=n.locale,r=n.args,a=n.choice);return{path:e,locale:t,args:r,choice:a}}(e.value),i=o.path,s=o.locale,l=o.args,c=o.choice;if(i||s||l)if(i){var p=t.context;n._vt=n.textContent=null!=c?(r=p.$i18n).tc.apply(r,[i,c].concat(nr(s,l))):(a=p.$i18n).t.apply(a,[i].concat(nr(s,l))),n._locale=p.$i18n.locale,n._localeMessage=p.$i18n.getLocaleMessage(p.$i18n.locale)}else It("`path` is required in v-t directive");else It("value type not supported")}function nr(n,e){var t=[];return n&&t.push(n),e&&(Array.isArray(e)||Rt(e))&&t.push(e),t}function er(n,e){void 0===e&&(e={bridge:!1}),er.installed=!0;var t;(Wt=n).version&&Number(Wt.version.split(".")[0]);(t=Wt).prototype.hasOwnProperty("$i18n")||Object.defineProperty(t.prototype,"$i18n",{get:function(){return this._i18n}}),t.prototype.$t=function(n){for(var e=[],t=arguments.length-1;t-- >0;)e[t]=arguments[t+1];var r=this.$i18n;return r._t.apply(r,[n,r.locale,r._getMessages(),this].concat(e))},t.prototype.$tc=function(n,e){for(var t=[],r=arguments.length-2;r-- >0;)t[r]=arguments[r+2];var a=this.$i18n;return a._tc.apply(a,[n,a.locale,a._getMessages(),this,e].concat(t))},t.prototype.$te=function(n,e){var t=this.$i18n;return t._te(n,t.locale,t._getMessages(),e)},t.prototype.$d=function(n){for(var e,t=[],r=arguments.length-1;r-- >0;)t[r]=arguments[r+1];return(e=this.$i18n).d.apply(e,[n].concat(t))},t.prototype.$n=function(n){for(var e,t=[],r=arguments.length-1;r-- >0;)t[r]=arguments[r+1];return(e=this.$i18n).n.apply(e,[n].concat(t))},Wt.mixin(function(n){function e(){this!==this.$root&&this.$options.__INTLIFY_META__&&this.$el&&this.$el.setAttribute("data-intlify",this.$options.__INTLIFY_META__)}return void 0===n&&(n=!1),n?{mounted:e}:{beforeCreate:function(){var n=this.$options;if(n.i18n=n.i18n||(n.__i18nBridge||n.__i18n?{}:null),n.i18n)if(n.i18n instanceof vr){if(n.__i18nBridge||n.__i18n)try{var e=n.i18n&&n.i18n.messages?n.i18n.messages:{};(n.__i18nBridge||n.__i18n).forEach((function(n){e=$t(e,JSON.parse(n))})),Object.keys(e).forEach((function(t){n.i18n.mergeLocaleMessage(t,e[t])}))}catch(n){0}this._i18n=n.i18n,this._i18nWatcher=this._i18n.watchI18nData()}else if(Rt(n.i18n)){var t=this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof vr?this.$root.$i18n:null;if(t&&(n.i18n.root=this.$root,n.i18n.formatter=t.formatter,n.i18n.fallbackLocale=t.fallbackLocale,n.i18n.formatFallbackMessages=t.formatFallbackMessages,n.i18n.silentTranslationWarn=t.silentTranslationWarn,n.i18n.silentFallbackWarn=t.silentFallbackWarn,n.i18n.pluralizationRules=t.pluralizationRules,n.i18n.preserveDirectiveContent=t.preserveDirectiveContent),n.__i18nBridge||n.__i18n)try{var r=n.i18n&&n.i18n.messages?n.i18n.messages:{};(n.__i18nBridge||n.__i18n).forEach((function(n){r=$t(r,JSON.parse(n))})),n.i18n.messages=r}catch(n){0}var a=n.i18n.sharedMessages;a&&Rt(a)&&(n.i18n.messages=$t(n.i18n.messages,a)),this._i18n=new vr(n.i18n),this._i18nWatcher=this._i18n.watchI18nData(),(void 0===n.i18n.sync||n.i18n.sync)&&(this._localeWatcher=this.$i18n.watchLocale()),t&&t.onComponentInstanceCreated(this._i18n)}else 0;else this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof vr?this._i18n=this.$root.$i18n:n.parent&&n.parent.$i18n&&n.parent.$i18n instanceof vr&&(this._i18n=n.parent.$i18n)},beforeMount:function(){var n=this.$options;n.i18n=n.i18n||(n.__i18nBridge||n.__i18n?{}:null),n.i18n?(n.i18n instanceof vr||Rt(n.i18n))&&(this._i18n.subscribeDataChanging(this),this._subscribing=!0):(this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof vr||n.parent&&n.parent.$i18n&&n.parent.$i18n instanceof vr)&&(this._i18n.subscribeDataChanging(this),this._subscribing=!0)},mounted:e,beforeDestroy:function(){if(this._i18n){var n=this;this.$nextTick((function(){n._subscribing&&(n._i18n.unsubscribeDataChanging(n),delete n._subscribing),n._i18nWatcher&&(n._i18nWatcher(),n._i18n.destroyVM(),delete n._i18nWatcher),n._localeWatcher&&(n._localeWatcher(),delete n._localeWatcher)}))}}}}(e.bridge)),Wt.directive("t",{bind:Jt,update:Yt,unbind:Qt}),Wt.component(Ht.name,Ht),Wt.component(Gt.name,Gt),Wt.config.optionMergeStrategies.i18n=function(n,e){return void 0===e?n:e}}var tr=function(){this._caches=Object.create(null)};tr.prototype.interpolate=function(n,e){if(!e)return[n];var t=this._caches[n];return t||(t=function(n){var e=[],t=0,r="";for(;t<n.length;){var a=n[t++];if("{"===a){r&&e.push({type:"text",value:r}),r="";var o="";for(a=n[t++];void 0!==a&&"}"!==a;)o+=a,a=n[t++];var i="}"===a,s=rr.test(o)?"list":i&&ar.test(o)?"named":"unknown";e.push({value:o,type:s})}else"%"===a?"{"!==n[t]&&(r+=a):r+=a}return r&&e.push({type:"text",value:r}),e}(n),this._caches[n]=t),function(n,e){var t=[],r=0,a=Array.isArray(e)?"list":Ot(e)?"named":"unknown";if("unknown"===a)return t;for(;r<n.length;){var o=n[r];switch(o.type){case"text":t.push(o.value);break;case"list":t.push(e[parseInt(o.value,10)]);break;case"named":"named"===a&&t.push(e[o.value]);break;case"unknown":0}r++}return t}(t,e)};var rr=/^(?:\d)+/,ar=/^(?:\w)+/;var or=[];or[0]={ws:[0],ident:[3,0],"[":[4],eof:[7]},or[1]={ws:[1],".":[2],"[":[4],eof:[7]},or[2]={ws:[2],ident:[3,0],0:[3,0],number:[3,0]},or[3]={ident:[3,0],0:[3,0],number:[3,0],ws:[1,1],".":[2,1],"[":[4,1],eof:[7,1]},or[4]={"'":[5,0],'"':[6,0],"[":[4,2],"]":[1,3],eof:8,else:[4,0]},or[5]={"'":[4,0],eof:8,else:[5,0]},or[6]={'"':[4,0],eof:8,else:[6,0]};var ir=/^\s?(?:true|false|-?[\d.]+|'[^']*'|"[^"]*")\s?$/;function sr(n){if(null==n)return"eof";switch(n.charCodeAt(0)){case 91:case 93:case 46:case 34:case 39:return n;case 95:case 36:case 45:return"ident";case 9:case 10:case 13:case 160:case 65279:case 8232:case 8233:return"ws"}return"ident"}function lr(n){var e,t,r,a=n.trim();return("0"!==n.charAt(0)||!isNaN(n))&&(r=a,ir.test(r)?(t=(e=a).charCodeAt(0))!==e.charCodeAt(e.length-1)||34!==t&&39!==t?e:e.slice(1,-1):"*"+a)}var cr=function(){this._cache=Object.create(null)};cr.prototype.parsePath=function(n){var e=this._cache[n];return e||(e=function(n){var e,t,r,a,o,i,s,l=[],c=-1,p=0,u=0,d=[];function m(){var e=n[c+1];if(5===p&&"'"===e||6===p&&'"'===e)return c++,r="\\"+e,d[0](),!0}for(d[1]=function(){void 0!==t&&(l.push(t),t=void 0)},d[0]=function(){void 0===t?t=r:t+=r},d[2]=function(){d[0](),u++},d[3]=function(){if(u>0)u--,p=4,d[0]();else{if(u=0,void 0===t)return!1;if(!1===(t=lr(t)))return!1;d[1]()}};null!==p;)if(c++,"\\"!==(e=n[c])||!m()){if(a=sr(e),8===(o=(s=or[p])[a]||s.else||8))return;if(p=o[0],(i=d[o[1]])&&(r=void 0===(r=o[2])?e:r,!1===i()))return;if(7===p)return l}}(n))&&(this._cache[n]=e),e||[]},cr.prototype.getPathValue=function(n,e){if(!Ot(n))return null;var t=this.parsePath(e);if(0===t.length)return null;for(var r=t.length,a=n,o=0;o<r;){var i=a[t[o]];if(null==i)return null;a=i,o++}return a};var pr,ur=/<\/?[\w\s="/.':;#-\/]+>/,dr=/(?:@(?:\.[a-zA-Z]+)?:(?:[\w\-_|./]+|\([\w\-_:|./]+\)))/g,mr=/^@(?:\.([a-zA-Z]+))?:/,gr=/[()]/g,fr={upper:function(n){return n.toLocaleUpperCase()},lower:function(n){return n.toLocaleLowerCase()},capitalize:function(n){return""+n.charAt(0).toLocaleUpperCase()+n.substr(1)}},hr=new tr,vr=function(n){var e=this;void 0===n&&(n={}),!Wt&&"undefined"!=typeof window&&window.Vue&&er(window.Vue);var t=n.locale||"en-US",r=!1!==n.fallbackLocale&&(n.fallbackLocale||"en-US"),a=n.messages||{},o=n.dateTimeFormats||n.datetimeFormats||{},i=n.numberFormats||{};this._vm=null,this._formatter=n.formatter||hr,this._modifiers=n.modifiers||{},this._missing=n.missing||null,this._root=n.root||null,this._sync=void 0===n.sync||!!n.sync,this._fallbackRoot=void 0===n.fallbackRoot||!!n.fallbackRoot,this._fallbackRootWithEmptyString=void 0===n.fallbackRootWithEmptyString||!!n.fallbackRootWithEmptyString,this._formatFallbackMessages=void 0!==n.formatFallbackMessages&&!!n.formatFallbackMessages,this._silentTranslationWarn=void 0!==n.silentTranslationWarn&&n.silentTranslationWarn,this._silentFallbackWarn=void 0!==n.silentFallbackWarn&&!!n.silentFallbackWarn,this._dateTimeFormatters={},this._numberFormatters={},this._path=new cr,this._dataListeners=new Set,this._componentInstanceCreatedListener=n.componentInstanceCreatedListener||null,this._preserveDirectiveContent=void 0!==n.preserveDirectiveContent&&!!n.preserveDirectiveContent,this.pluralizationRules=n.pluralizationRules||{},this._warnHtmlInMessage=n.warnHtmlInMessage||"off",this._postTranslation=n.postTranslation||null,this._escapeParameterHtml=n.escapeParameterHtml||!1,"__VUE_I18N_BRIDGE__"in n&&(this.__VUE_I18N_BRIDGE__=n.__VUE_I18N_BRIDGE__),this.getChoiceIndex=function(n,t){var r=Object.getPrototypeOf(e);if(r&&r.getChoiceIndex)return r.getChoiceIndex.call(e,n,t);var a,o;return e.locale in e.pluralizationRules?e.pluralizationRules[e.locale].apply(e,[n,t]):(a=n,o=t,a=Math.abs(a),2===o?a?a>1?1:0:1:a?Math.min(a,2):0)},this._exist=function(n,t){return!(!n||!t)&&(!Pt(e._path.getPathValue(n,t))||!!n[t])},"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||Object.keys(a).forEach((function(n){e._checkLocaleMessage(n,e._warnHtmlInMessage,a[n])})),this._initVM({locale:t,fallbackLocale:r,messages:a,dateTimeFormats:o,numberFormats:i})},br={vm:{configurable:!0},messages:{configurable:!0},dateTimeFormats:{configurable:!0},numberFormats:{configurable:!0},availableLocales:{configurable:!0},locale:{configurable:!0},fallbackLocale:{configurable:!0},formatFallbackMessages:{configurable:!0},missing:{configurable:!0},formatter:{configurable:!0},silentTranslationWarn:{configurable:!0},silentFallbackWarn:{configurable:!0},preserveDirectiveContent:{configurable:!0},warnHtmlInMessage:{configurable:!0},postTranslation:{configurable:!0},sync:{configurable:!0}};vr.prototype._checkLocaleMessage=function(n,e,t){var r=function(n,e,t,a){if(Rt(t))Object.keys(t).forEach((function(o){var i=t[o];Rt(i)?(a.push(o),a.push("."),r(n,e,i,a),a.pop(),a.pop()):(a.push(o),r(n,e,i,a),a.pop())}));else if(Tt(t))t.forEach((function(t,o){Rt(t)?(a.push("["+o+"]"),a.push("."),r(n,e,t,a),a.pop(),a.pop()):(a.push("["+o+"]"),r(n,e,t,a),a.pop())}));else if(At(t)){if(ur.test(t)){var o="Detected HTML in message '"+t+"' of keypath '"+a.join("")+"' at '"+e+"'. Consider component interpolation with '<i18n>' to avoid XSS. See https://bit.ly/2ZqJzkp";"warn"===n?It(o):"error"===n&&function(n,e){"undefined"!=typeof console&&(console.error("[vue-i18n] "+n),e&&console.error(e.stack))}(o)}}};r(e,n,t,[])},vr.prototype._initVM=function(n){var e=Wt.config.silent;Wt.config.silent=!0,this._vm=new Wt({data:n,__VUE18N__INSTANCE__:!0}),Wt.config.silent=e},vr.prototype.destroyVM=function(){this._vm.$destroy()},vr.prototype.subscribeDataChanging=function(n){this._dataListeners.add(n)},vr.prototype.unsubscribeDataChanging=function(n){!function(n,e){if(n.delete(e));}(this._dataListeners,n)},vr.prototype.watchI18nData=function(){var n=this;return this._vm.$watch("$data",(function(){for(var e,t,r=(e=n._dataListeners,t=[],e.forEach((function(n){return t.push(n)})),t),a=r.length;a--;)Wt.nextTick((function(){r[a]&&r[a].$forceUpdate()}))}),{deep:!0})},vr.prototype.watchLocale=function(n){if(n){if(!this.__VUE_I18N_BRIDGE__)return null;var e=this,t=this._vm;return this.vm.$watch("locale",(function(r){t.$set(t,"locale",r),e.__VUE_I18N_BRIDGE__&&n&&(n.locale.value=r),t.$forceUpdate()}),{immediate:!0})}if(!this._sync||!this._root)return null;var r=this._vm;return this._root.$i18n.vm.$watch("locale",(function(n){r.$set(r,"locale",n),r.$forceUpdate()}),{immediate:!0})},vr.prototype.onComponentInstanceCreated=function(n){this._componentInstanceCreatedListener&&this._componentInstanceCreatedListener(n,this)},br.vm.get=function(){return this._vm},br.messages.get=function(){return Mt(this._getMessages())},br.dateTimeFormats.get=function(){return Mt(this._getDateTimeFormats())},br.numberFormats.get=function(){return Mt(this._getNumberFormats())},br.availableLocales.get=function(){return Object.keys(this.messages).sort()},br.locale.get=function(){return this._vm.locale},br.locale.set=function(n){this._vm.$set(this._vm,"locale",n)},br.fallbackLocale.get=function(){return this._vm.fallbackLocale},br.fallbackLocale.set=function(n){this._localeChainCache={},this._vm.$set(this._vm,"fallbackLocale",n)},br.formatFallbackMessages.get=function(){return this._formatFallbackMessages},br.formatFallbackMessages.set=function(n){this._formatFallbackMessages=n},br.missing.get=function(){return this._missing},br.missing.set=function(n){this._missing=n},br.formatter.get=function(){return this._formatter},br.formatter.set=function(n){this._formatter=n},br.silentTranslationWarn.get=function(){return this._silentTranslationWarn},br.silentTranslationWarn.set=function(n){this._silentTranslationWarn=n},br.silentFallbackWarn.get=function(){return this._silentFallbackWarn},br.silentFallbackWarn.set=function(n){this._silentFallbackWarn=n},br.preserveDirectiveContent.get=function(){return this._preserveDirectiveContent},br.preserveDirectiveContent.set=function(n){this._preserveDirectiveContent=n},br.warnHtmlInMessage.get=function(){return this._warnHtmlInMessage},br.warnHtmlInMessage.set=function(n){var e=this,t=this._warnHtmlInMessage;if(this._warnHtmlInMessage=n,t!==n&&("warn"===n||"error"===n)){var r=this._getMessages();Object.keys(r).forEach((function(n){e._checkLocaleMessage(n,e._warnHtmlInMessage,r[n])}))}},br.postTranslation.get=function(){return this._postTranslation},br.postTranslation.set=function(n){this._postTranslation=n},br.sync.get=function(){return this._sync},br.sync.set=function(n){this._sync=n},vr.prototype._getMessages=function(){return this._vm.messages},vr.prototype._getDateTimeFormats=function(){return this._vm.dateTimeFormats},vr.prototype._getNumberFormats=function(){return this._vm.numberFormats},vr.prototype._warnDefault=function(n,e,t,r,a,o){if(!Pt(t))return t;if(this._missing){var i=this._missing.apply(null,[n,e,r,a]);if(At(i))return i}else 0;if(this._formatFallbackMessages){var s=Bt.apply(void 0,a);return this._render(e,o,s.params,e)}return e},vr.prototype._isFallbackRoot=function(n){return(this._fallbackRootWithEmptyString?!n:Pt(n))&&!Pt(this._root)&&this._fallbackRoot},vr.prototype._isSilentFallbackWarn=function(n){return this._silentFallbackWarn instanceof RegExp?this._silentFallbackWarn.test(n):this._silentFallbackWarn},vr.prototype._isSilentFallback=function(n,e){return this._isSilentFallbackWarn(e)&&(this._isFallbackRoot()||n!==this.fallbackLocale)},vr.prototype._isSilentTranslationWarn=function(n){return this._silentTranslationWarn instanceof RegExp?this._silentTranslationWarn.test(n):this._silentTranslationWarn},vr.prototype._interpolate=function(n,e,t,r,a,o,i){if(!e)return null;var s,l=this._path.getPathValue(e,t);if(Tt(l)||Rt(l))return l;if(Pt(l)){if(!Rt(e))return null;if(!At(s=e[t])&&!Ft(s))return null}else{if(!At(l)&&!Ft(l))return null;s=l}return At(s)&&(s.indexOf("@:")>=0||s.indexOf("@.")>=0)&&(s=this._link(n,e,s,r,"raw",o,i)),this._render(s,a,o,t)},vr.prototype._link=function(n,e,t,r,a,o,i){var s=t,l=s.match(dr);for(var c in l)if(l.hasOwnProperty(c)){var p=l[c],u=p.match(mr),d=u[0],m=u[1],g=p.replace(d,"").replace(gr,"");if(jt(i,g))return s;i.push(g);var f=this._interpolate(n,e,g,r,"raw"===a?"string":a,"raw"===a?void 0:o,i);if(this._isFallbackRoot(f)){if(!this._root)throw Error("unexpected error");var h=this._root.$i18n;f=h._translate(h._getMessages(),h.locale,h.fallbackLocale,g,r,a,o)}f=this._warnDefault(n,g,f,r,Tt(o)?o:[o],a),this._modifiers.hasOwnProperty(m)?f=this._modifiers[m](f):fr.hasOwnProperty(m)&&(f=fr[m](f)),i.pop(),s=f?s.replace(p,f):s}return s},vr.prototype._createMessageContext=function(n,e,t,r){var a=this,o=Tt(n)?n:[],i=Ot(n)?n:{},s=this._getMessages(),l=this.locale;return{list:function(n){return o[n]},named:function(n){return i[n]},values:n,formatter:e,path:t,messages:s,locale:l,linked:function(n){return a._interpolate(l,s[l]||{},n,null,r,void 0,[n])}}},vr.prototype._render=function(n,e,t,r){if(Ft(n))return n(this._createMessageContext(t,this._formatter||hr,r,e));var a=this._formatter.interpolate(n,t,r);return a||(a=hr.interpolate(n,t,r)),"string"!==e||At(a)?a:a.join("")},vr.prototype._appendItemToChain=function(n,e,t){var r=!1;return jt(n,e)||(r=!0,e&&(r="!"!==e[e.length-1],e=e.replace(/!/g,""),n.push(e),t&&t[e]&&(r=t[e]))),r},vr.prototype._appendLocaleToChain=function(n,e,t){var r,a=e.split("-");do{var o=a.join("-");r=this._appendItemToChain(n,o,t),a.splice(-1,1)}while(a.length&&!0===r);return r},vr.prototype._appendBlockToChain=function(n,e,t){for(var r=!0,a=0;a<e.length&&"boolean"==typeof r;a++){var o=e[a];At(o)&&(r=this._appendLocaleToChain(n,o,t))}return r},vr.prototype._getLocaleChain=function(n,e){if(""===n)return[];this._localeChainCache||(this._localeChainCache={});var t=this._localeChainCache[n];if(!t){e||(e=this.fallbackLocale),t=[];for(var r,a=[n];Tt(a);)a=this._appendBlockToChain(t,a,e);(a=At(r=Tt(e)?e:Ot(e)?e.default?e.default:null:e)?[r]:r)&&this._appendBlockToChain(t,a,null),this._localeChainCache[n]=t}return t},vr.prototype._translate=function(n,e,t,r,a,o,i){for(var s,l=this._getLocaleChain(e,t),c=0;c<l.length;c++){var p=l[c];if(!Pt(s=this._interpolate(p,n[p],r,a,o,i,[r])))return s}return null},vr.prototype._t=function(n,e,t,r){for(var a,o=[],i=arguments.length-4;i-- >0;)o[i]=arguments[i+4];if(!n)return"";var s=Bt.apply(void 0,o);this._escapeParameterHtml&&(s.params=zt(s.params));var l=s.locale||e,c=this._translate(t,l,this.fallbackLocale,n,r,"string",s.params);if(this._isFallbackRoot(c)){if(!this._root)throw Error("unexpected error");return(a=this._root).$t.apply(a,[n].concat(o))}return c=this._warnDefault(l,n,c,r,o,"string"),this._postTranslation&&null!=c&&(c=this._postTranslation(c,n)),c},vr.prototype.t=function(n){for(var e,t=[],r=arguments.length-1;r-- >0;)t[r]=arguments[r+1];return(e=this)._t.apply(e,[n,this.locale,this._getMessages(),null].concat(t))},vr.prototype._i=function(n,e,t,r,a){var o=this._translate(t,e,this.fallbackLocale,n,r,"raw",a);if(this._isFallbackRoot(o)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.i(n,e,a)}return this._warnDefault(e,n,o,r,[a],"raw")},vr.prototype.i=function(n,e,t){return n?(At(e)||(e=this.locale),this._i(n,e,this._getMessages(),null,t)):""},vr.prototype._tc=function(n,e,t,r,a){for(var o,i=[],s=arguments.length-5;s-- >0;)i[s]=arguments[s+5];if(!n)return"";void 0===a&&(a=1);var l={count:a,n:a},c=Bt.apply(void 0,i);return c.params=Object.assign(l,c.params),i=null===c.locale?[c.params]:[c.locale,c.params],this.fetchChoice((o=this)._t.apply(o,[n,e,t,r].concat(i)),a)},vr.prototype.fetchChoice=function(n,e){if(!n||!At(n))return null;var t=n.split("|");return t[e=this.getChoiceIndex(e,t.length)]?t[e].trim():n},vr.prototype.tc=function(n,e){for(var t,r=[],a=arguments.length-2;a-- >0;)r[a]=arguments[a+2];return(t=this)._tc.apply(t,[n,this.locale,this._getMessages(),null,e].concat(r))},vr.prototype._te=function(n,e,t){for(var r=[],a=arguments.length-3;a-- >0;)r[a]=arguments[a+3];var o=Bt.apply(void 0,r).locale||e;return this._exist(t[o],n)},vr.prototype.te=function(n,e){return this._te(n,this.locale,this._getMessages(),e)},vr.prototype.getLocaleMessage=function(n){return Mt(this._vm.messages[n]||{})},vr.prototype.setLocaleMessage=function(n,e){"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||this._checkLocaleMessage(n,this._warnHtmlInMessage,e),this._vm.$set(this._vm.messages,n,e)},vr.prototype.mergeLocaleMessage=function(n,e){"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||this._checkLocaleMessage(n,this._warnHtmlInMessage,e),this._vm.$set(this._vm.messages,n,$t(void 0!==this._vm.messages[n]&&Object.keys(this._vm.messages[n]).length?Object.assign({},this._vm.messages[n]):{},e))},vr.prototype.getDateTimeFormat=function(n){return Mt(this._vm.dateTimeFormats[n]||{})},vr.prototype.setDateTimeFormat=function(n,e){this._vm.$set(this._vm.dateTimeFormats,n,e),this._clearDateTimeFormat(n,e)},vr.prototype.mergeDateTimeFormat=function(n,e){this._vm.$set(this._vm.dateTimeFormats,n,$t(this._vm.dateTimeFormats[n]||{},e)),this._clearDateTimeFormat(n,e)},vr.prototype._clearDateTimeFormat=function(n,e){for(var t in e){var r=n+"__"+t;this._dateTimeFormatters.hasOwnProperty(r)&&delete this._dateTimeFormatters[r]}},vr.prototype._localizeDateTime=function(n,e,t,r,a,o){for(var i=e,s=r[i],l=this._getLocaleChain(e,t),c=0;c<l.length;c++){var p=l[c];if(i=p,!Pt(s=r[p])&&!Pt(s[a]))break}if(Pt(s)||Pt(s[a]))return null;var u,d=s[a];if(o)u=new Intl.DateTimeFormat(i,Object.assign({},d,o));else{var m=i+"__"+a;(u=this._dateTimeFormatters[m])||(u=this._dateTimeFormatters[m]=new Intl.DateTimeFormat(i,d))}return u.format(n)},vr.prototype._d=function(n,e,t,r){if(!t)return(r?new Intl.DateTimeFormat(e,r):new Intl.DateTimeFormat(e)).format(n);var a=this._localizeDateTime(n,e,this.fallbackLocale,this._getDateTimeFormats(),t,r);if(this._isFallbackRoot(a)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.d(n,t,e)}return a||""},vr.prototype.d=function(n){for(var e=[],t=arguments.length-1;t-- >0;)e[t]=arguments[t+1];var r=this.locale,a=null,o=null;return 1===e.length?(At(e[0])?a=e[0]:Ot(e[0])&&(e[0].locale&&(r=e[0].locale),e[0].key&&(a=e[0].key)),o=Object.keys(e[0]).reduce((function(n,t){var r;return jt(Ct,t)?Object.assign({},n,((r={})[t]=e[0][t],r)):n}),null)):2===e.length&&(At(e[0])&&(a=e[0]),At(e[1])&&(r=e[1])),this._d(n,r,a,o)},vr.prototype.getNumberFormat=function(n){return Mt(this._vm.numberFormats[n]||{})},vr.prototype.setNumberFormat=function(n,e){this._vm.$set(this._vm.numberFormats,n,e),this._clearNumberFormat(n,e)},vr.prototype.mergeNumberFormat=function(n,e){this._vm.$set(this._vm.numberFormats,n,$t(this._vm.numberFormats[n]||{},e)),this._clearNumberFormat(n,e)},vr.prototype._clearNumberFormat=function(n,e){for(var t in e){var r=n+"__"+t;this._numberFormatters.hasOwnProperty(r)&&delete this._numberFormatters[r]}},vr.prototype._getNumberFormatter=function(n,e,t,r,a,o){for(var i=e,s=r[i],l=this._getLocaleChain(e,t),c=0;c<l.length;c++){var p=l[c];if(i=p,!Pt(s=r[p])&&!Pt(s[a]))break}if(Pt(s)||Pt(s[a]))return null;var u,d=s[a];if(o)u=new Intl.NumberFormat(i,Object.assign({},d,o));else{var m=i+"__"+a;(u=this._numberFormatters[m])||(u=this._numberFormatters[m]=new Intl.NumberFormat(i,d))}return u},vr.prototype._n=function(n,e,t,r){if(!vr.availabilities.numberFormat)return"";if(!t)return(r?new Intl.NumberFormat(e,r):new Intl.NumberFormat(e)).format(n);var a=this._getNumberFormatter(n,e,this.fallbackLocale,this._getNumberFormats(),t,r),o=a&&a.format(n);if(this._isFallbackRoot(o)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.n(n,Object.assign({},{key:t,locale:e},r))}return o||""},vr.prototype.n=function(n){for(var e=[],t=arguments.length-1;t-- >0;)e[t]=arguments[t+1];var r=this.locale,a=null,o=null;return 1===e.length?At(e[0])?a=e[0]:Ot(e[0])&&(e[0].locale&&(r=e[0].locale),e[0].key&&(a=e[0].key),o=Object.keys(e[0]).reduce((function(n,t){var r;return jt(Dt,t)?Object.assign({},n,((r={})[t]=e[0][t],r)):n}),null)):2===e.length&&(At(e[0])&&(a=e[0]),At(e[1])&&(r=e[1])),this._n(n,r,a,o)},vr.prototype._ntp=function(n,e,t,r){if(!vr.availabilities.numberFormat)return[];if(!t)return(r?new Intl.NumberFormat(e,r):new Intl.NumberFormat(e)).formatToParts(n);var a=this._getNumberFormatter(n,e,this.fallbackLocale,this._getNumberFormats(),t,r),o=a&&a.formatToParts(n);if(this._isFallbackRoot(o)){if(!this._root)throw Error("unexpected error");return this._root.$i18n._ntp(n,e,t,r)}return o||[]},Object.defineProperties(vr.prototype,br),Object.defineProperty(vr,"availabilities",{get:function(){if(!pr){var n="undefined"!=typeof Intl;pr={dateTimeFormat:n&&void 0!==Intl.DateTimeFormat,numberFormat:n&&void 0!==Intl.NumberFormat}}return pr}}),vr.install=er,vr.version="8.28.2";var kr=vr;
/*!
 * vssue - A vue-powered issue-based comment plugin
 *
 * @version v1.4.8
 * @link https://vssue.js.org
 * @license MIT
 * @copyright 2018-2021 meteorlxy
 */
/*! *****************************************************************************
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use
this file except in compliance with the License. You may obtain a copy of the
License at http://www.apache.org/licenses/LICENSE-2.0

THIS CODE IS PROVIDED ON AN *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
MERCHANTABLITY OR NON-INFRINGEMENT.

See the Apache Version 2.0 License for specific language governing permissions
and limitations under the License.
***************************************************************************** */function yr(n,e,t,r){var a,o=arguments.length,i=o<3?e:null===r?r=Object.getOwnPropertyDescriptor(e,t):r;if("object"==typeof Reflect&&"function"==typeof Reflect.decorate)i=Reflect.decorate(n,e,t,r);else for(var s=n.length-1;s>=0;s--)(a=n[s])&&(i=(o<3?a(i):o>3?a(e,t,i):a(e,t))||i);return o>3&&i&&Object.defineProperty(e,t,i),i}var Sr=r.b.extend({name:"Iconfont"});function xr(n,e,t,r,a,o,i,s,l,c){"boolean"!=typeof i&&(l=s,s=i,i=!1);const p="function"==typeof t?t.options:t;let u;if(n&&n.render&&(p.render=n.render,p.staticRenderFns=n.staticRenderFns,p._compiled=!0,a&&(p.functional=!0)),r&&(p._scopeId=r),o?(u=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),e&&e.call(this,l(n)),n&&n._registeredComponents&&n._registeredComponents.add(o)},p._ssrRegister=u):e&&(u=i?function(n){e.call(this,c(n,this.$root.$options.shadowRoot))}:function(n){e.call(this,s(n))}),u)if(p.functional){const n=p.render;p.render=function(e,t){return u.call(t),n(e,t)}}else{const n=p.beforeCreate;p.beforeCreate=n?[].concat(n,u):[u]}return t}"undefined"!=typeof navigator&&/msie [6-9]\\b/.test(navigator.userAgent.toLowerCase());const wr=xr({render:function(n,e){var t=e._c;return t("svg",{directives:[{name:"show",rawName:"v-show",value:!1,expression:"false"}]},[t("symbol",{attrs:{id:"vssue-icon-bitbucket",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M579.5522464 489.45249493q4.8371808 38.38537173-30.81752427 61.55702827t-67.95459093 3.66689493q-23.79580907-10.37653333-32.6119616-35.34262826t-0.31207573-50.01020907 31.67573333-35.34262827q21.92335253-11.00068587 44.1587808-7.33379093t39.00952427 21.61127573 16.77409493 41.1160384zM647.19476053 476.65737173q-8.50407573-65.22392427-68.8908192-99.9424t-120.07131413-7.9579424q-38.38537173 17.08617173-61.24495253 53.9111616t-21.0651424 78.95527574q2.41859093 55.4715424 47.20152426 94.48106666t100.87862827 34.1723424q55.4715424-4.8371808 92.60860907-51.18049493t30.50544746-102.43900907zM792.93434133 146.32472427q-12.17097173-16.4620192-34.1723424-27.15062827t-35.34262826-13.41927573-43.30057174-7.64586667q-177.33729493-28.63299093-345.00022826 1.24830507-26.2144 4.29104747-40.25782827 7.33379093t-33.54819093 13.41927573-30.50544747 26.2144q18.2564576 17.08617173 46.34331413 27.6967616t44.78293334 13.41927574 53.36502826 7.02171413q138.95192427 17.71032427 273.06666667 0.62415253 38.38537173-4.8371808 54.53531413-7.33379093t44.1587808-13.1072 45.7191616-28.32091413zM827.65281813 777.10872427q-4.8371808 15.83786667-9.44030506 46.65539093t-8.50407574 51.18049493-17.39824746 42.6764192-35.34262827 34.4064q-52.4288 29.2571424-115.46819093 43.61264747t-123.1140576 13.41927573-122.8019808-11.3127616q-28.0088384-4.8371808-49.69813334-11.00068586t-46.65539093-16.4620192-44.4708576-26.52647574-31.67573333-37.4491424q-15.21371413-58.51428587-34.71847574-177.96144746l3.66689494-9.7523808 11.00068586-5.46133334q135.9091808 90.1900192 308.72137174 90.1900192t309.34552426-90.1900192q12.79512427 3.66689493 14.5895616 14.04342827t-3.0427424 27.46270507-4.8371808 22.54750506zM937.97175147 191.41973333q-15.83786667 101.8148576-67.64251414 399.22346667-3.0427424 18.2564576-16.4620192 34.1723424t-26.52647573 24.3419424-33.23611413 18.88060907q-153.61950507 76.7707424-371.8387808 53.67710506-151.12289493-16.4620192-240.14262827-84.72868586-9.12822827-7.33379093-15.52579093-16.1499424t-10.37653334-21.2992-5.46133333-20.75306667-3.66689493-24.10788587-3.3548192-21.2992q-5.46133333-30.50544747-16.1499424-91.43832426t-17.08617174-98.4600384-14.35550506-89.8779424-13.41927574-96.27550507q1.7944384-15.83786667 10.68860907-29.5692192t19.19268587-22.8595808 27.46270506-18.2564576 28.0088384-13.73135253 29.2571424-11.3127616q76.22460907-28.0088384 190.75657174-39.00952427 231.0144-22.54750507 412.01859093 30.50544747 94.48106667 28.0088384 131.072 74.35215253 9.7523808 12.17097173 10.0644576 31.0515808t-3.3548192 32.9240384z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitea",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M184.31868985 236.10860742C106.94832667 235.94086648 3.32655508 285.13080468 9.02973665 408.46209936c8.93218827 192.65010787 206.32096845 210.5144844 285.20099725 212.06608453 8.63864186 36.14810496 101.48307766 160.77938883 170.21479898 167.32127321h301.09442177c180.57278288-11.99345499 315.77172611-546.07960359 215.54670217-548.09249109-165.7696721 7.79993906-264.02374305 11.74184405-348.27147151 12.41280591v166.69224585l-26.25140843-11.61603761-0.16773997-154.99233728c-96.70246985-0.04193548-181.83083757-4.52899687-343.4069947-12.49667687-20.21274496-0.12580547-48.39316992-3.5644886-78.67035236-3.64835859z m10.94507577 68.14462849h9.22573371c10.98701124 98.75729283 28.85138778 156.50200291 64.99949274 244.73357185-92.25734394-10.90314029-170.75995634-37.69970509-185.18564974-137.75698809-7.46445813-51.78991757 17.69663558-105.84433456 110.96042329-107.01851827z m358.83913087 97.07988723c6.29027343 0.08386999 12.70635233 1.25805468 18.74501482 4.02577499l31.40943263 13.54505513-22.51917887 41.05451824a28.18042496 25.03528825 0 0 0-10.10637297 1.59353561 28.18042496 25.03528825 0 0 0-16.98373825 32.038459 28.18042496 25.03528825 0 0 0 4.69673781 7.29671718l-38.83195528 70.70267333a28.18042496 25.03528825 0 0 0-9.30960467 1.59353659 28.18042496 25.03528825 0 0 0-16.98373825 32.038459 28.18042496 25.03528825 0 0 0 36.06423497 15.09665623 28.18042496 25.03528825 0 0 0 16.94180276-32.08039449 28.18042496 25.03528825 0 0 0-6.62575434-9.22573468l37.82551056-68.85752581a28.18042496 25.03528825 0 0 0 12.28700044-1.25805469 28.18042496 25.03528825 0 0 0 8.93218826-4.69673783c14.59343435 6.12253248 26.54495386 11.11281671 35.14166122 15.34826717 12.91602778 6.37414341 17.48696012 10.60959485 18.87082027 15.30633169 1.38386015 4.61286685-0.12580547 13.50312062-7.42252263 29.10299872-5.45157063 11.61603859-14.46762889 28.09655497-25.11915823 47.51253164a28.18042496 25.03528825 0 0 0-10.52572486 1.59353659 28.18042496 25.03528825 0 0 0-16.98373826 32.038459 28.18042496 25.03528825 0 0 0 36.06423498 15.09665623 28.18042496 25.03528825 0 0 0 16.94180278-32.03845901 28.18042496 25.03528825 0 0 0-5.74511608-8.47090188c10.52572388-19.20630122 19.58371762-35.72875308 25.41270465-48.14155897 7.88380904-16.85793279 11.99345499-29.39654416 8.38703091-41.51580463-3.60642311-12.11926046-14.67730434-20.0030695-29.35460966-27.25785217-9.6450856-4.73867233-21.68047607-9.77089106-36.06423399-15.80955357a28.18042496 25.03528825 0 0 0-1.59353562-10.022502 28.18042496 25.03528825 0 0 0-6.08059796-8.7644483l22.14176246-40.38355541 122.61839638 52.96410227c22.14176247 9.6031511 31.2836262 33.12877372 20.54822685 52.8382968l-84.28966393 154.32137544c-10.77733482 19.66758857-37.23841869 27.80300855-59.38018118 18.24179293l-173.48574115-74.98005927c-22.14176247-9.5612156-31.32556167-33.12877372-20.54822687-52.83829679l84.28966395-154.27943995c7.38058716-13.54505513 22.22563246-21.59660511 37.951317-22.22563246h2.68384935z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitee",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M978.404275 409.561604H455.061338c-25.117645 0-45.499734 20.382089-45.499734 45.499734l-0.031997 113.781333c0 25.117645 20.350092 45.499734 45.499734 45.531731h318.594132c25.117645 0 45.499734 20.382089 45.499734 45.499735v22.749867a136.5312 136.5312 0 0 1-136.5312 136.5312H250.248539a45.499734 45.499734 0 0 1-45.499734-45.499734V341.343999a136.5312 136.5312 0 0 1 136.5312-136.5312L978.308284 204.780802c25.117645 0 45.499734-20.350092 45.499734-45.467738L1023.904009 45.531731h0.031997A45.499734 45.499734 0 0 0 978.468269 0h-0.031997L341.343999 0.031997C152.84967 0.031997 0.031997 152.84967 0.031997 341.343999v637.092273c0 25.117645 20.382089 45.499734 45.499734 45.499734h671.233072a307.171203 307.171203 0 0 0 307.171203-307.171203v-261.671468c0-25.117645-20.382089-45.499734-45.499734-45.499734z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-github",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M512 20.4425c-278.334 0-504 225.6345-504 504 0 222.6735 144.4275 411.6105 344.673 478.233 25.2 4.662 34.461-10.9305 34.461-24.255 0-12.0015-0.4725-51.723-0.693-93.8385-140.238 30.492-169.8165-59.472-169.8165-59.472-22.932-58.2435-55.944-73.7415-55.944-73.7415-45.738-31.2795 3.465-30.6495 3.465-30.6495 50.589 3.5595 77.238 51.9435 77.238 51.9435 44.9505 77.049 117.9045 54.7785 146.664 41.895 4.5045-32.571 17.577-54.81 32.004-67.41-111.951-12.726-229.635-55.9755-229.635-249.0705 0-55.0305 19.6875-99.981 51.9435-135.2925-5.229-12.6945-22.491-63.945 4.8825-133.371 0 0 42.336-13.545 138.6315 51.66 40.194-11.1825 83.3175-16.758 126.1575-16.9785 42.8085 0.189 85.9635 5.796 126.252 16.9785 96.201-65.205 138.4425-51.66 138.4425-51.66 27.4365 69.426 10.1745 120.6765 4.9455 133.371 32.319 35.28 51.8805 80.262 51.8805 135.2925 0 193.5675-117.9045 236.187-230.139 248.6925 18.081 15.6555 34.1775 46.305 34.1775 93.3345 0 67.4415-0.5985 121.716-0.5985 138.3165 0 13.419 9.072 29.1375 34.6185 24.192 200.151-66.717 344.3895-255.5595 344.3895-478.17 0-278.3655-225.666-504-504-504z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitlab",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M66.61375986 405.11600042L512.11376028 976.03999972 23.84576 621.65599958a39.312 39.312 0 0 1-14.07600042-43.30799944l56.8080007-173.26800028z m259.88400014 0h371.26800014L512.14975986 976.03999972zM215.11376 60.88400042l111.384 344.232H66.61375986l111.384-344.232a19.72800014 19.72800014 0 0 1 37.11600014 0z m742.49999972 344.232l56.8080007 173.2679993a39.23999986 39.23999986 0 0 1-14.07600042 43.30800042l-488.26800028 354.38400014 445.50000042-570.92400028z m0 0h-259.88400014l111.384-344.232a19.72800014 19.72800014 0 0 1 37.11600014 0z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-loading",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M843.307 742.24c0 3.217 2.607 5.824 5.824 5.824s5.824-2.607 5.824-5.824a5.823 5.823 0 0 0-5.824-5.824 5.823 5.823 0 0 0-5.824 5.824zM714.731 874.912c0 6.398 5.186 11.584 11.584 11.584s11.584-5.186 11.584-11.584-5.186-11.584-11.584-11.584-11.584 5.186-11.584 11.584zM541.419 943.2c0 9.614 7.794 17.408 17.408 17.408s17.408-7.794 17.408-17.408-7.794-17.408-17.408-17.408-17.408 7.794-17.408 17.408z m-186.56-9.152c0 12.795 10.373 23.168 23.168 23.168s23.168-10.373 23.168-23.168-10.373-23.168-23.168-23.168-23.168 10.373-23.168 23.168zM189.355 849.12c0 16.012 12.98 28.992 28.992 28.992s28.992-12.98 28.992-28.992-12.98-28.992-28.992-28.992-28.992 12.98-28.992 28.992zM74.731 704.736c0 19.228 15.588 34.816 34.816 34.816s34.816-15.588 34.816-34.816-15.588-34.816-34.816-34.816-34.816 15.588-34.816 34.816z m-43.008-177.28c0 22.41 18.166 40.576 40.576 40.576s40.576-18.166 40.576-40.576-18.166-40.576-40.576-40.576-40.576 18.166-40.576 40.576z m35.392-176.128c0 25.626 20.774 46.4 46.4 46.4s46.4-20.774 46.4-46.4c0-25.626-20.774-46.4-46.4-46.4-25.626 0-46.4 20.774-46.4 46.4z m106.176-142.016c0 28.843 23.381 52.224 52.224 52.224s52.224-23.381 52.224-52.224c0-28.843-23.381-52.224-52.224-52.224-28.843 0-52.224 23.381-52.224 52.224z m155.904-81.344c0 32.024 25.96 57.984 57.984 57.984s57.984-25.96 57.984-57.984-25.96-57.984-57.984-57.984-57.984 25.96-57.984 57.984z m175.104-5.056c0 35.24 28.568 63.808 63.808 63.808s63.808-28.568 63.808-63.808c0-35.24-28.568-63.808-63.808-63.808-35.24 0-63.808 28.568-63.808 63.808z m160.32 72.128c0 38.421 31.147 69.568 69.568 69.568s69.568-31.147 69.568-69.568-31.147-69.568-69.568-69.568-69.568 31.147-69.568 69.568z m113.92 135.488c0 41.638 33.754 75.392 75.392 75.392s75.392-33.754 75.392-75.392-33.754-75.392-75.392-75.392-75.392 33.754-75.392 75.392z m45.312 175.488c0 44.854 36.362 81.216 81.216 81.216s81.216-36.362 81.216-81.216c0-44.854-36.362-81.216-81.216-81.216-44.854 0-81.216 36.362-81.216 81.216z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-like",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M885.9 533.7c16.8-22.2 26.1-49.4 26.1-77.7 0-44.9-25.1-87.4-65.5-111.1a67.67 67.67 0 0 0-34.3-9.3H572.4l6-122.9c1.4-29.7-9.1-57.9-29.5-79.4-20.5-21.5-48.1-33.4-77.9-33.4-52 0-98 35-111.8 85.1l-85.9 311H144c-17.7 0-32 14.3-32 32v364c0 17.7 14.3 32 32 32h601.3c9.2 0 18.2-1.8 26.5-5.4 47.6-20.3 78.3-66.8 78.3-118.4 0-12.6-1.8-25-5.4-37 16.8-22.2 26.1-49.4 26.1-77.7 0-12.6-1.8-25-5.4-37 16.8-22.2 26.1-49.4 26.1-77.7-0.2-12.6-2-25.1-5.6-37.1zM184 852V568h81v284h-81z m636.4-353l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 16.5-7.2 32.2-19.6 43l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 16.5-7.2 32.2-19.6 43l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 22.4-13.2 42.6-33.6 51.8H329V564.8l99.5-360.5c5.2-18.9 22.5-32.2 42.2-32.3 7.6 0 15.1 2.2 21.1 6.7 9.9 7.4 15.2 18.6 14.6 30.5l-9.6 198.4h314.4C829 418.5 840 436.9 840 456c0 16.5-7.2 32.1-19.6 43z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-unlike",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M885.9 490.3c3.6-12 5.4-24.4 5.4-37 0-28.3-9.3-55.5-26.1-77.7 3.6-12 5.4-24.4 5.4-37 0-28.3-9.3-55.5-26.1-77.7 3.6-12 5.4-24.4 5.4-37 0-51.6-30.7-98.1-78.3-118.4-8.3-3.6-17.2-5.4-26.5-5.4H144c-17.7 0-32 14.3-32 32v364c0 17.7 14.3 32 32 32h129.3l85.8 310.8C372.9 889 418.9 924 470.9 924c29.7 0 57.4-11.8 77.9-33.4 20.5-21.5 31-49.7 29.5-79.4l-6-122.9h239.9c12.1 0 23.9-3.2 34.3-9.3 40.4-23.5 65.5-66.1 65.5-111 0-28.3-9.3-55.5-26.1-77.7zM184 456V172h81v284h-81z m627.2 160.4H496.8l9.6 198.4c0.6 11.9-4.7 23.1-14.6 30.5-6.1 4.5-13.6 6.8-21.1 6.7-19.6-0.1-36.9-13.4-42.2-32.3L329 459.2V172h415.4c20.4 9.2 33.6 29.4 33.6 51.8 0 9.7-2.3 18.9-6.9 27.3l-13.9 25.4 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 9.7-2.3 18.9-6.9 27.3l-13.9 25.4 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 9.7-2.3 18.9-6.9 27.3l-14 25.5 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 19.1-11 37.5-28.8 48.4z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-heart",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M923 283.6c-13.4-31.1-32.6-58.9-56.9-82.8-24.3-23.8-52.5-42.4-84-55.5-32.5-13.5-66.9-20.3-102.4-20.3-49.3 0-97.4 13.5-139.2 39-10 6.1-19.5 12.8-28.5 20.1-9-7.3-18.5-14-28.5-20.1-41.8-25.5-89.9-39-139.2-39-35.5 0-69.9 6.8-102.4 20.3-31.4 13-59.7 31.7-84 55.5-24.4 23.9-43.5 51.7-56.9 82.8-13.9 32.3-21 66.6-21 101.9 0 33.3 6.8 68 20.3 103.3 11.3 29.5 27.5 60.1 48.2 91 32.8 48.9 77.9 99.9 133.9 151.6 92.8 85.7 184.7 144.9 188.6 147.3l23.7 15.2c10.5 6.7 24 6.7 34.5 0l23.7-15.2c3.9-2.5 95.7-61.6 188.6-147.3 56-51.7 101.1-102.7 133.9-151.6 20.7-30.9 37-61.5 48.2-91 13.5-35.3 20.3-70 20.3-103.3 0.1-35.3-7-69.6-20.9-101.9zM512 814.8S156 586.7 156 385.5C156 283.6 240.3 201 344.3 201c73.1 0 136.5 40.8 167.7 100.4C543.2 241.8 606.6 201 679.7 201c104 0 188.3 82.6 188.3 184.5 0 201.2-356 429.3-356 429.3z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-edit",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M723.2 917.76H286.72c-65.28 0-118.4-51.2-118.4-113.92V261.76C168.32 198.4 221.44 147.2 286.72 147.2h375.04c17.92 0 32 14.08 32 32s-14.08 32-32 32H286.72c-30.08 0-54.4 22.4-54.4 49.92v542.08c0 27.52 24.32 49.92 54.4 49.92H723.2c30.08 0 54.4-22.4 54.4-49.92V440.32c0-17.92 14.08-32 32-32s32 14.08 32 32v363.52c0 62.72-53.12 113.92-118.4 113.92z"}}),e._v(" "),t("path",{attrs:{d:"M499.84 602.24c-7.68 0-14.72-2.56-21.12-7.68-13.44-11.52-14.72-32-3.2-45.44L780.16 198.4c11.52-13.44 32-14.72 45.44-3.2s14.72 32 3.2 45.44L524.16 591.36c-6.4 7.04-15.36 10.88-24.32 10.88z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-delete",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M677.647059 256l0-90.352941c0-37.436235-23.461647-60.235294-61.771294-60.235294L408.094118 105.411765c-38.249412 0-61.741176 22.799059-61.741176 60.235294l0 90.352941-180.705882 0 0 60.235294 60.235294 0 0 512c0 54.272 33.972706 90.352941 90.352941 90.352941l391.529412 0c55.085176 0 90.352941-33.490824 90.352941-90.352941l0-512 60.235294 0 0-60.235294L677.647059 256zM406.588235 165.647059l210.823529 0-1.264941 90.352941L406.588235 256 406.588235 165.647059zM737.882353 858.352941l-451.764706 0 0-542.117647 451.764706 0L737.882353 858.352941zM466.823529 376.470588l-58.729412 0-1.505882 391.529412 60.235294 0L466.823529 376.470588zM617.411765 376.470588l-60.235294 0 0 391.529412 60.235294 0L617.411765 376.470588z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-reply",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M426.666667 384 426.666667 213.333333 128 512 426.666667 810.666667 426.666667 635.733333C640 635.733333 789.333333 704 896 853.333333 853.333333 640 725.333333 426.666667 426.666667 384Z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-error",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M512 720m-48 0a48 48 0 1 0 96 0 48 48 0 1 0-96 0Z"}}),e._v(" "),t("path",{attrs:{d:"M480 416v184c0 4.4 3.6 8 8 8h48c4.4 0 8-3.6 8-8V416c0-4.4-3.6-8-8-8h-48c-4.4 0-8 3.6-8 8z"}}),e._v(" "),t("path",{attrs:{d:"M955.7 856l-416-720c-6.2-10.7-16.9-16-27.7-16s-21.6 5.3-27.7 16l-416 720C56 877.4 71.4 904 96 904h832c24.6 0 40-26.6 27.7-48z m-783.5-27.9L512 239.9l339.8 588.2H172.2z"}})])])},staticRenderFns:[]},void 0,Sr,void 0,!0,void 0,!1,void 0,void 0,void 0);const Er=xr({},void 0,r.b.extend({name:"TransitionFade",functional:!0,props:{group:{type:Boolean,required:!1,default:!1},tag:{type:String,required:!1,default:"div"}},render:(n,{props:e,children:t})=>n(e.group?"TransitionGroup":"Transition",{props:{name:"fade",mode:"out-in",appear:!0,tag:e.tag}},t)}),void 0,void 0,void 0,!1,void 0,void 0,void 0);const Dr=xr({},void 0,r.b.extend({name:"VssueIcon",functional:!0,props:{name:{type:String,required:!0},title:{type:String,required:!1,default:null}},render:(n,{props:e,data:t})=>n("svg",Object.assign(Object.assign({},t),{class:["vssue-icon","vssue-icon-"+e.name],attrs:{"aria-hidden":"true"}}),[n("title",e.title),n("use",{attrs:{"xlink:href":"#vssue-icon-"+e.name}})])}),void 0,void 0,void 0,!1,void 0,void 0,void 0);let Cr=class extends r.b{constructor(){super(...arguments),this.editMode=!1,this.editContent=this.comment.contentRaw,this.creatingReactions=[],this.isPutingComment=!1,this.isDeletingComment=!1}get currentUser(){return this.vssue.user?this.vssue.user.username:null}get content(){return this.comment.content}get author(){return this.comment.author}get createdAt(){return xt(this.comment.createdAt)}get updatedAt(){return xt(this.comment.updatedAt)}get showReactions(){return Boolean(this.vssue.API&&this.vssue.API.platform.meta.reactable&&this.comment.reactions&&!this.editMode)}get reactionKeys(){return["heart","like","unlike"]}get editContentRows(){return this.editContent.split("\n").length-1}get editInputRows(){return this.editContentRows<3?5:this.editContentRows+2}async postReaction({reaction:n}){try{if(this.creatingReactions.includes(n))return;this.creatingReactions.push(n);await this.vssue.postCommentReaction({commentId:this.comment.id,reaction:n})||this.vssue.$emit("error",new Error(this.vssue.$t("reactionGiven",{reaction:this.vssue.$t(n)})));const e=await this.vssue.getCommentReactions({commentId:this.comment.id});e&&(this.comment.reactions=e)}finally{this.creatingReactions.splice(this.creatingReactions.findIndex(e=>e===n),1)}}enterEdit(){this.editMode=!0,this.$nextTick(()=>{this.$refs.input.focus()})}resetEdit(){this.editMode=!1,this.editContent=this.comment.contentRaw}async putComment(){try{if(this.vssue.isPending)return;if(this.editContent!==this.comment.contentRaw){this.isPutingComment=!0,this.vssue.isUpdatingComment=!0;const n=await this.vssue.putComment({commentId:this.comment.id,content:this.editContent});n&&this.vssue.comments.data.splice(this.vssue.comments.data.findIndex(n=>n.id===this.comment.id),1,n)}this.editMode=!1}finally{this.isPutingComment=!1,this.vssue.isUpdatingComment=!1}}async deleteComment(){try{if(this.vssue.isPending)return;if(!window.confirm(this.vssue.$t("deleteConfirm")))return;this.isDeletingComment=!0,this.vssue.isUpdatingComment=!0;await this.vssue.deleteComment({commentId:this.comment.id})?(this.vssue.comments.count-=1,this.vssue.comments.data.length>1&&this.vssue.comments.data.splice(this.vssue.comments.data.findIndex(n=>n.id===this.comment.id),1),this.vssue.query.page>1&&this.vssue.query.page>Math.ceil(this.vssue.comments.count/this.vssue.query.perPage)?this.vssue.query.page-=1:await this.vssue.getComments()):this.vssue.$emit("error",new Error(this.vssue.$t("deleteFailed")))}finally{this.isDeletingComment=!1,this.vssue.isUpdatingComment=!1}}};yr([gt({type:Object,required:!0})],Cr.prototype,"comment",void 0),yr([st()],Cr.prototype,"vssue",void 0),Cr=yr([Object(it.b)({components:{VssueIcon:Dr}})],Cr);const Ir=xr({render:function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"vssue-comment",class:{"vssue-comment-edit-mode":n.editMode,"vssue-comment-disabled":n.isDeletingComment||n.isPutingComment}},[t("div",{staticClass:"vssue-comment-avatar"},[t("a",{attrs:{href:n.author.homepage,title:n.author.username,target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:n.author.avatar,alt:n.author.username}})])]),n._v(" "),t("div",{staticClass:"vssue-comment-body"},[n._t("body",[t("div",{staticClass:"vssue-comment-header"},[t("span",{staticClass:"vssue-comment-author"},[t("a",{attrs:{href:n.author.homepage,title:n.author.username,target:"_blank",rel:"noopener noreferrer"}},[n._v("\n            "+n._s(n.author.username)+"\n          ")])]),n._v(" "),t("span",{staticClass:"vssue-comment-created-at"},[n._v("\n          "+n._s(n.createdAt)+"\n        ")])]),n._v(" "),t("div",{staticClass:"vssue-comment-main"},[n.editMode?t("textarea",{directives:[{name:"model",rawName:"v-model",value:n.editContent,expression:"editContent"}],ref:"input",staticClass:"vssue-edit-comment-input",attrs:{rows:n.editInputRows},domProps:{value:n.editContent},on:{keyup:function(e){return!e.type.indexOf("key")&&n._k(e.keyCode,"enter",13,e.key,"Enter")?null:e.ctrlKey?n.putComment():null},input:function(e){e.target.composing||(n.editContent=e.target.value)}}}):t("article",{staticClass:"markdown-body",domProps:{innerHTML:n._s(n.content)}})]),n._v(" "),t("div",{staticClass:"vssue-comment-footer"},[n.editMode?t("span",{staticClass:"vssue-comment-hint"},[n._v("\n          "+n._s(n.vssue.$t("editMode"))+"\n        ")]):n._e(),n._v(" "),n.showReactions?t("span",{staticClass:"vssue-comment-reactions"},n._l(n.reactionKeys,(function(e){return t("span",{key:e,staticClass:"vssue-comment-reaction",attrs:{title:n.vssue.$t(n.creatingReactions.includes(e)?"loading":e)},on:{click:function(t){return n.postReaction({reaction:e})}}},[t("VssueIcon",{attrs:{name:n.creatingReactions.includes(e)?"loading":e,title:n.vssue.$t(n.creatingReactions.includes(e)?"loading":e)}}),n._v(" "),t("span",{staticClass:"vssue-comment-reaction-number"},[n._v("\n              "+n._s(n.comment.reactions[e])+"\n            ")])],1)})),0):n._e(),n._v(" "),t("span",{staticClass:"vssue-comment-operations"},[n.comment.author.username===n.currentUser&&n.editMode?t("span",{staticClass:"vssue-comment-operation",class:{"vssue-comment-operation-muted":n.isPutingComment},attrs:{title:n.vssue.$t(n.isPutingComment?"loading":"submit")},on:{click:function(e){return n.putComment()}}},[t("VssueIcon",{directives:[{name:"show",rawName:"v-show",value:n.isPutingComment,expression:"isPutingComment"}],attrs:{name:"loading",title:n.vssue.$t("loading")}}),n._v("\n\n            "+n._s(n.vssue.$t("submit"))+"\n          ")],1):n._e(),n._v(" "),n.comment.author.username===n.currentUser&&n.editMode?t("span",{staticClass:"vssue-comment-operation vssue-comment-operation-muted",attrs:{title:n.vssue.$t("cancel")},on:{click:function(e){return n.resetEdit()}}},[n._v("\n            "+n._s(n.vssue.$t("cancel"))+"\n          ")]):n._e(),n._v(" "),n.comment.author.username===n.currentUser?t("span",{directives:[{name:"show",rawName:"v-show",value:!n.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(e){return n.enterEdit()}}},[t("VssueIcon",{attrs:{name:"edit",title:n.vssue.$t("edit")}})],1):n._e(),n._v(" "),n.comment.author.username===n.currentUser||n.vssue.isAdmin?t("span",{directives:[{name:"show",rawName:"v-show",value:!n.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(e){return n.deleteComment()}}},[t("VssueIcon",{attrs:{name:n.isDeletingComment?"loading":"delete",title:n.vssue.$t(n.isDeletingComment?"loading":"delete")}})],1):n._e(),n._v(" "),t("span",{directives:[{name:"show",rawName:"v-show",value:!n.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(e){return n.vssue.$emit("reply-comment",n.comment)}}},[t("VssueIcon",{attrs:{name:"reply",title:n.vssue.$t("reply")}})],1)])])])],2)])},staticRenderFns:[]},void 0,Cr,void 0,!1,void 0,!1,void 0,void 0,void 0);let Tr=class extends r.b{get disabled(){return this.vssue.isPending}get pageCount(){const n=Math.ceil(this.vssue.comments.count/this.vssue.comments.perPage);return n>1?n:1}get perPageOptions(){const n=[5,10,20,50];return!n.includes(this.vssue.options.perPage)&&this.vssue.options.perPage<100&&n.push(this.vssue.options.perPage),n.sort((n,e)=>n-e)}get page(){return this.vssue.query.page>this.pageCount?this.pageCount:this.vssue.query.page}set page(n){n>0&&n<=this.pageCount&&(this.vssue.query.page=n)}get perPage(){return this.vssue.query.perPage}set perPage(n){this.perPageOptions.includes(n)&&(this.vssue.query.perPage=n)}};yr([st()],Tr.prototype,"vssue",void 0),Tr=yr([Object(it.b)({components:{VssueIcon:Dr}})],Tr);const Or=xr({render:function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"vssue-pagination"},[t("div",{staticClass:"vssue-pagination-per-page"},[t("label",[t("select",{directives:[{name:"model",rawName:"v-model",value:n.perPage,expression:"perPage"}],staticClass:"vssue-pagination-select",attrs:{disabled:n.disabled},on:{change:function(e){var t=Array.prototype.filter.call(e.target.options,(function(n){return n.selected})).map((function(n){return"_value"in n?n._value:n.value}));n.perPage=e.target.multiple?t:t[0]}}},n._l(n.perPageOptions,(function(e){return t("option",{key:e,domProps:{value:e}},[n._v("\n          "+n._s(e)+"\n        ")])})),0),n._v(" "),t("span",[n._v("\n        "+n._s(n.vssue.$t("perPage"))+"\n      ")])]),n._v(" "),n.vssue.API.platform.meta.sortable?t("span",{class:{"vssue-pagination-link":!0,disabled:n.disabled},attrs:{title:n.vssue.$t("sort")},on:{click:function(e){n.vssue.query.sort="asc"===n.vssue.query.sort?"desc":"asc"}}},[n._v("\n      "+n._s("asc"===n.vssue.query.sort?"↑":"↓")+"\n    ")]):n._e()]),n._v(" "),t("div",{staticClass:"vssue-pagination-page"},[t("span",{class:{"vssue-pagination-link":!0,disabled:1===n.page||n.disabled},attrs:{title:n.vssue.$t("prev")},domProps:{textContent:n._s("<")},on:{click:function(e){n.page-=1}}}),n._v(" "),t("label",[t("span",[n._v("\n        "+n._s(n.vssue.$t("page"))+"\n      ")]),n._v(" "),t("select",{directives:[{name:"show",rawName:"v-show",value:n.pageCount>1,expression:"pageCount > 1"},{name:"model",rawName:"v-model",value:n.page,expression:"page"}],staticClass:"vssue-pagination-select",attrs:{disabled:n.disabled},on:{change:function(e){var t=Array.prototype.filter.call(e.target.options,(function(n){return n.selected})).map((function(n){return"_value"in n?n._value:n.value}));n.page=e.target.multiple?t:t[0]}}},n._l(n.pageCount,(function(e){return t("option",{key:e,domProps:{value:e}},[n._v("\n          "+n._s(e)+"\n        ")])})),0),n._v(" "),t("span",{directives:[{name:"show",rawName:"v-show",value:n.pageCount<2,expression:"pageCount < 2"}],domProps:{textContent:n._s(n.page)}}),n._v(" "),t("span",{domProps:{textContent:n._s(" / "+n.pageCount+" ")}})]),n._v(" "),t("span",{class:{"vssue-pagination-link":!0,disabled:n.page===n.pageCount||n.disabled},attrs:{title:n.vssue.$t("next")},domProps:{textContent:n._s(">")},on:{click:function(e){n.page+=1}}})])])},staticRenderFns:[]},void 0,Tr,void 0,!1,void 0,!1,void 0,void 0,void 0);let Ar=class extends r.b{};yr([st()],Ar.prototype,"vssue",void 0),Ar=yr([Object(it.b)({components:{TransitionFade:Er,VssueComment:Ir,VssuePagination:Or}})],Ar);const _r=xr({render:function(){var n=this.$createElement,e=this._self._c||n;return e("div",{staticClass:"vssue-comments"},[e("VssuePagination"),this._v(" "),e("TransitionFade",{attrs:{group:""}},this._l(this.vssue.comments.data,(function(n){return e("VssueComment",{key:n.id,attrs:{comment:n}})})),1),this._v(" "),e("VssuePagination",{directives:[{name:"show",rawName:"v-show",value:this.vssue.comments.data.length>5,expression:"vssue.comments.data.length > 5"}]})],1)},staticRenderFns:[]},void 0,Ar,void 0,!1,void 0,!1,void 0,void 0,void 0);const Rr=xr({},void 0,r.b.extend({name:"VssueIcon",functional:!0,props:{type:{type:String,required:!1,default:"default"}},render:(n,{props:e,data:t,children:r})=>n("button",Object.assign(Object.assign({},t),{class:["vssue-button","vssue-button-"+e.type]}),r)}),void 0,void 0,void 0,!1,void 0,void 0,void 0);let Pr=class extends r.b{constructor(){super(...arguments),this.content=""}get user(){return this.vssue.user}get platform(){return this.vssue.API&&this.vssue.API.platform.name}get isInputDisabled(){return this.loading||null===this.user||null===this.vssue.issue}get isSubmitDisabled(){return""===this.content||this.vssue.isPending||null===this.vssue.issue}get loading(){return this.vssue.isCreatingComment}get contentRows(){return this.content.split("\n").length-1}get inputRows(){return this.contentRows<3?5:this.contentRows+2}created(){this.vssue.$on("reply-comment",n=>{const e=n.contentRaw.replace(/\n/g,"\n> "),t=`@${n.author.username}\n\n> ${e}\n\n`;this.content=this.content.concat(t),this.focus()})}beforeDestroy(){this.vssue.$off("reply-comment")}focus(){this.$refs.input.focus()}async submit(){this.isSubmitDisabled||(await this.vssue.postComment({content:this.content}),this.content="",await this.vssue.getComments())}};yr([st()],Pr.prototype,"vssue",void 0),Pr=yr([Object(it.b)({components:{VssueButton:Rr,VssueIcon:Dr}})],Pr);const Fr=xr({render:function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"vssue-new-comment"},[t("div",{staticClass:"vssue-comment-avatar"},[n.user?t("a",{attrs:{href:n.user.homepage,title:n.user.username,target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:n.user.avatar,alt:n.user.username}})]):t("VssueIcon",{attrs:{name:n.platform.toLowerCase(),title:n.vssue.$t("loginToComment",{platform:n.platform})},on:{click:function(e){return n.vssue.login()}}})],1),n._v(" "),t("div",{staticClass:"vssue-new-comment-body"},[t("textarea",{directives:[{name:"model",rawName:"v-model",value:n.content,expression:"content"}],ref:"input",staticClass:"vssue-new-comment-input",attrs:{rows:n.inputRows,disabled:n.isInputDisabled,placeholder:n.vssue.$t(n.user?"placeholder":"noLoginPlaceHolder"),spellcheck:!1,"aria-label":"leave a comment"},domProps:{value:n.content},on:{keyup:function(e){return!e.type.indexOf("key")&&n._k(e.keyCode,"enter",13,e.key,"Enter")?null:e.ctrlKey?n.submit():null},input:function(e){e.target.composing||(n.content=e.target.value)}}})]),n._v(" "),t("div",{staticClass:"vssue-new-comment-footer"},[n.user?t("span",{staticClass:"vssue-current-user"},[t("span",[n._v(n._s(n.vssue.$t("currentUser"))+" - "+n._s(n.user.username)+" - ")]),n._v(" "),t("a",{staticClass:"vssue-logout",on:{click:function(e){return n.vssue.logout()}}},[n._v("\n        "+n._s(n.vssue.$t("logout"))+"\n      ")])]):t("span",{staticClass:"vssue-current-user"},[n._v("\n      "+n._s(n.vssue.$t("loginToComment",{platform:n.platform}))+"\n    ")]),n._v(" "),t("div",{staticClass:"vssue-new-comment-operations"},[n.user?t("VssueButton",{staticClass:"vssue-button-submit-comment",attrs:{type:"primary",disabled:n.isSubmitDisabled},on:{click:function(e){return n.submit()}}},[t("VssueIcon",{directives:[{name:"show",rawName:"v-show",value:n.loading,expression:"loading"}],attrs:{name:"loading"}}),n._v("\n\n        "+n._s(n.vssue.$t(n.loading?"submitting":"submitComment"))+"\n      ")],1):t("VssueButton",{staticClass:"vssue-button-login",attrs:{type:"primary",title:n.vssue.$t("loginToComment",{platform:n.platform})},on:{click:function(e){return n.vssue.login()}}},[n._v("\n        "+n._s(n.vssue.$t("login",{platform:n.platform}))+"\n      ")])],1)])])},staticRenderFns:[]},void 0,Pr,void 0,!1,void 0,!1,void 0,void 0,void 0);let Br=class extends r.b{constructor(){super(...arguments),this.progress={show:!1,percent:0,timer:null,speed:200},this.alert={show:!1,message:null,timer:null}}onLoadingCommentsChange(n){this.vssue.comments&&(n?this.progressStart():this.progressDone())}created(){this.vssue.$on("error",n=>this.alertShow(n.message))}beforeDestroy(){this.vssue.$off("error"),null!==this.progress.timer&&window.clearTimeout(this.progress.timer),null!==this.alert.timer&&window.clearTimeout(this.alert.timer)}progressStart(){this.progress.show=!0,this.progress.percent=0,this.progress.timer=window.setInterval(()=>{this.progress.percent+=5,this.progress.percent>94&&null!==this.progress.timer&&window.clearInterval(this.progress.timer)},this.progress.speed)}progressDone(){this.progress.percent=100,null!==this.progress.timer&&window.clearTimeout(this.progress.timer),this.progress.timer=null,window.setTimeout(()=>{this.progress.show=!1},this.progress.speed)}alertShow(n){this.alert.show=!0,this.alert.message=n,null!==this.alert.timer&&window.clearTimeout(this.alert.timer),this.alert.timer=window.setTimeout(()=>{this.alertHide()},3e3)}alertHide(){this.alert.show=!1,null!==this.alert.timer&&window.clearTimeout(this.alert.timer),this.alert.timer=null}};yr([st()],Br.prototype,"vssue",void 0),yr([ft("vssue.isLoadingComments")],Br.prototype,"onLoadingCommentsChange",null),Br=yr([Object(it.b)({components:{TransitionFade:Er}})],Br);const Mr=xr({render:function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"vssue-notice"},[t("div",{directives:[{name:"show",rawName:"v-show",value:n.progress.show,expression:"progress.show"}],staticClass:"vssue-progress",style:{width:n.progress.percent+"%",transition:"all "+n.progress.speed+"ms linear"}}),n._v(" "),t("TransitionFade",[t("div",{directives:[{name:"show",rawName:"v-show",value:n.alert.show,expression:"alert.show"}],staticClass:"vssue-alert",domProps:{textContent:n._s(n.alert.message)},on:{click:function(e){return n.alertHide()}}})])],1)},staticRenderFns:[]},void 0,Br,void 0,!1,void 0,!1,void 0,void 0,void 0);let jr=class extends r.b{get status(){return this.vssue.isFailed?"failed":this.vssue.isInitializing?"initializing":this.vssue.isIssueNotCreated&&!this.vssue.isCreatingIssue?this.vssue.isAdmin||!this.vssue.isLogined?"issueNotCreated":"failed":this.vssue.isLoginRequired?"loginRequired":!this.vssue.comments||this.vssue.isCreatingIssue?"loadingComments":0===this.vssue.comments.data.length?"noComments":null}handleClick(){"issueNotCreated"===this.status?this.vssue.postIssue():"loginRequired"===this.status&&this.vssue.login()}};yr([st()],jr.prototype,"vssue",void 0),jr=yr([Object(it.b)({components:{TransitionFade:Er,VssueIcon:Dr}})],jr);const Lr=xr({render:function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("TransitionFade",[n.status?t("div",{key:n.status,staticClass:"vssue-status"},[["failed","loadingComments","initializing"].includes(n.status)?t("VssueIcon",{attrs:{name:"failed"===n.status?"error":"loading"}}):n._e(),n._v(" "),t("p",{staticClass:"vssue-status-info"},[t(["issueNotCreated","loginRequired"].includes(n.status)?"a":"span",{tag:"Component",on:{click:n.handleClick}},[n._v("\n        "+n._s(n.vssue.$t(n.status))+"\n      ")])],1)],1):n._e()])},staticRenderFns:[]},void 0,jr,void 0,!1,void 0,!1,void 0,void 0,void 0);let Nr=class extends r.b{};yr([st()],Nr.prototype,"vssue",void 0),Nr=yr([Object(it.b)({components:{TransitionFade:Er,VssueIcon:Dr,VssueComments:_r,VssueNewComment:Fr,VssueNotice:Mr,VssueStatus:Lr}})],Nr);const $r=xr({render:function(){var n=this.$createElement,e=this._self._c||n;return e("TransitionFade",[this.vssue.isInitializing?e("VssueStatus"):e("div",{staticClass:"vssue-body"},[this.vssue.API?e("VssueNewComment"):this._e(),this._v(" "),e("VssueNotice"),this._v(" "),e("TransitionFade",[this.vssue.comments&&this.vssue.comments.data.length>0?e("VssueComments"):e("VssueStatus")],1)],1)],1)},staticRenderFns:[]},void 0,Nr,void 0,!1,void 0,!1,void 0,void 0,void 0);let Ur=class extends r.b{};yr([st()],Ur.prototype,"vssue",void 0),Ur=yr([it.b],Ur);const zr=xr({render:function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"vssue-header"},[t("a",{staticClass:"vssue-header-comments-count",attrs:{href:n.vssue.issue?n.vssue.issue.link:null,target:"_blank",rel:"noopener noreferrer"}},[t("span",[n._v("\n      "+n._s(n.vssue.comments?n.vssue.$tc("comments",n.vssue.comments.count,{count:n.vssue.comments.count}):n.vssue.$tc("comments",0))+"\n    ")])]),n._v(" "),t("span",{staticClass:"vssue-header-powered-by"},[t("span",[n._v("Powered by")]),n._v(" "),n.vssue.API?t("span",[t("a",{attrs:{href:n.vssue.API.platform.link,title:n.vssue.API.platform.name+" API "+n.vssue.API.platform.version,target:"_blank",rel:"noopener noreferrer"}},[n._v("\n        "+n._s(n.vssue.API.platform.name)+"\n      ")]),n._v(" "),t("span",[n._v("&")])]):n._e(),n._v(" "),t("a",{attrs:{href:"https://github.com/meteorlxy/vssue",title:"Vssue v"+n.vssue.version,target:"_blank",rel:"noopener noreferrer"}},[n._v("\n      Vssue\n    ")])])])},staticRenderFns:[]},void 0,Ur,void 0,!1,void 0,!1,void 0,void 0,void 0),Hr={login:"Login with {platform}",logout:"Logout",currentUser:"Current User",loading:"Loading",submit:"Submit",submitting:"Submitting",submitComment:"Submit Comment",cancel:"Cancel",edit:"Edit",editMode:"Edit Mode",delete:"Delete",reply:"Reply",heart:"Heart",like:"Like",unlike:"Unlike",perPage:"Comments per page",sort:"Click to change the sort direction",page:"Page",prev:"Previous Page",next:"Next Page",comments:"Comments | {count} Comment | {count} Comments",loginToComment:"Login with {platform} account to leave a comment",placeholder:"Leave a comment. Styling with Markdown is supported. Ctrl + Enter to submit.",noLoginPlaceHolder:"Login to leave a comment. Styling with Markdown is supported. ",failed:"Failed to load comments",initializing:"Initializing...",issueNotCreated:"Click to create issue",loadingComments:"Loading comments...",loginRequired:"Login to view comments",noComments:"No comments yet. Leave the first comment !",reactionGiven:"Already given '{reaction}' reaction",deleteConfirm:"Confirm to delete this comment ?",deleteFailed:"Failed to delete comment"},qr={login:"使用 {platform} 登录",logout:"退出登录",currentUser:"当前用户",loading:"加载中",submit:"提交",submitting:"发表中",submitComment:"发表评论",cancel:"取消",edit:"编辑",editMode:"编辑模式",delete:"删除",reply:"回复",heart:"喜欢",like:"赞",unlike:"踩",perPage:"每页评论数",sort:"点击改变排序方式",page:"页数",prev:"上一页",next:"下一页",comments:"评论 | {count} 条评论 | {count} 条评论",loginToComment:"使用 {platform} 帐号登录后发表评论",placeholder:"留下你的评论丨支持 Markdown 语法丨Ctrl + Enter 发表评论",noLoginPlaceHolder:"登录后才能发表评论丨支持 Markdown 语法",failed:"评论加载失败",initializing:"正在初始化...",issueNotCreated:"点击创建 Issue",loadingComments:"正在加载评论...",loginRequired:"登录后查看评论",noComments:"还没有评论，来发表第一条评论吧！",reactionGiven:"已经添加过 '{reaction}' 了",deleteConfirm:"确认要删除该评论吗？",deleteFailed:"评论删除失败"},Vr={login:"Entrar com {platform}",logout:"Sair",currentUser:"Usuário Atual",loading:"Carregando",submit:"Enviar",submitting:"Enviando",submitComment:"Enviar Comentário",cancel:"Cancelar",edit:"Editar",editMode:"Modo de Edição",delete:"Apagar",reply:"Responder",heart:"Heart",like:"Like",unlike:"Unlike",perPage:"Comentários por página",sort:"Clique para alterar a ordenação",page:"Página",prev:"Página Anterior",next:"Próxima Página",comments:"Comentários | {count} Comentário | {count} Comentários",loginToComment:"Entre com uma conta {platform} para deixar um comentário",placeholder:"Deixe um comentário. Estilos com Markdown suportados. Ctrl + Enter para enviar.",noLoginPlaceHolder:"Entre para deixar um comentário. Estilos com Markdown suportados. ",failed:"Falha ao carregar comentários",initializing:"Inicializando...",issueNotCreated:"Click to create issue",loadingComments:"Carregando comentários...",loginRequired:"Entrar para visualizar comentários",noComments:"Nenhum comentário. Deixe o primeiro comentário!",reactionGiven:"Já reagiu com '{reaction}'",deleteConfirm:"Apagar este comentário?",deleteFailed:"Falha ao apagar comentário"},Kr={login:"{platform} でログイン",logout:"ログアウト",currentUser:"現在のユーザー",loading:"読み込み中",submit:"送信",submitting:"送信中",submitComment:"コメントを送信",cancel:"キャンセル",edit:"編集",editMode:"編集モード",delete:"削除",reply:"返信",heart:"ハート",like:"高評価",unlike:"低評価",perPage:"コメント/ページ",sort:"並び順を変更するにはクリックしてください",page:"ページ",prev:"前のページ",next:"次のページ",comments:"コメント | {count} コメント | {count} コメント",loginToComment:"コメントを残すには {platform} アカウントでログインしてください。",placeholder:"コメントを残してください。Markdown 記法をサポートしています。 Ctrl + Enter で送信できます。",noLoginPlaceHolder:"コメントを残すにはログインしてください。マークダウン記法をサポートしています。",failed:"コメントの読み込みに失敗しました",initializing:"初期化中...",issueNotCreated:"Click to create issue",loadingComments:"コメントの読み込み中...",loginRequired:"コメントを見るにはログインしてください",noComments:"まだコメントがありません。最初のコメントを残しましょう！",reactionGiven:"既に '{reaction}' のリアクションをしています",deleteConfirm:"本当にコメントを削除してもいいですか？",deleteFailed:"コメントの削除に失敗しました"},Wr={login:"התחברו עם {platform}",logout:"התנתקו",currentUser:"משתמש/ת נוכחי/ת",loading:"טוען",submit:"שליחה",submitting:"שולח",submitComment:"שליחת תגובה",cancel:"ביטל",edit:"עריכה",editMode:"מצב עריכה",delete:"מחיקה",reply:"תשובה",heart:"לב",like:"לייק",unlike:"אנלייק",perPage:"תגובות לדף",sort:"לחצו כדי לשנות את כיוון המיון",page:"דף",prev:"הדף הקודם",next:"הדף הבא",comments:"תגובות | {count} תגובה | {count} תגובות",loginToComment:"התחברו עם חשבון {platform} כדי להשאיר תגובה",placeholder:"השאירו תגובה. יש תמיכה בעיצוב בעזרת Markdown. Ctrl + Enter כדי לשלוח.",noLoginPlaceHolder:"התחברו כדי להשאיר תגובה. יש תמיכה בעיצוב בעזרת Markdown. ",failed:"כשלון בטעינת התגובות",initializing:"מאתחל...",issueNotCreated:"לחצו ליצירת issue",loadingComments:"טוען תגובות...",loginRequired:"התחברו כדי לצפות בתגובות",noComments:"עדיין אין תגובות. השאירו תגובה ראשונה !",reactionGiven:"כבר ניתן חיווי '{reaction}'",deleteConfirm:"בטוחים במחיקת התגובה ?",deleteFailed:"כשלון במחיקת התגובה"};Object.prototype.hasOwnProperty.call(r.b,"$i18n")||r.b.use(kr);const Gr=new kr({locale:"en",fallbackLocale:"en",messages:{en:Hr,"en-US":Hr,zh:qr,"zh-CN":qr,pt:Vr,"pt-BR":Vr,ja:Kr,"ja-JP":Kr,he:Wr,"he-IL":Wr}});let Jr=class extends r.b{constructor(){super(...arguments),this.title=n=>`${n.prefix}${document.title}`,this.issueId=null,this.options=null,this.API=null,this.accessToken=null,this.user=null,this.issue=null,this.comments=null,this.query={page:1,perPage:10,sort:"desc"},this.isInitializing=!0,this.isIssueNotCreated=!1,this.isLoginRequired=!1,this.isFailed=!1,this.isCreatingIssue=!1,this.isLoadingComments=!1,this.isCreatingComment=!1,this.isUpdatingComment=!1}get version(){return"1.4.8"}get issueTitle(){return null===this.options?"":"function"==typeof this.title?this.title(this.options):`${this.options.prefix}${this.title}`}get isPending(){return this.isLoadingComments||this.isCreatingComment||this.isUpdatingComment}get isLogined(){return null!==this.accessToken&&null!==this.user}get isAdmin(){return null!==this.options&&null!==this.accessToken&&null!==this.user&&(this.user.username===this.options.owner||this.options.admins.includes(this.user.username))}get accessTokenKey(){return this.API?`Vssue.${this.API.platform.name.toLowerCase()}.access_token`:""}onQueryPerPageChange(){this.query.page=1,this.getComments()}onQueryChange(){this.getComments()}setOptions(n){this.options=Object.assign({labels:["Vssue"],state:"Vssue",prefix:"[Vssue]",admins:[],perPage:10,proxy:n=>"https://cors-anywhere.azm.workers.dev/"+n,issueContent:({url:n})=>n,autoCreateIssue:!1},n);const e=["api","owner","repo","clientId"];for(const n of e)this.options[n]||console.warn(`[Vssue] the option '${n}' is required`);if(this.options.locale)this.$i18n.locale=this.options.locale;else{const n=Object.keys(this.$i18n.messages),e=window.navigator.languages;this.$i18n.locale=e.filter(e=>n.includes(e)).shift()||"en"}}async init(){try{await this.initStore(),await this.initComments()}catch(n){n.response&&[401,403].includes(n.response.status)?this.isLoginRequired=!0:this.isFailed=!0,console.error(n)}}async initStore(){try{if(!this.options)throw new Error("Options are required to initialize Vssue");this.API=null,this.accessToken=null,this.user=null,this.issue=null,this.comments=null,this.query={page:1,perPage:this.options.perPage,sort:"desc"},this.isInitializing=!0,this.isIssueNotCreated=!1,this.isLoginRequired=!1,this.isFailed=!1,this.isCreatingIssue=!1,this.isLoadingComments=!1,this.isCreatingComment=!1,this.isUpdatingComment=!1;const n=this.options.api;this.API=new n({baseURL:this.options.baseURL,labels:this.options.labels,state:this.options.state,owner:this.options.owner,repo:this.options.repo,clientId:this.options.clientId,clientSecret:this.options.clientSecret,proxy:this.options.proxy}),await this.handleAuth()}finally{this.isInitializing=!1}}async initComments(){if(this.API&&this.options)if(this.issueId){const[n,e]=await Promise.all([this.API.getIssue({accessToken:this.accessToken,issueId:this.issueId}),this.API.getComments({accessToken:this.accessToken,issueId:this.issueId,query:this.query})]);this.issue=n,this.comments=e}else this.issue=await this.API.getIssue({accessToken:this.accessToken,issueTitle:this.issueTitle}),null===this.issue?(this.isIssueNotCreated=!0,this.options.autoCreateIssue&&await this.postIssue()):await this.getComments()}async postIssue(){if(this.API&&this.options&&!this.issue&&!this.issueId&&(this.isLogined||this.login(),this.isAdmin))try{this.isCreatingIssue=!0;const n=await this.API.postIssue({title:this.issueTitle,content:await this.options.issueContent({options:this.options,url:wt(window.location.href)}),accessToken:this.accessToken});this.issue=n,this.isIssueNotCreated=!1,await this.getComments()}catch(n){this.isFailed=!0}finally{this.isCreatingIssue=!1}}async getComments(){try{if(!this.API||!this.issue||this.isLoadingComments)return;this.isLoadingComments=!0;const n=await this.API.getComments({accessToken:this.accessToken,issueId:this.issue.id,query:this.query});return this.comments=n,this.query.page!==n.page&&(this.query.page=n.page),this.query.perPage!==n.perPage&&(this.query.perPage=n.perPage),n}catch(n){if(!n.response||![401,403].includes(n.response.status)||this.isLogined)throw this.$emit("error",n),n;this.isLoginRequired=!0}finally{this.isLoadingComments=!1}}async postComment({content:n}){try{if(!this.API||!this.issue||this.isCreatingComment)return;this.isCreatingComment=!0;return await this.API.postComment({accessToken:this.accessToken,content:n,issueId:this.issue.id})}catch(n){throw this.$emit("error",n),n}finally{this.isCreatingComment=!1}}async putComment({commentId:n,content:e}){try{if(!this.API||!this.issue)return;return await this.API.putComment({accessToken:this.accessToken,issueId:this.issue.id,commentId:n,content:e})}catch(n){throw this.$emit("error",n),n}}async deleteComment({commentId:n}){try{if(!this.API||!this.issue)return;return await this.API.deleteComment({accessToken:this.accessToken,issueId:this.issue.id,commentId:n})}catch(n){throw this.$emit("error",n),n}}async getCommentReactions({commentId:n}){try{if(!this.API||!this.issue)return;return await this.API.getCommentReactions({accessToken:this.accessToken,issueId:this.issue.id,commentId:n})}catch(n){throw this.$emit("error",n),n}}async postCommentReaction({commentId:n,reaction:e}){try{if(!this.API||!this.issue)return!1;return await this.API.postCommentReaction({accessToken:this.accessToken,issueId:this.issue.id,commentId:n,reaction:e})}catch(n){throw this.$emit("error",n),n}}login(){this.API&&this.API.redirectAuth()}logout(){this.setAccessToken(null),this.user=null}async handleAuth(){if(!this.API)return;const n=await this.API.handleAuth();n?(this.setAccessToken(n),this.user=await this.API.getUser({accessToken:n})):this.getAccessToken()?this.user=await this.API.getUser({accessToken:this.accessToken}):(this.setAccessToken(null),this.user=null)}getAccessToken(){return this.accessToken=window.localStorage.getItem(this.accessTokenKey),this.accessToken}setAccessToken(n){null===n?window.localStorage.removeItem(this.accessTokenKey):window.localStorage.setItem(this.accessTokenKey,n),this.accessToken=n}};yr([ft("query.perPage")],Jr.prototype,"onQueryPerPageChange",null),yr([ft("query.page"),ft("query.sort")],Jr.prototype,"onQueryChange",null),Jr=yr([Object(it.b)({i18n:Gr})],Jr);var Yr=Jr;let Qr=class extends r.b{constructor(){super(...arguments),this.vssue=new Yr}onOptionsChange(n){this.vssue.setOptions(n)}mounted(){null!==this.title&&(this.vssue.title=this.title),null!==this.issueId&&(this.vssue.issueId=this.issueId),this.vssue.setOptions(this.options),this.vssue.init()}};var Xr;yr([gt({type:[String,Function],required:!1,default:null})],Qr.prototype,"title",void 0),yr([gt({type:[String,Number],required:!1,default:null})],Qr.prototype,"issueId",void 0),yr([gt({type:Object,required:!1,default:()=>({})})],Qr.prototype,"options",void 0),yr([(Xr="vssue",Object(it.a)((function(n,e){var t=n.provide;ut(n),lt(t)&&(t=n.provide=ct(t)),t.managed[e]=Xr||e})))],Qr.prototype,"vssue",void 0),yr([ft("options",{deep:!0})],Qr.prototype,"onOptionsChange",null),Qr=yr([Object(it.b)({components:{Iconfont:wr,VssueBody:$r,VssueHeader:zr}})],Qr);const Zr=xr({render:function(){var n=this.$createElement,e=this._self._c||n;return e("div",{staticClass:"vssue"},[e("Iconfont"),this._v(" "),e("VssueHeader"),this._v(" "),e("VssueBody")],1)},staticRenderFns:[]},void 0,Qr,void 0,!1,void 0,!1,void 0,void 0,void 0);var na=t(10),ea=t.n(na);function ta(n){return{username:n.login,avatar:n.avatar_url,homepage:n.html_url}}function ra(n){return{id:n.number,title:n.title,content:n.body,link:n.html_url}}function aa(n){return{like:n["+1"],unlike:n[-1],heart:n.heart}}function oa(n){return{id:n.id,content:n.body_html,contentRaw:n.body,author:ta(n.user),createdAt:n.created_at,updatedAt:n.updated_at,reactions:aa(n.reactions)}}function ia(n){return"like"===n?"+1":"unlike"===n?"-1":n}class sa{constructor({baseURL:n="https://github.com",owner:e,repo:t,labels:r,clientId:a,clientSecret:o,state:i,proxy:s}){if(void 0===o||void 0===s)throw new Error("clientSecret and proxy is required for GitHub V3");this.baseURL=n,this.owner=e,this.repo=t,this.labels=r,this.clientId=a,this.clientSecret=o,this.state=i,this.proxy=s,this.$http=ea.a.create({baseURL:"https://github.com"===n?"https://api.github.com":kt(n,"api/v3"),headers:{Accept:"application/vnd.github.v3+json"}}),this.$http.interceptors.response.use(n=>n.data&&n.data.error?Promise.reject(new Error(n.data.error_description)):n,n=>(void 0===n.response&&"Network Error"===n.message&&(n.response={status:403}),Promise.reject(n)))}get platform(){return{name:"GitHub",link:this.baseURL,version:"v3",meta:{reactable:!0,sortable:!1}}}redirectAuth(){window.location.href=bt(kt(this.baseURL,"login/oauth/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,scope:"public_repo",state:this.state})}async handleAuth(){const n=Et(window.location.search);if(n.code){if(n.state!==this.state)return null;const e=n.code;delete n.code,delete n.state;const t=bt(wt(window.location.href),n)+window.location.hash;window.history.replaceState(null,"",t);return await this.getAccessToken({code:e})}return null}async getAccessToken({code:n}){const e=kt(this.baseURL,"login/oauth/access_token"),t="function"==typeof this.proxy?this.proxy(e):this.proxy,{data:r}=await this.$http.post(t,{client_id:this.clientId,client_secret:this.clientSecret,code:n},{headers:{Accept:"application/json"}});return r.access_token}async getUser({accessToken:n}){const{data:e}=await this.$http.get("user",{headers:{Authorization:"token "+n}});return ta(e)}async getIssue({accessToken:n,issueId:e,issueTitle:t}){const r={};if(n&&(r.headers={Authorization:"token "+n}),!e){r.params={q:[`"${t}"`,"is:issue","in:title",`repo:${this.owner}/${this.repo}`,"is:public",...this.labels.map(n=>"label:"+n)].join(" "),timestamp:Date.now()};const{data:n}=await this.$http.get("search/issues",r);return n.items.map(ra).find(n=>n.title===t)||null}try{r.params={timestamp:Date.now()};const{data:n}=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/${e}`,r);return ra(n)}catch(n){if(n.response&&404===n.response.status)return null;throw n}}async postIssue({accessToken:n,title:e,content:t}){const{data:r}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues`,{title:e,body:t,labels:this.labels},{headers:{Authorization:"token "+n}});return ra(r)}async getComments({accessToken:n,issueId:e,query:{page:t=1,perPage:r=10}={}}){const a={params:{timestamp:Date.now()}},o={params:{page:t,per_page:r,timestamp:Date.now()},headers:{Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}};n&&(a.headers={Authorization:"token "+n},o.headers.Authorization="token "+n);const[i,s]=await Promise.all([this.$http.get(`repos/${this.owner}/${this.repo}/issues/${e}`,a),this.$http.get(`repos/${this.owner}/${this.repo}/issues/${e}/comments`,o)]),l=s.headers.link||null,c=/rel="next"/.test(l)?Number(l.replace(/^.*[^_]page=(\d*).*rel="next".*$/,"$1"))-1:/rel="prev"/.test(l)?Number(l.replace(/^.*[^_]page=(\d*).*rel="prev".*$/,"$1"))+1:1,p=l?Number(l.replace(/^.*per_page=(\d*).*$/,"$1")):r;return{count:Number(i.data.comments),page:c,perPage:p,data:s.data.map(oa)}}async postComment({accessToken:n,issueId:e,content:t}){const{data:r}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues/${e}/comments`,{body:t},{headers:{Authorization:"token "+n,Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}});return oa(r)}async putComment({accessToken:n,commentId:e,content:t}){const{data:r}=await this.$http.patch(`repos/${this.owner}/${this.repo}/issues/comments/${e}`,{body:t},{headers:{Authorization:"token "+n,Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}});return oa(r)}async deleteComment({accessToken:n,commentId:e}){const{status:t}=await this.$http.delete(`repos/${this.owner}/${this.repo}/issues/comments/${e}`,{headers:{Authorization:"token "+n}});return 204===t}async getCommentReactions({accessToken:n,commentId:e}){const{data:t}=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/comments/${e}`,{params:{timestamp:Date.now()},headers:{Authorization:"token "+n,Accept:"application/vnd.github.squirrel-girl-preview"}});return aa(t.reactions)}async postCommentReaction({accessToken:n,commentId:e,reaction:t}){const r=await this.$http.post(`repos/${this.owner}/${this.repo}/issues/comments/${e}/reactions`,{content:ia(t)},{headers:{Authorization:"token "+n,Accept:"application/vnd.github.squirrel-girl-preview"}});return 200===r.status?this.deleteCommentReaction({accessToken:n,commentId:e,reactionId:r.data.id}):201===r.status}async deleteCommentReaction({accessToken:n,commentId:e,reactionId:t}){return 204===(await this.$http.delete(`repos/${this.owner}/${this.repo}/issues/comments/${e}/reactions/${t}`,{headers:{Authorization:"token "+n,Accept:"application/vnd.github.squirrel-girl-preview"}})).status}}function la(n){return null===n?{username:"ghost",avatar:"https://avatars3.githubusercontent.com/u/10137?v=4",homepage:"https://github.com/ghost"}:{username:n.login,avatar:n.avatarUrl,homepage:n.url}}function ca(n){return{id:n.number,title:n.title,content:n.body,link:n.url}}function pa(n){return{like:n.find(n=>"THUMBS_UP"===n.content).users.totalCount,unlike:n.find(n=>"THUMBS_DOWN"===n.content).users.totalCount,heart:n.find(n=>"HEART"===n.content).users.totalCount}}function ua(n){return{id:n.id,content:n.bodyHTML,contentRaw:n.body,author:la(n.author),createdAt:n.createdAt,updatedAt:n.updatedAt,reactions:pa(n.reactionGroups)}}function da(n){return"like"===n?"THUMBS_UP":"unlike"===n?"THUMBS_DOWN":"heart"===n?"HEART":n}class ma{constructor({baseURL:n="https://github.com",owner:e,repo:t,labels:r,clientId:a,clientSecret:o,state:i,proxy:s}){if(void 0===o||void 0===s)throw new Error("clientSecret and proxy is required for GitHub V4");this.baseURL=n,this.owner=e,this.repo=t,this.labels=r,this.clientId=a,this.clientSecret=o,this.state=i,this.proxy=s,this._pageInfo={page:1,startCursor:null,endCursor:null,sort:null,perPage:null},this._issueNodeId=null,this.$http=ea.a.create({baseURL:"https://github.com"===n?"https://api.github.com":kt(n,"api"),headers:{Accept:"application/vnd.github.v3+json"}}),this.$http.interceptors.response.use(n=>n.data.error?Promise.reject(n.data.error_description):n.data.errors?Promise.reject(n.data.errors[0].message):n)}get platform(){return{name:"GitHub",link:this.baseURL,version:"v4",meta:{reactable:!0,sortable:!0}}}redirectAuth(){window.location.href=bt(kt(this.baseURL,"login/oauth/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,scope:"public_repo",state:this.state})}async handleAuth(){const n=Et(window.location.search);if(n.code){if(n.state!==this.state)return null;const e=n.code;delete n.code,delete n.state;const t=bt(wt(window.location.href),n)+window.location.hash;window.history.replaceState(null,"",t);return await this.getAccessToken({code:e})}return null}async getAccessToken({code:n}){const e=kt(this.baseURL,"login/oauth/access_token"),t="function"==typeof this.proxy?this.proxy(e):this.proxy,{data:r}=await this.$http.post(t,{client_id:this.clientId,client_secret:this.clientSecret,code:n},{headers:{Accept:"application/json"}});return r.access_token}async getUser({accessToken:n}){const{data:e}=await this.$http.post("graphql",{query:"query getUser {\n  viewer {\n    login\n    avatarUrl\n    url\n  }\n}"},{headers:{Authorization:"token "+n}});return la(e.data.viewer)}async getIssue({accessToken:n,issueId:e,issueTitle:t}){const r={};if(n&&(r.headers={Authorization:"token "+n}),!e){const n=[`"${t}"`,"in:title",`repo:${this.owner}/${this.repo}`,"is:public",...this.labels.map(n=>"label:"+n)].join(" "),{data:e}=await this.$http.post("graphql",{variables:{query:n},query:"query getIssueByTitle(\n  $query: String!\n) {\n  search(\n    query: $query\n    type: ISSUE\n    first: 20\n    ) {\n      nodes {\n      ... on Issue {\n        id\n        number\n        title\n        body\n        url\n      }\n    }\n  }\n}"},r),a=e.data.search.nodes.find(n=>n.title===t);return a?(this._issueNodeId=a.id,ca(a)):null}try{const{data:n}=await this.$http.post("graphql",{query:`query getIssueById {\n  repository(owner: "${this.owner}", name: "${this.repo}") {\n    issue (number: ${e}) {\n      id\n      number\n      title\n      body\n      url\n    }\n  }\n}`},r);return this._issueNodeId=n.data.repository.issue.id,ca(n.data.repository.issue)}catch(n){if(n.response&&404===n.response.status)return null;throw n}}async postIssue({accessToken:n,title:e,content:t}){const{data:r}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues`,{title:e,body:t,labels:this.labels},{headers:{Authorization:"token "+n}});return r.url=r.html_url,this._issueNodeId=r.node_id,ca(r)}async getComments({accessToken:n,issueId:e,query:{page:t=1,perPage:r=10,sort:a="desc"}={}}){const o={};n&&(o.headers={Authorization:"token "+n}),null!==this._pageInfo.sort&&a!==this._pageInfo.sort&&(t=1);const{firstOrLast:i,afterOrBefore:s,cursor:l}=this._getQueryParams({page:t,sort:a}),{data:c}=await this.$http.post("graphql",{variables:{owner:this.owner,repo:this.repo,issueId:e,perPage:r},query:`query getComments(\n  $owner: String!\n  $repo: String!\n  $issueId: Int!\n  $perPage: Int!\n) {\n  repository(owner: $owner, name: $repo) {\n    issue(number: $issueId) {\n      comments(\n        ${i}: $perPage\n        ${null===s?"":`${s}: "${l}"`}\n      ) {\n        totalCount\n        pageInfo {\n          endCursor\n          startCursor\n        }\n        nodes {\n          id\n          body\n          bodyHTML\n          createdAt\n          updatedAt\n          author {\n            avatarUrl\n            login\n            url\n          }\n          reactionGroups {\n            users (first: 0) {\n              totalCount\n            }\n            content\n          }\n        }\n      }\n    }\n  }\n}`},o),p=c.data.repository.issue.comments;return"desc"===a&&p.nodes.reverse(),this._pageInfo={page:t,startCursor:p.pageInfo.startCursor,endCursor:p.pageInfo.endCursor,sort:a,perPage:r},{count:p.totalCount,page:t,perPage:r,data:p.nodes.map(ua)}}async postComment({accessToken:n,content:e}){const{data:t}=await this.$http.post("graphql",{variables:{issueNodeId:this._issueNodeId,content:e},query:"mutation postComment(\n  $issueNodeId: ID!\n  $content: String!\n) {\n  addComment(\n    input: {\n      subjectId: $issueNodeId\n      body: $content\n    }\n  ) {\n    commentEdge {\n      node {\n        id\n        body\n        bodyHTML\n        createdAt\n        updatedAt\n        author {\n          avatarUrl\n          login\n          url\n        }\n        reactionGroups {\n          users (first: 0) {\n            totalCount\n          }\n          content\n        }\n      }\n    }\n  }\n}"},{headers:{Authorization:"token "+n}});return ua(t.data.addComment.commentEdge.node)}async putComment({accessToken:n,commentId:e,content:t}){const{data:r}=await this.$http.post("graphql",{variables:{commentId:e,content:t},query:"mutation putComment(\n  $commentId: ID!,\n  $content: String!,\n) {\n  updateIssueComment(input: {\n    id: $commentId\n    body: $content\n  }) {\n    issueComment {\n      id\n      body\n      bodyHTML\n      createdAt\n      updatedAt\n      author {\n        avatarUrl\n        login\n        url\n      }\n      reactionGroups {\n        users (first: 0) {\n          totalCount\n        }\n        content\n      }\n    }\n  }\n}"},{headers:{Authorization:"token "+n}});return ua(r.data.updateIssueComment.issueComment)}async deleteComment({accessToken:n,commentId:e}){return await this.$http.post("graphql",{variables:{commentId:e},query:"mutation deleteComment(\n  $commentId: ID!,\n) {\n  deleteIssueComment(input: {\n    id: $commentId\n  }) {\n    clientMutationId\n  }\n}"},{headers:{Authorization:"token "+n}}),!0}async getCommentReactions({accessToken:n,issueId:e,commentId:t}){const{firstOrLast:r,afterOrBefore:a,cursor:o}=this._getQueryParams(),{data:i}=await this.$http.post("graphql",{variables:{owner:this.owner,repo:this.repo,issueId:e,perPage:this._pageInfo.perPage},query:`query getComments(\n  $owner: String!\n  $repo: String!\n  $issueId: Int!\n  $perPage: Int!\n) {\n  repository(owner: $owner, name: $repo) {\n    issue(number: $issueId) {\n      comments(\n        ${r}: $perPage\n        ${null===a?"":`${a}: "${o}"`}\n      ) {\n        nodes {\n          id\n          reactionGroups {\n            users (first: 0) {\n              totalCount\n            }\n            content\n          }\n        }\n      }\n    }\n  }\n}`},{headers:{Authorization:"token "+n}});return pa(i.data.repository.issue.comments.nodes.find(n=>n.id===t).reactionGroups)}async postCommentReaction({accessToken:n,commentId:e,reaction:t}){return await this.$http.post("graphql",{variables:{commentId:e,content:da(t)},query:"mutation postCommentReaction(\n  $commentId: ID!,\n  $content: ReactionContent!,\n) {\n  addReaction(input: {\n    subjectId: $commentId\n    content: $content\n  }) {\n    reaction {\n      databaseId\n    }\n  }\n}"},{headers:{Authorization:"token "+n}}),!0}_getQueryParams({page:n=this._pageInfo.page,sort:e=this._pageInfo.sort}={}){let t,r,a;return 1===n?(t="asc"===e?"first":"last",r=null,a=null):"asc"===e?n>this._pageInfo.page?(t="first",r="after",a=this._pageInfo.endCursor):(t="last",r="before",a=this._pageInfo.startCursor):n>this._pageInfo.page?(t="last",r="before",a=this._pageInfo.startCursor):(t="first",r="after",a=this._pageInfo.endCursor),{firstOrLast:t,afterOrBefore:r,cursor:a}}}function fa(n){return{username:n.username,avatar:n.avatar_url,homepage:n.web_url}}function ha(n){return{id:n.iid,title:n.title,content:n.description,link:n.web_url}}function va(n){return{id:n.id,content:n.body_html||"",contentRaw:n.body,author:fa(n.author),createdAt:n.created_at,updatedAt:n.updated_at,reactions:n.reactions}}function ba(n){return{like:n.filter(n=>"thumbsup"===n.name).length,unlike:n.filter(n=>"thumbsdown"===n.name).length,heart:n.filter(n=>"heart"===n.name).length}}function ka(n){return"like"===n?"thumbsup":"unlike"===n?"thumbsdown":n}class ya{constructor({baseURL:n="https://gitlab.com",owner:e,repo:t,labels:r,clientId:a,state:o}){this.baseURL=n,this.owner=e,this.repo=t,this.labels=r,this.clientId=a,this.state=o,this._encodedRepo=encodeURIComponent(`${this.owner}/${this.repo}`),this.$http=ea.a.create({baseURL:kt(n,"api/v4"),headers:{Accept:"application/json"}})}get platform(){return{name:"GitLab",link:this.baseURL,version:"v4",meta:{reactable:!0,sortable:!0}}}redirectAuth(){window.location.href=bt(kt(this.baseURL,"oauth/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,response_type:"token",state:this.state})}async handleAuth(){const n=Et(window.location.hash.slice(1));if(!n.access_token||n.state!==this.state)return null;const e=n.access_token;delete n.access_token,delete n.token_type,delete n.expires_in,delete n.state;const t=vt(n),r=t?"#"+t:"",a=`${wt(window.location.href)}${window.location.search}${r}`;return window.history.replaceState(null,"",a),e}async getUser({accessToken:n}){const{data:e}=await this.$http.get("user",{headers:{Authorization:"Bearer "+n}});return fa(e)}async getIssue({accessToken:n,issueId:e,issueTitle:t}){const r={};if(n&&(r.headers={Authorization:"Bearer "+n}),!e){r.params={labels:this.labels.join(","),order_by:"created_at",sort:"asc",search:t};const{data:n}=await this.$http.get(`projects/${this._encodedRepo}/issues`,r);return n.map(ha).find(n=>n.title===t)||null}try{const{data:n}=await this.$http.get(`projects/${this._encodedRepo}/issues/${e}`,r);return ha(n)}catch(n){if(n.response&&404===n.response.status)return null;throw n}}async postIssue({accessToken:n,title:e,content:t}){const{data:r}=await this.$http.post(`projects/${this._encodedRepo}/issues`,{title:e,description:t,labels:this.labels.join(",")},{headers:{Authorization:"Bearer "+n}});return ha(r)}async getComments({accessToken:n,issueId:e,query:{page:t=1,perPage:r=10,sort:a="desc"}={}}){const o={params:{page:t,per_page:r,order_by:"created_at",sort:a}};n&&(o.headers={Authorization:"Bearer "+n});const i=await this.$http.get(`projects/${this._encodedRepo}/issues/${e}/notes`,o),s=i.data,l=[];for(const t of s)l.push((async()=>{t.body_html=await this.getMarkdownContent({accessToken:n,contentRaw:t.body})})()),l.push((async()=>{t.reactions=await this.getCommentReactions({accessToken:n,issueId:e,commentId:t.id})})());return await Promise.all(l),{count:Number(i.headers["x-total"]),page:Number(i.headers["x-page"]),perPage:Number(i.headers["x-per-page"]),data:s.map(va)}}async postComment({accessToken:n,issueId:e,content:t}){const{data:r}=await this.$http.post(`projects/${this._encodedRepo}/issues/${e}/notes`,{body:t},{headers:{Authorization:"Bearer "+n}});return va(r)}async putComment({accessToken:n,issueId:e,commentId:t,content:r}){const{data:a}=await this.$http.put(`projects/${this._encodedRepo}/issues/${e}/notes/${t}`,{body:r},{headers:{Authorization:"Bearer "+n}}),[o,i]=await Promise.all([this.getMarkdownContent({accessToken:n,contentRaw:a.body}),this.getCommentReactions({accessToken:n,issueId:e,commentId:a.id})]);return a.body_html=o,a.reactions=i,va(a)}async deleteComment({accessToken:n,issueId:e,commentId:t}){const{status:r}=await this.$http.delete(`projects/${this._encodedRepo}/issues/${e}/notes/${t}`,{headers:{Authorization:"Bearer "+n}});return 204===r}async getCommentReactions({accessToken:n,issueId:e,commentId:t}){const{data:r}=await this.$http.get(`projects/${this._encodedRepo}/issues/${e}/notes/${t}/award_emoji`,{headers:{Authorization:"Bearer "+n}});return ba(r)}async postCommentReaction({issueId:n,commentId:e,reaction:t,accessToken:r}){try{return 201===(await this.$http.post(`projects/${this._encodedRepo}/issues/${n}/notes/${e}/award_emoji`,{name:ka(t)},{headers:{Authorization:"Bearer "+r}})).status}catch(n){if(n.response&&404===n.response.status)return!1;throw n}}async getMarkdownContent({accessToken:n,contentRaw:e}){const t={};n&&(t.headers={Authorization:"Bearer "+n});const{data:r}=await this.$http.post("markdown",{text:e,gfm:!0},t);return r.html}}function Sa(n){return{username:n.nickname,avatar:n.links.avatar.href,homepage:n.links.html.href}}function xa(n){return{id:n.id,title:n.title,content:n.content.raw,link:n.links.html.href}}function wa(n){return{id:n.id,content:n.content.html,contentRaw:n.content.raw,author:Sa(n.user),createdAt:n.created_on,updatedAt:n.updated_on,reactions:null}}class Ea{constructor({baseURL:n="https://bitbucket.org",owner:e,repo:t,clientId:r,state:a}){this.baseURL=n,this.owner=e,this.repo=t,this.clientId=r,this.state=a,this.$http=ea.a.create({baseURL:"https://api.bitbucket.org/2.0",headers:{Accept:"application/json"}})}get platform(){return{name:"Bitbucket",link:this.baseURL,version:"v2",meta:{reactable:!1,sortable:!0}}}redirectAuth(){window.location.href=bt(kt(this.baseURL,"site/oauth2/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,response_type:"token",state:this.state})}async handleAuth(){const n=Et(window.location.hash.slice(1));if(!n.access_token||n.state!==this.state)return null;const e=n.access_token;delete n.access_token,delete n.token_type,delete n.expires_in,delete n.state,delete n.scopes;const t=vt(n),r=t?"#"+t:"",a=`${wt(window.location.href)}${window.location.search}${r}`;return window.history.replaceState(null,"",a),e}async getUser({accessToken:n}){const{data:e}=await this.$http.get("user",{headers:{Authorization:"Bearer "+n}});return Sa(e)}async getIssue({accessToken:n,issueId:e,issueTitle:t}){const r={};if(n&&(r.headers={Authorization:"Bearer "+n}),!e){r.params={sort:"created_on",q:`title="${t}"`,timestamp:Date.now()};const{data:n}=await this.$http.get(`repositories/${this.owner}/${this.repo}/issues`,r);return n.size>0?xa(n.values[0]):null}try{r.params={timestamp:Date.now()};const{data:n}=await this.$http.get(`repositories/${this.owner}/${this.repo}/issues/${e}`,r);return xa(n)}catch(n){if(n.response&&404===n.response.status)return null;throw n}}async postIssue({accessToken:n,title:e,content:t}){const{data:r}=await this.$http.post(`repositories/${this.owner}/${this.repo}/issues`,{title:e,content:{raw:t},priority:"trivial",kind:"task"},{headers:{Authorization:"Bearer "+n}});return r.links.html={href:kt(this.baseURL,`${this.owner}/${this.repo}/issues/${r.id}`)},xa(r)}async getComments({accessToken:n,issueId:e,query:{page:t=1,perPage:r=10,sort:a="desc"}={}}){const o={params:{page:t,pagelen:r,sort:"desc"===a?"-created_on":"created_on",timestamp:Date.now()}};n&&(o.headers={Authorization:"Bearer "+n});const{data:i}=await this.$http.get(`repositories/${this.owner}/${this.repo}/issues/${e}/comments`,o);return{count:i.size,page:i.page,perPage:i.pagelen,data:i.values.filter(n=>null!==n.content.raw).map(wa)}}async postComment({accessToken:n,issueId:e,content:t}){const{data:r}=await this.$http.post(`repositories/${this.owner}/${this.repo}/issues/${e}/comments`,{content:{raw:t}},{headers:{Authorization:"Bearer "+n}});return wa(r)}async putComment({accessToken:n,issueId:e,commentId:t,content:r}){const{data:a}=await this.$http.put(`repositories/${this.owner}/${this.repo}/issues/${e}/comments/${t}`,{content:{raw:r}},{headers:{Authorization:"Bearer "+n}});return wa(a)}async deleteComment({accessToken:n,issueId:e,commentId:t}){const{status:r}=await this.$http.delete(`repositories/${this.owner}/${this.repo}/issues/${e}/comments/${t}`,{headers:{Authorization:"Bearer "+n}});return 204===r}async getCommentReactions(n){throw new Error("501 Not Implemented")}async postCommentReaction(n){throw new Error("501 Not Implemented")}}function Da(n){return{username:n.login,avatar:n.avatar_url,homepage:n.html_url}}function Ca(n){return{id:n.number,title:n.title,content:n.body,link:n.html_url}}function Ia(n){return{id:n.id,content:n.body_html||"",contentRaw:n.body,author:Da(n.user),createdAt:n.created_at,updatedAt:n.updated_at||"",reactions:null}}class Ta{constructor({baseURL:n="https://gitee.com",owner:e,repo:t,labels:r,clientId:a,clientSecret:o,state:i,proxy:s}){if(void 0===o||void 0===s)throw new Error("clientSecret and proxy is required for Gitee V5");this.baseURL=n,this.owner=e,this.repo=t,this.labels=r,this.clientId=a,this.clientSecret=o,this.state=i,this.proxy=s,this.$http=ea.a.create({baseURL:kt(n,"api/v5")}),this.$http.interceptors.response.use(n=>n,n=>(n.response.data&&n.response.data.message&&(n.message=n.response.data.message),Promise.reject(n)))}get platform(){return{name:"Gitee",link:this.baseURL,version:"v5",meta:{reactable:!1,sortable:!1}}}redirectAuth(){window.location.href=bt(kt(this.baseURL,"oauth/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,scope:"user_info issues notes",response_type:"code",state:this.state})}async handleAuth(){const n=Et(window.location.search);if(n.code){if(n.state!==this.state)return null;const e=n.code;delete n.code,delete n.state;const t=bt(wt(window.location.href),n)+window.location.hash;window.history.replaceState(null,"",t);return await this.getAccessToken({code:e})}return null}async getAccessToken({code:n}){const e=kt(this.baseURL,"oauth/token"),t="function"==typeof this.proxy?this.proxy(e):this.proxy,{data:r}=await this.$http.post(t,{client_id:this.clientId,client_secret:this.clientSecret,code:n,grant_type:"authorization_code",redirect_uri:window.location.href});return r.access_token}async getUser({accessToken:n}){const{data:e}=await this.$http.get("user",{params:{access_token:n}});return Da(e)}async getIssue({accessToken:n,issueId:e,issueTitle:t}){const r={params:{timestamp:Date.now()}};if(n&&(r.params.access_token=n),!e){Object.assign(r.params,{q:t,repo:`${this.owner}/${this.repo}`,per_page:1});const{data:n}=await this.$http.get("search/issues",r);return n.map(Ca).find(n=>n.title===t)||null}try{const{data:n}=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/${e}`,r);return Ca(n)}catch(n){if(n.response&&404===n.response.status)return null;throw n}}async postIssue({accessToken:n,title:e,content:t}){const{data:r}=await this.$http.post(`repos/${this.owner}/issues`,{access_token:n,repo:this.repo,title:e,body:t,labels:this.labels.join(",")});return Ca(r)}async getComments({accessToken:n,issueId:e,query:{page:t=1,perPage:r=10}={}}){const a={params:{page:t,per_page:r,timestamp:Date.now()},headers:{Accept:["application/vnd.gitee.html+json"]}};n&&(a.params.access_token=n);const o=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/${e}/comments`,a);return{count:Number(o.headers.total_count),page:t,perPage:r,data:o.data.map(Ia)}}async postComment({accessToken:n,issueId:e,content:t}){const{data:r}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues/${e}/comments`,{body:t,access_token:n},{headers:{Accept:["application/vnd.gitee.html+json"]}});return Ia(r)}async putComment({accessToken:n,commentId:e,content:t}){const{data:r}=await this.$http.patch(`repos/${this.owner}/${this.repo}/issues/comments/${e}`,{body:t,access_token:n},{headers:{Accept:["application/vnd.gitee.html+json"]}});return Ia(r)}async deleteComment({accessToken:n,commentId:e}){const{status:t}=await this.$http.delete(`repos/${this.owner}/${this.repo}/issues/comments/${e}`,{params:{access_token:n}});return 204===t}async getCommentReactions(n){throw new Error("501 Not Implemented")}async postCommentReaction(n){throw new Error("501 Not Implemented")}}t(331);var Oa={name:"Vssue",components:{VssueComponent:Zr},props:{options:{type:Object,default:()=>({})}},data:()=>({key:"key",platformOptions:{github:sa,"github-v4":ma,gitlab:ya,bitbucket:Ea,gitee:Ta}}),computed:{vssueOptions(){const{platformOptions:n,options:e}=this,t=n[e.platform];return{...e,api:t}}},watch:{$route(n,e){n.path!==e.path&&setTimeout(()=>{this.key="reco-"+(new Date).getTime()},300)}}},Aa=(t(332),{components:{Valine:ot,Vssue:Object(Ce.a)(Oa,(function(){return(0,this._self._c)("VssueComponent",{key:this.key,staticClass:"vssue-wrapper",attrs:{options:this.vssueOptions}})}),[],!1,null,null,null).exports},props:{isShowComments:{type:Boolean,default:!0}},data:()=>({commentsOptions:{}}),computed:{solution(){const{commentsOptions:{solution:n},$themeConfig:{valineConfig:e,vssueConfig:t},$themeLocaleConfig:{valineConfig:r,vssueConfig:a}}=this;let o="";return void 0!==n?o=n:void 0!==r||void 0!==e?o="valine":void 0===a&&void 0===t||(o="vssue"),o},options(){const{commentsOptions:{options:n},$themeConfig:{valineConfig:e,vssueConfig:t},$themeLocaleConfig:{valineConfig:r,vssueConfig:a}}=this;return void 0!==n?n:void 0!==r||void 0!==e?r||e:void 0!==a||void 0!==t?a||t:null},componentName(){const n=this.solution;return"valine"===n?"Valine":"vssue"===n?"Vssue":""}},mounted(){this.$themeConfig.commentsSolution=this.solution}}),_a=Object(Ce.a)(Aa,(function(){var n=this._self._c;return n("div",{directives:[{name:"show",rawName:"v-show",value:this.isShowComments,expression:"isShowComments"}],staticClass:"comments-wrapper"},[n("ClientOnly",[n(this.componentName,{tag:"component",attrs:{options:this.options}})],1)],1)}),[],!1,null,null,null).exports,Ra={props:{idVal:String,numStyle:Object,flagTitle:{type:String,default:"Your Article Title"}},methods:{getIdVal(n){const e=this.$site.base;return e.slice(0,e.length-1)+n}}},Pa=Object(Ce.a)(Ra,(function(){var n=this._self._c;return n("span",{staticClass:"leancloud-visitors",attrs:{id:this.getIdVal(this.idVal),"data-flag-title":this.flagTitle}},[n("a",{staticClass:"leancloud-visitors-count",style:this.numStyle})])}),[],!1,null,null,null).exports,Fa=(t(333),{tags:{markdown:{key:"markdown",scope:"tags",path:"/tag/markdown/",pageKeys:["v-0cbdd054"]},"推荐":{key:"推荐",scope:"tags",path:"/tag/推荐/",pageKeys:["v-6048fa40"]},"资源分享":{key:"资源分享",scope:"tags",path:"/tag/资源分享/",pageKeys:["v-6048fa40"]},"消息队列":{key:"消息队列",scope:"tags",path:"/tag/消息队列/",pageKeys:["v-7d8ca27f"]},"生产者-消费者":{key:"生产者-消费者",scope:"tags",path:"/tag/生产者-消费者/",pageKeys:["v-7d8ca27f"]},"云原生":{key:"云原生",scope:"tags",path:"/tag/云原生/",pageKeys:["v-33a310e8","v-83d8af48","v-4f7f9be4"]},"容器技术":{key:"容器技术",scope:"tags",path:"/tag/容器技术/",pageKeys:["v-33a310e8","v-4f7f9be4"]},"远程过程调用":{key:"远程过程调用",scope:"tags",path:"/tag/远程过程调用/",pageKeys:["v-700a68a1"]},"索引":{key:"索引",scope:"tags",path:"/tag/索引/",pageKeys:["v-01787dc2"]},"空间索引":{key:"空间索引",scope:"tags",path:"/tag/空间索引/",pageKeys:["v-01787dc2"]},"附近的人":{key:"附近的人",scope:"tags",path:"/tag/附近的人/",pageKeys:["v-01787dc2"]},k8s:{key:"k8s",scope:"tags",path:"/tag/k8s/",pageKeys:["v-83d8af48"]},Flink:{key:"Flink",scope:"tags",path:"/tag/Flink/",pageKeys:["v-83d8af48"]},beam:{key:"beam",scope:"tags",path:"/tag/beam/",pageKeys:["v-83d8af48"]},"k8s部署":{key:"k8s部署",scope:"tags",path:"/tag/k8s部署/",pageKeys:["v-c38fec2a"]},"离线同步数据":{key:"离线同步数据",scope:"tags",path:"/tag/离线同步数据/",pageKeys:["v-7e69d236"]},"近实时":{key:"近实时",scope:"tags",path:"/tag/近实时/",pageKeys:["v-7e69d236","v-24ffb3db"]},"linux命令":{key:"linux命令",scope:"tags",path:"/tag/linux命令/",pageKeys:["v-cdeeea2e"]},CICD:{key:"CICD",scope:"tags",path:"/tag/CICD/",pageKeys:["v-10e8b782"]},"持续集成部署":{key:"持续集成部署",scope:"tags",path:"/tag/持续集成部署/",pageKeys:["v-10e8b782"]},pipeline:{key:"pipeline",scope:"tags",path:"/tag/pipeline/",pageKeys:["v-10e8b782","v-36184f82"]},redis:{key:"redis",scope:"tags",path:"/tag/redis/",pageKeys:["v-851571e2"]},"一致性hash":{key:"一致性hash",scope:"tags",path:"/tag/一致性hash/",pageKeys:["v-851571e2"]},"水平扩容":{key:"水平扩容",scope:"tags",path:"/tag/水平扩容/",pageKeys:["v-851571e2"]},"黑名单过滤":{key:"黑名单过滤",scope:"tags",path:"/tag/黑名单过滤/",pageKeys:["v-7350f07e"]},"缓存穿透优化":{key:"缓存穿透优化",scope:"tags",path:"/tag/缓存穿透优化/",pageKeys:["v-7350f07e"]},"分布式":{key:"分布式",scope:"tags",path:"/tag/分布式/",pageKeys:["v-22a43ff6"]},"一致性":{key:"一致性",scope:"tags",path:"/tag/一致性/",pageKeys:["v-22a43ff6"]},"增量更新":{key:"增量更新",scope:"tags",path:"/tag/增量更新/",pageKeys:["v-0ddfb2e2"]},"bisdiff/bispatch":{key:"bisdiff/bispatch",scope:"tags",path:"/tag/bisdiff/bispatch/",pageKeys:["v-0ddfb2e2"]},"序列化":{key:"序列化",scope:"tags",path:"/tag/序列化/",pageKeys:["v-41ccc7e2","v-e463dc58"]},"缓存":{key:"缓存",scope:"tags",path:"/tag/缓存/",pageKeys:["v-29f87cb0"]},"一级缓存":{key:"一级缓存",scope:"tags",path:"/tag/一级缓存/",pageKeys:["v-29f87cb0"]},"分布式id生成":{key:"分布式id生成",scope:"tags",path:"/tag/分布式id生成/",pageKeys:["v-2e09059c"]},"kafka的高性能原理":{key:"kafka的高性能原理",scope:"tags",path:"/tag/kafka的高性能原理/",pageKeys:["v-61c5b94b"]},"服务器小文件传输":{key:"服务器小文件传输",scope:"tags",path:"/tag/服务器小文件传输/",pageKeys:["v-61c5b94b"]},"线程池设计":{key:"线程池设计",scope:"tags",path:"/tag/线程池设计/",pageKeys:["v-5692179e"]},spring:{key:"spring",scope:"tags",path:"/tag/spring/",pageKeys:["v-5692179e"]},"网络IO":{key:"网络IO",scope:"tags",path:"/tag/网络IO/",pageKeys:["v-227baaf0"]},"列式存储":{key:"列式存储",scope:"tags",path:"/tag/列式存储/",pageKeys:["v-24ffb3db"]},"搜索":{key:"搜索",scope:"tags",path:"/tag/搜索/",pageKeys:["v-007b24d3"]},"倒排索引":{key:"倒排索引",scope:"tags",path:"/tag/倒排索引/",pageKeys:["v-007b24d3"]},"流批一体编程框架":{key:"流批一体编程框架",scope:"tags",path:"/tag/流批一体编程框架/",pageKeys:["v-36184f82"]},"实时计算":{key:"实时计算",scope:"tags",path:"/tag/实时计算/",pageKeys:["v-49f14a1b"]},"流批一体":{key:"流批一体",scope:"tags",path:"/tag/流批一体/",pageKeys:["v-49f14a1b"]},"离线计算":{key:"离线计算",scope:"tags",path:"/tag/离线计算/",pageKeys:["v-9f20b3be"]},sparksql:{key:"sparksql",scope:"tags",path:"/tag/sparksql/",pageKeys:["v-9f20b3be"]}},categories:{tool:{key:"tool",scope:"categories",path:"/categories/tool/",pageKeys:["v-0cbdd054"]},"软件资源":{key:"软件资源",scope:"categories",path:"/categories/软件资源/",pageKeys:["v-6048fa40"]},"随笔":{key:"随笔",scope:"categories",path:"/categories/随笔/",pageKeys:["v-6048fa40"]},"中间件":{key:"中间件",scope:"categories",path:"/categories/中间件/",pageKeys:["v-7d8ca27f","v-7e69d236"]},"消息队列":{key:"消息队列",scope:"categories",path:"/categories/消息队列/",pageKeys:["v-7d8ca27f"]},"云原生":{key:"云原生",scope:"categories",path:"/categories/云原生/",pageKeys:["v-33a310e8","v-83d8af48","v-4f7f9be4"]},"其他":{key:"其他",scope:"categories",path:"/categories/其他/",pageKeys:["v-700a68a1","v-c38fec2a","v-cdeeea2e","v-22a43ff6","v-41ccc7e2","v-29f87cb0","v-61c5b94b","v-5692179e","v-227baaf0","v-e463dc58"]},"算法":{key:"算法",scope:"categories",path:"/categories/算法/",pageKeys:["v-01787dc2","v-851571e2","v-7350f07e","v-0ddfb2e2","v-2e09059c"]},"数据同步":{key:"数据同步",scope:"categories",path:"/categories/数据同步/",pageKeys:["v-7e69d236"]},CICD:{key:"CICD",scope:"categories",path:"/categories/CICD/",pageKeys:["v-10e8b782"]},"存储引擎":{key:"存储引擎",scope:"categories",path:"/categories/存储引擎/",pageKeys:["v-24ffb3db","v-007b24d3"]},nosql:{key:"nosql",scope:"categories",path:"/categories/nosql/",pageKeys:["v-24ffb3db"]},"计算引擎":{key:"计算引擎",scope:"categories",path:"/categories/计算引擎/",pageKeys:["v-36184f82","v-49f14a1b","v-9f20b3be"]}},timeline:{}});class Ba{constructor(n,e){this._metaMap=Object.assign({},n),Object.keys(this._metaMap).forEach(n=>{const{pageKeys:t}=this._metaMap[n];this._metaMap[n].pages=t.map(n=>Object(Jn.b)(e,n))})}get length(){return Object.keys(this._metaMap).length}get map(){return this._metaMap}get pages(){return this.list}get list(){return this.toArray()}toArray(){const n=[];return Object.keys(this._metaMap).forEach(e=>{const{pages:t,path:r}=this._metaMap[e];n.push({name:e,pages:t,path:r})}),n}getItemByName(n){return this._metaMap[n]}}var Ma={tags:(n,e)=>{const r=t(126);return r(n.frontmatter.date)-r(e.frontmatter.date)>0?-1:1},categories:(n,e)=>{const r=t(126);return r(n.frontmatter.date)-r(e.frontmatter.date)>0?-1:1}},ja={tags:function(n,e,t){const r=e;return["tags"].some(e=>{const t=n.frontmatter[e];return Array.isArray(t)?t.some(n=>n===r):t===r})},categories:function(n,e,t){const r=e;return["categories"].some(e=>{const t=n.frontmatter[e];return Array.isArray(t)?t.some(n=>n===r):t===r})}},La=[{pid:"tags",id:"markdown",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/markdown/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"推荐",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/推荐/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"资源分享",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/资源分享/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"消息队列",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/消息队列/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"生产者-消费者",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/生产者-消费者/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"云原生",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/云原生/",interval:[0,3]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"容器技术",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/容器技术/",interval:[0,2]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"远程过程调用",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/远程过程调用/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"索引",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/索引/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"空间索引",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/空间索引/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"附近的人",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/附近的人/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"k8s",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/k8s/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"Flink",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/Flink/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"beam",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/beam/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"k8s部署",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/k8s部署/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"离线同步数据",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/离线同步数据/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"近实时",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/近实时/",interval:[0,2]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"linux命令",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/linux命令/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"CICD",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/CICD/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"持续集成部署",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/持续集成部署/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"pipeline",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/pipeline/",interval:[0,2]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"redis",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/redis/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"一致性hash",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/一致性hash/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"水平扩容",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/水平扩容/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"黑名单过滤",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/黑名单过滤/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"缓存穿透优化",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/缓存穿透优化/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"分布式",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/分布式/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"一致性",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/一致性/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"增量更新",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/增量更新/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"bisdiff/bispatch",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/bisdiff/bispatch/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"序列化",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/序列化/",interval:[0,2]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"缓存",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/缓存/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"一级缓存",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/一级缓存/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"分布式id生成",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/分布式id生成/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"kafka的高性能原理",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/kafka的高性能原理/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"服务器小文件传输",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/服务器小文件传输/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"线程池设计",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/线程池设计/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"spring",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/spring/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"网络IO",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/网络IO/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"列式存储",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/列式存储/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"搜索",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/搜索/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"倒排索引",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/倒排索引/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"流批一体编程框架",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/流批一体编程框架/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"实时计算",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/实时计算/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"流批一体",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/流批一体/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"离线计算",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/离线计算/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"sparksql",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/sparksql/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"tool",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/tool/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"软件资源",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/软件资源/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"随笔",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/随笔/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"中间件",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/中间件/",interval:[0,2]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"消息队列",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/消息队列/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"云原生",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/云原生/",interval:[0,3]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"其他",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/其他/",interval:[0,9]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"算法",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/算法/",interval:[0,5]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"数据同步",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/数据同步/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"CICD",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/CICD/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"存储引擎",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/存储引擎/",interval:[0,2]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"nosql",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/nosql/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"计算引擎",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/计算引擎/",interval:[0,3]}],prevText:"Prev",nextText:"Next"}],Na=t(135);const $a=t.n(Na)()("plugin-blog:pagination");class Ua{constructor(n,e,t){$a("pagination",n);const{pages:r,prevText:a,nextText:o}=n,{path:i}=t;this._prevText=a,this._nextText=o;for(let n=0,e=r.length;n<e;n++){if(r[n].path===i){this.paginationIndex=n;break}}this.paginationIndex||(this.paginationIndex=0),this._paginationPages=r,this._currentPage=r[this.paginationIndex],this._matchedPages=e.filter(e=>n.filter(e,n.id,n.pid)).sort(n.sorter)}setIndexPage(n){this._indexPage=n}get length(){return this._paginationPages.length}get pages(){const[n,e]=this._currentPage.interval;return this._matchedPages.slice(n,e+1)}get hasPrev(){return 0!==this.paginationIndex}get prevLink(){return this.hasPrev?this.paginationIndex-1==0&&this._indexPage?this._indexPage:this._paginationPages[this.paginationIndex-1].path:null}get hasNext(){return this.paginationIndex!==this.length-1}get nextLink(){return this.hasNext?this._paginationPages[this.paginationIndex+1].path:null}get prevText(){return this._prevText}get nextText(){return this._nextText}getSpecificPageLink(n){return this._paginationPages[n].path}}const za=new class{constructor(n){this.paginations=n}get pages(){return r.b.$vuepress.$get("siteData").pages}getPagination(n,e,t){$a("id",e),$a("pid",n);const r=this.paginations.filter(t=>t.id===e&&t.pid===n)[0];return new Ua(r,this.pages,t)}}(La);var Ha={comment:{enabled:!1,service:""},email:{enabled:!1},feed:{rss:!1,atom:!1,json:!1}};t(336);function qa(n){const e=document.documentElement.getBoundingClientRect(),t=n.getBoundingClientRect();return{x:t.left-e.left,y:t.top-e.top}}class Va{constructor(n){Object.defineProperty(this,"registration",{value:n,configurable:!0,writable:!0})}update(){return this.registration.update()}skipWaiting(){const n=this.registration.waiting;return n?(console.log("[vuepress:sw] Doing worker.skipWaiting()."),new Promise((e,t)=>{const r=new MessageChannel;r.port1.onmessage=n=>{console.log("[vuepress:sw] Done worker.skipWaiting()."),n.data.error?t(n.data.error):e(n.data)},n.postMessage({type:"skip-waiting"},[r.port2])})):Promise.resolve()}}var Ka=t(22);r.b.component("SWUpdatePopup",()=>Promise.all([t.e(0),t.e(34)]).then(t.bind(null,1558)));var Wa=[({Vue:n,options:e,router:r,siteData:a})=>{n.mixin({mounted(){const e=()=>{(function(){var n=JSON.parse(localStorage.getItem("user_auth_xxxxxxxxxxxx"));if(console.log(n),n&&n.time){var e=new Date(n.time);return!((new Date).setHours(-1)>e)&&(n&&Object.keys(n).length)}return!1})()||this.$dlg.modal(je,{width:400,height:350,title:"请登录您的账号",singletonKey:"user-login",maxButton:!1,closeButton:!1,callback:n=>{}})};this.$dlg?e():t.e(56).then(t.t.bind(null,1554,7)).then(t=>{n.use(t.default),this.$nextTick(()=>{e()})})}})},({Vue:n,siteData:e,isServer:t,router:r})=>{n.mixin(Ne),n.mixin(We),Object(Ge.c)(r),Object(Ge.a)(r)},{},({Vue:n})=>{n.mixin({computed:{$dataBlock(){return this.$options.__data__block__}}})},{},({Vue:n})=>{n.component("BackToTop",Ye)},({Vue:n})=>{n.component("Pagation",rt)},({Vue:n})=>{n.mixin({computed:{$perPage:()=>10}})},({Vue:n})=>{n.component("Comments",_a),n.component("AccessNumber",Pa)},{},({Vue:n})=>{const e=Object.keys(Fa).map(n=>{const e=Fa[n],t="$"+n;return{[t](){const{pages:n}=this.$site;return new Ba(e,n)},["$current"+(n.charAt(0).toUpperCase()+n.slice(1))](){const n=this.$route.meta.id;return this[t].getItemByName(n)}}}).reduce((n,e)=>(Object.assign(n,e),n),{});e.$frontmatterKey=function(){const n=this["$"+this.$route.meta.id];return n||null},n.mixin({computed:e})},({Vue:n})=>{n.mixin({computed:{$pagination(){return this.$route.meta.pid&&this.$route.meta.id?this.$getPagination(this.$route.meta.pid,this.$route.meta.id):null}},methods:{$getPagination(n,e){return e=e||n,za.getPagination(n,e,this.$route)}}})},({Vue:n})=>{const e={$service:()=>Ha};n.mixin({computed:e})},({Vue:n,router:e})=>{e.options.scrollBehavior=(e,t,r)=>{if(r)return window.scrollTo({top:r.y,behavior:"smooth"});if(e.hash){if(n.$vuepress.$get("disableScrollBehavior"))return!1;const t=document.querySelector(e.hash);return!!t&&window.scrollTo({top:qa(t).y,behavior:"smooth"})}return window.scrollTo({top:0,behavior:"smooth"})}},async({router:n,isServer:e})=>{if(!e){const{register:e}=await t.e(55).then(t.bind(null,1555));n.onReady(()=>{e("./service-worker.js",{registrationOptions:{},ready(){console.log("[vuepress:sw] Service worker is active."),Ka.a.$emit("sw-ready")},cached(n){console.log("[vuepress:sw] Content has been cached for offline use."),Ka.a.$emit("sw-cached",new Va(n))},updated(n){console.log("[vuepress:sw] Content updated."),Ka.a.$emit("sw-updated",new Va(n))},offline(){console.log("[vuepress:sw] No internet connection found. App is running in offline mode."),Ka.a.$emit("sw-offline")},error(n){console.error("[vuepress:sw] Error during service worker registration:",n),Ka.a.$emit("sw-error",n),GA_ID&&ga("send","exception",{exDescription:n.message,exFatal:!1})}})})}},({Vue:n})=>{n.component("CodeCopy",Ae)}],Ga=["BackToTop","SWUpdatePopup"];class Ja extends class{constructor(){this.store=new r.b({data:{state:{}}})}$get(n){return this.store.state[n]}$set(n,e){r.b.set(this.store.state,n,e)}$emit(...n){this.store.$emit(...n)}$on(...n){this.store.$on(...n)}}{}Object.assign(Ja.prototype,{getPageAsyncComponent:Jn.e,getLayoutAsyncComponent:Jn.d,getAsyncComponent:Jn.c,getVueComponent:Jn.f});var Ya={install(n){const e=new Ja;n.$vuepress=e,n.prototype.$vuepress=e}};function Qa(n,e){const t=e.toLowerCase();return n.options.routes.some(n=>n.path.toLowerCase()===t)}var Xa={props:{pageKey:String,slotKey:{type:String,default:"default"}},render(n){const e=this.pageKey||this.$parent.$page.key;return Object(Jn.i)("pageKey",e),r.b.component(e)||r.b.component(e,Object(Jn.e)(e)),r.b.component(e)?n(e):n("")}},Za={functional:!0,props:{slotKey:String,required:!0},render:(n,{props:e,slots:t})=>n("div",{class:["content__"+e.slotKey]},t()[e.slotKey])},no={computed:{openInNewWindowTitle(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},eo=(t(337),t(338),Object(Ce.a)(no,(function(){var n=this._self._c;return n("span",[n("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[n("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),n("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),n("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports),to={functional:!0,render(n,{parent:e,children:t}){if(e._isMounted)return t;e.$once("hook:mounted",()=>{e.$forceUpdate()})}};r.b.config.productionTip=!1,r.b.use(Wn),r.b.use(Ya),r.b.mixin(function(n,e,t=r.b){!function(n){n.locales&&Object.keys(n.locales).forEach(e=>{n.locales[e].path=e});Object.freeze(n)}(e),t.$vuepress.$set("siteData",e);const a=new(n(t.$vuepress.$get("siteData"))),o=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(a)),i={};return Object.keys(o).reduce((n,e)=>(e.startsWith("$")&&(n[e]=o[e].get),n),i),{computed:i}}(n=>class{setPage(n){this.__page=n}get $site(){return n}get $themeConfig(){return this.$site.themeConfig}get $frontmatter(){return this.$page.frontmatter}get $localeConfig(){const{locales:n={}}=this.$site;let e,t;for(const r in n)"/"===r?t=n[r]:0===this.$page.path.indexOf(r)&&(e=n[r]);return e||t||{}}get $siteTitle(){return this.$localeConfig.title||this.$site.title||""}get $canonicalUrl(){const{canonicalUrl:n}=this.$page.frontmatter;return"string"==typeof n&&n}get $title(){const n=this.$page,{metaTitle:e}=this.$page.frontmatter;if("string"==typeof e)return e;const t=this.$siteTitle,r=n.frontmatter.home?null:n.frontmatter.title||n.title;return t?r?r+" | "+t:t:r||"VuePress"}get $description(){const n=function(n){if(n){const e=n.filter(n=>"description"===n.name)[0];if(e)return e.content}}(this.$page.frontmatter.meta);return n||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}get $lang(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}get $localePath(){return this.$localeConfig.path||"/"}get $themeLocaleConfig(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}get $page(){return this.__page?this.__page:function(n,e){for(let t=0;t<n.length;t++){const r=n[t];if(r.path.toLowerCase()===e.toLowerCase())return r}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}},Be)),r.b.component("Content",Xa),r.b.component("ContentSlotsDistributor",Za),r.b.component("OutboundLink",eo),r.b.component("ClientOnly",to),r.b.component("Layout",Object(Jn.d)("Layout")),r.b.component("NotFound",Object(Jn.d)("NotFound")),r.b.prototype.$withBase=function(n){const e=this.$site.base;return"/"===n.charAt(0)?e+n.slice(1):n},window.__VUEPRESS__={version:"1.9.9",hash:"793c1e2"},async function(n){const e="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:Be.routerBase||Be.base,t=new Wn({base:e,fallback:!1,routes:Fe,scrollBehavior:(n,e,t)=>t||(n.hash?!r.b.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(n.hash)}:{x:0,y:0})});!function(n){n.beforeEach((e,t,r)=>{if(Qa(n,e.path))r();else if(/(\/|\.html)$/.test(e.path))if(/\/$/.test(e.path)){const t=e.path.replace(/\/$/,"")+".html";Qa(n,t)?r(t):r()}else r();else{const t=e.path+"/",a=e.path+".html";Qa(n,a)?r(a):Qa(n,t)?r(t):r()}})}(t);const a={};try{await Promise.all(Wa.filter(n=>"function"==typeof n).map(e=>e({Vue:r.b,options:a,router:t,siteData:Be,isServer:n})))}catch(n){console.error(n)}return{app:new r.b(Object.assign(a,{router:t,render:n=>n("div",{attrs:{id:"app"}},[n("RouterView",{ref:"layout"}),n("div",{class:"global-ui"},Ga.map(e=>n(e)))])})),router:t}}(!1).then(({app:n,router:e})=>{e.onReady(()=>{n.$mount("#app")})})}]);