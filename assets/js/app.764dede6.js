(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(n){function e(e){for(var r,i,s=e[0],l=e[1],c=e[2],u=0,d=[];u<s.length;u++)i=s[u],Object.prototype.hasOwnProperty.call(a,i)&&a[i]&&d.push(a[i][0]),a[i]=0;for(r in l)Object.prototype.hasOwnProperty.call(l,r)&&(n[r]=l[r]);for(p&&p(e);d.length;)d.shift()();return o.push.apply(o,c||[]),t()}function t(){for(var n,e=0;e<o.length;e++){for(var t=o[e],r=!0,s=1;s<t.length;s++){var l=t[s];0!==a[l]&&(r=!1)}r&&(o.splice(e--,1),n=i(i.s=t[0]))}return n}var r={},a={2:0},o=[];function i(e){if(r[e])return r[e].exports;var t=r[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,i),t.l=!0,t.exports}i.e=function(n){var e=[],t=a[n];if(0!==t)if(t)e.push(t[2]);else{var r=new Promise((function(e,r){t=a[n]=[e,r]}));e.push(t[2]=r);var o,s=document.createElement("script");s.charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.src=function(n){return i.p+"assets/js/"+({}[n]||n)+"."+{1:"8757f81f",3:"9efa3b27",4:"95c562bd",5:"a1a0df33",6:"e534639a",7:"bc868936",8:"ce0f4f25",9:"a57a9cf0",10:"36ed4133",11:"18e1c33d",12:"b32071c5",13:"20e964b9",14:"d0932ff0",15:"7747d51d",16:"487f9c88",17:"9ae2b89b",18:"801c9b12",19:"867ef46b",20:"ec371d7f",21:"73f810bc",22:"8c016772",23:"9aa3d5ef",24:"a6dcda10",25:"2fe13775",26:"2296389e",27:"21f2d66f",28:"3209357f",29:"c7065b7f",30:"92914cc9",31:"13c12633",32:"b016f60e",33:"ed48a654",34:"ec572062",35:"5b55607b",36:"f50b6e38",37:"34409ac4",38:"c7073f77",39:"12f063e9",40:"1bbd5377",41:"6331a8cf",42:"28acd6f5",43:"905d2114",44:"041132c5",45:"d1a66b92",46:"1c9100fe",47:"3e75f768",48:"7d7248fb",49:"8d465809",50:"4aa56564",51:"59674bf5",52:"f3990683",53:"092fe0ad",54:"ac62e2a7",55:"3530fa5a",56:"6e9bc6f3"}[n]+".js"}(n);var l=new Error;o=function(e){s.onerror=s.onload=null,clearTimeout(c);var t=a[n];if(0!==t){if(t){var r=e&&("load"===e.type?"missing":e.type),o=e&&e.target&&e.target.src;l.message="Loading chunk "+n+" failed.\n("+r+": "+o+")",l.name="ChunkLoadError",l.type=r,l.request=o,t[1](l)}a[n]=void 0}};var c=setTimeout((function(){o({type:"timeout",target:s})}),12e4);s.onerror=s.onload=o,document.head.appendChild(s)}return Promise.all(e)},i.m=n,i.c=r,i.d=function(n,e,t){i.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},i.r=function(n){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},i.t=function(n,e){if(1&e&&(n=i(n)),8&e)return n;if(4&e&&"object"==typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(i.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var r in n)i.d(t,r,function(e){return n[e]}.bind(null,r));return t},i.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return i.d(e,"a",e),e},i.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},i.p="./",i.oe=function(n){throw console.error(n),n};var s=window.webpackJsonp=window.webpackJsonp||[],l=s.push.bind(s);s.push=e,s=s.slice();for(var c=0;c<s.length;c++)e(s[c]);var p=l;o.push([137,0]),t()}([function(n,e,t){"use strict";t.d(e,"e",(function(){return b})),t.d(e,"d",(function(){return k})),t.d(e,"c",(function(){return y})),t.d(e,"f",(function(){return S})),t.d(e,"a",(function(){return x})),t.d(e,"g",(function(){return w})),t.d(e,"b",(function(){return E})),t.d(e,"h",(function(){return D})),t.d(e,"i",(function(){return C}));t(17),t(136);var r=t(1),a={NotFound:()=>Promise.all([t.e(0),t.e(28)]).then(t.bind(null,1559)),Category:()=>Promise.all([t.e(0),t.e(1),t.e(20)]).then(t.bind(null,1560)),Layout:()=>Promise.all([t.e(0),t.e(1),t.e(13)]).then(t.bind(null,1557)),Tag:()=>Promise.all([t.e(0),t.e(1),t.e(17)]).then(t.bind(null,1561)),Tags:()=>Promise.all([t.e(0),t.e(1),t.e(18)]).then(t.bind(null,1562)),TimeLines:()=>Promise.all([t.e(0),t.e(1),t.e(29)]).then(t.bind(null,1563))},o={"v-aae13ec4":()=>t.e(39).then(t.bind(null,1565)),"v-af65573c":()=>t.e(37).then(t.bind(null,1566)),"v-0ea853f1":()=>t.e(40).then(t.bind(null,1567)),"v-0cbdd054":()=>t.e(43).then(t.bind(null,1568)),"v-0bf5ebde":()=>t.e(42).then(t.bind(null,1569)),"v-46e67ace":()=>t.e(38).then(t.bind(null,1570)),"v-6048fa40":()=>t.e(44).then(t.bind(null,1571)),"v-5ed1fdaa":()=>t.e(41).then(t.bind(null,1572)),"v-7d8ca27f":()=>t.e(7).then(t.bind(null,1573)),"v-dffff514":()=>t.e(27).then(t.bind(null,1574)),"v-33a310e8":()=>t.e(5).then(t.bind(null,1575)),"v-700a68a1":()=>t.e(30).then(t.bind(null,1576)),"v-2c8e9208":()=>t.e(46).then(t.bind(null,1577)),"v-01787dc2":()=>t.e(25).then(t.bind(null,1578)),"v-83d8af48":()=>t.e(9).then(t.bind(null,1579)),"v-c38fec2a":()=>t.e(26).then(t.bind(null,1580)),"v-7e69d236":()=>t.e(45).then(t.bind(null,1581)),"v-cdeeea2e":()=>t.e(47).then(t.bind(null,1582)),"v-10e8b782":()=>t.e(22).then(t.bind(null,1583)),"v-851571e2":()=>t.e(21).then(t.bind(null,1584)),"v-7350f07e":()=>t.e(15).then(t.bind(null,1585)),"v-22a43ff6":()=>t.e(19).then(t.bind(null,1586)),"v-0ddfb2e2":()=>t.e(12).then(t.bind(null,1587)),"v-41ccc7e2":()=>t.e(48).then(t.bind(null,1588)),"v-29f87cb0":()=>t.e(23).then(t.bind(null,1589)),"v-2e09059c":()=>t.e(32).then(t.bind(null,1590)),"v-7ea9e72a":()=>t.e(49).then(t.bind(null,1591)),"v-63f06f0b":()=>t.e(11).then(t.bind(null,1592)),"v-61c5b94b":()=>t.e(24).then(t.bind(null,1593)),"v-5692179e":()=>t.e(31).then(t.bind(null,1594)),"v-227baaf0":()=>t.e(14).then(t.bind(null,1595)),"v-e7d27b94":()=>t.e(51).then(t.bind(null,1596)),"v-7f30b557":()=>t.e(8).then(t.bind(null,1597)),"v-4f7f9be4":()=>t.e(36).then(t.bind(null,1598)),"v-24ffb3db":()=>t.e(6).then(t.bind(null,1599)),"v-e463dc58":()=>t.e(52).then(t.bind(null,1600)),"v-007b24d3":()=>t.e(10).then(t.bind(null,1601)),"v-305ac8e0":()=>t.e(50).then(t.bind(null,1602)),"v-4fd15c58":()=>t.e(53).then(t.bind(null,1603)),"v-36184f82":()=>t.e(16).then(t.bind(null,1604)),"v-49f14a1b":()=>t.e(4).then(t.bind(null,1605)),"v-9f20b3be":()=>t.e(3).then(t.bind(null,1606))};function i(n){const e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}const s=/-(\w)/g,l=i(n=>n.replace(s,(n,e)=>e?e.toUpperCase():"")),c=/\B([A-Z])/g,p=i(n=>n.replace(c,"-$1").toLowerCase()),u=i(n=>n.charAt(0).toUpperCase()+n.slice(1));function d(n,e){if(!e)return;if(n(e))return n(e);return e.includes("-")?n(u(l(e))):n(u(e))||n(p(e))}const m=Object.assign({},a,o),g=n=>m[n],f=n=>o[n],h=n=>a[n],v=n=>r.b.component(n);function b(n){return d(f,n)}function k(n){return d(h,n)}function y(n){return d(g,n)}function S(n){return d(v,n)}function x(...n){return Promise.all(n.filter(n=>n).map(async n=>{if(!S(n)&&y(n)){const e=await y(n)();r.b.component(n,e.default)}}))}function w(n,e,t){switch(e){case"components":n[e]||(n[e]={}),Object.assign(n[e],t);break;case"mixins":n[e]||(n[e]=[]),n[e].push(...t);break;default:throw new Error("Unknown option name.")}}function E(n,e){for(let t=0;t<n.length;t++){const r=n[t];if(r.key===e)return r}return{path:"",frontmatter:{}}}function D(n,e){const{$localePath:t}=n;return"object"==typeof e&&e[t]?e[t]:e}function C(n,e){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[n]=e)}},function(n,e,t){"use strict";t.d(e,"a",(function(){return Wn})),t.d(e,"b",(function(){return tr})),t.d(e,"c",(function(){return Ye})),t.d(e,"d",(function(){return dn})),t.d(e,"e",(function(){return Ge})),t.d(e,"f",(function(){return Je})),t.d(e,"g",(function(){return jn})),t.d(e,"h",(function(){return zn})),t.d(e,"i",(function(){return Vn}));
/*!
 * Vue.js v2.7.14
 * (c) 2014-2022 Evan You
 * Released under the MIT License.
 */
var r=Object.freeze({}),a=Array.isArray;function o(n){return null==n}function i(n){return null!=n}function s(n){return!0===n}function l(n){return"string"==typeof n||"number"==typeof n||"symbol"==typeof n||"boolean"==typeof n}function c(n){return"function"==typeof n}function p(n){return null!==n&&"object"==typeof n}var u=Object.prototype.toString;function d(n){return"[object Object]"===u.call(n)}function m(n){return"[object RegExp]"===u.call(n)}function g(n){var e=parseFloat(String(n));return e>=0&&Math.floor(e)===e&&isFinite(n)}function f(n){return i(n)&&"function"==typeof n.then&&"function"==typeof n.catch}function h(n){return null==n?"":Array.isArray(n)||d(n)&&n.toString===u?JSON.stringify(n,null,2):String(n)}function v(n){var e=parseFloat(n);return isNaN(e)?n:e}function b(n,e){for(var t=Object.create(null),r=n.split(","),a=0;a<r.length;a++)t[r[a]]=!0;return e?function(n){return t[n.toLowerCase()]}:function(n){return t[n]}}b("slot,component",!0);var k=b("key,ref,slot,slot-scope,is");function y(n,e){var t=n.length;if(t){if(e===n[t-1])return void(n.length=t-1);var r=n.indexOf(e);if(r>-1)return n.splice(r,1)}}var S=Object.prototype.hasOwnProperty;function x(n,e){return S.call(n,e)}function w(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var E=/-(\w)/g,D=w((function(n){return n.replace(E,(function(n,e){return e?e.toUpperCase():""}))})),C=w((function(n){return n.charAt(0).toUpperCase()+n.slice(1)})),I=/\B([A-Z])/g,T=w((function(n){return n.replace(I,"-$1").toLowerCase()}));var O=Function.prototype.bind?function(n,e){return n.bind(e)}:function(n,e){function t(t){var r=arguments.length;return r?r>1?n.apply(e,arguments):n.call(e,t):n.call(e)}return t._length=n.length,t};function A(n,e){e=e||0;for(var t=n.length-e,r=new Array(t);t--;)r[t]=n[t+e];return r}function _(n,e){for(var t in e)n[t]=e[t];return n}function R(n){for(var e={},t=0;t<n.length;t++)n[t]&&_(e,n[t]);return e}function P(n,e,t){}var F=function(n,e,t){return!1},B=function(n){return n};function M(n,e){if(n===e)return!0;var t=p(n),r=p(e);if(!t||!r)return!t&&!r&&String(n)===String(e);try{var a=Array.isArray(n),o=Array.isArray(e);if(a&&o)return n.length===e.length&&n.every((function(n,t){return M(n,e[t])}));if(n instanceof Date&&e instanceof Date)return n.getTime()===e.getTime();if(a||o)return!1;var i=Object.keys(n),s=Object.keys(e);return i.length===s.length&&i.every((function(t){return M(n[t],e[t])}))}catch(n){return!1}}function j(n,e){for(var t=0;t<n.length;t++)if(M(n[t],e))return t;return-1}function L(n){var e=!1;return function(){e||(e=!0,n.apply(this,arguments))}}function N(n,e){return n===e?0===n&&1/n!=1/e:n==n||e==e}var $=["component","directive","filter"],U=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],z={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:F,isReservedAttr:F,isUnknownElement:F,getTagNamespace:P,parsePlatformTagName:B,mustUseProp:F,async:!0,_lifecycleHooks:U},H=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function q(n){var e=(n+"").charCodeAt(0);return 36===e||95===e}function V(n,e,t,r){Object.defineProperty(n,e,{value:t,enumerable:!!r,writable:!0,configurable:!0})}var K=new RegExp("[^".concat(H.source,".$_\\d]"));var W="__proto__"in{},G="undefined"!=typeof window,J=G&&window.navigator.userAgent.toLowerCase(),Y=J&&/msie|trident/.test(J),Q=J&&J.indexOf("msie 9.0")>0,X=J&&J.indexOf("edge/")>0;J&&J.indexOf("android");var Z=J&&/iphone|ipad|ipod|ios/.test(J);J&&/chrome\/\d+/.test(J),J&&/phantomjs/.test(J);var nn,en=J&&J.match(/firefox\/(\d+)/),tn={}.watch,rn=!1;if(G)try{var an={};Object.defineProperty(an,"passive",{get:function(){rn=!0}}),window.addEventListener("test-passive",null,an)}catch(n){}var on=function(){return void 0===nn&&(nn=!G&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),nn},sn=G&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function ln(n){return"function"==typeof n&&/native code/.test(n.toString())}var cn,pn="undefined"!=typeof Symbol&&ln(Symbol)&&"undefined"!=typeof Reflect&&ln(Reflect.ownKeys);cn="undefined"!=typeof Set&&ln(Set)?Set:function(){function n(){this.set=Object.create(null)}return n.prototype.has=function(n){return!0===this.set[n]},n.prototype.add=function(n){this.set[n]=!0},n.prototype.clear=function(){this.set=Object.create(null)},n}();var un=null;function dn(){return un&&{proxy:un}}function mn(n){void 0===n&&(n=null),n||un&&un._scope.off(),un=n,n&&n._scope.on()}var gn=function(){function n(n,e,t,r,a,o,i,s){this.tag=n,this.data=e,this.children=t,this.text=r,this.elm=a,this.ns=void 0,this.context=o,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=i,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(n.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),n}(),fn=function(n){void 0===n&&(n="");var e=new gn;return e.text=n,e.isComment=!0,e};function hn(n){return new gn(void 0,void 0,void 0,String(n))}function vn(n){var e=new gn(n.tag,n.data,n.children&&n.children.slice(),n.text,n.elm,n.context,n.componentOptions,n.asyncFactory);return e.ns=n.ns,e.isStatic=n.isStatic,e.key=n.key,e.isComment=n.isComment,e.fnContext=n.fnContext,e.fnOptions=n.fnOptions,e.fnScopeId=n.fnScopeId,e.asyncMeta=n.asyncMeta,e.isCloned=!0,e}var bn=0,kn=[],yn=function(){function n(){this._pending=!1,this.id=bn++,this.subs=[]}return n.prototype.addSub=function(n){this.subs.push(n)},n.prototype.removeSub=function(n){this.subs[this.subs.indexOf(n)]=null,this._pending||(this._pending=!0,kn.push(this))},n.prototype.depend=function(e){n.target&&n.target.addDep(this)},n.prototype.notify=function(n){var e=this.subs.filter((function(n){return n}));for(var t=0,r=e.length;t<r;t++){0,e[t].update()}},n}();yn.target=null;var Sn=[];function xn(n){Sn.push(n),yn.target=n}function wn(){Sn.pop(),yn.target=Sn[Sn.length-1]}var En=Array.prototype,Dn=Object.create(En);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(n){var e=En[n];V(Dn,n,(function(){for(var t=[],r=0;r<arguments.length;r++)t[r]=arguments[r];var a,o=e.apply(this,t),i=this.__ob__;switch(n){case"push":case"unshift":a=t;break;case"splice":a=t.slice(2)}return a&&i.observeArray(a),i.dep.notify(),o}))}));var Cn=Object.getOwnPropertyNames(Dn),In={},Tn=!0;function On(n){Tn=n}var An={notify:P,depend:P,addSub:P,removeSub:P},_n=function(){function n(n,e,t){if(void 0===e&&(e=!1),void 0===t&&(t=!1),this.value=n,this.shallow=e,this.mock=t,this.dep=t?An:new yn,this.vmCount=0,V(n,"__ob__",this),a(n)){if(!t)if(W)n.__proto__=Dn;else for(var r=0,o=Cn.length;r<o;r++){V(n,s=Cn[r],Dn[s])}e||this.observeArray(n)}else{var i=Object.keys(n);for(r=0;r<i.length;r++){var s;Pn(n,s=i[r],In,void 0,e,t)}}}return n.prototype.observeArray=function(n){for(var e=0,t=n.length;e<t;e++)Rn(n[e],!1,this.mock)},n}();function Rn(n,e,t){return n&&x(n,"__ob__")&&n.__ob__ instanceof _n?n.__ob__:!Tn||!t&&on()||!a(n)&&!d(n)||!Object.isExtensible(n)||n.__v_skip||Un(n)||n instanceof gn?void 0:new _n(n,e,t)}function Pn(n,e,t,r,o,i){var s=new yn,l=Object.getOwnPropertyDescriptor(n,e);if(!l||!1!==l.configurable){var c=l&&l.get,p=l&&l.set;c&&!p||t!==In&&2!==arguments.length||(t=n[e]);var u=!o&&Rn(t,!1,i);return Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){var e=c?c.call(n):t;return yn.target&&(s.depend(),u&&(u.dep.depend(),a(e)&&Mn(e))),Un(e)&&!o?e.value:e},set:function(e){var r=c?c.call(n):t;if(N(r,e)){if(p)p.call(n,e);else{if(c)return;if(!o&&Un(r)&&!Un(e))return void(r.value=e);t=e}u=!o&&Rn(e,!1,i),s.notify()}}}),s}}function Fn(n,e,t){if(!$n(n)){var r=n.__ob__;return a(n)&&g(e)?(n.length=Math.max(n.length,e),n.splice(e,1,t),r&&!r.shallow&&r.mock&&Rn(t,!1,!0),t):e in n&&!(e in Object.prototype)?(n[e]=t,t):n._isVue||r&&r.vmCount?t:r?(Pn(r.value,e,t,void 0,r.shallow,r.mock),r.dep.notify(),t):(n[e]=t,t)}}function Bn(n,e){if(a(n)&&g(e))n.splice(e,1);else{var t=n.__ob__;n._isVue||t&&t.vmCount||$n(n)||x(n,e)&&(delete n[e],t&&t.dep.notify())}}function Mn(n){for(var e=void 0,t=0,r=n.length;t<r;t++)(e=n[t])&&e.__ob__&&e.__ob__.dep.depend(),a(e)&&Mn(e)}function jn(n){return Nn(n,!1),n}function Ln(n){return Nn(n,!0),V(n,"__v_isShallow",!0),n}function Nn(n,e){if(!$n(n)){Rn(n,e,on());0}}function $n(n){return!(!n||!n.__v_isReadonly)}function Un(n){return!(!n||!0!==n.__v_isRef)}function zn(n){return Hn(n,!1)}function Hn(n,e){if(Un(n))return n;var t={};return V(t,"__v_isRef",!0),V(t,"__v_isShallow",e),V(t,"dep",Pn(t,"value",n,null,e,on())),t}function qn(n,e,t){Object.defineProperty(n,t,{enumerable:!0,configurable:!0,get:function(){var n=e[t];if(Un(n))return n.value;var r=n&&n.__ob__;return r&&r.dep.depend(),n},set:function(n){var r=e[t];Un(r)&&!Un(n)?r.value=n:e[t]=n}})}function Vn(n){var e=a(n)?new Array(n.length):{};for(var t in n)e[t]=Kn(n,t);return e}function Kn(n,e,t){var r=n[e];if(Un(r))return r;var a={get value(){var r=n[e];return void 0===r?t:r},set value(t){n[e]=t}};return V(a,"__v_isRef",!0),a}function Wn(n,e){var t,r,a=c(n);a?(t=n,r=P):(t=n.get,r=n.set);var o=on()?null:new et(un,t,P,{lazy:!0});var i={effect:o,get value(){return o?(o.dirty&&o.evaluate(),yn.target&&o.depend(),o.value):t()},set value(n){r(n)}};return V(i,"__v_isRef",!0),V(i,"__v_isReadonly",a),i}"".concat("watcher"," callback"),"".concat("watcher"," getter"),"".concat("watcher"," cleanup");var Gn;var Jn=function(){function n(n){void 0===n&&(n=!1),this.detached=n,this.active=!0,this.effects=[],this.cleanups=[],this.parent=Gn,!n&&Gn&&(this.index=(Gn.scopes||(Gn.scopes=[])).push(this)-1)}return n.prototype.run=function(n){if(this.active){var e=Gn;try{return Gn=this,n()}finally{Gn=e}}else 0},n.prototype.on=function(){Gn=this},n.prototype.off=function(){Gn=this.parent},n.prototype.stop=function(n){if(this.active){var e=void 0,t=void 0;for(e=0,t=this.effects.length;e<t;e++)this.effects[e].teardown();for(e=0,t=this.cleanups.length;e<t;e++)this.cleanups[e]();if(this.scopes)for(e=0,t=this.scopes.length;e<t;e++)this.scopes[e].stop(!0);if(!this.detached&&this.parent&&!n){var r=this.parent.scopes.pop();r&&r!==this&&(this.parent.scopes[this.index]=r,r.index=this.index)}this.parent=void 0,this.active=!1}},n}();function Yn(n){var e=n._provided,t=n.$parent&&n.$parent._provided;return t===e?n._provided=Object.create(t):e}var Qn=w((function(n){var e="&"===n.charAt(0),t="~"===(n=e?n.slice(1):n).charAt(0),r="!"===(n=t?n.slice(1):n).charAt(0);return{name:n=r?n.slice(1):n,once:t,capture:r,passive:e}}));function Xn(n,e){function t(){var n=t.fns;if(!a(n))return Fe(n,null,arguments,e,"v-on handler");for(var r=n.slice(),o=0;o<r.length;o++)Fe(r[o],null,arguments,e,"v-on handler")}return t.fns=n,t}function Zn(n,e,t,r,a,i){var l,c,p,u;for(l in n)c=n[l],p=e[l],u=Qn(l),o(c)||(o(p)?(o(c.fns)&&(c=n[l]=Xn(c,i)),s(u.once)&&(c=n[l]=a(u.name,c,u.capture)),t(u.name,c,u.capture,u.passive,u.params)):c!==p&&(p.fns=c,n[l]=p));for(l in e)o(n[l])&&r((u=Qn(l)).name,e[l],u.capture)}function ne(n,e,t){var r;n instanceof gn&&(n=n.data.hook||(n.data.hook={}));var a=n[e];function l(){t.apply(this,arguments),y(r.fns,l)}o(a)?r=Xn([l]):i(a.fns)&&s(a.merged)?(r=a).fns.push(l):r=Xn([a,l]),r.merged=!0,n[e]=r}function ee(n,e,t,r,a){if(i(e)){if(x(e,t))return n[t]=e[t],a||delete e[t],!0;if(x(e,r))return n[t]=e[r],a||delete e[r],!0}return!1}function te(n){return l(n)?[hn(n)]:a(n)?function n(e,t){var r,c,p,u,d=[];for(r=0;r<e.length;r++)o(c=e[r])||"boolean"==typeof c||(p=d.length-1,u=d[p],a(c)?c.length>0&&(re((c=n(c,"".concat(t||"","_").concat(r)))[0])&&re(u)&&(d[p]=hn(u.text+c[0].text),c.shift()),d.push.apply(d,c)):l(c)?re(u)?d[p]=hn(u.text+c):""!==c&&d.push(hn(c)):re(c)&&re(u)?d[p]=hn(u.text+c.text):(s(e._isVList)&&i(c.tag)&&o(c.key)&&i(t)&&(c.key="__vlist".concat(t,"_").concat(r,"__")),d.push(c)));return d}(n):void 0}function re(n){return i(n)&&i(n.text)&&!1===n.isComment}function ae(n,e){var t,r,o,s,l=null;if(a(n)||"string"==typeof n)for(l=new Array(n.length),t=0,r=n.length;t<r;t++)l[t]=e(n[t],t);else if("number"==typeof n)for(l=new Array(n),t=0;t<n;t++)l[t]=e(t+1,t);else if(p(n))if(pn&&n[Symbol.iterator]){l=[];for(var c=n[Symbol.iterator](),u=c.next();!u.done;)l.push(e(u.value,l.length)),u=c.next()}else for(o=Object.keys(n),l=new Array(o.length),t=0,r=o.length;t<r;t++)s=o[t],l[t]=e(n[s],s,t);return i(l)||(l=[]),l._isVList=!0,l}function oe(n,e,t,r){var a,o=this.$scopedSlots[n];o?(t=t||{},r&&(t=_(_({},r),t)),a=o(t)||(c(e)?e():e)):a=this.$slots[n]||(c(e)?e():e);var i=t&&t.slot;return i?this.$createElement("template",{slot:i},a):a}function ie(n){return $t(this.$options,"filters",n,!0)||B}function se(n,e){return a(n)?-1===n.indexOf(e):n!==e}function le(n,e,t,r,a){var o=z.keyCodes[e]||t;return a&&r&&!z.keyCodes[e]?se(a,r):o?se(o,n):r?T(r)!==e:void 0===n}function ce(n,e,t,r,o){if(t)if(p(t)){a(t)&&(t=R(t));var i=void 0,s=function(a){if("class"===a||"style"===a||k(a))i=n;else{var s=n.attrs&&n.attrs.type;i=r||z.mustUseProp(e,s,a)?n.domProps||(n.domProps={}):n.attrs||(n.attrs={})}var l=D(a),c=T(a);l in i||c in i||(i[a]=t[a],o&&((n.on||(n.on={}))["update:".concat(a)]=function(n){t[a]=n}))};for(var l in t)s(l)}else;return n}function pe(n,e){var t=this._staticTrees||(this._staticTrees=[]),r=t[n];return r&&!e||de(r=t[n]=this.$options.staticRenderFns[n].call(this._renderProxy,this._c,this),"__static__".concat(n),!1),r}function ue(n,e,t){return de(n,"__once__".concat(e).concat(t?"_".concat(t):""),!0),n}function de(n,e,t){if(a(n))for(var r=0;r<n.length;r++)n[r]&&"string"!=typeof n[r]&&me(n[r],"".concat(e,"_").concat(r),t);else me(n,e,t)}function me(n,e,t){n.isStatic=!0,n.key=e,n.isOnce=t}function ge(n,e){if(e)if(d(e)){var t=n.on=n.on?_({},n.on):{};for(var r in e){var a=t[r],o=e[r];t[r]=a?[].concat(a,o):o}}else;return n}function fe(n,e,t,r){e=e||{$stable:!t};for(var o=0;o<n.length;o++){var i=n[o];a(i)?fe(i,e,t):i&&(i.proxy&&(i.fn.proxy=!0),e[i.key]=i.fn)}return r&&(e.$key=r),e}function he(n,e){for(var t=0;t<e.length;t+=2){var r=e[t];"string"==typeof r&&r&&(n[e[t]]=e[t+1])}return n}function ve(n,e){return"string"==typeof n?e+n:n}function be(n){n._o=ue,n._n=v,n._s=h,n._l=ae,n._t=oe,n._q=M,n._i=j,n._m=pe,n._f=ie,n._k=le,n._b=ce,n._v=hn,n._e=fn,n._u=fe,n._g=ge,n._d=he,n._p=ve}function ke(n,e){if(!n||!n.length)return{};for(var t={},r=0,a=n.length;r<a;r++){var o=n[r],i=o.data;if(i&&i.attrs&&i.attrs.slot&&delete i.attrs.slot,o.context!==e&&o.fnContext!==e||!i||null==i.slot)(t.default||(t.default=[])).push(o);else{var s=i.slot,l=t[s]||(t[s]=[]);"template"===o.tag?l.push.apply(l,o.children||[]):l.push(o)}}for(var c in t)t[c].every(ye)&&delete t[c];return t}function ye(n){return n.isComment&&!n.asyncFactory||" "===n.text}function Se(n){return n.isComment&&n.asyncFactory}function xe(n,e,t,a){var o,i=Object.keys(t).length>0,s=e?!!e.$stable:!i,l=e&&e.$key;if(e){if(e._normalized)return e._normalized;if(s&&a&&a!==r&&l===a.$key&&!i&&!a.$hasNormal)return a;for(var c in o={},e)e[c]&&"$"!==c[0]&&(o[c]=we(n,t,c,e[c]))}else o={};for(var p in t)p in o||(o[p]=Ee(t,p));return e&&Object.isExtensible(e)&&(e._normalized=o),V(o,"$stable",s),V(o,"$key",l),V(o,"$hasNormal",i),o}function we(n,e,t,r){var o=function(){var e=un;mn(n);var t=arguments.length?r.apply(null,arguments):r({}),o=(t=t&&"object"==typeof t&&!a(t)?[t]:te(t))&&t[0];return mn(e),t&&(!o||1===t.length&&o.isComment&&!Se(o))?void 0:t};return r.proxy&&Object.defineProperty(e,t,{get:o,enumerable:!0,configurable:!0}),o}function Ee(n,e){return function(){return n[e]}}function De(n){return{get attrs(){if(!n._attrsProxy){var e=n._attrsProxy={};V(e,"_v_attr_proxy",!0),Ce(e,n.$attrs,r,n,"$attrs")}return n._attrsProxy},get listeners(){n._listenersProxy||Ce(n._listenersProxy={},n.$listeners,r,n,"$listeners");return n._listenersProxy},get slots(){return function(n){n._slotsProxy||Te(n._slotsProxy={},n.$scopedSlots);return n._slotsProxy}(n)},emit:O(n.$emit,n),expose:function(e){e&&Object.keys(e).forEach((function(t){return qn(n,e,t)}))}}}function Ce(n,e,t,r,a){var o=!1;for(var i in e)i in n?e[i]!==t[i]&&(o=!0):(o=!0,Ie(n,i,r,a));for(var i in n)i in e||(o=!0,delete n[i]);return o}function Ie(n,e,t,r){Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){return t[r][e]}})}function Te(n,e){for(var t in e)n[t]=e[t];for(var t in n)t in e||delete n[t]}var Oe=null;function Ae(n,e){return(n.__esModule||pn&&"Module"===n[Symbol.toStringTag])&&(n=n.default),p(n)?e.extend(n):n}function _e(n){if(a(n))for(var e=0;e<n.length;e++){var t=n[e];if(i(t)&&(i(t.componentOptions)||Se(t)))return t}}function Re(n,e,t,r,u,d){return(a(t)||l(t))&&(u=r,r=t,t=void 0),s(d)&&(u=2),function(n,e,t,r,l){if(i(t)&&i(t.__ob__))return fn();i(t)&&i(t.is)&&(e=t.is);if(!e)return fn();0;a(r)&&c(r[0])&&((t=t||{}).scopedSlots={default:r[0]},r.length=0);2===l?r=te(r):1===l&&(r=function(n){for(var e=0;e<n.length;e++)if(a(n[e]))return Array.prototype.concat.apply([],n);return n}(r));var u,d;if("string"==typeof e){var m=void 0;d=n.$vnode&&n.$vnode.ns||z.getTagNamespace(e),u=z.isReservedTag(e)?new gn(z.parsePlatformTagName(e),t,r,void 0,void 0,n):t&&t.pre||!i(m=$t(n.$options,"components",e))?new gn(e,t,r,void 0,void 0,n):At(m,t,n,r,e)}else u=At(e,t,n,r);return a(u)?u:i(u)?(i(d)&&function n(e,t,r){e.ns=t,"foreignObject"===e.tag&&(t=void 0,r=!0);if(i(e.children))for(var a=0,l=e.children.length;a<l;a++){var c=e.children[a];i(c.tag)&&(o(c.ns)||s(r)&&"svg"!==c.tag)&&n(c,t,r)}}(u,d),i(t)&&function(n){p(n.style)&&Xe(n.style);p(n.class)&&Xe(n.class)}(t),u):fn()}(n,e,t,r,u)}function Pe(n,e,t){xn();try{if(e)for(var r=e;r=r.$parent;){var a=r.$options.errorCaptured;if(a)for(var o=0;o<a.length;o++)try{if(!1===a[o].call(r,n,e,t))return}catch(n){Be(n,r,"errorCaptured hook")}}Be(n,e,t)}finally{wn()}}function Fe(n,e,t,r,a){var o;try{(o=t?n.apply(e,t):n.call(e))&&!o._isVue&&f(o)&&!o._handled&&(o.catch((function(n){return Pe(n,r,a+" (Promise/async)")})),o._handled=!0)}catch(n){Pe(n,r,a)}return o}function Be(n,e,t){if(z.errorHandler)try{return z.errorHandler.call(null,n,e,t)}catch(e){e!==n&&Me(e,null,"config.errorHandler")}Me(n,e,t)}function Me(n,e,t){if(!G||"undefined"==typeof console)throw n;console.error(n)}var je,Le=!1,Ne=[],$e=!1;function Ue(){$e=!1;var n=Ne.slice(0);Ne.length=0;for(var e=0;e<n.length;e++)n[e]()}if("undefined"!=typeof Promise&&ln(Promise)){var ze=Promise.resolve();je=function(){ze.then(Ue),Z&&setTimeout(P)},Le=!0}else if(Y||"undefined"==typeof MutationObserver||!ln(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())je="undefined"!=typeof setImmediate&&ln(setImmediate)?function(){setImmediate(Ue)}:function(){setTimeout(Ue,0)};else{var He=1,qe=new MutationObserver(Ue),Ve=document.createTextNode(String(He));qe.observe(Ve,{characterData:!0}),je=function(){He=(He+1)%2,Ve.data=String(He)},Le=!0}function Ke(n,e){var t;if(Ne.push((function(){if(n)try{n.call(e)}catch(n){Pe(n,e,"nextTick")}else t&&t(e)})),$e||($e=!0,je()),!n&&"undefined"!=typeof Promise)return new Promise((function(n){t=n}))}function We(n){return function(e,t){if(void 0===t&&(t=un),t)return function(n,e,t){var r=n.$options;r[e]=Mt(r[e],t)}(t,n,e)}}We("beforeMount");var Ge=We("mounted"),Je=(We("beforeUpdate"),We("updated"));We("beforeDestroy"),We("destroyed"),We("activated"),We("deactivated"),We("serverPrefetch"),We("renderTracked"),We("renderTriggered"),We("errorCaptured");function Ye(n){return n}var Qe=new cn;function Xe(n){return function n(e,t){var r,o,i=a(e);if(!i&&!p(e)||e.__v_skip||Object.isFrozen(e)||e instanceof gn)return;if(e.__ob__){var s=e.__ob__.dep.id;if(t.has(s))return;t.add(s)}if(i)for(r=e.length;r--;)n(e[r],t);else if(Un(e))n(e.value,t);else for(o=Object.keys(e),r=o.length;r--;)n(e[o[r]],t)}(n,Qe),Qe.clear(),n}var Ze,nt=0,et=function(){function n(n,e,t,r,a){var o,i;o=this,void 0===(i=Gn&&!Gn._vm?Gn:n?n._scope:void 0)&&(i=Gn),i&&i.active&&i.effects.push(o),(this.vm=n)&&a&&(n._watcher=this),r?(this.deep=!!r.deep,this.user=!!r.user,this.lazy=!!r.lazy,this.sync=!!r.sync,this.before=r.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++nt,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new cn,this.newDepIds=new cn,this.expression="",c(e)?this.getter=e:(this.getter=function(n){if(!K.test(n)){var e=n.split(".");return function(n){for(var t=0;t<e.length;t++){if(!n)return;n=n[e[t]]}return n}}}(e),this.getter||(this.getter=P)),this.value=this.lazy?void 0:this.get()}return n.prototype.get=function(){var n;xn(this);var e=this.vm;try{n=this.getter.call(e,e)}catch(n){if(!this.user)throw n;Pe(n,e,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&Xe(n),wn(),this.cleanupDeps()}return n},n.prototype.addDep=function(n){var e=n.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(n),this.depIds.has(e)||n.addSub(this))},n.prototype.cleanupDeps=function(){for(var n=this.deps.length;n--;){var e=this.deps[n];this.newDepIds.has(e.id)||e.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},n.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():xt(this)},n.prototype.run=function(){if(this.active){var n=this.get();if(n!==this.value||p(n)||this.deep){var e=this.value;if(this.value=n,this.user){var t='callback for watcher "'.concat(this.expression,'"');Fe(this.cb,this.vm,[n,e],this.vm,t)}else this.cb.call(this.vm,n,e)}}},n.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},n.prototype.depend=function(){for(var n=this.deps.length;n--;)this.deps[n].depend()},n.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&y(this.vm._scope.effects,this),this.active){for(var n=this.deps.length;n--;)this.deps[n].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},n}();function tt(n,e){Ze.$on(n,e)}function rt(n,e){Ze.$off(n,e)}function at(n,e){var t=Ze;return function r(){var a=e.apply(null,arguments);null!==a&&t.$off(n,r)}}function ot(n,e,t){Ze=n,Zn(e,t||{},tt,rt,at,n),Ze=void 0}var it=null;function st(n){var e=it;return it=n,function(){it=e}}function lt(n){for(;n&&(n=n.$parent);)if(n._inactive)return!0;return!1}function ct(n,e){if(e){if(n._directInactive=!1,lt(n))return}else if(n._directInactive)return;if(n._inactive||null===n._inactive){n._inactive=!1;for(var t=0;t<n.$children.length;t++)ct(n.$children[t]);pt(n,"activated")}}function pt(n,e,t,r){void 0===r&&(r=!0),xn();var a=un;r&&mn(n);var o=n.$options[e],i="".concat(e," hook");if(o)for(var s=0,l=o.length;s<l;s++)Fe(o[s],n,t||null,n,i);n._hasHookEvent&&n.$emit("hook:"+e),r&&mn(a),wn()}var ut=[],dt=[],mt={},gt=!1,ft=!1,ht=0;var vt=0,bt=Date.now;if(G&&!Y){var kt=window.performance;kt&&"function"==typeof kt.now&&bt()>document.createEvent("Event").timeStamp&&(bt=function(){return kt.now()})}var yt=function(n,e){if(n.post){if(!e.post)return 1}else if(e.post)return-1;return n.id-e.id};function St(){var n,e;for(vt=bt(),ft=!0,ut.sort(yt),ht=0;ht<ut.length;ht++)(n=ut[ht]).before&&n.before(),e=n.id,mt[e]=null,n.run();var t=dt.slice(),r=ut.slice();ht=ut.length=dt.length=0,mt={},gt=ft=!1,function(n){for(var e=0;e<n.length;e++)n[e]._inactive=!0,ct(n[e],!0)}(t),function(n){var e=n.length;for(;e--;){var t=n[e],r=t.vm;r&&r._watcher===t&&r._isMounted&&!r._isDestroyed&&pt(r,"updated")}}(r),function(){for(var n=0;n<kn.length;n++){var e=kn[n];e.subs=e.subs.filter((function(n){return n})),e._pending=!1}kn.length=0}(),sn&&z.devtools&&sn.emit("flush")}function xt(n){var e=n.id;if(null==mt[e]&&(n!==yn.target||!n.noRecurse)){if(mt[e]=!0,ft){for(var t=ut.length-1;t>ht&&ut[t].id>n.id;)t--;ut.splice(t+1,0,n)}else ut.push(n);gt||(gt=!0,Ke(St))}}function wt(n,e){if(n){for(var t=Object.create(null),r=pn?Reflect.ownKeys(n):Object.keys(n),a=0;a<r.length;a++){var o=r[a];if("__ob__"!==o){var i=n[o].from;if(i in e._provided)t[o]=e._provided[i];else if("default"in n[o]){var s=n[o].default;t[o]=c(s)?s.call(e):s}else 0}}return t}}function Et(n,e,t,o,i){var l,c=this,p=i.options;x(o,"_uid")?(l=Object.create(o))._original=o:(l=o,o=o._original);var u=s(p._compiled),d=!u;this.data=n,this.props=e,this.children=t,this.parent=o,this.listeners=n.on||r,this.injections=wt(p.inject,o),this.slots=function(){return c.$slots||xe(o,n.scopedSlots,c.$slots=ke(t,o)),c.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return xe(o,n.scopedSlots,this.slots())}}),u&&(this.$options=p,this.$slots=this.slots(),this.$scopedSlots=xe(o,n.scopedSlots,this.$slots)),p._scopeId?this._c=function(n,e,t,r){var i=Re(l,n,e,t,r,d);return i&&!a(i)&&(i.fnScopeId=p._scopeId,i.fnContext=o),i}:this._c=function(n,e,t,r){return Re(l,n,e,t,r,d)}}function Dt(n,e,t,r,a){var o=vn(n);return o.fnContext=t,o.fnOptions=r,e.slot&&((o.data||(o.data={})).slot=e.slot),o}function Ct(n,e){for(var t in e)n[D(t)]=e[t]}function It(n){return n.name||n.__name||n._componentTag}be(Et.prototype);var Tt={init:function(n,e){if(n.componentInstance&&!n.componentInstance._isDestroyed&&n.data.keepAlive){var t=n;Tt.prepatch(t,t)}else{(n.componentInstance=function(n,e){var t={_isComponent:!0,_parentVnode:n,parent:e},r=n.data.inlineTemplate;i(r)&&(t.render=r.render,t.staticRenderFns=r.staticRenderFns);return new n.componentOptions.Ctor(t)}(n,it)).$mount(e?n.elm:void 0,e)}},prepatch:function(n,e){var t=e.componentOptions;!function(n,e,t,a,o){var i=a.data.scopedSlots,s=n.$scopedSlots,l=!!(i&&!i.$stable||s!==r&&!s.$stable||i&&n.$scopedSlots.$key!==i.$key||!i&&n.$scopedSlots.$key),c=!!(o||n.$options._renderChildren||l),p=n.$vnode;n.$options._parentVnode=a,n.$vnode=a,n._vnode&&(n._vnode.parent=a),n.$options._renderChildren=o;var u=a.data.attrs||r;n._attrsProxy&&Ce(n._attrsProxy,u,p.data&&p.data.attrs||r,n,"$attrs")&&(c=!0),n.$attrs=u,t=t||r;var d=n.$options._parentListeners;if(n._listenersProxy&&Ce(n._listenersProxy,t,d||r,n,"$listeners"),n.$listeners=n.$options._parentListeners=t,ot(n,t,d),e&&n.$options.props){On(!1);for(var m=n._props,g=n.$options._propKeys||[],f=0;f<g.length;f++){var h=g[f],v=n.$options.props;m[h]=Ut(h,v,e,n)}On(!0),n.$options.propsData=e}c&&(n.$slots=ke(o,a.context),n.$forceUpdate())}(e.componentInstance=n.componentInstance,t.propsData,t.listeners,e,t.children)},insert:function(n){var e,t=n.context,r=n.componentInstance;r._isMounted||(r._isMounted=!0,pt(r,"mounted")),n.data.keepAlive&&(t._isMounted?((e=r)._inactive=!1,dt.push(e)):ct(r,!0))},destroy:function(n){var e=n.componentInstance;e._isDestroyed||(n.data.keepAlive?function n(e,t){if(!(t&&(e._directInactive=!0,lt(e))||e._inactive)){e._inactive=!0;for(var r=0;r<e.$children.length;r++)n(e.$children[r]);pt(e,"deactivated")}}(e,!0):e.$destroy())}},Ot=Object.keys(Tt);function At(n,e,t,l,c){if(!o(n)){var u=t.$options._base;if(p(n)&&(n=u.extend(n)),"function"==typeof n){var d;if(o(n.cid)&&void 0===(n=function(n,e){if(s(n.error)&&i(n.errorComp))return n.errorComp;if(i(n.resolved))return n.resolved;var t=Oe;if(t&&i(n.owners)&&-1===n.owners.indexOf(t)&&n.owners.push(t),s(n.loading)&&i(n.loadingComp))return n.loadingComp;if(t&&!i(n.owners)){var r=n.owners=[t],a=!0,l=null,c=null;t.$on("hook:destroyed",(function(){return y(r,t)}));var u=function(n){for(var e=0,t=r.length;e<t;e++)r[e].$forceUpdate();n&&(r.length=0,null!==l&&(clearTimeout(l),l=null),null!==c&&(clearTimeout(c),c=null))},d=L((function(t){n.resolved=Ae(t,e),a?r.length=0:u(!0)})),m=L((function(e){i(n.errorComp)&&(n.error=!0,u(!0))})),g=n(d,m);return p(g)&&(f(g)?o(n.resolved)&&g.then(d,m):f(g.component)&&(g.component.then(d,m),i(g.error)&&(n.errorComp=Ae(g.error,e)),i(g.loading)&&(n.loadingComp=Ae(g.loading,e),0===g.delay?n.loading=!0:l=setTimeout((function(){l=null,o(n.resolved)&&o(n.error)&&(n.loading=!0,u(!1))}),g.delay||200)),i(g.timeout)&&(c=setTimeout((function(){c=null,o(n.resolved)&&m(null)}),g.timeout)))),a=!1,n.loading?n.loadingComp:n.resolved}}(d=n,u)))return function(n,e,t,r,a){var o=fn();return o.asyncFactory=n,o.asyncMeta={data:e,context:t,children:r,tag:a},o}(d,e,t,l,c);e=e||{},er(n),i(e.model)&&function(n,e){var t=n.model&&n.model.prop||"value",r=n.model&&n.model.event||"input";(e.attrs||(e.attrs={}))[t]=e.model.value;var o=e.on||(e.on={}),s=o[r],l=e.model.callback;i(s)?(a(s)?-1===s.indexOf(l):s!==l)&&(o[r]=[l].concat(s)):o[r]=l}(n.options,e);var m=function(n,e,t){var r=e.options.props;if(!o(r)){var a={},s=n.attrs,l=n.props;if(i(s)||i(l))for(var c in r){var p=T(c);ee(a,l,c,p,!0)||ee(a,s,c,p,!1)}return a}}(e,n);if(s(n.options.functional))return function(n,e,t,o,s){var l=n.options,c={},p=l.props;if(i(p))for(var u in p)c[u]=Ut(u,p,e||r);else i(t.attrs)&&Ct(c,t.attrs),i(t.props)&&Ct(c,t.props);var d=new Et(t,c,s,o,n),m=l.render.call(null,d._c,d);if(m instanceof gn)return Dt(m,t,d.parent,l,d);if(a(m)){for(var g=te(m)||[],f=new Array(g.length),h=0;h<g.length;h++)f[h]=Dt(g[h],t,d.parent,l,d);return f}}(n,m,e,t,l);var g=e.on;if(e.on=e.nativeOn,s(n.options.abstract)){var h=e.slot;e={},h&&(e.slot=h)}!function(n){for(var e=n.hook||(n.hook={}),t=0;t<Ot.length;t++){var r=Ot[t],a=e[r],o=Tt[r];a===o||a&&a._merged||(e[r]=a?_t(o,a):o)}}(e);var v=It(n.options)||c;return new gn("vue-component-".concat(n.cid).concat(v?"-".concat(v):""),e,void 0,void 0,void 0,t,{Ctor:n,propsData:m,listeners:g,tag:c,children:l},d)}}}function _t(n,e){var t=function(t,r){n(t,r),e(t,r)};return t._merged=!0,t}var Rt=P,Pt=z.optionMergeStrategies;function Ft(n,e,t){if(void 0===t&&(t=!0),!e)return n;for(var r,a,o,i=pn?Reflect.ownKeys(e):Object.keys(e),s=0;s<i.length;s++)"__ob__"!==(r=i[s])&&(a=n[r],o=e[r],t&&x(n,r)?a!==o&&d(a)&&d(o)&&Ft(a,o):Fn(n,r,o));return n}function Bt(n,e,t){return t?function(){var r=c(e)?e.call(t,t):e,a=c(n)?n.call(t,t):n;return r?Ft(r,a):a}:e?n?function(){return Ft(c(e)?e.call(this,this):e,c(n)?n.call(this,this):n)}:e:n}function Mt(n,e){var t=e?n?n.concat(e):a(e)?e:[e]:n;return t?function(n){for(var e=[],t=0;t<n.length;t++)-1===e.indexOf(n[t])&&e.push(n[t]);return e}(t):t}function jt(n,e,t,r){var a=Object.create(n||null);return e?_(a,e):a}Pt.data=function(n,e,t){return t?Bt(n,e,t):e&&"function"!=typeof e?n:Bt(n,e)},U.forEach((function(n){Pt[n]=Mt})),$.forEach((function(n){Pt[n+"s"]=jt})),Pt.watch=function(n,e,t,r){if(n===tn&&(n=void 0),e===tn&&(e=void 0),!e)return Object.create(n||null);if(!n)return e;var o={};for(var i in _(o,n),e){var s=o[i],l=e[i];s&&!a(s)&&(s=[s]),o[i]=s?s.concat(l):a(l)?l:[l]}return o},Pt.props=Pt.methods=Pt.inject=Pt.computed=function(n,e,t,r){if(!n)return e;var a=Object.create(null);return _(a,n),e&&_(a,e),a},Pt.provide=function(n,e){return n?function(){var t=Object.create(null);return Ft(t,c(n)?n.call(this):n),e&&Ft(t,c(e)?e.call(this):e,!1),t}:e};var Lt=function(n,e){return void 0===e?n:e};function Nt(n,e,t){if(c(e)&&(e=e.options),function(n,e){var t=n.props;if(t){var r,o,i={};if(a(t))for(r=t.length;r--;)"string"==typeof(o=t[r])&&(i[D(o)]={type:null});else if(d(t))for(var s in t)o=t[s],i[D(s)]=d(o)?o:{type:o};else 0;n.props=i}}(e),function(n,e){var t=n.inject;if(t){var r=n.inject={};if(a(t))for(var o=0;o<t.length;o++)r[t[o]]={from:t[o]};else if(d(t))for(var i in t){var s=t[i];r[i]=d(s)?_({from:i},s):{from:s}}else 0}}(e),function(n){var e=n.directives;if(e)for(var t in e){var r=e[t];c(r)&&(e[t]={bind:r,update:r})}}(e),!e._base&&(e.extends&&(n=Nt(n,e.extends,t)),e.mixins))for(var r=0,o=e.mixins.length;r<o;r++)n=Nt(n,e.mixins[r],t);var i,s={};for(i in n)l(i);for(i in e)x(n,i)||l(i);function l(r){var a=Pt[r]||Lt;s[r]=a(n[r],e[r],t,r)}return s}function $t(n,e,t,r){if("string"==typeof t){var a=n[e];if(x(a,t))return a[t];var o=D(t);if(x(a,o))return a[o];var i=C(o);return x(a,i)?a[i]:a[t]||a[o]||a[i]}}function Ut(n,e,t,r){var a=e[n],o=!x(t,n),i=t[n],s=Vt(Boolean,a.type);if(s>-1)if(o&&!x(a,"default"))i=!1;else if(""===i||i===T(n)){var l=Vt(String,a.type);(l<0||s<l)&&(i=!0)}if(void 0===i){i=function(n,e,t){if(!x(e,"default"))return;var r=e.default;0;if(n&&n.$options.propsData&&void 0===n.$options.propsData[t]&&void 0!==n._props[t])return n._props[t];return c(r)&&"Function"!==Ht(e.type)?r.call(n):r}(r,a,n);var p=Tn;On(!0),Rn(i),On(p)}return i}var zt=/^\s*function (\w+)/;function Ht(n){var e=n&&n.toString().match(zt);return e?e[1]:""}function qt(n,e){return Ht(n)===Ht(e)}function Vt(n,e){if(!a(e))return qt(e,n)?0:-1;for(var t=0,r=e.length;t<r;t++)if(qt(e[t],n))return t;return-1}var Kt={enumerable:!0,configurable:!0,get:P,set:P};function Wt(n,e,t){Kt.get=function(){return this[e][t]},Kt.set=function(n){this[e][t]=n},Object.defineProperty(n,t,Kt)}function Gt(n){var e=n.$options;if(e.props&&function(n,e){var t=n.$options.propsData||{},r=n._props=Ln({}),a=n.$options._propKeys=[];n.$parent&&On(!1);var o=function(o){a.push(o);var i=Ut(o,e,t,n);Pn(r,o,i),o in n||Wt(n,"_props",o)};for(var i in e)o(i);On(!0)}(n,e.props),function(n){var e=n.$options,t=e.setup;if(t){var r=n._setupContext=De(n);mn(n),xn();var a=Fe(t,null,[n._props||Ln({}),r],n,"setup");if(wn(),mn(),c(a))e.render=a;else if(p(a))if(n._setupState=a,a.__sfc){var o=n._setupProxy={};for(var i in a)"__sfc"!==i&&qn(o,a,i)}else for(var i in a)q(i)||qn(n,a,i);else 0}}(n),e.methods&&function(n,e){n.$options.props;for(var t in e)n[t]="function"!=typeof e[t]?P:O(e[t],n)}(n,e.methods),e.data)!function(n){var e=n.$options.data;d(e=n._data=c(e)?function(n,e){xn();try{return n.call(e,e)}catch(n){return Pe(n,e,"data()"),{}}finally{wn()}}(e,n):e||{})||(e={});var t=Object.keys(e),r=n.$options.props,a=(n.$options.methods,t.length);for(;a--;){var o=t[a];0,r&&x(r,o)||q(o)||Wt(n,"_data",o)}var i=Rn(e);i&&i.vmCount++}(n);else{var t=Rn(n._data={});t&&t.vmCount++}e.computed&&function(n,e){var t=n._computedWatchers=Object.create(null),r=on();for(var a in e){var o=e[a],i=c(o)?o:o.get;0,r||(t[a]=new et(n,i||P,P,Jt)),a in n||Yt(n,a,o)}}(n,e.computed),e.watch&&e.watch!==tn&&function(n,e){for(var t in e){var r=e[t];if(a(r))for(var o=0;o<r.length;o++)Zt(n,t,r[o]);else Zt(n,t,r)}}(n,e.watch)}var Jt={lazy:!0};function Yt(n,e,t){var r=!on();c(t)?(Kt.get=r?Qt(e):Xt(t),Kt.set=P):(Kt.get=t.get?r&&!1!==t.cache?Qt(e):Xt(t.get):P,Kt.set=t.set||P),Object.defineProperty(n,e,Kt)}function Qt(n){return function(){var e=this._computedWatchers&&this._computedWatchers[n];if(e)return e.dirty&&e.evaluate(),yn.target&&e.depend(),e.value}}function Xt(n){return function(){return n.call(this,this)}}function Zt(n,e,t,r){return d(t)&&(r=t,t=t.handler),"string"==typeof t&&(t=n[t]),n.$watch(e,t,r)}var nr=0;function er(n){var e=n.options;if(n.super){var t=er(n.super);if(t!==n.superOptions){n.superOptions=t;var r=function(n){var e,t=n.options,r=n.sealedOptions;for(var a in t)t[a]!==r[a]&&(e||(e={}),e[a]=t[a]);return e}(n);r&&_(n.extendOptions,r),(e=n.options=Nt(t,n.extendOptions)).name&&(e.components[e.name]=n)}}return e}function tr(n){this._init(n)}function rr(n){n.cid=0;var e=1;n.extend=function(n){n=n||{};var t=this,r=t.cid,a=n._Ctor||(n._Ctor={});if(a[r])return a[r];var o=It(n)||It(t.options);var i=function(n){this._init(n)};return(i.prototype=Object.create(t.prototype)).constructor=i,i.cid=e++,i.options=Nt(t.options,n),i.super=t,i.options.props&&function(n){var e=n.options.props;for(var t in e)Wt(n.prototype,"_props",t)}(i),i.options.computed&&function(n){var e=n.options.computed;for(var t in e)Yt(n.prototype,t,e[t])}(i),i.extend=t.extend,i.mixin=t.mixin,i.use=t.use,$.forEach((function(n){i[n]=t[n]})),o&&(i.options.components[o]=i),i.superOptions=t.options,i.extendOptions=n,i.sealedOptions=_({},i.options),a[r]=i,i}}function ar(n){return n&&(It(n.Ctor.options)||n.tag)}function or(n,e){return a(n)?n.indexOf(e)>-1:"string"==typeof n?n.split(",").indexOf(e)>-1:!!m(n)&&n.test(e)}function ir(n,e){var t=n.cache,r=n.keys,a=n._vnode;for(var o in t){var i=t[o];if(i){var s=i.name;s&&!e(s)&&sr(t,o,r,a)}}}function sr(n,e,t,r){var a=n[e];!a||r&&a.tag===r.tag||a.componentInstance.$destroy(),n[e]=null,y(t,e)}!function(n){n.prototype._init=function(n){var e=this;e._uid=nr++,e._isVue=!0,e.__v_skip=!0,e._scope=new Jn(!0),e._scope._vm=!0,n&&n._isComponent?function(n,e){var t=n.$options=Object.create(n.constructor.options),r=e._parentVnode;t.parent=e.parent,t._parentVnode=r;var a=r.componentOptions;t.propsData=a.propsData,t._parentListeners=a.listeners,t._renderChildren=a.children,t._componentTag=a.tag,e.render&&(t.render=e.render,t.staticRenderFns=e.staticRenderFns)}(e,n):e.$options=Nt(er(e.constructor),n||{},e),e._renderProxy=e,e._self=e,function(n){var e=n.$options,t=e.parent;if(t&&!e.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(n)}n.$parent=t,n.$root=t?t.$root:n,n.$children=[],n.$refs={},n._provided=t?t._provided:Object.create(null),n._watcher=null,n._inactive=null,n._directInactive=!1,n._isMounted=!1,n._isDestroyed=!1,n._isBeingDestroyed=!1}(e),function(n){n._events=Object.create(null),n._hasHookEvent=!1;var e=n.$options._parentListeners;e&&ot(n,e)}(e),function(n){n._vnode=null,n._staticTrees=null;var e=n.$options,t=n.$vnode=e._parentVnode,a=t&&t.context;n.$slots=ke(e._renderChildren,a),n.$scopedSlots=t?xe(n.$parent,t.data.scopedSlots,n.$slots):r,n._c=function(e,t,r,a){return Re(n,e,t,r,a,!1)},n.$createElement=function(e,t,r,a){return Re(n,e,t,r,a,!0)};var o=t&&t.data;Pn(n,"$attrs",o&&o.attrs||r,null,!0),Pn(n,"$listeners",e._parentListeners||r,null,!0)}(e),pt(e,"beforeCreate",void 0,!1),function(n){var e=wt(n.$options.inject,n);e&&(On(!1),Object.keys(e).forEach((function(t){Pn(n,t,e[t])})),On(!0))}(e),Gt(e),function(n){var e=n.$options.provide;if(e){var t=c(e)?e.call(n):e;if(!p(t))return;for(var r=Yn(n),a=pn?Reflect.ownKeys(t):Object.keys(t),o=0;o<a.length;o++){var i=a[o];Object.defineProperty(r,i,Object.getOwnPropertyDescriptor(t,i))}}}(e),pt(e,"created"),e.$options.el&&e.$mount(e.$options.el)}}(tr),function(n){var e={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(n.prototype,"$data",e),Object.defineProperty(n.prototype,"$props",t),n.prototype.$set=Fn,n.prototype.$delete=Bn,n.prototype.$watch=function(n,e,t){if(d(e))return Zt(this,n,e,t);(t=t||{}).user=!0;var r=new et(this,n,e,t);if(t.immediate){var a='callback for immediate watcher "'.concat(r.expression,'"');xn(),Fe(e,this,[r.value],this,a),wn()}return function(){r.teardown()}}}(tr),function(n){var e=/^hook:/;n.prototype.$on=function(n,t){var r=this;if(a(n))for(var o=0,i=n.length;o<i;o++)r.$on(n[o],t);else(r._events[n]||(r._events[n]=[])).push(t),e.test(n)&&(r._hasHookEvent=!0);return r},n.prototype.$once=function(n,e){var t=this;function r(){t.$off(n,r),e.apply(t,arguments)}return r.fn=e,t.$on(n,r),t},n.prototype.$off=function(n,e){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(a(n)){for(var r=0,o=n.length;r<o;r++)t.$off(n[r],e);return t}var i,s=t._events[n];if(!s)return t;if(!e)return t._events[n]=null,t;for(var l=s.length;l--;)if((i=s[l])===e||i.fn===e){s.splice(l,1);break}return t},n.prototype.$emit=function(n){var e=this,t=e._events[n];if(t){t=t.length>1?A(t):t;for(var r=A(arguments,1),a='event handler for "'.concat(n,'"'),o=0,i=t.length;o<i;o++)Fe(t[o],e,r,e,a)}return e}}(tr),function(n){n.prototype._update=function(n,e){var t=this,r=t.$el,a=t._vnode,o=st(t);t._vnode=n,t.$el=a?t.__patch__(a,n):t.__patch__(t.$el,n,e,!1),o(),r&&(r.__vue__=null),t.$el&&(t.$el.__vue__=t);for(var i=t;i&&i.$vnode&&i.$parent&&i.$vnode===i.$parent._vnode;)i.$parent.$el=i.$el,i=i.$parent},n.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},n.prototype.$destroy=function(){var n=this;if(!n._isBeingDestroyed){pt(n,"beforeDestroy"),n._isBeingDestroyed=!0;var e=n.$parent;!e||e._isBeingDestroyed||n.$options.abstract||y(e.$children,n),n._scope.stop(),n._data.__ob__&&n._data.__ob__.vmCount--,n._isDestroyed=!0,n.__patch__(n._vnode,null),pt(n,"destroyed"),n.$off(),n.$el&&(n.$el.__vue__=null),n.$vnode&&(n.$vnode.parent=null)}}}(tr),function(n){be(n.prototype),n.prototype.$nextTick=function(n){return Ke(n,this)},n.prototype._render=function(){var n,e=this,t=e.$options,r=t.render,o=t._parentVnode;o&&e._isMounted&&(e.$scopedSlots=xe(e.$parent,o.data.scopedSlots,e.$slots,e.$scopedSlots),e._slotsProxy&&Te(e._slotsProxy,e.$scopedSlots)),e.$vnode=o;try{mn(e),Oe=e,n=r.call(e._renderProxy,e.$createElement)}catch(t){Pe(t,e,"render"),n=e._vnode}finally{Oe=null,mn()}return a(n)&&1===n.length&&(n=n[0]),n instanceof gn||(n=fn()),n.parent=o,n}}(tr);var lr=[String,RegExp,Array],cr={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:lr,exclude:lr,max:[String,Number]},methods:{cacheVNode:function(){var n=this.cache,e=this.keys,t=this.vnodeToCache,r=this.keyToCache;if(t){var a=t.tag,o=t.componentInstance,i=t.componentOptions;n[r]={name:ar(i),tag:a,componentInstance:o},e.push(r),this.max&&e.length>parseInt(this.max)&&sr(n,e[0],e,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var n in this.cache)sr(this.cache,n,this.keys)},mounted:function(){var n=this;this.cacheVNode(),this.$watch("include",(function(e){ir(n,(function(n){return or(e,n)}))})),this.$watch("exclude",(function(e){ir(n,(function(n){return!or(e,n)}))}))},updated:function(){this.cacheVNode()},render:function(){var n=this.$slots.default,e=_e(n),t=e&&e.componentOptions;if(t){var r=ar(t),a=this.include,o=this.exclude;if(a&&(!r||!or(a,r))||o&&r&&or(o,r))return e;var i=this.cache,s=this.keys,l=null==e.key?t.Ctor.cid+(t.tag?"::".concat(t.tag):""):e.key;i[l]?(e.componentInstance=i[l].componentInstance,y(s,l),s.push(l)):(this.vnodeToCache=e,this.keyToCache=l),e.data.keepAlive=!0}return e||n&&n[0]}}};!function(n){var e={get:function(){return z}};Object.defineProperty(n,"config",e),n.util={warn:Rt,extend:_,mergeOptions:Nt,defineReactive:Pn},n.set=Fn,n.delete=Bn,n.nextTick=Ke,n.observable=function(n){return Rn(n),n},n.options=Object.create(null),$.forEach((function(e){n.options[e+"s"]=Object.create(null)})),n.options._base=n,_(n.options.components,cr),function(n){n.use=function(n){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(n)>-1)return this;var t=A(arguments,1);return t.unshift(this),c(n.install)?n.install.apply(n,t):c(n)&&n.apply(null,t),e.push(n),this}}(n),function(n){n.mixin=function(n){return this.options=Nt(this.options,n),this}}(n),rr(n),function(n){$.forEach((function(e){n[e]=function(n,t){return t?("component"===e&&d(t)&&(t.name=t.name||n,t=this.options._base.extend(t)),"directive"===e&&c(t)&&(t={bind:t,update:t}),this.options[e+"s"][n]=t,t):this.options[e+"s"][n]}}))}(n)}(tr),Object.defineProperty(tr.prototype,"$isServer",{get:on}),Object.defineProperty(tr.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(tr,"FunctionalRenderContext",{value:Et}),tr.version="2.7.14";var pr=b("style,class"),ur=b("input,textarea,option,select,progress"),dr=b("contenteditable,draggable,spellcheck"),mr=b("events,caret,typing,plaintext-only"),gr=b("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),fr="http://www.w3.org/1999/xlink",hr=function(n){return":"===n.charAt(5)&&"xlink"===n.slice(0,5)},vr=function(n){return hr(n)?n.slice(6,n.length):""},br=function(n){return null==n||!1===n};function kr(n){for(var e=n.data,t=n,r=n;i(r.componentInstance);)(r=r.componentInstance._vnode)&&r.data&&(e=yr(r.data,e));for(;i(t=t.parent);)t&&t.data&&(e=yr(e,t.data));return function(n,e){if(i(n)||i(e))return Sr(n,xr(e));return""}(e.staticClass,e.class)}function yr(n,e){return{staticClass:Sr(n.staticClass,e.staticClass),class:i(n.class)?[n.class,e.class]:e.class}}function Sr(n,e){return n?e?n+" "+e:n:e||""}function xr(n){return Array.isArray(n)?function(n){for(var e,t="",r=0,a=n.length;r<a;r++)i(e=xr(n[r]))&&""!==e&&(t&&(t+=" "),t+=e);return t}(n):p(n)?function(n){var e="";for(var t in n)n[t]&&(e&&(e+=" "),e+=t);return e}(n):"string"==typeof n?n:""}var wr={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},Er=b("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),Dr=b("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Cr=function(n){return Er(n)||Dr(n)};var Ir=Object.create(null);var Tr=b("text,number,password,search,email,tel,url");var Or=Object.freeze({__proto__:null,createElement:function(n,e){var t=document.createElement(n);return"select"!==n||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(n,e){return document.createElementNS(wr[n],e)},createTextNode:function(n){return document.createTextNode(n)},createComment:function(n){return document.createComment(n)},insertBefore:function(n,e,t){n.insertBefore(e,t)},removeChild:function(n,e){n.removeChild(e)},appendChild:function(n,e){n.appendChild(e)},parentNode:function(n){return n.parentNode},nextSibling:function(n){return n.nextSibling},tagName:function(n){return n.tagName},setTextContent:function(n,e){n.textContent=e},setStyleScope:function(n,e){n.setAttribute(e,"")}}),Ar={create:function(n,e){_r(e)},update:function(n,e){n.data.ref!==e.data.ref&&(_r(n,!0),_r(e))},destroy:function(n){_r(n,!0)}};function _r(n,e){var t=n.data.ref;if(i(t)){var r=n.context,o=n.componentInstance||n.elm,s=e?null:o,l=e?void 0:o;if(c(t))Fe(t,r,[s],r,"template ref function");else{var p=n.data.refInFor,u="string"==typeof t||"number"==typeof t,d=Un(t),m=r.$refs;if(u||d)if(p){var g=u?m[t]:t.value;e?a(g)&&y(g,o):a(g)?g.includes(o)||g.push(o):u?(m[t]=[o],Rr(r,t,m[t])):t.value=[o]}else if(u){if(e&&m[t]!==o)return;m[t]=l,Rr(r,t,s)}else if(d){if(e&&t.value!==o)return;t.value=s}else 0}}}function Rr(n,e,t){var r=n._setupState;r&&x(r,e)&&(Un(r[e])?r[e].value=t:r[e]=t)}var Pr=new gn("",{},[]),Fr=["create","activate","update","remove","destroy"];function Br(n,e){return n.key===e.key&&n.asyncFactory===e.asyncFactory&&(n.tag===e.tag&&n.isComment===e.isComment&&i(n.data)===i(e.data)&&function(n,e){if("input"!==n.tag)return!0;var t,r=i(t=n.data)&&i(t=t.attrs)&&t.type,a=i(t=e.data)&&i(t=t.attrs)&&t.type;return r===a||Tr(r)&&Tr(a)}(n,e)||s(n.isAsyncPlaceholder)&&o(e.asyncFactory.error))}function Mr(n,e,t){var r,a,o={};for(r=e;r<=t;++r)i(a=n[r].key)&&(o[a]=r);return o}var jr={create:Lr,update:Lr,destroy:function(n){Lr(n,Pr)}};function Lr(n,e){(n.data.directives||e.data.directives)&&function(n,e){var t,r,a,o=n===Pr,i=e===Pr,s=$r(n.data.directives,n.context),l=$r(e.data.directives,e.context),c=[],p=[];for(t in l)r=s[t],a=l[t],r?(a.oldValue=r.value,a.oldArg=r.arg,zr(a,"update",e,n),a.def&&a.def.componentUpdated&&p.push(a)):(zr(a,"bind",e,n),a.def&&a.def.inserted&&c.push(a));if(c.length){var u=function(){for(var t=0;t<c.length;t++)zr(c[t],"inserted",e,n)};o?ne(e,"insert",u):u()}p.length&&ne(e,"postpatch",(function(){for(var t=0;t<p.length;t++)zr(p[t],"componentUpdated",e,n)}));if(!o)for(t in s)l[t]||zr(s[t],"unbind",n,n,i)}(n,e)}var Nr=Object.create(null);function $r(n,e){var t,r,a=Object.create(null);if(!n)return a;for(t=0;t<n.length;t++){if((r=n[t]).modifiers||(r.modifiers=Nr),a[Ur(r)]=r,e._setupState&&e._setupState.__sfc){var o=r.def||$t(e,"_setupState","v-"+r.name);r.def="function"==typeof o?{bind:o,update:o}:o}r.def=r.def||$t(e.$options,"directives",r.name)}return a}function Ur(n){return n.rawName||"".concat(n.name,".").concat(Object.keys(n.modifiers||{}).join("."))}function zr(n,e,t,r,a){var o=n.def&&n.def[e];if(o)try{o(t.elm,n,t,r,a)}catch(r){Pe(r,t.context,"directive ".concat(n.name," ").concat(e," hook"))}}var Hr=[Ar,jr];function qr(n,e){var t=e.componentOptions;if(!(i(t)&&!1===t.Ctor.options.inheritAttrs||o(n.data.attrs)&&o(e.data.attrs))){var r,a,l=e.elm,c=n.data.attrs||{},p=e.data.attrs||{};for(r in(i(p.__ob__)||s(p._v_attr_proxy))&&(p=e.data.attrs=_({},p)),p)a=p[r],c[r]!==a&&Vr(l,r,a,e.data.pre);for(r in(Y||X)&&p.value!==c.value&&Vr(l,"value",p.value),c)o(p[r])&&(hr(r)?l.removeAttributeNS(fr,vr(r)):dr(r)||l.removeAttribute(r))}}function Vr(n,e,t,r){r||n.tagName.indexOf("-")>-1?Kr(n,e,t):gr(e)?br(t)?n.removeAttribute(e):(t="allowfullscreen"===e&&"EMBED"===n.tagName?"true":e,n.setAttribute(e,t)):dr(e)?n.setAttribute(e,function(n,e){return br(e)||"false"===e?"false":"contenteditable"===n&&mr(e)?e:"true"}(e,t)):hr(e)?br(t)?n.removeAttributeNS(fr,vr(e)):n.setAttributeNS(fr,e,t):Kr(n,e,t)}function Kr(n,e,t){if(br(t))n.removeAttribute(e);else{if(Y&&!Q&&"TEXTAREA"===n.tagName&&"placeholder"===e&&""!==t&&!n.__ieph){var r=function(e){e.stopImmediatePropagation(),n.removeEventListener("input",r)};n.addEventListener("input",r),n.__ieph=!0}n.setAttribute(e,t)}}var Wr={create:qr,update:qr};function Gr(n,e){var t=e.elm,r=e.data,a=n.data;if(!(o(r.staticClass)&&o(r.class)&&(o(a)||o(a.staticClass)&&o(a.class)))){var s=kr(e),l=t._transitionClasses;i(l)&&(s=Sr(s,xr(l))),s!==t._prevClass&&(t.setAttribute("class",s),t._prevClass=s)}}var Jr,Yr={create:Gr,update:Gr};function Qr(n,e,t){var r=Jr;return function a(){var o=e.apply(null,arguments);null!==o&&na(n,a,t,r)}}var Xr=Le&&!(en&&Number(en[1])<=53);function Zr(n,e,t,r){if(Xr){var a=vt,o=e;e=o._wrapper=function(n){if(n.target===n.currentTarget||n.timeStamp>=a||n.timeStamp<=0||n.target.ownerDocument!==document)return o.apply(this,arguments)}}Jr.addEventListener(n,e,rn?{capture:t,passive:r}:t)}function na(n,e,t,r){(r||Jr).removeEventListener(n,e._wrapper||e,t)}function ea(n,e){if(!o(n.data.on)||!o(e.data.on)){var t=e.data.on||{},r=n.data.on||{};Jr=e.elm||n.elm,function(n){if(i(n.__r)){var e=Y?"change":"input";n[e]=[].concat(n.__r,n[e]||[]),delete n.__r}i(n.__c)&&(n.change=[].concat(n.__c,n.change||[]),delete n.__c)}(t),Zn(t,r,Zr,na,Qr,e.context),Jr=void 0}}var ta,ra={create:ea,update:ea,destroy:function(n){return ea(n,Pr)}};function aa(n,e){if(!o(n.data.domProps)||!o(e.data.domProps)){var t,r,a=e.elm,l=n.data.domProps||{},c=e.data.domProps||{};for(t in(i(c.__ob__)||s(c._v_attr_proxy))&&(c=e.data.domProps=_({},c)),l)t in c||(a[t]="");for(t in c){if(r=c[t],"textContent"===t||"innerHTML"===t){if(e.children&&(e.children.length=0),r===l[t])continue;1===a.childNodes.length&&a.removeChild(a.childNodes[0])}if("value"===t&&"PROGRESS"!==a.tagName){a._value=r;var p=o(r)?"":String(r);oa(a,p)&&(a.value=p)}else if("innerHTML"===t&&Dr(a.tagName)&&o(a.innerHTML)){(ta=ta||document.createElement("div")).innerHTML="<svg>".concat(r,"</svg>");for(var u=ta.firstChild;a.firstChild;)a.removeChild(a.firstChild);for(;u.firstChild;)a.appendChild(u.firstChild)}else if(r!==l[t])try{a[t]=r}catch(n){}}}}function oa(n,e){return!n.composing&&("OPTION"===n.tagName||function(n,e){var t=!0;try{t=document.activeElement!==n}catch(n){}return t&&n.value!==e}(n,e)||function(n,e){var t=n.value,r=n._vModifiers;if(i(r)){if(r.number)return v(t)!==v(e);if(r.trim)return t.trim()!==e.trim()}return t!==e}(n,e))}var ia={create:aa,update:aa},sa=w((function(n){var e={},t=/:(.+)/;return n.split(/;(?![^(]*\))/g).forEach((function(n){if(n){var r=n.split(t);r.length>1&&(e[r[0].trim()]=r[1].trim())}})),e}));function la(n){var e=ca(n.style);return n.staticStyle?_(n.staticStyle,e):e}function ca(n){return Array.isArray(n)?R(n):"string"==typeof n?sa(n):n}var pa,ua=/^--/,da=/\s*!important$/,ma=function(n,e,t){if(ua.test(e))n.style.setProperty(e,t);else if(da.test(t))n.style.setProperty(T(e),t.replace(da,""),"important");else{var r=fa(e);if(Array.isArray(t))for(var a=0,o=t.length;a<o;a++)n.style[r]=t[a];else n.style[r]=t}},ga=["Webkit","Moz","ms"],fa=w((function(n){if(pa=pa||document.createElement("div").style,"filter"!==(n=D(n))&&n in pa)return n;for(var e=n.charAt(0).toUpperCase()+n.slice(1),t=0;t<ga.length;t++){var r=ga[t]+e;if(r in pa)return r}}));function ha(n,e){var t=e.data,r=n.data;if(!(o(t.staticStyle)&&o(t.style)&&o(r.staticStyle)&&o(r.style))){var a,s,l=e.elm,c=r.staticStyle,p=r.normalizedStyle||r.style||{},u=c||p,d=ca(e.data.style)||{};e.data.normalizedStyle=i(d.__ob__)?_({},d):d;var m=function(n,e){var t,r={};if(e)for(var a=n;a.componentInstance;)(a=a.componentInstance._vnode)&&a.data&&(t=la(a.data))&&_(r,t);(t=la(n.data))&&_(r,t);for(var o=n;o=o.parent;)o.data&&(t=la(o.data))&&_(r,t);return r}(e,!0);for(s in u)o(m[s])&&ma(l,s,"");for(s in m)(a=m[s])!==u[s]&&ma(l,s,null==a?"":a)}}var va={create:ha,update:ha},ba=/\s+/;function ka(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(ba).forEach((function(e){return n.classList.add(e)})):n.classList.add(e);else{var t=" ".concat(n.getAttribute("class")||""," ");t.indexOf(" "+e+" ")<0&&n.setAttribute("class",(t+e).trim())}}function ya(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(ba).forEach((function(e){return n.classList.remove(e)})):n.classList.remove(e),n.classList.length||n.removeAttribute("class");else{for(var t=" ".concat(n.getAttribute("class")||""," "),r=" "+e+" ";t.indexOf(r)>=0;)t=t.replace(r," ");(t=t.trim())?n.setAttribute("class",t):n.removeAttribute("class")}}function Sa(n){if(n){if("object"==typeof n){var e={};return!1!==n.css&&_(e,xa(n.name||"v")),_(e,n),e}return"string"==typeof n?xa(n):void 0}}var xa=w((function(n){return{enterClass:"".concat(n,"-enter"),enterToClass:"".concat(n,"-enter-to"),enterActiveClass:"".concat(n,"-enter-active"),leaveClass:"".concat(n,"-leave"),leaveToClass:"".concat(n,"-leave-to"),leaveActiveClass:"".concat(n,"-leave-active")}})),wa=G&&!Q,Ea="transition",Da="transitionend",Ca="animation",Ia="animationend";wa&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Ea="WebkitTransition",Da="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(Ca="WebkitAnimation",Ia="webkitAnimationEnd"));var Ta=G?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(n){return n()};function Oa(n){Ta((function(){Ta(n)}))}function Aa(n,e){var t=n._transitionClasses||(n._transitionClasses=[]);t.indexOf(e)<0&&(t.push(e),ka(n,e))}function _a(n,e){n._transitionClasses&&y(n._transitionClasses,e),ya(n,e)}function Ra(n,e,t){var r=Fa(n,e),a=r.type,o=r.timeout,i=r.propCount;if(!a)return t();var s="transition"===a?Da:Ia,l=0,c=function(){n.removeEventListener(s,p),t()},p=function(e){e.target===n&&++l>=i&&c()};setTimeout((function(){l<i&&c()}),o+1),n.addEventListener(s,p)}var Pa=/\b(transform|all)(,|$)/;function Fa(n,e){var t,r=window.getComputedStyle(n),a=(r[Ea+"Delay"]||"").split(", "),o=(r[Ea+"Duration"]||"").split(", "),i=Ba(a,o),s=(r[Ca+"Delay"]||"").split(", "),l=(r[Ca+"Duration"]||"").split(", "),c=Ba(s,l),p=0,u=0;return"transition"===e?i>0&&(t="transition",p=i,u=o.length):"animation"===e?c>0&&(t="animation",p=c,u=l.length):u=(t=(p=Math.max(i,c))>0?i>c?"transition":"animation":null)?"transition"===t?o.length:l.length:0,{type:t,timeout:p,propCount:u,hasTransform:"transition"===t&&Pa.test(r[Ea+"Property"])}}function Ba(n,e){for(;n.length<e.length;)n=n.concat(n);return Math.max.apply(null,e.map((function(e,t){return Ma(e)+Ma(n[t])})))}function Ma(n){return 1e3*Number(n.slice(0,-1).replace(",","."))}function ja(n,e){var t=n.elm;i(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var r=Sa(n.data.transition);if(!o(r)&&!i(t._enterCb)&&1===t.nodeType){for(var a=r.css,s=r.type,l=r.enterClass,u=r.enterToClass,d=r.enterActiveClass,m=r.appearClass,g=r.appearToClass,f=r.appearActiveClass,h=r.beforeEnter,b=r.enter,k=r.afterEnter,y=r.enterCancelled,S=r.beforeAppear,x=r.appear,w=r.afterAppear,E=r.appearCancelled,D=r.duration,C=it,I=it.$vnode;I&&I.parent;)C=I.context,I=I.parent;var T=!C._isMounted||!n.isRootInsert;if(!T||x||""===x){var O=T&&m?m:l,A=T&&f?f:d,_=T&&g?g:u,R=T&&S||h,P=T&&c(x)?x:b,F=T&&w||k,B=T&&E||y,M=v(p(D)?D.enter:D);0;var j=!1!==a&&!Q,N=$a(P),$=t._enterCb=L((function(){j&&(_a(t,_),_a(t,A)),$.cancelled?(j&&_a(t,O),B&&B(t)):F&&F(t),t._enterCb=null}));n.data.show||ne(n,"insert",(function(){var e=t.parentNode,r=e&&e._pending&&e._pending[n.key];r&&r.tag===n.tag&&r.elm._leaveCb&&r.elm._leaveCb(),P&&P(t,$)})),R&&R(t),j&&(Aa(t,O),Aa(t,A),Oa((function(){_a(t,O),$.cancelled||(Aa(t,_),N||(Na(M)?setTimeout($,M):Ra(t,s,$)))}))),n.data.show&&(e&&e(),P&&P(t,$)),j||N||$()}}}function La(n,e){var t=n.elm;i(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var r=Sa(n.data.transition);if(o(r)||1!==t.nodeType)return e();if(!i(t._leaveCb)){var a=r.css,s=r.type,l=r.leaveClass,c=r.leaveToClass,u=r.leaveActiveClass,d=r.beforeLeave,m=r.leave,g=r.afterLeave,f=r.leaveCancelled,h=r.delayLeave,b=r.duration,k=!1!==a&&!Q,y=$a(m),S=v(p(b)?b.leave:b);0;var x=t._leaveCb=L((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[n.key]=null),k&&(_a(t,c),_a(t,u)),x.cancelled?(k&&_a(t,l),f&&f(t)):(e(),g&&g(t)),t._leaveCb=null}));h?h(w):w()}function w(){x.cancelled||(!n.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[n.key]=n),d&&d(t),k&&(Aa(t,l),Aa(t,u),Oa((function(){_a(t,l),x.cancelled||(Aa(t,c),y||(Na(S)?setTimeout(x,S):Ra(t,s,x)))}))),m&&m(t,x),k||y||x())}}function Na(n){return"number"==typeof n&&!isNaN(n)}function $a(n){if(o(n))return!1;var e=n.fns;return i(e)?$a(Array.isArray(e)?e[0]:e):(n._length||n.length)>1}function Ua(n,e){!0!==e.data.show&&ja(e)}var za=function(n){var e,t,r={},c=n.modules,p=n.nodeOps;for(e=0;e<Fr.length;++e)for(r[Fr[e]]=[],t=0;t<c.length;++t)i(c[t][Fr[e]])&&r[Fr[e]].push(c[t][Fr[e]]);function u(n){var e=p.parentNode(n);i(e)&&p.removeChild(e,n)}function d(n,e,t,a,o,l,c){if(i(n.elm)&&i(l)&&(n=l[c]=vn(n)),n.isRootInsert=!o,!function(n,e,t,a){var o=n.data;if(i(o)){var l=i(n.componentInstance)&&o.keepAlive;if(i(o=o.hook)&&i(o=o.init)&&o(n,!1),i(n.componentInstance))return m(n,e),g(t,n.elm,a),s(l)&&function(n,e,t,a){var o,s=n;for(;s.componentInstance;)if(s=s.componentInstance._vnode,i(o=s.data)&&i(o=o.transition)){for(o=0;o<r.activate.length;++o)r.activate[o](Pr,s);e.push(s);break}g(t,n.elm,a)}(n,e,t,a),!0}}(n,e,t,a)){var u=n.data,d=n.children,h=n.tag;i(h)?(n.elm=n.ns?p.createElementNS(n.ns,h):p.createElement(h,n),k(n),f(n,d,e),i(u)&&v(n,e),g(t,n.elm,a)):s(n.isComment)?(n.elm=p.createComment(n.text),g(t,n.elm,a)):(n.elm=p.createTextNode(n.text),g(t,n.elm,a))}}function m(n,e){i(n.data.pendingInsert)&&(e.push.apply(e,n.data.pendingInsert),n.data.pendingInsert=null),n.elm=n.componentInstance.$el,h(n)?(v(n,e),k(n)):(_r(n),e.push(n))}function g(n,e,t){i(n)&&(i(t)?p.parentNode(t)===n&&p.insertBefore(n,e,t):p.appendChild(n,e))}function f(n,e,t){if(a(e)){0;for(var r=0;r<e.length;++r)d(e[r],t,n.elm,null,!0,e,r)}else l(n.text)&&p.appendChild(n.elm,p.createTextNode(String(n.text)))}function h(n){for(;n.componentInstance;)n=n.componentInstance._vnode;return i(n.tag)}function v(n,t){for(var a=0;a<r.create.length;++a)r.create[a](Pr,n);i(e=n.data.hook)&&(i(e.create)&&e.create(Pr,n),i(e.insert)&&t.push(n))}function k(n){var e;if(i(e=n.fnScopeId))p.setStyleScope(n.elm,e);else for(var t=n;t;)i(e=t.context)&&i(e=e.$options._scopeId)&&p.setStyleScope(n.elm,e),t=t.parent;i(e=it)&&e!==n.context&&e!==n.fnContext&&i(e=e.$options._scopeId)&&p.setStyleScope(n.elm,e)}function y(n,e,t,r,a,o){for(;r<=a;++r)d(t[r],o,n,e,!1,t,r)}function S(n){var e,t,a=n.data;if(i(a))for(i(e=a.hook)&&i(e=e.destroy)&&e(n),e=0;e<r.destroy.length;++e)r.destroy[e](n);if(i(e=n.children))for(t=0;t<n.children.length;++t)S(n.children[t])}function x(n,e,t){for(;e<=t;++e){var r=n[e];i(r)&&(i(r.tag)?(w(r),S(r)):u(r.elm))}}function w(n,e){if(i(e)||i(n.data)){var t,a=r.remove.length+1;for(i(e)?e.listeners+=a:e=function(n,e){function t(){0==--t.listeners&&u(n)}return t.listeners=e,t}(n.elm,a),i(t=n.componentInstance)&&i(t=t._vnode)&&i(t.data)&&w(t,e),t=0;t<r.remove.length;++t)r.remove[t](n,e);i(t=n.data.hook)&&i(t=t.remove)?t(n,e):e()}else u(n.elm)}function E(n,e,t,r){for(var a=t;a<r;a++){var o=e[a];if(i(o)&&Br(n,o))return a}}function D(n,e,t,a,l,c){if(n!==e){i(e.elm)&&i(a)&&(e=a[l]=vn(e));var u=e.elm=n.elm;if(s(n.isAsyncPlaceholder))i(e.asyncFactory.resolved)?T(n.elm,e,t):e.isAsyncPlaceholder=!0;else if(s(e.isStatic)&&s(n.isStatic)&&e.key===n.key&&(s(e.isCloned)||s(e.isOnce)))e.componentInstance=n.componentInstance;else{var m,g=e.data;i(g)&&i(m=g.hook)&&i(m=m.prepatch)&&m(n,e);var f=n.children,v=e.children;if(i(g)&&h(e)){for(m=0;m<r.update.length;++m)r.update[m](n,e);i(m=g.hook)&&i(m=m.update)&&m(n,e)}o(e.text)?i(f)&&i(v)?f!==v&&function(n,e,t,r,a){var s,l,c,u=0,m=0,g=e.length-1,f=e[0],h=e[g],v=t.length-1,b=t[0],k=t[v],S=!a;for(0;u<=g&&m<=v;)o(f)?f=e[++u]:o(h)?h=e[--g]:Br(f,b)?(D(f,b,r,t,m),f=e[++u],b=t[++m]):Br(h,k)?(D(h,k,r,t,v),h=e[--g],k=t[--v]):Br(f,k)?(D(f,k,r,t,v),S&&p.insertBefore(n,f.elm,p.nextSibling(h.elm)),f=e[++u],k=t[--v]):Br(h,b)?(D(h,b,r,t,m),S&&p.insertBefore(n,h.elm,f.elm),h=e[--g],b=t[++m]):(o(s)&&(s=Mr(e,u,g)),o(l=i(b.key)?s[b.key]:E(b,e,u,g))?d(b,r,n,f.elm,!1,t,m):Br(c=e[l],b)?(D(c,b,r,t,m),e[l]=void 0,S&&p.insertBefore(n,c.elm,f.elm)):d(b,r,n,f.elm,!1,t,m),b=t[++m]);u>g?y(n,o(t[v+1])?null:t[v+1].elm,t,m,v,r):m>v&&x(e,u,g)}(u,f,v,t,c):i(v)?(i(n.text)&&p.setTextContent(u,""),y(u,null,v,0,v.length-1,t)):i(f)?x(f,0,f.length-1):i(n.text)&&p.setTextContent(u,""):n.text!==e.text&&p.setTextContent(u,e.text),i(g)&&i(m=g.hook)&&i(m=m.postpatch)&&m(n,e)}}}function C(n,e,t){if(s(t)&&i(n.parent))n.parent.data.pendingInsert=e;else for(var r=0;r<e.length;++r)e[r].data.hook.insert(e[r])}var I=b("attrs,class,staticClass,staticStyle,key");function T(n,e,t,r){var a,o=e.tag,l=e.data,c=e.children;if(r=r||l&&l.pre,e.elm=n,s(e.isComment)&&i(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(i(l)&&(i(a=l.hook)&&i(a=a.init)&&a(e,!0),i(a=e.componentInstance)))return m(e,t),!0;if(i(o)){if(i(c))if(n.hasChildNodes())if(i(a=l)&&i(a=a.domProps)&&i(a=a.innerHTML)){if(a!==n.innerHTML)return!1}else{for(var p=!0,u=n.firstChild,d=0;d<c.length;d++){if(!u||!T(u,c[d],t,r)){p=!1;break}u=u.nextSibling}if(!p||u)return!1}else f(e,c,t);if(i(l)){var g=!1;for(var h in l)if(!I(h)){g=!0,v(e,t);break}!g&&l.class&&Xe(l.class)}}else n.data!==e.text&&(n.data=e.text);return!0}return function(n,e,t,a){if(!o(e)){var l,c=!1,u=[];if(o(n))c=!0,d(e,u);else{var m=i(n.nodeType);if(!m&&Br(n,e))D(n,e,u,null,null,a);else{if(m){if(1===n.nodeType&&n.hasAttribute("data-server-rendered")&&(n.removeAttribute("data-server-rendered"),t=!0),s(t)&&T(n,e,u))return C(e,u,!0),n;l=n,n=new gn(p.tagName(l).toLowerCase(),{},[],void 0,l)}var g=n.elm,f=p.parentNode(g);if(d(e,u,g._leaveCb?null:f,p.nextSibling(g)),i(e.parent))for(var v=e.parent,b=h(e);v;){for(var k=0;k<r.destroy.length;++k)r.destroy[k](v);if(v.elm=e.elm,b){for(var y=0;y<r.create.length;++y)r.create[y](Pr,v);var w=v.data.hook.insert;if(w.merged)for(var E=1;E<w.fns.length;E++)w.fns[E]()}else _r(v);v=v.parent}i(f)?x([n],0,0):i(n.tag)&&S(n)}}return C(e,u,c),e.elm}i(n)&&S(n)}}({nodeOps:Or,modules:[Wr,Yr,ra,ia,va,G?{create:Ua,activate:Ua,remove:function(n,e){!0!==n.data.show?La(n,e):e()}}:{}].concat(Hr)});Q&&document.addEventListener("selectionchange",(function(){var n=document.activeElement;n&&n.vmodel&&Ya(n,"input")}));var Ha={inserted:function(n,e,t,r){"select"===t.tag?(r.elm&&!r.elm._vOptions?ne(t,"postpatch",(function(){Ha.componentUpdated(n,e,t)})):qa(n,e,t.context),n._vOptions=[].map.call(n.options,Wa)):("textarea"===t.tag||Tr(n.type))&&(n._vModifiers=e.modifiers,e.modifiers.lazy||(n.addEventListener("compositionstart",Ga),n.addEventListener("compositionend",Ja),n.addEventListener("change",Ja),Q&&(n.vmodel=!0)))},componentUpdated:function(n,e,t){if("select"===t.tag){qa(n,e,t.context);var r=n._vOptions,a=n._vOptions=[].map.call(n.options,Wa);if(a.some((function(n,e){return!M(n,r[e])})))(n.multiple?e.value.some((function(n){return Ka(n,a)})):e.value!==e.oldValue&&Ka(e.value,a))&&Ya(n,"change")}}};function qa(n,e,t){Va(n,e,t),(Y||X)&&setTimeout((function(){Va(n,e,t)}),0)}function Va(n,e,t){var r=e.value,a=n.multiple;if(!a||Array.isArray(r)){for(var o,i,s=0,l=n.options.length;s<l;s++)if(i=n.options[s],a)o=j(r,Wa(i))>-1,i.selected!==o&&(i.selected=o);else if(M(Wa(i),r))return void(n.selectedIndex!==s&&(n.selectedIndex=s));a||(n.selectedIndex=-1)}}function Ka(n,e){return e.every((function(e){return!M(e,n)}))}function Wa(n){return"_value"in n?n._value:n.value}function Ga(n){n.target.composing=!0}function Ja(n){n.target.composing&&(n.target.composing=!1,Ya(n.target,"input"))}function Ya(n,e){var t=document.createEvent("HTMLEvents");t.initEvent(e,!0,!0),n.dispatchEvent(t)}function Qa(n){return!n.componentInstance||n.data&&n.data.transition?n:Qa(n.componentInstance._vnode)}var Xa={model:Ha,show:{bind:function(n,e,t){var r=e.value,a=(t=Qa(t)).data&&t.data.transition,o=n.__vOriginalDisplay="none"===n.style.display?"":n.style.display;r&&a?(t.data.show=!0,ja(t,(function(){n.style.display=o}))):n.style.display=r?o:"none"},update:function(n,e,t){var r=e.value;!r!=!e.oldValue&&((t=Qa(t)).data&&t.data.transition?(t.data.show=!0,r?ja(t,(function(){n.style.display=n.__vOriginalDisplay})):La(t,(function(){n.style.display="none"}))):n.style.display=r?n.__vOriginalDisplay:"none")},unbind:function(n,e,t,r,a){a||(n.style.display=n.__vOriginalDisplay)}}},Za={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function no(n){var e=n&&n.componentOptions;return e&&e.Ctor.options.abstract?no(_e(e.children)):n}function eo(n){var e={},t=n.$options;for(var r in t.propsData)e[r]=n[r];var a=t._parentListeners;for(var r in a)e[D(r)]=a[r];return e}function to(n,e){if(/\d-keep-alive$/.test(e.tag))return n("keep-alive",{props:e.componentOptions.propsData})}var ro=function(n){return n.tag||Se(n)},ao=function(n){return"show"===n.name},oo={name:"transition",props:Za,abstract:!0,render:function(n){var e=this,t=this.$slots.default;if(t&&(t=t.filter(ro)).length){0;var r=this.mode;0;var a=t[0];if(function(n){for(;n=n.parent;)if(n.data.transition)return!0}(this.$vnode))return a;var o=no(a);if(!o)return a;if(this._leaving)return to(n,a);var i="__transition-".concat(this._uid,"-");o.key=null==o.key?o.isComment?i+"comment":i+o.tag:l(o.key)?0===String(o.key).indexOf(i)?o.key:i+o.key:o.key;var s=(o.data||(o.data={})).transition=eo(this),c=this._vnode,p=no(c);if(o.data.directives&&o.data.directives.some(ao)&&(o.data.show=!0),p&&p.data&&!function(n,e){return e.key===n.key&&e.tag===n.tag}(o,p)&&!Se(p)&&(!p.componentInstance||!p.componentInstance._vnode.isComment)){var u=p.data.transition=_({},s);if("out-in"===r)return this._leaving=!0,ne(u,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),to(n,a);if("in-out"===r){if(Se(o))return c;var d,m=function(){d()};ne(s,"afterEnter",m),ne(s,"enterCancelled",m),ne(u,"delayLeave",(function(n){d=n}))}}return a}}},io=_({tag:String,moveClass:String},Za);function so(n){n.elm._moveCb&&n.elm._moveCb(),n.elm._enterCb&&n.elm._enterCb()}function lo(n){n.data.newPos=n.elm.getBoundingClientRect()}function co(n){var e=n.data.pos,t=n.data.newPos,r=e.left-t.left,a=e.top-t.top;if(r||a){n.data.moved=!0;var o=n.elm.style;o.transform=o.WebkitTransform="translate(".concat(r,"px,").concat(a,"px)"),o.transitionDuration="0s"}}delete io.mode;var po={Transition:oo,TransitionGroup:{props:io,beforeMount:function(){var n=this,e=this._update;this._update=function(t,r){var a=st(n);n.__patch__(n._vnode,n.kept,!1,!0),n._vnode=n.kept,a(),e.call(n,t,r)}},render:function(n){for(var e=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),r=this.prevChildren=this.children,a=this.$slots.default||[],o=this.children=[],i=eo(this),s=0;s<a.length;s++){if((p=a[s]).tag)if(null!=p.key&&0!==String(p.key).indexOf("__vlist"))o.push(p),t[p.key]=p,(p.data||(p.data={})).transition=i;else;}if(r){var l=[],c=[];for(s=0;s<r.length;s++){var p;(p=r[s]).data.transition=i,p.data.pos=p.elm.getBoundingClientRect(),t[p.key]?l.push(p):c.push(p)}this.kept=n(e,null,l),this.removed=c}return n(e,null,o)},updated:function(){var n=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";n.length&&this.hasMove(n[0].elm,e)&&(n.forEach(so),n.forEach(lo),n.forEach(co),this._reflow=document.body.offsetHeight,n.forEach((function(n){if(n.data.moved){var t=n.elm,r=t.style;Aa(t,e),r.transform=r.WebkitTransform=r.transitionDuration="",t.addEventListener(Da,t._moveCb=function n(r){r&&r.target!==t||r&&!/transform$/.test(r.propertyName)||(t.removeEventListener(Da,n),t._moveCb=null,_a(t,e))})}})))},methods:{hasMove:function(n,e){if(!wa)return!1;if(this._hasMove)return this._hasMove;var t=n.cloneNode();n._transitionClasses&&n._transitionClasses.forEach((function(n){ya(t,n)})),ka(t,e),t.style.display="none",this.$el.appendChild(t);var r=Fa(t);return this.$el.removeChild(t),this._hasMove=r.hasTransform}}}};tr.config.mustUseProp=function(n,e,t){return"value"===t&&ur(n)&&"button"!==e||"selected"===t&&"option"===n||"checked"===t&&"input"===n||"muted"===t&&"video"===n},tr.config.isReservedTag=Cr,tr.config.isReservedAttr=pr,tr.config.getTagNamespace=function(n){return Dr(n)?"svg":"math"===n?"math":void 0},tr.config.isUnknownElement=function(n){if(!G)return!0;if(Cr(n))return!1;if(n=n.toLowerCase(),null!=Ir[n])return Ir[n];var e=document.createElement(n);return n.indexOf("-")>-1?Ir[n]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:Ir[n]=/HTMLUnknownElement/.test(e.toString())},_(tr.options.directives,Xa),_(tr.options.components,po),tr.prototype.__patch__=G?za:P,tr.prototype.$mount=function(n,e){return function(n,e,t){var r;n.$el=e,n.$options.render||(n.$options.render=fn),pt(n,"beforeMount"),r=function(){n._update(n._render(),t)},new et(n,r,P,{before:function(){n._isMounted&&!n._isDestroyed&&pt(n,"beforeUpdate")}},!0),t=!1;var a=n._preWatchers;if(a)for(var o=0;o<a.length;o++)a[o].run();return null==n.$vnode&&(n._isMounted=!0,pt(n,"mounted")),n}(this,n=n&&G?function(n){if("string"==typeof n){var e=document.querySelector(n);return e||document.createElement("div")}return n}(n):void 0,e)},G&&setTimeout((function(){z.devtools&&sn&&sn.emit("init",tr)}),0)},function(n,e,t){"use strict";t.d(e,"a",(function(){return u}));var r=t(1);
/**
  * vue-class-component v7.2.6
  * (c) 2015-present Evan You
  * @license MIT
  */function a(n){return(a="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n})(n)}function o(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}function i(n){return function(n){if(Array.isArray(n)){for(var e=0,t=new Array(n.length);e<n.length;e++)t[e]=n[e];return t}}(n)||function(n){if(Symbol.iterator in Object(n)||"[object Arguments]"===Object.prototype.toString.call(n))return Array.from(n)}(n)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance")}()}function s(){return"undefined"!=typeof Reflect&&Reflect.defineMetadata&&Reflect.getOwnMetadataKeys}function l(n,e){c(n,e),Object.getOwnPropertyNames(e.prototype).forEach((function(t){c(n.prototype,e.prototype,t)})),Object.getOwnPropertyNames(e).forEach((function(t){c(n,e,t)}))}function c(n,e,t){(t?Reflect.getOwnMetadataKeys(e,t):Reflect.getOwnMetadataKeys(e)).forEach((function(r){var a=t?Reflect.getOwnMetadata(r,e,t):Reflect.getOwnMetadata(r,e);t?Reflect.defineMetadata(r,a,n,t):Reflect.defineMetadata(r,a,n)}))}var p={__proto__:[]}instanceof Array;function u(n){return function(e,t,r){var a="function"==typeof e?e:e.constructor;a.__decorators__||(a.__decorators__=[]),"number"!=typeof r&&(r=void 0),a.__decorators__.push((function(e){return n(e,t,r)}))}}function d(n,e){var t=e.prototype._init;e.prototype._init=function(){var e=this,t=Object.getOwnPropertyNames(n);if(n.$options.props)for(var r in n.$options.props)n.hasOwnProperty(r)||t.push(r);t.forEach((function(t){Object.defineProperty(e,t,{get:function(){return n[t]},set:function(e){n[t]=e},configurable:!0})}))};var r=new e;e.prototype._init=t;var a={};return Object.keys(r).forEach((function(n){void 0!==r[n]&&(a[n]=r[n])})),a}var m=["data","beforeCreate","created","beforeMount","mounted","beforeDestroy","destroyed","beforeUpdate","updated","activated","deactivated","render","errorCaptured","serverPrefetch"];function g(n){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};e.name=e.name||n._componentTag||n.name;var t=n.prototype;Object.getOwnPropertyNames(t).forEach((function(n){if("constructor"!==n)if(m.indexOf(n)>-1)e[n]=t[n];else{var r=Object.getOwnPropertyDescriptor(t,n);void 0!==r.value?"function"==typeof r.value?(e.methods||(e.methods={}))[n]=r.value:(e.mixins||(e.mixins=[])).push({data:function(){return o({},n,r.value)}}):(r.get||r.set)&&((e.computed||(e.computed={}))[n]={get:r.get,set:r.set})}})),(e.mixins||(e.mixins=[])).push({data:function(){return d(this,n)}});var a=n.__decorators__;a&&(a.forEach((function(n){return n(e)})),delete n.__decorators__);var i=Object.getPrototypeOf(n.prototype),c=i instanceof r.b?i.constructor:r.b,p=c.extend(e);return h(p,n,c),s()&&l(p,n),p}var f={prototype:!0,arguments:!0,callee:!0,caller:!0};function h(n,e,t){Object.getOwnPropertyNames(e).forEach((function(r){if(!f[r]){var o=Object.getOwnPropertyDescriptor(n,r);if(!o||o.configurable){var i,s,l=Object.getOwnPropertyDescriptor(e,r);if(!p){if("cid"===r)return;var c=Object.getOwnPropertyDescriptor(t,r);if(i=l.value,s=a(i),null!=i&&("object"===s||"function"===s)&&c&&c.value===l.value)return}0,Object.defineProperty(n,r,l)}}}))}function v(n){return"function"==typeof n?g(n):function(e){return g(e,n)}}v.registerHooks=function(n){m.push.apply(m,i(n))},e.b=v},function(n,e,t){"use strict";function r(n,e,t,r,a,o,i,s){var l,c="function"==typeof n?n.options:n;if(e&&(c.render=e,c.staticRenderFns=t,c._compiled=!0),r&&(c.functional=!0),o&&(c._scopeId="data-v-"+o),i?(l=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),a&&a.call(this,n),n&&n._registeredComponents&&n._registeredComponents.add(i)},c._ssrRegister=l):a&&(l=s?function(){a.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:a),l)if(c.functional){c._injectStyles=l;var p=c.render;c.render=function(n,e){return l.call(e),p(n,e)}}else{var u=c.beforeCreate;c.beforeCreate=u?[].concat(u,l):[l]}return{exports:n,options:c}}t.d(e,"a",(function(){return r}))},function(n,e,t){var r=t(72),a=r.all;n.exports=r.IS_HTMLDDA?function(n){return"function"==typeof n||n===a}:function(n){return"function"==typeof n}},function(n,e,t){"use strict";var r=t(117),a=Object.prototype.toString;function o(n){return"[object Array]"===a.call(n)}function i(n){return void 0===n}function s(n){return null!==n&&"object"==typeof n}function l(n){if("[object Object]"!==a.call(n))return!1;var e=Object.getPrototypeOf(n);return null===e||e===Object.prototype}function c(n){return"[object Function]"===a.call(n)}function p(n,e){if(null!=n)if("object"!=typeof n&&(n=[n]),o(n))for(var t=0,r=n.length;t<r;t++)e.call(null,n[t],t,n);else for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&e.call(null,n[a],a,n)}n.exports={isArray:o,isArrayBuffer:function(n){return"[object ArrayBuffer]"===a.call(n)},isBuffer:function(n){return null!==n&&!i(n)&&null!==n.constructor&&!i(n.constructor)&&"function"==typeof n.constructor.isBuffer&&n.constructor.isBuffer(n)},isFormData:function(n){return"undefined"!=typeof FormData&&n instanceof FormData},isArrayBufferView:function(n){return"undefined"!=typeof ArrayBuffer&&ArrayBuffer.isView?ArrayBuffer.isView(n):n&&n.buffer&&n.buffer instanceof ArrayBuffer},isString:function(n){return"string"==typeof n},isNumber:function(n){return"number"==typeof n},isObject:s,isPlainObject:l,isUndefined:i,isDate:function(n){return"[object Date]"===a.call(n)},isFile:function(n){return"[object File]"===a.call(n)},isBlob:function(n){return"[object Blob]"===a.call(n)},isFunction:c,isStream:function(n){return s(n)&&c(n.pipe)},isURLSearchParams:function(n){return"undefined"!=typeof URLSearchParams&&n instanceof URLSearchParams},isStandardBrowserEnv:function(){return("undefined"==typeof navigator||"ReactNative"!==navigator.product&&"NativeScript"!==navigator.product&&"NS"!==navigator.product)&&("undefined"!=typeof window&&"undefined"!=typeof document)},forEach:p,merge:function n(){var e={};function t(t,r){l(e[r])&&l(t)?e[r]=n(e[r],t):l(t)?e[r]=n({},t):o(t)?e[r]=t.slice():e[r]=t}for(var r=0,a=arguments.length;r<a;r++)p(arguments[r],t);return e},extend:function(n,e,t){return p(e,(function(e,a){n[a]=t&&"function"==typeof e?r(e,t):e})),n},trim:function(n){return n.trim?n.trim():n.replace(/^\s+|\s+$/g,"")},stripBOM:function(n){return 65279===n.charCodeAt(0)&&(n=n.slice(1)),n}}},function(n,e){n.exports=function(n){try{return!!n()}catch(n){return!0}}},function(n,e,t){var r=t(44),a=Function.prototype,o=a.call,i=r&&a.bind.bind(o,o);n.exports=r?i:function(n){return function(){return o.apply(n,arguments)}}},function(n,e){var t=function(n){return n&&n.Math==Math&&n};n.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||this||Function("return this")()},function(n,e,t){var r=t(6);n.exports=!r((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(n,e,t){n.exports=t(313)},function(n,e,t){var r=t(90),a="object"==typeof self&&self&&self.Object===Object&&self,o=r||a||Function("return this")();n.exports=o},function(n,e){var t=Array.isArray;n.exports=t},function(n,e,t){var r=t(7),a=t(33),o=r({}.hasOwnProperty);n.exports=Object.hasOwn||function(n,e){return o(a(n),e)}},function(n,e,t){var r=t(4),a=t(72),o=a.all;n.exports=a.IS_HTMLDDA?function(n){return"object"==typeof n?null!==n:r(n)||n===o}:function(n){return"object"==typeof n?null!==n:r(n)}},function(n,e,t){var r=t(195),a=t(198);n.exports=function(n,e){var t=a(n,e);return r(t)?t:void 0}},function(n,e,t){var r=t(301),a=t(115),o=/[T ]/,i=/:/,s=/^(\d{2})$/,l=[/^([+-]\d{2})$/,/^([+-]\d{3})$/,/^([+-]\d{4})$/],c=/^(\d{4})/,p=[/^([+-]\d{4})/,/^([+-]\d{5})/,/^([+-]\d{6})/],u=/^-(\d{2})$/,d=/^-?(\d{3})$/,m=/^-?(\d{2})-?(\d{2})$/,g=/^-?W(\d{2})$/,f=/^-?W(\d{2})-?(\d{1})$/,h=/^(\d{2}([.,]\d*)?)$/,v=/^(\d{2}):?(\d{2}([.,]\d*)?)$/,b=/^(\d{2}):?(\d{2}):?(\d{2}([.,]\d*)?)$/,k=/([Z+-].*)$/,y=/^(Z)$/,S=/^([+-])(\d{2})$/,x=/^([+-])(\d{2}):?(\d{2})$/;function w(n,e,t){e=e||0,t=t||0;var r=new Date(0);r.setUTCFullYear(n,0,4);var a=7*e+t+1-(r.getUTCDay()||7);return r.setUTCDate(r.getUTCDate()+a),r}n.exports=function(n,e){if(a(n))return new Date(n.getTime());if("string"!=typeof n)return new Date(n);var t=(e||{}).additionalDigits;t=null==t?2:Number(t);var E=function(n){var e,t={},r=n.split(o);i.test(r[0])?(t.date=null,e=r[0]):(t.date=r[0],e=r[1]);if(e){var a=k.exec(e);a?(t.time=e.replace(a[1],""),t.timezone=a[1]):t.time=e}return t}(n),D=function(n,e){var t,r=l[e],a=p[e];if(t=c.exec(n)||a.exec(n)){var o=t[1];return{year:parseInt(o,10),restDateString:n.slice(o.length)}}if(t=s.exec(n)||r.exec(n)){var i=t[1];return{year:100*parseInt(i,10),restDateString:n.slice(i.length)}}return{year:null}}(E.date,t),C=D.year,I=function(n,e){if(null===e)return null;var t,r,a,o;if(0===n.length)return(r=new Date(0)).setUTCFullYear(e),r;if(t=u.exec(n))return r=new Date(0),a=parseInt(t[1],10)-1,r.setUTCFullYear(e,a),r;if(t=d.exec(n)){r=new Date(0);var i=parseInt(t[1],10);return r.setUTCFullYear(e,0,i),r}if(t=m.exec(n)){r=new Date(0),a=parseInt(t[1],10)-1;var s=parseInt(t[2],10);return r.setUTCFullYear(e,a,s),r}if(t=g.exec(n))return o=parseInt(t[1],10)-1,w(e,o);if(t=f.exec(n)){o=parseInt(t[1],10)-1;var l=parseInt(t[2],10)-1;return w(e,o,l)}return null}(D.restDateString,C);if(I){var T,O=I.getTime(),A=0;if(E.time&&(A=function(n){var e,t,r;if(e=h.exec(n))return(t=parseFloat(e[1].replace(",",".")))%24*36e5;if(e=v.exec(n))return t=parseInt(e[1],10),r=parseFloat(e[2].replace(",",".")),t%24*36e5+6e4*r;if(e=b.exec(n)){t=parseInt(e[1],10),r=parseInt(e[2],10);var a=parseFloat(e[3].replace(",","."));return t%24*36e5+6e4*r+1e3*a}return null}(E.time)),E.timezone)T=6e4*function(n){var e,t;if(e=y.exec(n))return 0;if(e=S.exec(n))return t=60*parseInt(e[2],10),"+"===e[1]?-t:t;if(e=x.exec(n))return t=60*parseInt(e[2],10)+parseInt(e[3],10),"+"===e[1]?-t:t;return 0}(E.timezone);else{var _=O+A,R=new Date(_);T=r(R);var P=new Date(_);P.setDate(R.getDate()+1);var F=r(P)-r(R);F>0&&(T+=F)}return new Date(O+A+T)}return new Date(n)}},function(n,e,t){"use strict";var r=t(23),a=t(33),o=t(35),i=t(161),s=t(163);r({target:"Array",proto:!0,arity:1,forced:t(6)((function(){return 4294967297!==[].push.call({length:4294967296},1)}))||!function(){try{Object.defineProperty([],"length",{writable:!1}).push()}catch(n){return n instanceof TypeError}}()},{push:function(n){var e=a(this),t=o(e),r=arguments.length;s(t+r);for(var l=0;l<r;l++)e[t]=arguments[l],t++;return i(e,t),t}})},function(n,e,t){"use strict";t.d(e,"c",(function(){return o})),t.d(e,"i",(function(){return i})),t.d(e,"f",(function(){return l})),t.d(e,"g",(function(){return c})),t.d(e,"h",(function(){return p})),t.d(e,"d",(function(){return u})),t.d(e,"e",(function(){return d})),t.d(e,"k",(function(){return m})),t.d(e,"l",(function(){return g})),t.d(e,"j",(function(){return f})),t.d(e,"b",(function(){return v})),t.d(e,"a",(function(){return b}));t(17);const r=/#.*$/,a=/\.(md|html)$/,o=/\/$/,i=/^(https?:|mailto:|tel:)/;function s(n){return decodeURI(n).replace(r,"").replace(a,"")}function l(n){return i.test(n)}function c(n){return/^mailto:/.test(n)}function p(n){return/^tel:/.test(n)}function u(n){if(l(n))return n;const e=n.match(r),t=e?e[0]:"",a=s(n);return o.test(a)?n:a+".html"+t}function d(n,e){const t=n.hash,a=function(n){const e=n.match(r);if(e)return e[0]}(e);if(a&&t!==a)return!1;return s(n.path)===s(e)}function m(n,e,t){t&&(e=function(n,e,t){const r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;const a=e.split("/");t&&a[a.length-1]||a.pop();const o=n.replace(/^\//,"").split("/");for(let n=0;n<o.length;n++){const e=o[n];".."===e?a.pop():"."!==e&&a.push(e)}""!==a[0]&&a.unshift("");return a.join("/")}(e,t));const r=s(e);for(let e=0;e<n.length;e++)if(s(n[e].regularPath)===r)return Object.assign({},n[e],{type:"page",path:u(n[e].path)});return console.error(`[vuepress] No matching page found for sidebar item "${e}"`),{}}function g(n,e,t,r){const{pages:a,themeConfig:o}=t,i=(r&&o.locales&&o.locales[r]||o).sidebar||o.sidebar,{base:s,config:l}=function(n,e){if(Array.isArray(e))return{base:"/",config:e};for(const r in e)if(0===(t=n,/(\.html|\/)$/.test(t)?t:t+"/").indexOf(encodeURI(r)))return{base:r,config:e[r]};var t;return{}}(e,i);return l?l.map(n=>function n(e,t,r,a=1){if("string"==typeof e)return m(t,e,r);if(Array.isArray(e))return Object.assign(m(t,e[0],r),{title:e[1]});{a>3&&console.error("[vuepress] detected a too deep nested sidebar group.");const o=e.children||[];return 0===o.length&&e.path?Object.assign(m(t,e.path,r),{title:e.title}):{type:"group",path:e.path,title:e.title,sidebarDepth:e.sidebarDepth,children:o.map(e=>n(e,t,r,a+1)),collapsable:!1!==e.collapsable}}}(n,a,s)):[]}function f(n){return Object.assign(n,{type:n.items&&n.items.length?"links":"link"})}function h(n){return n?new Date(n).getTime():0}function v(n,e){const t=h(n.frontmatter.date),r=h(e.frontmatter.date);return 0===t||0===r?0:r-t}function b(n){const e=document.createElement("link");e.rel="stylesheet",e.href=n,document.head.append(e)}},function(n,e,t){var r=t(9),a=t(81),o=t(83),i=t(25),s=t(71),l=TypeError,c=Object.defineProperty,p=Object.getOwnPropertyDescriptor;e.f=r?o?function(n,e,t){if(i(n),e=s(e),i(t),"function"==typeof n&&"prototype"===e&&"value"in t&&"writable"in t&&!t.writable){var r=p(n,e);r&&r.writable&&(n[e]=t.value,t={configurable:"configurable"in t?t.configurable:r.configurable,enumerable:"enumerable"in t?t.enumerable:r.enumerable,writable:!1})}return c(n,e,t)}:c:function(n,e,t){if(i(n),e=s(e),i(t),a)try{return c(n,e,t)}catch(n){}if("get"in t||"set"in t)throw l("Accessors not supported");return"value"in t&&(n[e]=t.value),n}},function(n,e){n.exports=function(n){return null!=n&&"object"==typeof n}},function(n,e,t){"use strict";var r=t(288),a=t(299),o=t(63);n.exports={formats:o,parse:a,stringify:r}},function(n,e,t){"use strict";var r=t(1);e.a=new r.b},function(n,e,t){var r=t(8),a=t(68).f,o=t(24),i=t(144),s=t(49),l=t(85),c=t(157);n.exports=function(n,e){var t,p,u,d,m,g=n.target,f=n.global,h=n.stat;if(t=f?r:h?r[g]||s(g,{}):(r[g]||{}).prototype)for(p in e){if(d=e[p],u=n.dontCallGetSet?(m=a(t,p))&&m.value:t[p],!c(f?p:g+(h?".":"#")+p,n.forced)&&void 0!==u){if(typeof d==typeof u)continue;l(d,u)}(n.sham||u&&u.sham)&&o(d,"sham",!0),i(t,p,d,n)}}},function(n,e,t){var r=t(9),a=t(19),o=t(45);n.exports=r?function(n,e,t){return a.f(n,e,o(1,t))}:function(n,e,t){return n[e]=t,n}},function(n,e,t){var r=t(14),a=String,o=TypeError;n.exports=function(n){if(r(n))return n;throw o(a(n)+" is not an object")}},function(n,e,t){var r=t(11).Symbol;n.exports=r},function(n,e,t){var r=t(26),a=t(180),o=t(181),i=r?r.toStringTag:void 0;n.exports=function(n){return null==n?void 0===n?"[object Undefined]":"[object Null]":i&&i in Object(n)?a(n):o(n)}},function(n,e,t){"use strict";t.d(e,"a",(function(){return a})),t.d(e,"c",(function(){return o})),t.d(e,"b",(function(){return i}));var r=t(18);function a(n,e){return n=n.filter((t,r)=>{const{title:a,frontmatter:{home:o,date:i,publish:s}}=t;if(n.indexOf(t)!==r)return!1;{const n=!0===o||null==a||!1===s;return!0===e?!(n||void 0===i):!n}})}function o(n){n.sort((n,e)=>{const t=n.frontmatter.sticky,a=e.frontmatter.sticky;return t&&a?t==a?Object(r.b)(n,e):t-a:t&&!a?-1:!t&&a?1:Object(r.b)(n,e)})}function i(n){n.sort((n,e)=>Object(r.b)(n,e))}},function(n,e,t){var r=t(8),a=t(78),o=t(13),i=t(80),s=t(76),l=t(75),c=r.Symbol,p=a("wks"),u=l?c.for||c:c&&c.withoutSetter||i;n.exports=function(n){return o(p,n)||(p[n]=s&&o(c,n)?c[n]:u("Symbol."+n)),p[n]}},function(n,e,t){var r=t(69),a=t(46);n.exports=function(n){return r(a(n))}},function(n,e,t){var r=t(7),a=r({}.toString),o=r("".slice);n.exports=function(n){return o(a(n),8,-1)}},function(n,e,t){var r=t(8),a=t(4),o=function(n){return a(n)?n:void 0};n.exports=function(n,e){return arguments.length<2?o(r[n]):r[n]&&r[n][e]}},function(n,e,t){var r=t(46),a=Object;n.exports=function(n){return a(r(n))}},function(n,e,t){var r=t(154);n.exports=function(n){var e=+n;return e!=e||0===e?0:r(e)}},function(n,e,t){var r=t(155);n.exports=function(n){return r(n.length)}},function(n,e,t){var r=t(185),a=t(186),o=t(187),i=t(188),s=t(189);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=a,l.prototype.get=o,l.prototype.has=i,l.prototype.set=s,n.exports=l},function(n,e,t){var r=t(92);n.exports=function(n,e){for(var t=n.length;t--;)if(r(n[t][0],e))return t;return-1}},function(n,e,t){var r=t(15)(Object,"create");n.exports=r},function(n,e,t){var r=t(207);n.exports=function(n,e){var t=n.__data__;return r(e)?t["string"==typeof e?"string":"hash"]:t.map}},function(n,e,t){var r=t(59);n.exports=function(n){if("string"==typeof n||r(n))return n;var e=n+"";return"0"==e&&1/n==-1/0?"-0":e}},function(n,e,t){var r,a;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(a="function"==typeof(r=function(){var n,e,t={version:"0.2.0"},r=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function a(n,e,t){return n<e?e:n>t?t:n}function o(n){return 100*(-1+n)}t.configure=function(n){var e,t;for(e in n)void 0!==(t=n[e])&&n.hasOwnProperty(e)&&(r[e]=t);return this},t.status=null,t.set=function(n){var e=t.isStarted();n=a(n,r.minimum,1),t.status=1===n?null:n;var l=t.render(!e),c=l.querySelector(r.barSelector),p=r.speed,u=r.easing;return l.offsetWidth,i((function(e){""===r.positionUsing&&(r.positionUsing=t.getPositioningCSS()),s(c,function(n,e,t){var a;return(a="translate3d"===r.positionUsing?{transform:"translate3d("+o(n)+"%,0,0)"}:"translate"===r.positionUsing?{transform:"translate("+o(n)+"%,0)"}:{"margin-left":o(n)+"%"}).transition="all "+e+"ms "+t,a}(n,p,u)),1===n?(s(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){s(l,{transition:"all "+p+"ms linear",opacity:0}),setTimeout((function(){t.remove(),e()}),p)}),p)):setTimeout(e,p)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var n=function(){setTimeout((function(){t.status&&(t.trickle(),n())}),r.trickleSpeed)};return r.trickle&&n(),this},t.done=function(n){return n||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(n){var e=t.status;return e?("number"!=typeof n&&(n=(1-e)*a(Math.random()*e,.1,.95)),e=a(e+n,0,.994),t.set(e)):t.start()},t.trickle=function(){return t.inc(Math.random()*r.trickleRate)},n=0,e=0,t.promise=function(r){return r&&"resolved"!==r.state()?(0===e&&t.start(),n++,e++,r.always((function(){0==--e?(n=0,t.done()):t.set((n-e)/n)})),this):this},t.render=function(n){if(t.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var e=document.createElement("div");e.id="nprogress",e.innerHTML=r.template;var a,i=e.querySelector(r.barSelector),l=n?"-100":o(t.status||0),p=document.querySelector(r.parent);return s(i,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),r.showSpinner||(a=e.querySelector(r.spinnerSelector))&&d(a),p!=document.body&&c(p,"nprogress-custom-parent"),p.appendChild(e),e},t.remove=function(){p(document.documentElement,"nprogress-busy"),p(document.querySelector(r.parent),"nprogress-custom-parent");var n=document.getElementById("nprogress");n&&d(n)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var n=document.body.style,e="WebkitTransform"in n?"Webkit":"MozTransform"in n?"Moz":"msTransform"in n?"ms":"OTransform"in n?"O":"";return e+"Perspective"in n?"translate3d":e+"Transform"in n?"translate":"margin"};var i=function(){var n=[];function e(){var t=n.shift();t&&t(e)}return function(t){n.push(t),1==n.length&&e()}}(),s=function(){var n=["Webkit","O","Moz","ms"],e={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(n,e){return e.toUpperCase()})),e[t]||(e[t]=function(e){var t=document.body.style;if(e in t)return e;for(var r,a=n.length,o=e.charAt(0).toUpperCase()+e.slice(1);a--;)if((r=n[a]+o)in t)return r;return e}(t))}function r(n,e,r){e=t(e),n.style[e]=r}return function(n,e){var t,a,o=arguments;if(2==o.length)for(t in e)void 0!==(a=e[t])&&e.hasOwnProperty(t)&&r(n,t,a);else r(n,o[1],o[2])}}();function l(n,e){return("string"==typeof n?n:u(n)).indexOf(" "+e+" ")>=0}function c(n,e){var t=u(n),r=t+e;l(t,e)||(n.className=r.substring(1))}function p(n,e){var t,r=u(n);l(n,e)&&(t=r.replace(" "+e+" "," "),n.className=t.substring(1,t.length-1))}function u(n){return(" "+(n.className||"")+" ").replace(/\s+/gi," ")}function d(n){n&&n.parentNode&&n.parentNode.removeChild(n)}return t})?r.call(e,t,e,n):r)||(n.exports=a)},function(n,e,t){"use strict";t.d(e,"b",(function(){return r})),t.d(e,"c",(function(){return a})),t.d(e,"a",(function(){return o}));t(17),t(18);function r(){const n=["#e15b64","#f47e60","#f8b26a","#abbd81","#849b87","#e15b64","#f47e60","#f8b26a","#f26d6d","#67cc86","#fb9b5f","#3498db"];return n[Math.floor(Math.random()*n.length)]}function a(n){const e=n.__proto__.push;n.__proto__.push=function(n){return e.call(this,n).catch(n=>n)}}function o(n){const e=n.getRoutes();n.beforeEach((n,t,r)=>{const a=e.find(e=>e.regex.test(n.path));return/\.html$/.test(n.path)||a&&"*"!==a.path&&!a.redirect?r():decodeURIComponent(n.path)!==n.path?r(Object.assign({},n,{path:decodeURIComponent(n.path),fullPath:decodeURIComponent(n.fullPath)})):void r()})}},function(n,e,t){var r=t(44),a=Function.prototype.call;n.exports=r?a.bind(a):function(){return a.apply(a,arguments)}},function(n,e,t){var r=t(6);n.exports=!r((function(){var n=function(){}.bind();return"function"!=typeof n||n.hasOwnProperty("prototype")}))},function(n,e){n.exports=function(n,e){return{enumerable:!(1&n),configurable:!(2&n),writable:!(4&n),value:e}}},function(n,e,t){var r=t(70),a=TypeError;n.exports=function(n){if(r(n))throw a("Can't call method on "+n);return n}},function(n,e,t){var r=t(4),a=t(142),o=TypeError;n.exports=function(n){if(r(n))return n;throw o(a(n)+" is not a function")}},function(n,e,t){var r=t(8),a=t(49),o=r["__core-js_shared__"]||a("__core-js_shared__",{});n.exports=o},function(n,e,t){var r=t(8),a=Object.defineProperty;n.exports=function(n,e){try{a(r,n,{value:e,configurable:!0,writable:!0})}catch(t){r[n]=e}return e}},function(n,e){n.exports={}},function(n,e){n.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(n,e,t){var r=t(179),a=t(20),o=Object.prototype,i=o.hasOwnProperty,s=o.propertyIsEnumerable,l=r(function(){return arguments}())?r:function(n){return a(n)&&i.call(n,"callee")&&!s.call(n,"callee")};n.exports=l},function(n,e,t){var r=t(15)(t(11),"Map");n.exports=r},function(n,e){n.exports=function(n){var e=typeof n;return null!=n&&("object"==e||"function"==e)}},function(n,e,t){var r=t(199),a=t(206),o=t(208),i=t(209),s=t(210);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=a,l.prototype.get=o,l.prototype.has=i,l.prototype.set=s,n.exports=l},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n){t[++e]=n})),t}},function(n,e){n.exports=function(n){return"number"==typeof n&&n>-1&&n%1==0&&n<=9007199254740991}},function(n,e,t){var r=t(12),a=t(59),o=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,i=/^\w*$/;n.exports=function(n,e){if(r(n))return!1;var t=typeof n;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=n&&!a(n))||(i.test(n)||!o.test(n)||null!=e&&n in Object(e))}},function(n,e,t){var r=t(27),a=t(20);n.exports=function(n){return"symbol"==typeof n||a(n)&&"[object Symbol]"==r(n)}},function(n,e){n.exports=function(n){return n}},function(n,e,t){"use strict";var r=SyntaxError,a=Function,o=TypeError,i=function(n){try{return a('"use strict"; return ('+n+").constructor;")()}catch(n){}},s=Object.getOwnPropertyDescriptor;if(s)try{s({},"")}catch(n){s=null}var l=function(){throw new o},c=s?function(){try{return l}catch(n){try{return s(arguments,"callee").get}catch(n){return l}}}():l,p=t(290)(),u=t(292)(),d=Object.getPrototypeOf||(u?function(n){return n.__proto__}:null),m={},g="undefined"!=typeof Uint8Array&&d?d(Uint8Array):void 0,f={"%AggregateError%":"undefined"==typeof AggregateError?void 0:AggregateError,"%Array%":Array,"%ArrayBuffer%":"undefined"==typeof ArrayBuffer?void 0:ArrayBuffer,"%ArrayIteratorPrototype%":p&&d?d([][Symbol.iterator]()):void 0,"%AsyncFromSyncIteratorPrototype%":void 0,"%AsyncFunction%":m,"%AsyncGenerator%":m,"%AsyncGeneratorFunction%":m,"%AsyncIteratorPrototype%":m,"%Atomics%":"undefined"==typeof Atomics?void 0:Atomics,"%BigInt%":"undefined"==typeof BigInt?void 0:BigInt,"%BigInt64Array%":"undefined"==typeof BigInt64Array?void 0:BigInt64Array,"%BigUint64Array%":"undefined"==typeof BigUint64Array?void 0:BigUint64Array,"%Boolean%":Boolean,"%DataView%":"undefined"==typeof DataView?void 0:DataView,"%Date%":Date,"%decodeURI%":decodeURI,"%decodeURIComponent%":decodeURIComponent,"%encodeURI%":encodeURI,"%encodeURIComponent%":encodeURIComponent,"%Error%":Error,"%eval%":eval,"%EvalError%":EvalError,"%Float32Array%":"undefined"==typeof Float32Array?void 0:Float32Array,"%Float64Array%":"undefined"==typeof Float64Array?void 0:Float64Array,"%FinalizationRegistry%":"undefined"==typeof FinalizationRegistry?void 0:FinalizationRegistry,"%Function%":a,"%GeneratorFunction%":m,"%Int8Array%":"undefined"==typeof Int8Array?void 0:Int8Array,"%Int16Array%":"undefined"==typeof Int16Array?void 0:Int16Array,"%Int32Array%":"undefined"==typeof Int32Array?void 0:Int32Array,"%isFinite%":isFinite,"%isNaN%":isNaN,"%IteratorPrototype%":p&&d?d(d([][Symbol.iterator]())):void 0,"%JSON%":"object"==typeof JSON?JSON:void 0,"%Map%":"undefined"==typeof Map?void 0:Map,"%MapIteratorPrototype%":"undefined"!=typeof Map&&p&&d?d((new Map)[Symbol.iterator]()):void 0,"%Math%":Math,"%Number%":Number,"%Object%":Object,"%parseFloat%":parseFloat,"%parseInt%":parseInt,"%Promise%":"undefined"==typeof Promise?void 0:Promise,"%Proxy%":"undefined"==typeof Proxy?void 0:Proxy,"%RangeError%":RangeError,"%ReferenceError%":ReferenceError,"%Reflect%":"undefined"==typeof Reflect?void 0:Reflect,"%RegExp%":RegExp,"%Set%":"undefined"==typeof Set?void 0:Set,"%SetIteratorPrototype%":"undefined"!=typeof Set&&p&&d?d((new Set)[Symbol.iterator]()):void 0,"%SharedArrayBuffer%":"undefined"==typeof SharedArrayBuffer?void 0:SharedArrayBuffer,"%String%":String,"%StringIteratorPrototype%":p&&d?d(""[Symbol.iterator]()):void 0,"%Symbol%":p?Symbol:void 0,"%SyntaxError%":r,"%ThrowTypeError%":c,"%TypedArray%":g,"%TypeError%":o,"%Uint8Array%":"undefined"==typeof Uint8Array?void 0:Uint8Array,"%Uint8ClampedArray%":"undefined"==typeof Uint8ClampedArray?void 0:Uint8ClampedArray,"%Uint16Array%":"undefined"==typeof Uint16Array?void 0:Uint16Array,"%Uint32Array%":"undefined"==typeof Uint32Array?void 0:Uint32Array,"%URIError%":URIError,"%WeakMap%":"undefined"==typeof WeakMap?void 0:WeakMap,"%WeakRef%":"undefined"==typeof WeakRef?void 0:WeakRef,"%WeakSet%":"undefined"==typeof WeakSet?void 0:WeakSet};if(d)try{null.error}catch(n){var h=d(d(n));f["%Error.prototype%"]=h}var v={"%ArrayBufferPrototype%":["ArrayBuffer","prototype"],"%ArrayPrototype%":["Array","prototype"],"%ArrayProto_entries%":["Array","prototype","entries"],"%ArrayProto_forEach%":["Array","prototype","forEach"],"%ArrayProto_keys%":["Array","prototype","keys"],"%ArrayProto_values%":["Array","prototype","values"],"%AsyncFunctionPrototype%":["AsyncFunction","prototype"],"%AsyncGenerator%":["AsyncGeneratorFunction","prototype"],"%AsyncGeneratorPrototype%":["AsyncGeneratorFunction","prototype","prototype"],"%BooleanPrototype%":["Boolean","prototype"],"%DataViewPrototype%":["DataView","prototype"],"%DatePrototype%":["Date","prototype"],"%ErrorPrototype%":["Error","prototype"],"%EvalErrorPrototype%":["EvalError","prototype"],"%Float32ArrayPrototype%":["Float32Array","prototype"],"%Float64ArrayPrototype%":["Float64Array","prototype"],"%FunctionPrototype%":["Function","prototype"],"%Generator%":["GeneratorFunction","prototype"],"%GeneratorPrototype%":["GeneratorFunction","prototype","prototype"],"%Int8ArrayPrototype%":["Int8Array","prototype"],"%Int16ArrayPrototype%":["Int16Array","prototype"],"%Int32ArrayPrototype%":["Int32Array","prototype"],"%JSONParse%":["JSON","parse"],"%JSONStringify%":["JSON","stringify"],"%MapPrototype%":["Map","prototype"],"%NumberPrototype%":["Number","prototype"],"%ObjectPrototype%":["Object","prototype"],"%ObjProto_toString%":["Object","prototype","toString"],"%ObjProto_valueOf%":["Object","prototype","valueOf"],"%PromisePrototype%":["Promise","prototype"],"%PromiseProto_then%":["Promise","prototype","then"],"%Promise_all%":["Promise","all"],"%Promise_reject%":["Promise","reject"],"%Promise_resolve%":["Promise","resolve"],"%RangeErrorPrototype%":["RangeError","prototype"],"%ReferenceErrorPrototype%":["ReferenceError","prototype"],"%RegExpPrototype%":["RegExp","prototype"],"%SetPrototype%":["Set","prototype"],"%SharedArrayBufferPrototype%":["SharedArrayBuffer","prototype"],"%StringPrototype%":["String","prototype"],"%SymbolPrototype%":["Symbol","prototype"],"%SyntaxErrorPrototype%":["SyntaxError","prototype"],"%TypedArrayPrototype%":["TypedArray","prototype"],"%TypeErrorPrototype%":["TypeError","prototype"],"%Uint8ArrayPrototype%":["Uint8Array","prototype"],"%Uint8ClampedArrayPrototype%":["Uint8ClampedArray","prototype"],"%Uint16ArrayPrototype%":["Uint16Array","prototype"],"%Uint32ArrayPrototype%":["Uint32Array","prototype"],"%URIErrorPrototype%":["URIError","prototype"],"%WeakMapPrototype%":["WeakMap","prototype"],"%WeakSetPrototype%":["WeakSet","prototype"]},b=t(62),k=t(294),y=b.call(Function.call,Array.prototype.concat),S=b.call(Function.apply,Array.prototype.splice),x=b.call(Function.call,String.prototype.replace),w=b.call(Function.call,String.prototype.slice),E=b.call(Function.call,RegExp.prototype.exec),D=/[^%.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|%$))/g,C=/\\(\\)?/g,I=function(n){var e=w(n,0,1),t=w(n,-1);if("%"===e&&"%"!==t)throw new r("invalid intrinsic syntax, expected closing `%`");if("%"===t&&"%"!==e)throw new r("invalid intrinsic syntax, expected opening `%`");var a=[];return x(n,D,(function(n,e,t,r){a[a.length]=t?x(r,C,"$1"):e||n})),a},T=function(n,e){var t,a=n;if(k(v,a)&&(a="%"+(t=v[a])[0]+"%"),k(f,a)){var s=f[a];if(s===m&&(s=function n(e){var t;if("%AsyncFunction%"===e)t=i("async function () {}");else if("%GeneratorFunction%"===e)t=i("function* () {}");else if("%AsyncGeneratorFunction%"===e)t=i("async function* () {}");else if("%AsyncGenerator%"===e){var r=n("%AsyncGeneratorFunction%");r&&(t=r.prototype)}else if("%AsyncIteratorPrototype%"===e){var a=n("%AsyncGenerator%");a&&d&&(t=d(a.prototype))}return f[e]=t,t}(a)),void 0===s&&!e)throw new o("intrinsic "+n+" exists, but is not available. Please file an issue!");return{alias:t,name:a,value:s}}throw new r("intrinsic "+n+" does not exist!")};n.exports=function(n,e){if("string"!=typeof n||0===n.length)throw new o("intrinsic name must be a non-empty string");if(arguments.length>1&&"boolean"!=typeof e)throw new o('"allowMissing" argument must be a boolean');if(null===E(/^%?[^%]*%?$/,n))throw new r("`%` may not be present anywhere but at the beginning and end of the intrinsic name");var t=I(n),a=t.length>0?t[0]:"",i=T("%"+a+"%",e),l=i.name,c=i.value,p=!1,u=i.alias;u&&(a=u[0],S(t,y([0,1],u)));for(var d=1,m=!0;d<t.length;d+=1){var g=t[d],h=w(g,0,1),v=w(g,-1);if(('"'===h||"'"===h||"`"===h||'"'===v||"'"===v||"`"===v)&&h!==v)throw new r("property names with quotes must have matching quotes");if("constructor"!==g&&m||(p=!0),k(f,l="%"+(a+="."+g)+"%"))c=f[l];else if(null!=c){if(!(g in c)){if(!e)throw new o("base intrinsic for "+n+" exists, but the property is not available.");return}if(s&&d+1>=t.length){var b=s(c,g);c=(m=!!b)&&"get"in b&&!("originalValue"in b.get)?b.get:c[g]}else m=k(c,g),c=c[g];m&&!p&&(f[l]=c)}}return c}},function(n,e,t){"use strict";var r=t(293);n.exports=Function.prototype.bind||r},function(n,e,t){"use strict";var r=String.prototype.replace,a=/%20/g,o="RFC1738",i="RFC3986";n.exports={default:i,formatters:{RFC1738:function(n){return r.call(n,a,"+")},RFC3986:function(n){return String(n)}},RFC1738:o,RFC3986:i}},function(n,e,t){var r=t(306);n.exports=function(n){return r(n,{weekStartsOn:1})}},function(n,e,t){"use strict";var r=t(5),a=t(318),o=t(119),i={"Content-Type":"application/x-www-form-urlencoded"};function s(n,e){!r.isUndefined(n)&&r.isUndefined(n["Content-Type"])&&(n["Content-Type"]=e)}var l,c={transitional:{silentJSONParsing:!0,forcedJSONParsing:!0,clarifyTimeoutError:!1},adapter:(("undefined"!=typeof XMLHttpRequest||"undefined"!=typeof process&&"[object process]"===Object.prototype.toString.call(process))&&(l=t(120)),l),transformRequest:[function(n,e){return a(e,"Accept"),a(e,"Content-Type"),r.isFormData(n)||r.isArrayBuffer(n)||r.isBuffer(n)||r.isStream(n)||r.isFile(n)||r.isBlob(n)?n:r.isArrayBufferView(n)?n.buffer:r.isURLSearchParams(n)?(s(e,"application/x-www-form-urlencoded;charset=utf-8"),n.toString()):r.isObject(n)||e&&"application/json"===e["Content-Type"]?(s(e,"application/json"),function(n,e,t){if(r.isString(n))try{return(e||JSON.parse)(n),r.trim(n)}catch(n){if("SyntaxError"!==n.name)throw n}return(t||JSON.stringify)(n)}(n)):n}],transformResponse:[function(n){var e=this.transitional,t=e&&e.silentJSONParsing,a=e&&e.forcedJSONParsing,i=!t&&"json"===this.responseType;if(i||a&&r.isString(n)&&n.length)try{return JSON.parse(n)}catch(n){if(i){if("SyntaxError"===n.name)throw o(n,this,"E_JSON_PARSE");throw n}}return n}],timeout:0,xsrfCookieName:"XSRF-TOKEN",xsrfHeaderName:"X-XSRF-TOKEN",maxContentLength:-1,maxBodyLength:-1,validateStatus:function(n){return n>=200&&n<300}};c.headers={common:{Accept:"application/json, text/plain, */*"}},r.forEach(["delete","get","head"],(function(n){c.headers[n]={}})),r.forEach(["post","put","patch"],(function(n){c.headers[n]=r.merge(i)})),n.exports=c},function(n,e){n.exports=function(n){return n.webpackPolyfill||(n.deprecate=function(){},n.paths=[],n.children||(n.children=[]),Object.defineProperty(n,"loaded",{enumerable:!0,get:function(){return n.l}}),Object.defineProperty(n,"id",{enumerable:!0,get:function(){return n.i}}),n.webpackPolyfill=1),n}},function(n,e,t){"use strict";var r=t(23),a=t(158).left,o=t(159),i=t(77);r({target:"Array",proto:!0,forced:!t(160)&&i>79&&i<83||!o("reduce")},{reduce:function(n){var e=arguments.length;return a(this,n,e,e>1?arguments[1]:void 0)}})},function(n,e,t){var r=t(9),a=t(43),o=t(138),i=t(45),s=t(30),l=t(71),c=t(13),p=t(81),u=Object.getOwnPropertyDescriptor;e.f=r?u:function(n,e){if(n=s(n),e=l(e),p)try{return u(n,e)}catch(n){}if(c(n,e))return i(!a(o.f,n,e),n[e])}},function(n,e,t){var r=t(7),a=t(6),o=t(31),i=Object,s=r("".split);n.exports=a((function(){return!i("z").propertyIsEnumerable(0)}))?function(n){return"String"==o(n)?s(n,""):i(n)}:i},function(n,e){n.exports=function(n){return null==n}},function(n,e,t){var r=t(139),a=t(73);n.exports=function(n){var e=r(n,"string");return a(e)?e:e+""}},function(n,e){var t="object"==typeof document&&document.all,r=void 0===t&&void 0!==t;n.exports={all:t,IS_HTMLDDA:r}},function(n,e,t){var r=t(32),a=t(4),o=t(74),i=t(75),s=Object;n.exports=i?function(n){return"symbol"==typeof n}:function(n){var e=r("Symbol");return a(e)&&o(e.prototype,s(n))}},function(n,e,t){var r=t(7);n.exports=r({}.isPrototypeOf)},function(n,e,t){var r=t(76);n.exports=r&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(n,e,t){var r=t(77),a=t(6),o=t(8).String;n.exports=!!Object.getOwnPropertySymbols&&!a((function(){var n=Symbol();return!o(n)||!(Object(n)instanceof Symbol)||!Symbol.sham&&r&&r<41}))},function(n,e,t){var r,a,o=t(8),i=t(140),s=o.process,l=o.Deno,c=s&&s.versions||l&&l.version,p=c&&c.v8;p&&(a=(r=p.split("."))[0]>0&&r[0]<4?1:+(r[0]+r[1])),!a&&i&&(!(r=i.match(/Edge\/(\d+)/))||r[1]>=74)&&(r=i.match(/Chrome\/(\d+)/))&&(a=+r[1]),n.exports=a},function(n,e,t){var r=t(79),a=t(48);(n.exports=function(n,e){return a[n]||(a[n]=void 0!==e?e:{})})("versions",[]).push({version:"3.31.0",mode:r?"pure":"global",copyright:" 2014-2023 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.31.0/LICENSE",source:"https://github.com/zloirock/core-js"})},function(n,e){n.exports=!1},function(n,e,t){var r=t(7),a=0,o=Math.random(),i=r(1..toString);n.exports=function(n){return"Symbol("+(void 0===n?"":n)+")_"+i(++a+o,36)}},function(n,e,t){var r=t(9),a=t(6),o=t(82);n.exports=!r&&!a((function(){return 7!=Object.defineProperty(o("div"),"a",{get:function(){return 7}}).a}))},function(n,e,t){var r=t(8),a=t(14),o=r.document,i=a(o)&&a(o.createElement);n.exports=function(n){return i?o.createElement(n):{}}},function(n,e,t){var r=t(9),a=t(6);n.exports=r&&a((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(n,e,t){var r=t(78),a=t(80),o=r("keys");n.exports=function(n){return o[n]||(o[n]=a(n))}},function(n,e,t){var r=t(13),a=t(150),o=t(68),i=t(19);n.exports=function(n,e,t){for(var s=a(e),l=i.f,c=o.f,p=0;p<s.length;p++){var u=s[p];r(n,u)||t&&r(t,u)||l(n,u,c(e,u))}}},function(n,e,t){var r=t(7),a=t(13),o=t(30),i=t(152).indexOf,s=t(50),l=r([].push);n.exports=function(n,e){var t,r=o(n),c=0,p=[];for(t in r)!a(s,t)&&a(r,t)&&l(p,t);for(;e.length>c;)a(r,t=e[c++])&&(~i(p,t)||l(p,t));return p}},function(n,e,t){var r=t(166),a=t(25),o=t(167);n.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var n,e=!1,t={};try{(n=r(Object.prototype,"__proto__","set"))(t,[]),e=t instanceof Array}catch(n){}return function(t,r){return a(t),o(r),e?n(t,r):t.__proto__=r,t}}():void 0)},function(n,e,t){var r=t(171),a=String;n.exports=function(n){if("Symbol"===r(n))throw TypeError("Cannot convert a Symbol value to a string");return a(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,r=e.length,a=n.length;++t<r;)n[a+t]=e[t];return n}},function(n,e){var t="object"==typeof global&&global&&global.Object===Object&&global;n.exports=t},function(n,e,t){var r=t(36),a=t(190),o=t(191),i=t(192),s=t(193),l=t(194);function c(n){var e=this.__data__=new r(n);this.size=e.size}c.prototype.clear=a,c.prototype.delete=o,c.prototype.get=i,c.prototype.has=s,c.prototype.set=l,n.exports=c},function(n,e){n.exports=function(n,e){return n===e||n!=n&&e!=e}},function(n,e,t){var r=t(27),a=t(54);n.exports=function(n){if(!a(n))return!1;var e=r(n);return"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e}},function(n,e){var t=Function.prototype.toString;n.exports=function(n){if(null!=n){try{return t.call(n)}catch(n){}try{return n+""}catch(n){}}return""}},function(n,e,t){var r=t(211),a=t(20);n.exports=function n(e,t,o,i,s){return e===t||(null==e||null==t||!a(e)&&!a(t)?e!=e&&t!=t:r(e,t,o,i,n,s))}},function(n,e,t){var r=t(97),a=t(214),o=t(98);n.exports=function(n,e,t,i,s,l){var c=1&t,p=n.length,u=e.length;if(p!=u&&!(c&&u>p))return!1;var d=l.get(n),m=l.get(e);if(d&&m)return d==e&&m==n;var g=-1,f=!0,h=2&t?new r:void 0;for(l.set(n,e),l.set(e,n);++g<p;){var v=n[g],b=e[g];if(i)var k=c?i(b,v,g,e,n,l):i(v,b,g,n,e,l);if(void 0!==k){if(k)continue;f=!1;break}if(h){if(!a(e,(function(n,e){if(!o(h,e)&&(v===n||s(v,n,t,i,l)))return h.push(e)}))){f=!1;break}}else if(v!==b&&!s(v,b,t,i,l)){f=!1;break}}return l.delete(n),l.delete(e),f}},function(n,e,t){var r=t(55),a=t(212),o=t(213);function i(n){var e=-1,t=null==n?0:n.length;for(this.__data__=new r;++e<t;)this.add(n[e])}i.prototype.add=i.prototype.push=a,i.prototype.has=o,n.exports=i},function(n,e){n.exports=function(n,e){return n.has(e)}},function(n,e,t){var r=t(224),a=t(230),o=t(103);n.exports=function(n){return o(n)?r(n):a(n)}},function(n,e,t){(function(n){var r=t(11),a=t(226),o=e&&!e.nodeType&&e,i=o&&"object"==typeof n&&n&&!n.nodeType&&n,s=i&&i.exports===o?r.Buffer:void 0,l=(s?s.isBuffer:void 0)||a;n.exports=l}).call(this,t(66)(n))},function(n,e){var t=/^(?:0|[1-9]\d*)$/;n.exports=function(n,e){var r=typeof n;return!!(e=null==e?9007199254740991:e)&&("number"==r||"symbol"!=r&&t.test(n))&&n>-1&&n%1==0&&n<e}},function(n,e,t){var r=t(227),a=t(228),o=t(229),i=o&&o.isTypedArray,s=i?a(i):r;n.exports=s},function(n,e,t){var r=t(93),a=t(57);n.exports=function(n){return null!=n&&a(n.length)&&!r(n)}},function(n,e,t){var r=t(15)(t(11),"Set");n.exports=r},function(n,e,t){var r=t(54);n.exports=function(n){return n==n&&!r(n)}},function(n,e){n.exports=function(n,e){return function(t){return null!=t&&(t[n]===e&&(void 0!==e||n in Object(t)))}}},function(n,e,t){var r=t(108),a=t(40);n.exports=function(n,e){for(var t=0,o=(e=r(e,n)).length;null!=n&&t<o;)n=n[a(e[t++])];return t&&t==o?n:void 0}},function(n,e,t){var r=t(12),a=t(58),o=t(241),i=t(244);n.exports=function(n,e){return r(n)?n:a(n,e)?[n]:o(i(n))}},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){"use strict";var r=t(63),a=Object.prototype.hasOwnProperty,o=Array.isArray,i=function(){for(var n=[],e=0;e<256;++e)n.push("%"+((e<16?"0":"")+e.toString(16)).toUpperCase());return n}(),s=function(n,e){for(var t=e&&e.plainObjects?Object.create(null):{},r=0;r<n.length;++r)void 0!==n[r]&&(t[r]=n[r]);return t};n.exports={arrayToObject:s,assign:function(n,e){return Object.keys(e).reduce((function(n,t){return n[t]=e[t],n}),n)},combine:function(n,e){return[].concat(n,e)},compact:function(n){for(var e=[{obj:{o:n},prop:"o"}],t=[],r=0;r<e.length;++r)for(var a=e[r],i=a.obj[a.prop],s=Object.keys(i),l=0;l<s.length;++l){var c=s[l],p=i[c];"object"==typeof p&&null!==p&&-1===t.indexOf(p)&&(e.push({obj:i,prop:c}),t.push(p))}return function(n){for(;n.length>1;){var e=n.pop(),t=e.obj[e.prop];if(o(t)){for(var r=[],a=0;a<t.length;++a)void 0!==t[a]&&r.push(t[a]);e.obj[e.prop]=r}}}(e),n},decode:function(n,e,t){var r=n.replace(/\+/g," ");if("iso-8859-1"===t)return r.replace(/%[0-9a-f]{2}/gi,unescape);try{return decodeURIComponent(r)}catch(n){return r}},encode:function(n,e,t,a,o){if(0===n.length)return n;var s=n;if("symbol"==typeof n?s=Symbol.prototype.toString.call(n):"string"!=typeof n&&(s=String(n)),"iso-8859-1"===t)return escape(s).replace(/%u[0-9a-f]{4}/gi,(function(n){return"%26%23"+parseInt(n.slice(2),16)+"%3B"}));for(var l="",c=0;c<s.length;++c){var p=s.charCodeAt(c);45===p||46===p||95===p||126===p||p>=48&&p<=57||p>=65&&p<=90||p>=97&&p<=122||o===r.RFC1738&&(40===p||41===p)?l+=s.charAt(c):p<128?l+=i[p]:p<2048?l+=i[192|p>>6]+i[128|63&p]:p<55296||p>=57344?l+=i[224|p>>12]+i[128|p>>6&63]+i[128|63&p]:(c+=1,p=65536+((1023&p)<<10|1023&s.charCodeAt(c)),l+=i[240|p>>18]+i[128|p>>12&63]+i[128|p>>6&63]+i[128|63&p])}return l},isBuffer:function(n){return!(!n||"object"!=typeof n)&&!!(n.constructor&&n.constructor.isBuffer&&n.constructor.isBuffer(n))},isRegExp:function(n){return"[object RegExp]"===Object.prototype.toString.call(n)},maybeMap:function(n,e){if(o(n)){for(var t=[],r=0;r<n.length;r+=1)t.push(e(n[r]));return t}return e(n)},merge:function n(e,t,r){if(!t)return e;if("object"!=typeof t){if(o(e))e.push(t);else{if(!e||"object"!=typeof e)return[e,t];(r&&(r.plainObjects||r.allowPrototypes)||!a.call(Object.prototype,t))&&(e[t]=!0)}return e}if(!e||"object"!=typeof e)return[e].concat(t);var i=e;return o(e)&&!o(t)&&(i=s(e,r)),o(e)&&o(t)?(t.forEach((function(t,o){if(a.call(e,o)){var i=e[o];i&&"object"==typeof i&&t&&"object"==typeof t?e[o]=n(i,t,r):e.push(t)}else e[o]=t})),e):Object.keys(t).reduce((function(e,o){var i=t[o];return a.call(e,o)?e[o]=n(e[o],i,r):e[o]=i,e}),i)}}},function(n,e){n.exports=function(n){return n instanceof Date}},function(n,e,t){var r=t(16),a=t(64);n.exports=function(n){var e=r(n),t=e.getFullYear(),o=new Date(0);o.setFullYear(t+1,0,4),o.setHours(0,0,0,0);var i=a(o),s=new Date(0);s.setFullYear(t,0,4),s.setHours(0,0,0,0);var l=a(s);return e.getTime()>=i.getTime()?t+1:e.getTime()>=l.getTime()?t:t-1}},function(n,e,t){"use strict";n.exports=function(n,e){return function(){for(var t=new Array(arguments.length),r=0;r<t.length;r++)t[r]=arguments[r];return n.apply(e,t)}}},function(n,e,t){"use strict";var r=t(5);function a(n){return encodeURIComponent(n).replace(/%3A/gi,":").replace(/%24/g,"$").replace(/%2C/gi,",").replace(/%20/g,"+").replace(/%5B/gi,"[").replace(/%5D/gi,"]")}n.exports=function(n,e,t){if(!e)return n;var o;if(t)o=t(e);else if(r.isURLSearchParams(e))o=e.toString();else{var i=[];r.forEach(e,(function(n,e){null!=n&&(r.isArray(n)?e+="[]":n=[n],r.forEach(n,(function(n){r.isDate(n)?n=n.toISOString():r.isObject(n)&&(n=JSON.stringify(n)),i.push(a(e)+"="+a(n))})))})),o=i.join("&")}if(o){var s=n.indexOf("#");-1!==s&&(n=n.slice(0,s)),n+=(-1===n.indexOf("?")?"?":"&")+o}return n}},function(n,e,t){"use strict";n.exports=function(n,e,t,r,a){return n.config=e,t&&(n.code=t),n.request=r,n.response=a,n.isAxiosError=!0,n.toJSON=function(){return{message:this.message,name:this.name,description:this.description,number:this.number,fileName:this.fileName,lineNumber:this.lineNumber,columnNumber:this.columnNumber,stack:this.stack,config:this.config,code:this.code}},n}},function(n,e,t){"use strict";var r=t(5),a=t(319),o=t(320),i=t(118),s=t(321),l=t(324),c=t(325),p=t(121);n.exports=function(n){return new Promise((function(e,t){var u=n.data,d=n.headers,m=n.responseType;r.isFormData(u)&&delete d["Content-Type"];var g=new XMLHttpRequest;if(n.auth){var f=n.auth.username||"",h=n.auth.password?unescape(encodeURIComponent(n.auth.password)):"";d.Authorization="Basic "+btoa(f+":"+h)}var v=s(n.baseURL,n.url);function b(){if(g){var r="getAllResponseHeaders"in g?l(g.getAllResponseHeaders()):null,o={data:m&&"text"!==m&&"json"!==m?g.response:g.responseText,status:g.status,statusText:g.statusText,headers:r,config:n,request:g};a(e,t,o),g=null}}if(g.open(n.method.toUpperCase(),i(v,n.params,n.paramsSerializer),!0),g.timeout=n.timeout,"onloadend"in g?g.onloadend=b:g.onreadystatechange=function(){g&&4===g.readyState&&(0!==g.status||g.responseURL&&0===g.responseURL.indexOf("file:"))&&setTimeout(b)},g.onabort=function(){g&&(t(p("Request aborted",n,"ECONNABORTED",g)),g=null)},g.onerror=function(){t(p("Network Error",n,null,g)),g=null},g.ontimeout=function(){var e="timeout of "+n.timeout+"ms exceeded";n.timeoutErrorMessage&&(e=n.timeoutErrorMessage),t(p(e,n,n.transitional&&n.transitional.clarifyTimeoutError?"ETIMEDOUT":"ECONNABORTED",g)),g=null},r.isStandardBrowserEnv()){var k=(n.withCredentials||c(v))&&n.xsrfCookieName?o.read(n.xsrfCookieName):void 0;k&&(d[n.xsrfHeaderName]=k)}"setRequestHeader"in g&&r.forEach(d,(function(n,e){void 0===u&&"content-type"===e.toLowerCase()?delete d[e]:g.setRequestHeader(e,n)})),r.isUndefined(n.withCredentials)||(g.withCredentials=!!n.withCredentials),m&&"json"!==m&&(g.responseType=n.responseType),"function"==typeof n.onDownloadProgress&&g.addEventListener("progress",n.onDownloadProgress),"function"==typeof n.onUploadProgress&&g.upload&&g.upload.addEventListener("progress",n.onUploadProgress),n.cancelToken&&n.cancelToken.promise.then((function(n){g&&(g.abort(),t(n),g=null)})),u||(u=null),g.send(u)}))}},function(n,e,t){"use strict";var r=t(119);n.exports=function(n,e,t,a,o){var i=new Error(n);return r(i,e,t,a,o)}},function(n,e,t){"use strict";n.exports=function(n){return!(!n||!n.__CANCEL__)}},function(n,e,t){"use strict";var r=t(5);n.exports=function(n,e){e=e||{};var t={},a=["url","method","data"],o=["headers","auth","proxy","params"],i=["baseURL","transformRequest","transformResponse","paramsSerializer","timeout","timeoutMessage","withCredentials","adapter","responseType","xsrfCookieName","xsrfHeaderName","onUploadProgress","onDownloadProgress","decompress","maxContentLength","maxBodyLength","maxRedirects","transport","httpAgent","httpsAgent","cancelToken","socketPath","responseEncoding"],s=["validateStatus"];function l(n,e){return r.isPlainObject(n)&&r.isPlainObject(e)?r.merge(n,e):r.isPlainObject(e)?r.merge({},e):r.isArray(e)?e.slice():e}function c(a){r.isUndefined(e[a])?r.isUndefined(n[a])||(t[a]=l(void 0,n[a])):t[a]=l(n[a],e[a])}r.forEach(a,(function(n){r.isUndefined(e[n])||(t[n]=l(void 0,e[n]))})),r.forEach(o,c),r.forEach(i,(function(a){r.isUndefined(e[a])?r.isUndefined(n[a])||(t[a]=l(void 0,n[a])):t[a]=l(void 0,e[a])})),r.forEach(s,(function(r){r in e?t[r]=l(n[r],e[r]):r in n&&(t[r]=l(void 0,n[r]))}));var p=a.concat(o).concat(i).concat(s),u=Object.keys(n).concat(Object.keys(e)).filter((function(n){return-1===p.indexOf(n)}));return r.forEach(u,c),t}},function(n,e,t){"use strict";function r(n){this.message=n}r.prototype.toString=function(){return"Cancel"+(this.message?": "+this.message:"")},r.prototype.__CANCEL__=!0,n.exports=r},function(n,e,t){},function(n,e,t){n.exports=function(){"use strict";var n=6e4,e=36e5,t="millisecond",r="second",a="minute",o="hour",i="day",s="week",l="month",c="quarter",p="year",u="date",d="Invalid Date",m=/^(\d{4})[-/]?(\d{1,2})?[-/]?(\d{0,2})[Tt\s]*(\d{1,2})?:?(\d{1,2})?:?(\d{1,2})?[.:]?(\d+)?$/,g=/\[([^\]]+)]|Y{1,4}|M{1,4}|D{1,2}|d{1,4}|H{1,2}|h{1,2}|a|A|m{1,2}|s{1,2}|Z{1,2}|SSS/g,f={name:"en",weekdays:"Sunday_Monday_Tuesday_Wednesday_Thursday_Friday_Saturday".split("_"),months:"January_February_March_April_May_June_July_August_September_October_November_December".split("_"),ordinal:function(n){var e=["th","st","nd","rd"],t=n%100;return"["+n+(e[(t-20)%10]||e[t]||e[0])+"]"}},h=function(n,e,t){var r=String(n);return!r||r.length>=e?n:""+Array(e+1-r.length).join(t)+n},v={s:h,z:function(n){var e=-n.utcOffset(),t=Math.abs(e),r=Math.floor(t/60),a=t%60;return(e<=0?"+":"-")+h(r,2,"0")+":"+h(a,2,"0")},m:function n(e,t){if(e.date()<t.date())return-n(t,e);var r=12*(t.year()-e.year())+(t.month()-e.month()),a=e.clone().add(r,l),o=t-a<0,i=e.clone().add(r+(o?-1:1),l);return+(-(r+(t-a)/(o?a-i:i-a))||0)},a:function(n){return n<0?Math.ceil(n)||0:Math.floor(n)},p:function(n){return{M:l,y:p,w:s,d:i,D:u,h:o,m:a,s:r,ms:t,Q:c}[n]||String(n||"").toLowerCase().replace(/s$/,"")},u:function(n){return void 0===n}},b="en",k={};k[b]=f;var y=function(n){return n instanceof E},S=function n(e,t,r){var a;if(!e)return b;if("string"==typeof e){var o=e.toLowerCase();k[o]&&(a=o),t&&(k[o]=t,a=o);var i=e.split("-");if(!a&&i.length>1)return n(i[0])}else{var s=e.name;k[s]=e,a=s}return!r&&a&&(b=a),a||!r&&b},x=function(n,e){if(y(n))return n.clone();var t="object"==typeof e?e:{};return t.date=n,t.args=arguments,new E(t)},w=v;w.l=S,w.i=y,w.w=function(n,e){return x(n,{locale:e.$L,utc:e.$u,x:e.$x,$offset:e.$offset})};var E=function(){function f(n){this.$L=S(n.locale,null,!0),this.parse(n)}var h=f.prototype;return h.parse=function(n){this.$d=function(n){var e=n.date,t=n.utc;if(null===e)return new Date(NaN);if(w.u(e))return new Date;if(e instanceof Date)return new Date(e);if("string"==typeof e&&!/Z$/i.test(e)){var r=e.match(m);if(r){var a=r[2]-1||0,o=(r[7]||"0").substring(0,3);return t?new Date(Date.UTC(r[1],a,r[3]||1,r[4]||0,r[5]||0,r[6]||0,o)):new Date(r[1],a,r[3]||1,r[4]||0,r[5]||0,r[6]||0,o)}}return new Date(e)}(n),this.$x=n.x||{},this.init()},h.init=function(){var n=this.$d;this.$y=n.getFullYear(),this.$M=n.getMonth(),this.$D=n.getDate(),this.$W=n.getDay(),this.$H=n.getHours(),this.$m=n.getMinutes(),this.$s=n.getSeconds(),this.$ms=n.getMilliseconds()},h.$utils=function(){return w},h.isValid=function(){return!(this.$d.toString()===d)},h.isSame=function(n,e){var t=x(n);return this.startOf(e)<=t&&t<=this.endOf(e)},h.isAfter=function(n,e){return x(n)<this.startOf(e)},h.isBefore=function(n,e){return this.endOf(e)<x(n)},h.$g=function(n,e,t){return w.u(n)?this[e]:this.set(t,n)},h.unix=function(){return Math.floor(this.valueOf()/1e3)},h.valueOf=function(){return this.$d.getTime()},h.startOf=function(n,e){var t=this,c=!!w.u(e)||e,d=w.p(n),m=function(n,e){var r=w.w(t.$u?Date.UTC(t.$y,e,n):new Date(t.$y,e,n),t);return c?r:r.endOf(i)},g=function(n,e){return w.w(t.toDate()[n].apply(t.toDate("s"),(c?[0,0,0,0]:[23,59,59,999]).slice(e)),t)},f=this.$W,h=this.$M,v=this.$D,b="set"+(this.$u?"UTC":"");switch(d){case p:return c?m(1,0):m(31,11);case l:return c?m(1,h):m(0,h+1);case s:var k=this.$locale().weekStart||0,y=(f<k?f+7:f)-k;return m(c?v-y:v+(6-y),h);case i:case u:return g(b+"Hours",0);case o:return g(b+"Minutes",1);case a:return g(b+"Seconds",2);case r:return g(b+"Milliseconds",3);default:return this.clone()}},h.endOf=function(n){return this.startOf(n,!1)},h.$set=function(n,e){var s,c=w.p(n),d="set"+(this.$u?"UTC":""),m=(s={},s[i]=d+"Date",s[u]=d+"Date",s[l]=d+"Month",s[p]=d+"FullYear",s[o]=d+"Hours",s[a]=d+"Minutes",s[r]=d+"Seconds",s[t]=d+"Milliseconds",s)[c],g=c===i?this.$D+(e-this.$W):e;if(c===l||c===p){var f=this.clone().set(u,1);f.$d[m](g),f.init(),this.$d=f.set(u,Math.min(this.$D,f.daysInMonth())).$d}else m&&this.$d[m](g);return this.init(),this},h.set=function(n,e){return this.clone().$set(n,e)},h.get=function(n){return this[w.p(n)]()},h.add=function(t,c){var u,d=this;t=Number(t);var m=w.p(c),g=function(n){var e=x(d);return w.w(e.date(e.date()+Math.round(n*t)),d)};if(m===l)return this.set(l,this.$M+t);if(m===p)return this.set(p,this.$y+t);if(m===i)return g(1);if(m===s)return g(7);var f=(u={},u[a]=n,u[o]=e,u[r]=1e3,u)[m]||1,h=this.$d.getTime()+t*f;return w.w(h,this)},h.subtract=function(n,e){return this.add(-1*n,e)},h.format=function(n){var e=this,t=this.$locale();if(!this.isValid())return t.invalidDate||d;var r=n||"YYYY-MM-DDTHH:mm:ssZ",a=w.z(this),o=this.$H,i=this.$m,s=this.$M,l=t.weekdays,c=t.months,p=function(n,t,a,o){return n&&(n[t]||n(e,r))||a[t].slice(0,o)},u=function(n){return w.s(o%12||12,n,"0")},m=t.meridiem||function(n,e,t){var r=n<12?"AM":"PM";return t?r.toLowerCase():r},f={YY:String(this.$y).slice(-2),YYYY:w.s(this.$y,4,"0"),M:s+1,MM:w.s(s+1,2,"0"),MMM:p(t.monthsShort,s,c,3),MMMM:p(c,s),D:this.$D,DD:w.s(this.$D,2,"0"),d:String(this.$W),dd:p(t.weekdaysMin,this.$W,l,2),ddd:p(t.weekdaysShort,this.$W,l,3),dddd:l[this.$W],H:String(o),HH:w.s(o,2,"0"),h:u(1),hh:u(2),a:m(o,i,!0),A:m(o,i,!1),m:String(i),mm:w.s(i,2,"0"),s:String(this.$s),ss:w.s(this.$s,2,"0"),SSS:w.s(this.$ms,3,"0"),Z:a};return r.replace(g,(function(n,e){return e||f[n]||a.replace(":","")}))},h.utcOffset=function(){return 15*-Math.round(this.$d.getTimezoneOffset()/15)},h.diff=function(t,u,d){var m,g=w.p(u),f=x(t),h=(f.utcOffset()-this.utcOffset())*n,v=this-f,b=w.m(this,f);return b=(m={},m[p]=b/12,m[l]=b,m[c]=b/3,m[s]=(v-h)/6048e5,m[i]=(v-h)/864e5,m[o]=v/e,m[a]=v/n,m[r]=v/1e3,m)[g]||v,d?b:w.a(b)},h.daysInMonth=function(){return this.endOf(l).$D},h.$locale=function(){return k[this.$L]},h.locale=function(n,e){if(!n)return this.$L;var t=this.clone(),r=S(n,e,!0);return r&&(t.$L=r),t},h.clone=function(){return w.w(this.$d,this)},h.toDate=function(){return new Date(this.valueOf())},h.toJSON=function(){return this.isValid()?this.toISOString():null},h.toISOString=function(){return this.$d.toISOString()},h.toString=function(){return this.$d.toUTCString()},f}(),D=E.prototype;return x.prototype=D,[["$ms",t],["$s",r],["$m",a],["$H",o],["$W",i],["$M",l],["$y",p],["$D",u]].forEach((function(n){D[n[1]]=function(e){return this.$g(e,n[0],n[1])}})),x.extend=function(n,e){return n.$i||(n(e,E,x),n.$i=!0),x},x.locale=S,x.isDayjs=y,x.unix=function(n){return x(1e3*n)},x.en=k[b],x.Ls=k,x.p={},x}()},function(n,e,t){},function(n,e,t){},function(n,e,t){var r=t(177),a=t(182),o=t(253),i=t(261),s=t(270),l=t(271),c=o((function(n){var e=l(n);return s(e)&&(e=void 0),i(r(n,1,s,!0),a(e,2))}));n.exports=c},function(n,e,t){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var r=/["'&<>]/;n.exports=function(n){var e,t=""+n,a=r.exec(t);if(!a)return t;var o="",i=0,s=0;for(i=a.index;i<t.length;i++){switch(t.charCodeAt(i)){case 34:e="&quot;";break;case 38:e="&amp;";break;case 39:e="&#39;";break;case 60:e="&lt;";break;case 62:e="&gt;";break;default:continue}s!==i&&(o+=t.substring(s,i)),s=i+1,o+=e}return s!==i?o+t.substring(s,i):o}},function(n,e){var t=/^\s+|\s+$/g,r=/^[-+]0x[0-9a-f]+$/i,a=/^0b[01]+$/i,o=/^0o[0-7]+$/i,i=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,c=s||l||Function("return this")(),p=Object.prototype.toString,u=Math.max,d=Math.min,m=function(){return c.Date.now()};function g(n){var e=typeof n;return!!n&&("object"==e||"function"==e)}function f(n){if("number"==typeof n)return n;if(function(n){return"symbol"==typeof n||function(n){return!!n&&"object"==typeof n}(n)&&"[object Symbol]"==p.call(n)}(n))return NaN;if(g(n)){var e="function"==typeof n.valueOf?n.valueOf():n;n=g(e)?e+"":e}if("string"!=typeof n)return 0===n?n:+n;n=n.replace(t,"");var s=a.test(n);return s||o.test(n)?i(n.slice(2),s?2:8):r.test(n)?NaN:+n}n.exports=function(n,e,t){var r,a,o,i,s,l,c=0,p=!1,h=!1,v=!0;if("function"!=typeof n)throw new TypeError("Expected a function");function b(e){var t=r,o=a;return r=a=void 0,c=e,i=n.apply(o,t)}function k(n){return c=n,s=setTimeout(S,e),p?b(n):i}function y(n){var t=n-l;return void 0===l||t>=e||t<0||h&&n-c>=o}function S(){var n=m();if(y(n))return x(n);s=setTimeout(S,function(n){var t=e-(n-l);return h?d(t,o-(n-c)):t}(n))}function x(n){return s=void 0,v&&r?b(n):(r=a=void 0,i)}function w(){var n=m(),t=y(n);if(r=arguments,a=this,l=n,t){if(void 0===s)return k(l);if(h)return s=setTimeout(S,e),b(l)}return void 0===s&&(s=setTimeout(S,e)),i}return e=f(e)||0,g(t)&&(p=!!t.leading,o=(h="maxWait"in t)?u(f(t.maxWait)||0,e):o,v="trailing"in t?!!t.trailing:v),w.cancel=function(){void 0!==s&&clearTimeout(s),c=0,r=l=a=s=void 0},w.flush=function(){return void 0===s?i:x(m())},w}},function(n,e,t){!function(){"use strict";n.exports={polyfill:function(){var n=window,e=document;if(!("scrollBehavior"in e.documentElement.style)||!0===n.__forceSmoothScrollPolyfill__){var t,r=n.HTMLElement||n.Element,a={scroll:n.scroll||n.scrollTo,scrollBy:n.scrollBy,elementScroll:r.prototype.scroll||s,scrollIntoView:r.prototype.scrollIntoView},o=n.performance&&n.performance.now?n.performance.now.bind(n.performance):Date.now,i=(t=n.navigator.userAgent,new RegExp(["MSIE ","Trident/","Edge/"].join("|")).test(t)?1:0);n.scroll=n.scrollTo=function(){void 0!==arguments[0]&&(!0!==l(arguments[0])?g.call(n,e.body,void 0!==arguments[0].left?~~arguments[0].left:n.scrollX||n.pageXOffset,void 0!==arguments[0].top?~~arguments[0].top:n.scrollY||n.pageYOffset):a.scroll.call(n,void 0!==arguments[0].left?arguments[0].left:"object"!=typeof arguments[0]?arguments[0]:n.scrollX||n.pageXOffset,void 0!==arguments[0].top?arguments[0].top:void 0!==arguments[1]?arguments[1]:n.scrollY||n.pageYOffset))},n.scrollBy=function(){void 0!==arguments[0]&&(l(arguments[0])?a.scrollBy.call(n,void 0!==arguments[0].left?arguments[0].left:"object"!=typeof arguments[0]?arguments[0]:0,void 0!==arguments[0].top?arguments[0].top:void 0!==arguments[1]?arguments[1]:0):g.call(n,e.body,~~arguments[0].left+(n.scrollX||n.pageXOffset),~~arguments[0].top+(n.scrollY||n.pageYOffset)))},r.prototype.scroll=r.prototype.scrollTo=function(){if(void 0!==arguments[0])if(!0!==l(arguments[0])){var n=arguments[0].left,e=arguments[0].top;g.call(this,this,void 0===n?this.scrollLeft:~~n,void 0===e?this.scrollTop:~~e)}else{if("number"==typeof arguments[0]&&void 0===arguments[1])throw new SyntaxError("Value could not be converted");a.elementScroll.call(this,void 0!==arguments[0].left?~~arguments[0].left:"object"!=typeof arguments[0]?~~arguments[0]:this.scrollLeft,void 0!==arguments[0].top?~~arguments[0].top:void 0!==arguments[1]?~~arguments[1]:this.scrollTop)}},r.prototype.scrollBy=function(){void 0!==arguments[0]&&(!0!==l(arguments[0])?this.scroll({left:~~arguments[0].left+this.scrollLeft,top:~~arguments[0].top+this.scrollTop,behavior:arguments[0].behavior}):a.elementScroll.call(this,void 0!==arguments[0].left?~~arguments[0].left+this.scrollLeft:~~arguments[0]+this.scrollLeft,void 0!==arguments[0].top?~~arguments[0].top+this.scrollTop:~~arguments[1]+this.scrollTop))},r.prototype.scrollIntoView=function(){if(!0!==l(arguments[0])){var t=d(this),r=t.getBoundingClientRect(),o=this.getBoundingClientRect();t!==e.body?(g.call(this,t,t.scrollLeft+o.left-r.left,t.scrollTop+o.top-r.top),"fixed"!==n.getComputedStyle(t).position&&n.scrollBy({left:r.left,top:r.top,behavior:"smooth"})):n.scrollBy({left:o.left,top:o.top,behavior:"smooth"})}else a.scrollIntoView.call(this,void 0===arguments[0]||arguments[0])}}function s(n,e){this.scrollLeft=n,this.scrollTop=e}function l(n){if(null===n||"object"!=typeof n||void 0===n.behavior||"auto"===n.behavior||"instant"===n.behavior)return!0;if("object"==typeof n&&"smooth"===n.behavior)return!1;throw new TypeError("behavior member of ScrollOptions "+n.behavior+" is not a valid value for enumeration ScrollBehavior.")}function c(n,e){return"Y"===e?n.clientHeight+i<n.scrollHeight:"X"===e?n.clientWidth+i<n.scrollWidth:void 0}function p(e,t){var r=n.getComputedStyle(e,null)["overflow"+t];return"auto"===r||"scroll"===r}function u(n){var e=c(n,"Y")&&p(n,"Y"),t=c(n,"X")&&p(n,"X");return e||t}function d(n){for(;n!==e.body&&!1===u(n);)n=n.parentNode||n.host;return n}function m(e){var t,r,a,i,s=(o()-e.startTime)/468;i=s=s>1?1:s,t=.5*(1-Math.cos(Math.PI*i)),r=e.startX+(e.x-e.startX)*t,a=e.startY+(e.y-e.startY)*t,e.method.call(e.scrollable,r,a),r===e.x&&a===e.y||n.requestAnimationFrame(m.bind(n,e))}function g(t,r,i){var l,c,p,u,d=o();t===e.body?(l=n,c=n.scrollX||n.pageXOffset,p=n.scrollY||n.pageYOffset,u=a.scroll):(l=t,c=t.scrollLeft,p=t.scrollTop,u=s),m({scrollable:l,method:u,startTime:d,startX:c,startY:p,x:r,y:i})}}}}()},function(n){n.exports=JSON.parse('{"en-US":{"author":"author","beforeAuthor":"Copyright  ","afterAuthor":"\\nLink: "},"zh-CN":{"author":"","beforeAuthor":"","afterAuthor":"\\n"}}')},function(n,e,t){var r=t(300),a=t(305),o=t(116),i=t(16),s=t(308),l=t(309);var c={M:function(n){return n.getMonth()+1},MM:function(n){return d(n.getMonth()+1,2)},Q:function(n){return Math.ceil((n.getMonth()+1)/3)},D:function(n){return n.getDate()},DD:function(n){return d(n.getDate(),2)},DDD:function(n){return r(n)},DDDD:function(n){return d(r(n),3)},d:function(n){return n.getDay()},E:function(n){return n.getDay()||7},W:function(n){return a(n)},WW:function(n){return d(a(n),2)},YY:function(n){return d(n.getFullYear(),4).substr(2)},YYYY:function(n){return d(n.getFullYear(),4)},GG:function(n){return String(o(n)).substr(2)},GGGG:function(n){return o(n)},H:function(n){return n.getHours()},HH:function(n){return d(n.getHours(),2)},h:function(n){var e=n.getHours();return 0===e?12:e>12?e%12:e},hh:function(n){return d(c.h(n),2)},m:function(n){return n.getMinutes()},mm:function(n){return d(n.getMinutes(),2)},s:function(n){return n.getSeconds()},ss:function(n){return d(n.getSeconds(),2)},S:function(n){return Math.floor(n.getMilliseconds()/100)},SS:function(n){return d(Math.floor(n.getMilliseconds()/10),2)},SSS:function(n){return d(n.getMilliseconds(),3)},Z:function(n){return u(n.getTimezoneOffset(),":")},ZZ:function(n){return u(n.getTimezoneOffset())},X:function(n){return Math.floor(n.getTime()/1e3)},x:function(n){return n.getTime()}};function p(n){return n.match(/\[[\s\S]/)?n.replace(/^\[|]$/g,""):n.replace(/\\/g,"")}function u(n,e){e=e||"";var t=n>0?"-":"+",r=Math.abs(n),a=r%60;return t+d(Math.floor(r/60),2)+e+d(a,2)}function d(n,e){for(var t=Math.abs(n).toString();t.length<e;)t="0"+t;return t}n.exports=function(n,e,t){var r=e?String(e):"YYYY-MM-DDTHH:mm:ss.SSSZ",a=(t||{}).locale,o=l.format.formatters,u=l.format.formattingTokensRegExp;a&&a.format&&a.format.formatters&&(o=a.format.formatters,a.format.formattingTokensRegExp&&(u=a.format.formattingTokensRegExp));var d=i(n);return s(d)?function(n,e,t){var r,a,o=n.match(t),i=o.length;for(r=0;r<i;r++)a=e[o[r]]||c[o[r]],o[r]=a||p(o[r]);return function(n){for(var e="",t=0;t<i;t++)o[t]instanceof Function?e+=o[t](n,c):e+=o[t];return e}}(r,o,u)(d):"Invalid Date"}},function(n,e,t){function r(){var n;try{n=e.storage.debug}catch(n){}return!n&&"undefined"!=typeof process&&"env"in process&&(n=process.env.DEBUG),n}(e=n.exports=t(334)).log=function(){return"object"==typeof console&&console.log&&Function.prototype.apply.call(console.log,console,arguments)},e.formatArgs=function(n){var t=this.useColors;if(n[0]=(t?"%c":"")+this.namespace+(t?" %c":" ")+n[0]+(t?"%c ":" ")+"+"+e.humanize(this.diff),!t)return;var r="color: "+this.color;n.splice(1,0,r,"color: inherit");var a=0,o=0;n[0].replace(/%[a-zA-Z%]/g,(function(n){"%%"!==n&&(a++,"%c"===n&&(o=a))})),n.splice(o,0,r)},e.save=function(n){try{null==n?e.storage.removeItem("debug"):e.storage.debug=n}catch(n){}},e.load=r,e.useColors=function(){if("undefined"!=typeof window&&window.process&&"renderer"===window.process.type)return!0;return"undefined"!=typeof document&&document.documentElement&&document.documentElement.style&&document.documentElement.style.WebkitAppearance||"undefined"!=typeof window&&window.console&&(window.console.firebug||window.console.exception&&window.console.table)||"undefined"!=typeof navigator&&navigator.userAgent&&navigator.userAgent.toLowerCase().match(/firefox\/(\d+)/)&&parseInt(RegExp.$1,10)>=31||"undefined"!=typeof navigator&&navigator.userAgent&&navigator.userAgent.toLowerCase().match(/applewebkit\/(\d+)/)},e.storage="undefined"!=typeof chrome&&void 0!==chrome.storage?chrome.storage.local:function(){try{return window.localStorage}catch(n){}}(),e.colors=["lightseagreen","forestgreen","goldenrod","dodgerblue","darkorchid","crimson"],e.formatters.j=function(n){try{return JSON.stringify(n)}catch(n){return"[UnexpectedJSONParseError]: "+n.message}},e.enable(r())},function(n,e,t){var r=t(23),a=t(8),o=t(164),i=t(165),s=a.WebAssembly,l=7!==Error("e",{cause:7}).cause,c=function(n,e){var t={};t[n]=i(n,e,l),r({global:!0,constructor:!0,arity:1,forced:l},t)},p=function(n,e){if(s&&s[n]){var t={};t[n]=i("WebAssembly."+n,e,l),r({target:"WebAssembly",stat:!0,constructor:!0,arity:1,forced:l},t)}};c("Error",(function(n){return function(e){return o(n,this,arguments)}})),c("EvalError",(function(n){return function(e){return o(n,this,arguments)}})),c("RangeError",(function(n){return function(e){return o(n,this,arguments)}})),c("ReferenceError",(function(n){return function(e){return o(n,this,arguments)}})),c("SyntaxError",(function(n){return function(e){return o(n,this,arguments)}})),c("TypeError",(function(n){return function(e){return o(n,this,arguments)}})),c("URIError",(function(n){return function(e){return o(n,this,arguments)}})),p("CompileError",(function(n){return function(e){return o(n,this,arguments)}})),p("LinkError",(function(n){return function(e){return o(n,this,arguments)}})),p("RuntimeError",(function(n){return function(e){return o(n,this,arguments)}}))},function(n,e,t){n.exports=t(339)},function(n,e,t){"use strict";var r={}.propertyIsEnumerable,a=Object.getOwnPropertyDescriptor,o=a&&!r.call({1:2},1);e.f=o?function(n){var e=a(this,n);return!!e&&e.enumerable}:r},function(n,e,t){var r=t(43),a=t(14),o=t(73),i=t(141),s=t(143),l=t(29),c=TypeError,p=l("toPrimitive");n.exports=function(n,e){if(!a(n)||o(n))return n;var t,l=i(n,p);if(l){if(void 0===e&&(e="default"),t=r(l,n,e),!a(t)||o(t))return t;throw c("Can't convert object to primitive value")}return void 0===e&&(e="number"),s(n,e)}},function(n,e){n.exports="undefined"!=typeof navigator&&String(navigator.userAgent)||""},function(n,e,t){var r=t(47),a=t(70);n.exports=function(n,e){var t=n[e];return a(t)?void 0:r(t)}},function(n,e){var t=String;n.exports=function(n){try{return t(n)}catch(n){return"Object"}}},function(n,e,t){var r=t(43),a=t(4),o=t(14),i=TypeError;n.exports=function(n,e){var t,s;if("string"===e&&a(t=n.toString)&&!o(s=r(t,n)))return s;if(a(t=n.valueOf)&&!o(s=r(t,n)))return s;if("string"!==e&&a(t=n.toString)&&!o(s=r(t,n)))return s;throw i("Can't convert object to primitive value")}},function(n,e,t){var r=t(4),a=t(19),o=t(145),i=t(49);n.exports=function(n,e,t,s){s||(s={});var l=s.enumerable,c=void 0!==s.name?s.name:e;if(r(t)&&o(t,c,s),s.global)l?n[e]=t:i(e,t);else{try{s.unsafe?n[e]&&(l=!0):delete n[e]}catch(n){}l?n[e]=t:a.f(n,e,{value:t,enumerable:!1,configurable:!s.nonConfigurable,writable:!s.nonWritable})}return n}},function(n,e,t){var r=t(7),a=t(6),o=t(4),i=t(13),s=t(9),l=t(146).CONFIGURABLE,c=t(147),p=t(148),u=p.enforce,d=p.get,m=String,g=Object.defineProperty,f=r("".slice),h=r("".replace),v=r([].join),b=s&&!a((function(){return 8!==g((function(){}),"length",{value:8}).length})),k=String(String).split("String"),y=n.exports=function(n,e,t){"Symbol("===f(m(e),0,7)&&(e="["+h(m(e),/^Symbol\(([^)]*)\)/,"$1")+"]"),t&&t.getter&&(e="get "+e),t&&t.setter&&(e="set "+e),(!i(n,"name")||l&&n.name!==e)&&(s?g(n,"name",{value:e,configurable:!0}):n.name=e),b&&t&&i(t,"arity")&&n.length!==t.arity&&g(n,"length",{value:t.arity});try{t&&i(t,"constructor")&&t.constructor?s&&g(n,"prototype",{writable:!1}):n.prototype&&(n.prototype=void 0)}catch(n){}var r=u(n);return i(r,"source")||(r.source=v(k,"string"==typeof e?e:"")),n};Function.prototype.toString=y((function(){return o(this)&&d(this).source||c(this)}),"toString")},function(n,e,t){var r=t(9),a=t(13),o=Function.prototype,i=r&&Object.getOwnPropertyDescriptor,s=a(o,"name"),l=s&&"something"===function(){}.name,c=s&&(!r||r&&i(o,"name").configurable);n.exports={EXISTS:s,PROPER:l,CONFIGURABLE:c}},function(n,e,t){var r=t(7),a=t(4),o=t(48),i=r(Function.toString);a(o.inspectSource)||(o.inspectSource=function(n){return i(n)}),n.exports=o.inspectSource},function(n,e,t){var r,a,o,i=t(149),s=t(8),l=t(14),c=t(24),p=t(13),u=t(48),d=t(84),m=t(50),g=s.TypeError,f=s.WeakMap;if(i||u.state){var h=u.state||(u.state=new f);h.get=h.get,h.has=h.has,h.set=h.set,r=function(n,e){if(h.has(n))throw g("Object already initialized");return e.facade=n,h.set(n,e),e},a=function(n){return h.get(n)||{}},o=function(n){return h.has(n)}}else{var v=d("state");m[v]=!0,r=function(n,e){if(p(n,v))throw g("Object already initialized");return e.facade=n,c(n,v,e),e},a=function(n){return p(n,v)?n[v]:{}},o=function(n){return p(n,v)}}n.exports={set:r,get:a,has:o,enforce:function(n){return o(n)?a(n):r(n,{})},getterFor:function(n){return function(e){var t;if(!l(e)||(t=a(e)).type!==n)throw g("Incompatible receiver, "+n+" required");return t}}}},function(n,e,t){var r=t(8),a=t(4),o=r.WeakMap;n.exports=a(o)&&/native code/.test(String(o))},function(n,e,t){var r=t(32),a=t(7),o=t(151),i=t(156),s=t(25),l=a([].concat);n.exports=r("Reflect","ownKeys")||function(n){var e=o.f(s(n)),t=i.f;return t?l(e,t(n)):e}},function(n,e,t){var r=t(86),a=t(51).concat("length","prototype");e.f=Object.getOwnPropertyNames||function(n){return r(n,a)}},function(n,e,t){var r=t(30),a=t(153),o=t(35),i=function(n){return function(e,t,i){var s,l=r(e),c=o(l),p=a(i,c);if(n&&t!=t){for(;c>p;)if((s=l[p++])!=s)return!0}else for(;c>p;p++)if((n||p in l)&&l[p]===t)return n||p||0;return!n&&-1}};n.exports={includes:i(!0),indexOf:i(!1)}},function(n,e,t){var r=t(34),a=Math.max,o=Math.min;n.exports=function(n,e){var t=r(n);return t<0?a(t+e,0):o(t,e)}},function(n,e){var t=Math.ceil,r=Math.floor;n.exports=Math.trunc||function(n){var e=+n;return(e>0?r:t)(e)}},function(n,e,t){var r=t(34),a=Math.min;n.exports=function(n){return n>0?a(r(n),9007199254740991):0}},function(n,e){e.f=Object.getOwnPropertySymbols},function(n,e,t){var r=t(6),a=t(4),o=/#|\.prototype\./,i=function(n,e){var t=l[s(n)];return t==p||t!=c&&(a(e)?r(e):!!e)},s=i.normalize=function(n){return String(n).replace(o,".").toLowerCase()},l=i.data={},c=i.NATIVE="N",p=i.POLYFILL="P";n.exports=i},function(n,e,t){var r=t(47),a=t(33),o=t(69),i=t(35),s=TypeError,l=function(n){return function(e,t,l,c){r(t);var p=a(e),u=o(p),d=i(p),m=n?d-1:0,g=n?-1:1;if(l<2)for(;;){if(m in u){c=u[m],m+=g;break}if(m+=g,n?m<0:d<=m)throw s("Reduce of empty array with no initial value")}for(;n?m>=0:d>m;m+=g)m in u&&(c=t(c,u[m],m,p));return c}};n.exports={left:l(!1),right:l(!0)}},function(n,e,t){"use strict";var r=t(6);n.exports=function(n,e){var t=[][n];return!!t&&r((function(){t.call(null,e||function(){return 1},1)}))}},function(n,e,t){var r=t(31);n.exports="undefined"!=typeof process&&"process"==r(process)},function(n,e,t){"use strict";var r=t(9),a=t(162),o=TypeError,i=Object.getOwnPropertyDescriptor,s=r&&!function(){if(void 0!==this)return!0;try{Object.defineProperty([],"length",{writable:!1}).length=1}catch(n){return n instanceof TypeError}}();n.exports=s?function(n,e){if(a(n)&&!i(n,"length").writable)throw o("Cannot set read only .length");return n.length=e}:function(n,e){return n.length=e}},function(n,e,t){var r=t(31);n.exports=Array.isArray||function(n){return"Array"==r(n)}},function(n,e){var t=TypeError;n.exports=function(n){if(n>9007199254740991)throw t("Maximum allowed index exceeded");return n}},function(n,e,t){var r=t(44),a=Function.prototype,o=a.apply,i=a.call;n.exports="object"==typeof Reflect&&Reflect.apply||(r?i.bind(o):function(){return i.apply(o,arguments)})},function(n,e,t){"use strict";var r=t(32),a=t(13),o=t(24),i=t(74),s=t(87),l=t(85),c=t(168),p=t(169),u=t(170),d=t(173),m=t(174),g=t(9),f=t(79);n.exports=function(n,e,t,h){var v=h?2:1,b=n.split("."),k=b[b.length-1],y=r.apply(null,b);if(y){var S=y.prototype;if(!f&&a(S,"cause")&&delete S.cause,!t)return y;var x=r("Error"),w=e((function(n,e){var t=u(h?e:n,void 0),r=h?new y(n):new y;return void 0!==t&&o(r,"message",t),m(r,w,r.stack,2),this&&i(S,this)&&p(r,this,w),arguments.length>v&&d(r,arguments[v]),r}));if(w.prototype=S,"Error"!==k?s?s(w,x):l(w,x,{name:!0}):g&&"stackTraceLimit"in y&&(c(w,y,"stackTraceLimit"),c(w,y,"prepareStackTrace")),l(w,y),!f)try{S.name!==k&&o(S,"name",k),S.constructor=w}catch(n){}return w}}},function(n,e,t){var r=t(7),a=t(47);n.exports=function(n,e,t){try{return r(a(Object.getOwnPropertyDescriptor(n,e)[t]))}catch(n){}}},function(n,e,t){var r=t(4),a=String,o=TypeError;n.exports=function(n){if("object"==typeof n||r(n))return n;throw o("Can't set "+a(n)+" as a prototype")}},function(n,e,t){var r=t(19).f;n.exports=function(n,e,t){t in n||r(n,t,{configurable:!0,get:function(){return e[t]},set:function(n){e[t]=n}})}},function(n,e,t){var r=t(4),a=t(14),o=t(87);n.exports=function(n,e,t){var i,s;return o&&r(i=e.constructor)&&i!==t&&a(s=i.prototype)&&s!==t.prototype&&o(n,s),n}},function(n,e,t){var r=t(88);n.exports=function(n,e){return void 0===n?arguments.length<2?"":e:r(n)}},function(n,e,t){var r=t(172),a=t(4),o=t(31),i=t(29)("toStringTag"),s=Object,l="Arguments"==o(function(){return arguments}());n.exports=r?o:function(n){var e,t,r;return void 0===n?"Undefined":null===n?"Null":"string"==typeof(t=function(n,e){try{return n[e]}catch(n){}}(e=s(n),i))?t:l?o(e):"Object"==(r=o(e))&&a(e.callee)?"Arguments":r}},function(n,e,t){var r={};r[t(29)("toStringTag")]="z",n.exports="[object z]"===String(r)},function(n,e,t){var r=t(14),a=t(24);n.exports=function(n,e){r(e)&&"cause"in e&&a(n,"cause",e.cause)}},function(n,e,t){var r=t(24),a=t(175),o=t(176),i=Error.captureStackTrace;n.exports=function(n,e,t,s){o&&(i?i(n,e):r(n,"stack",a(t,s)))}},function(n,e,t){var r=t(7),a=Error,o=r("".replace),i=String(a("zxcasd").stack),s=/\n\s*at [^:]*:[^\n]*/,l=s.test(i);n.exports=function(n,e){if(l&&"string"==typeof n&&!a.prepareStackTrace)for(;e--;)n=o(n,s,"");return n}},function(n,e,t){var r=t(6),a=t(45);n.exports=!r((function(){var n=Error("a");return!("stack"in n)||(Object.defineProperty(n,"stack",a(1,7)),7!==n.stack)}))},function(n,e,t){var r=t(89),a=t(178);n.exports=function n(e,t,o,i,s){var l=-1,c=e.length;for(o||(o=a),s||(s=[]);++l<c;){var p=e[l];t>0&&o(p)?t>1?n(p,t-1,o,i,s):r(s,p):i||(s[s.length]=p)}return s}},function(n,e,t){var r=t(26),a=t(52),o=t(12),i=r?r.isConcatSpreadable:void 0;n.exports=function(n){return o(n)||a(n)||!!(i&&n&&n[i])}},function(n,e,t){var r=t(27),a=t(20);n.exports=function(n){return a(n)&&"[object Arguments]"==r(n)}},function(n,e,t){var r=t(26),a=Object.prototype,o=a.hasOwnProperty,i=a.toString,s=r?r.toStringTag:void 0;n.exports=function(n){var e=o.call(n,s),t=n[s];try{n[s]=void 0;var r=!0}catch(n){}var a=i.call(n);return r&&(e?n[s]=t:delete n[s]),a}},function(n,e){var t=Object.prototype.toString;n.exports=function(n){return t.call(n)}},function(n,e,t){var r=t(183),a=t(239),o=t(60),i=t(12),s=t(250);n.exports=function(n){return"function"==typeof n?n:null==n?o:"object"==typeof n?i(n)?a(n[0],n[1]):r(n):s(n)}},function(n,e,t){var r=t(184),a=t(238),o=t(106);n.exports=function(n){var e=a(n);return 1==e.length&&e[0][2]?o(e[0][0],e[0][1]):function(t){return t===n||r(t,n,e)}}},function(n,e,t){var r=t(91),a=t(95);n.exports=function(n,e,t,o){var i=t.length,s=i,l=!o;if(null==n)return!s;for(n=Object(n);i--;){var c=t[i];if(l&&c[2]?c[1]!==n[c[0]]:!(c[0]in n))return!1}for(;++i<s;){var p=(c=t[i])[0],u=n[p],d=c[1];if(l&&c[2]){if(void 0===u&&!(p in n))return!1}else{var m=new r;if(o)var g=o(u,d,p,n,e,m);if(!(void 0===g?a(d,u,3,o,m):g))return!1}}return!0}},function(n,e){n.exports=function(){this.__data__=[],this.size=0}},function(n,e,t){var r=t(37),a=Array.prototype.splice;n.exports=function(n){var e=this.__data__,t=r(e,n);return!(t<0)&&(t==e.length-1?e.pop():a.call(e,t,1),--this.size,!0)}},function(n,e,t){var r=t(37);n.exports=function(n){var e=this.__data__,t=r(e,n);return t<0?void 0:e[t][1]}},function(n,e,t){var r=t(37);n.exports=function(n){return r(this.__data__,n)>-1}},function(n,e,t){var r=t(37);n.exports=function(n,e){var t=this.__data__,a=r(t,n);return a<0?(++this.size,t.push([n,e])):t[a][1]=e,this}},function(n,e,t){var r=t(36);n.exports=function(){this.__data__=new r,this.size=0}},function(n,e){n.exports=function(n){var e=this.__data__,t=e.delete(n);return this.size=e.size,t}},function(n,e){n.exports=function(n){return this.__data__.get(n)}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e,t){var r=t(36),a=t(53),o=t(55);n.exports=function(n,e){var t=this.__data__;if(t instanceof r){var i=t.__data__;if(!a||i.length<199)return i.push([n,e]),this.size=++t.size,this;t=this.__data__=new o(i)}return t.set(n,e),this.size=t.size,this}},function(n,e,t){var r=t(93),a=t(196),o=t(54),i=t(94),s=/^\[object .+?Constructor\]$/,l=Function.prototype,c=Object.prototype,p=l.toString,u=c.hasOwnProperty,d=RegExp("^"+p.call(u).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");n.exports=function(n){return!(!o(n)||a(n))&&(r(n)?d:s).test(i(n))}},function(n,e,t){var r,a=t(197),o=(r=/[^.]+$/.exec(a&&a.keys&&a.keys.IE_PROTO||""))?"Symbol(src)_1."+r:"";n.exports=function(n){return!!o&&o in n}},function(n,e,t){var r=t(11)["__core-js_shared__"];n.exports=r},function(n,e){n.exports=function(n,e){return null==n?void 0:n[e]}},function(n,e,t){var r=t(200),a=t(36),o=t(53);n.exports=function(){this.size=0,this.__data__={hash:new r,map:new(o||a),string:new r}}},function(n,e,t){var r=t(201),a=t(202),o=t(203),i=t(204),s=t(205);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var r=n[e];this.set(r[0],r[1])}}l.prototype.clear=r,l.prototype.delete=a,l.prototype.get=o,l.prototype.has=i,l.prototype.set=s,n.exports=l},function(n,e,t){var r=t(38);n.exports=function(){this.__data__=r?r(null):{},this.size=0}},function(n,e){n.exports=function(n){var e=this.has(n)&&delete this.__data__[n];return this.size-=e?1:0,e}},function(n,e,t){var r=t(38),a=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;if(r){var t=e[n];return"__lodash_hash_undefined__"===t?void 0:t}return a.call(e,n)?e[n]:void 0}},function(n,e,t){var r=t(38),a=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;return r?void 0!==e[n]:a.call(e,n)}},function(n,e,t){var r=t(38);n.exports=function(n,e){var t=this.__data__;return this.size+=this.has(n)?0:1,t[n]=r&&void 0===e?"__lodash_hash_undefined__":e,this}},function(n,e,t){var r=t(39);n.exports=function(n){var e=r(this,n).delete(n);return this.size-=e?1:0,e}},function(n,e){n.exports=function(n){var e=typeof n;return"string"==e||"number"==e||"symbol"==e||"boolean"==e?"__proto__"!==n:null===n}},function(n,e,t){var r=t(39);n.exports=function(n){return r(this,n).get(n)}},function(n,e,t){var r=t(39);n.exports=function(n){return r(this,n).has(n)}},function(n,e,t){var r=t(39);n.exports=function(n,e){var t=r(this,n),a=t.size;return t.set(n,e),this.size+=t.size==a?0:1,this}},function(n,e,t){var r=t(91),a=t(96),o=t(215),i=t(218),s=t(234),l=t(12),c=t(100),p=t(102),u="[object Object]",d=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,m,g,f){var h=l(n),v=l(e),b=h?"[object Array]":s(n),k=v?"[object Array]":s(e),y=(b="[object Arguments]"==b?u:b)==u,S=(k="[object Arguments]"==k?u:k)==u,x=b==k;if(x&&c(n)){if(!c(e))return!1;h=!0,y=!1}if(x&&!y)return f||(f=new r),h||p(n)?a(n,e,t,m,g,f):o(n,e,b,t,m,g,f);if(!(1&t)){var w=y&&d.call(n,"__wrapped__"),E=S&&d.call(e,"__wrapped__");if(w||E){var D=w?n.value():n,C=E?e.value():e;return f||(f=new r),g(D,C,t,m,f)}}return!!x&&(f||(f=new r),i(n,e,t,m,g,f))}},function(n,e){n.exports=function(n){return this.__data__.set(n,"__lodash_hash_undefined__"),this}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length;++t<r;)if(e(n[t],t,n))return!0;return!1}},function(n,e,t){var r=t(26),a=t(216),o=t(92),i=t(96),s=t(217),l=t(56),c=r?r.prototype:void 0,p=c?c.valueOf:void 0;n.exports=function(n,e,t,r,c,u,d){switch(t){case"[object DataView]":if(n.byteLength!=e.byteLength||n.byteOffset!=e.byteOffset)return!1;n=n.buffer,e=e.buffer;case"[object ArrayBuffer]":return!(n.byteLength!=e.byteLength||!u(new a(n),new a(e)));case"[object Boolean]":case"[object Date]":case"[object Number]":return o(+n,+e);case"[object Error]":return n.name==e.name&&n.message==e.message;case"[object RegExp]":case"[object String]":return n==e+"";case"[object Map]":var m=s;case"[object Set]":var g=1&r;if(m||(m=l),n.size!=e.size&&!g)return!1;var f=d.get(n);if(f)return f==e;r|=2,d.set(n,e);var h=i(m(n),m(e),r,c,u,d);return d.delete(n),h;case"[object Symbol]":if(p)return p.call(n)==p.call(e)}return!1}},function(n,e,t){var r=t(11).Uint8Array;n.exports=r},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n,r){t[++e]=[r,n]})),t}},function(n,e,t){var r=t(219),a=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,o,i,s){var l=1&t,c=r(n),p=c.length;if(p!=r(e).length&&!l)return!1;for(var u=p;u--;){var d=c[u];if(!(l?d in e:a.call(e,d)))return!1}var m=s.get(n),g=s.get(e);if(m&&g)return m==e&&g==n;var f=!0;s.set(n,e),s.set(e,n);for(var h=l;++u<p;){var v=n[d=c[u]],b=e[d];if(o)var k=l?o(b,v,d,e,n,s):o(v,b,d,n,e,s);if(!(void 0===k?v===b||i(v,b,t,o,s):k)){f=!1;break}h||(h="constructor"==d)}if(f&&!h){var y=n.constructor,S=e.constructor;y==S||!("constructor"in n)||!("constructor"in e)||"function"==typeof y&&y instanceof y&&"function"==typeof S&&S instanceof S||(f=!1)}return s.delete(n),s.delete(e),f}},function(n,e,t){var r=t(220),a=t(221),o=t(99);n.exports=function(n){return r(n,o,a)}},function(n,e,t){var r=t(89),a=t(12);n.exports=function(n,e,t){var o=e(n);return a(n)?o:r(o,t(n))}},function(n,e,t){var r=t(222),a=t(223),o=Object.prototype.propertyIsEnumerable,i=Object.getOwnPropertySymbols,s=i?function(n){return null==n?[]:(n=Object(n),r(i(n),(function(e){return o.call(n,e)})))}:a;n.exports=s},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,a=0,o=[];++t<r;){var i=n[t];e(i,t,n)&&(o[a++]=i)}return o}},function(n,e){n.exports=function(){return[]}},function(n,e,t){var r=t(225),a=t(52),o=t(12),i=t(100),s=t(101),l=t(102),c=Object.prototype.hasOwnProperty;n.exports=function(n,e){var t=o(n),p=!t&&a(n),u=!t&&!p&&i(n),d=!t&&!p&&!u&&l(n),m=t||p||u||d,g=m?r(n.length,String):[],f=g.length;for(var h in n)!e&&!c.call(n,h)||m&&("length"==h||u&&("offset"==h||"parent"==h)||d&&("buffer"==h||"byteLength"==h||"byteOffset"==h)||s(h,f))||g.push(h);return g}},function(n,e){n.exports=function(n,e){for(var t=-1,r=Array(n);++t<n;)r[t]=e(t);return r}},function(n,e){n.exports=function(){return!1}},function(n,e,t){var r=t(27),a=t(57),o=t(20),i={};i["[object Float32Array]"]=i["[object Float64Array]"]=i["[object Int8Array]"]=i["[object Int16Array]"]=i["[object Int32Array]"]=i["[object Uint8Array]"]=i["[object Uint8ClampedArray]"]=i["[object Uint16Array]"]=i["[object Uint32Array]"]=!0,i["[object Arguments]"]=i["[object Array]"]=i["[object ArrayBuffer]"]=i["[object Boolean]"]=i["[object DataView]"]=i["[object Date]"]=i["[object Error]"]=i["[object Function]"]=i["[object Map]"]=i["[object Number]"]=i["[object Object]"]=i["[object RegExp]"]=i["[object Set]"]=i["[object String]"]=i["[object WeakMap]"]=!1,n.exports=function(n){return o(n)&&a(n.length)&&!!i[r(n)]}},function(n,e){n.exports=function(n){return function(e){return n(e)}}},function(n,e,t){(function(n){var r=t(90),a=e&&!e.nodeType&&e,o=a&&"object"==typeof n&&n&&!n.nodeType&&n,i=o&&o.exports===a&&r.process,s=function(){try{var n=o&&o.require&&o.require("util").types;return n||i&&i.binding&&i.binding("util")}catch(n){}}();n.exports=s}).call(this,t(66)(n))},function(n,e,t){var r=t(231),a=t(232),o=Object.prototype.hasOwnProperty;n.exports=function(n){if(!r(n))return a(n);var e=[];for(var t in Object(n))o.call(n,t)&&"constructor"!=t&&e.push(t);return e}},function(n,e){var t=Object.prototype;n.exports=function(n){var e=n&&n.constructor;return n===("function"==typeof e&&e.prototype||t)}},function(n,e,t){var r=t(233)(Object.keys,Object);n.exports=r},function(n,e){n.exports=function(n,e){return function(t){return n(e(t))}}},function(n,e,t){var r=t(235),a=t(53),o=t(236),i=t(104),s=t(237),l=t(27),c=t(94),p=c(r),u=c(a),d=c(o),m=c(i),g=c(s),f=l;(r&&"[object DataView]"!=f(new r(new ArrayBuffer(1)))||a&&"[object Map]"!=f(new a)||o&&"[object Promise]"!=f(o.resolve())||i&&"[object Set]"!=f(new i)||s&&"[object WeakMap]"!=f(new s))&&(f=function(n){var e=l(n),t="[object Object]"==e?n.constructor:void 0,r=t?c(t):"";if(r)switch(r){case p:return"[object DataView]";case u:return"[object Map]";case d:return"[object Promise]";case m:return"[object Set]";case g:return"[object WeakMap]"}return e}),n.exports=f},function(n,e,t){var r=t(15)(t(11),"DataView");n.exports=r},function(n,e,t){var r=t(15)(t(11),"Promise");n.exports=r},function(n,e,t){var r=t(15)(t(11),"WeakMap");n.exports=r},function(n,e,t){var r=t(105),a=t(99);n.exports=function(n){for(var e=a(n),t=e.length;t--;){var o=e[t],i=n[o];e[t]=[o,i,r(i)]}return e}},function(n,e,t){var r=t(95),a=t(240),o=t(247),i=t(58),s=t(105),l=t(106),c=t(40);n.exports=function(n,e){return i(n)&&s(e)?l(c(n),e):function(t){var i=a(t,n);return void 0===i&&i===e?o(t,n):r(e,i,3)}}},function(n,e,t){var r=t(107);n.exports=function(n,e,t){var a=null==n?void 0:r(n,e);return void 0===a?t:a}},function(n,e,t){var r=t(242),a=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,o=/\\(\\)?/g,i=r((function(n){var e=[];return 46===n.charCodeAt(0)&&e.push(""),n.replace(a,(function(n,t,r,a){e.push(r?a.replace(o,"$1"):t||n)})),e}));n.exports=i},function(n,e,t){var r=t(243);n.exports=function(n){var e=r(n,(function(n){return 500===t.size&&t.clear(),n})),t=e.cache;return e}},function(n,e,t){var r=t(55);function a(n,e){if("function"!=typeof n||null!=e&&"function"!=typeof e)throw new TypeError("Expected a function");var t=function(){var r=arguments,a=e?e.apply(this,r):r[0],o=t.cache;if(o.has(a))return o.get(a);var i=n.apply(this,r);return t.cache=o.set(a,i)||o,i};return t.cache=new(a.Cache||r),t}a.Cache=r,n.exports=a},function(n,e,t){var r=t(245);n.exports=function(n){return null==n?"":r(n)}},function(n,e,t){var r=t(26),a=t(246),o=t(12),i=t(59),s=r?r.prototype:void 0,l=s?s.toString:void 0;n.exports=function n(e){if("string"==typeof e)return e;if(o(e))return a(e,n)+"";if(i(e))return l?l.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(n,e){n.exports=function(n,e){for(var t=-1,r=null==n?0:n.length,a=Array(r);++t<r;)a[t]=e(n[t],t,n);return a}},function(n,e,t){var r=t(248),a=t(249);n.exports=function(n,e){return null!=n&&a(n,e,r)}},function(n,e){n.exports=function(n,e){return null!=n&&e in Object(n)}},function(n,e,t){var r=t(108),a=t(52),o=t(12),i=t(101),s=t(57),l=t(40);n.exports=function(n,e,t){for(var c=-1,p=(e=r(e,n)).length,u=!1;++c<p;){var d=l(e[c]);if(!(u=null!=n&&t(n,d)))break;n=n[d]}return u||++c!=p?u:!!(p=null==n?0:n.length)&&s(p)&&i(d,p)&&(o(n)||a(n))}},function(n,e,t){var r=t(251),a=t(252),o=t(58),i=t(40);n.exports=function(n){return o(n)?r(i(n)):a(n)}},function(n,e){n.exports=function(n){return function(e){return null==e?void 0:e[n]}}},function(n,e,t){var r=t(107);n.exports=function(n){return function(e){return r(e,n)}}},function(n,e,t){var r=t(60),a=t(254),o=t(256);n.exports=function(n,e){return o(a(n,e,r),n+"")}},function(n,e,t){var r=t(255),a=Math.max;n.exports=function(n,e,t){return e=a(void 0===e?n.length-1:e,0),function(){for(var o=arguments,i=-1,s=a(o.length-e,0),l=Array(s);++i<s;)l[i]=o[e+i];i=-1;for(var c=Array(e+1);++i<e;)c[i]=o[i];return c[e]=t(l),r(n,this,c)}}},function(n,e){n.exports=function(n,e,t){switch(t.length){case 0:return n.call(e);case 1:return n.call(e,t[0]);case 2:return n.call(e,t[0],t[1]);case 3:return n.call(e,t[0],t[1],t[2])}return n.apply(e,t)}},function(n,e,t){var r=t(257),a=t(260)(r);n.exports=a},function(n,e,t){var r=t(258),a=t(259),o=t(60),i=a?function(n,e){return a(n,"toString",{configurable:!0,enumerable:!1,value:r(e),writable:!0})}:o;n.exports=i},function(n,e){n.exports=function(n){return function(){return n}}},function(n,e,t){var r=t(15),a=function(){try{var n=r(Object,"defineProperty");return n({},"",{}),n}catch(n){}}();n.exports=a},function(n,e){var t=Date.now;n.exports=function(n){var e=0,r=0;return function(){var a=t(),o=16-(a-r);if(r=a,o>0){if(++e>=800)return arguments[0]}else e=0;return n.apply(void 0,arguments)}}},function(n,e,t){var r=t(97),a=t(262),o=t(267),i=t(98),s=t(268),l=t(56);n.exports=function(n,e,t){var c=-1,p=a,u=n.length,d=!0,m=[],g=m;if(t)d=!1,p=o;else if(u>=200){var f=e?null:s(n);if(f)return l(f);d=!1,p=i,g=new r}else g=e?[]:m;n:for(;++c<u;){var h=n[c],v=e?e(h):h;if(h=t||0!==h?h:0,d&&v==v){for(var b=g.length;b--;)if(g[b]===v)continue n;e&&g.push(v),m.push(h)}else p(g,v,t)||(g!==m&&g.push(v),m.push(h))}return m}},function(n,e,t){var r=t(263);n.exports=function(n,e){return!!(null==n?0:n.length)&&r(n,e,0)>-1}},function(n,e,t){var r=t(264),a=t(265),o=t(266);n.exports=function(n,e,t){return e==e?o(n,e,t):r(n,a,t)}},function(n,e){n.exports=function(n,e,t,r){for(var a=n.length,o=t+(r?1:-1);r?o--:++o<a;)if(e(n[o],o,n))return o;return-1}},function(n,e){n.exports=function(n){return n!=n}},function(n,e){n.exports=function(n,e,t){for(var r=t-1,a=n.length;++r<a;)if(n[r]===e)return r;return-1}},function(n,e){n.exports=function(n,e,t){for(var r=-1,a=null==n?0:n.length;++r<a;)if(t(e,n[r]))return!0;return!1}},function(n,e,t){var r=t(104),a=t(269),o=t(56),i=r&&1/o(new r([,-0]))[1]==1/0?function(n){return new r(n)}:a;n.exports=i},function(n,e){n.exports=function(){}},function(n,e,t){var r=t(103),a=t(20);n.exports=function(n){return a(n)&&r(n)}},function(n,e){n.exports=function(n){var e=null==n?0:n.length;return e?n[e-1]:void 0}},function(n,e,t){},function(n,e,t){"use strict";t(109)},function(n,e,t){},function(n,e,t){"use strict";t(110)},function(n,e,t){},function(n,e,t){"use strict";t(111)},function(n,e,t){"use strict";var r=t(23),a=t(33),o=t(35),i=t(34),s=t(279);r({target:"Array",proto:!0},{at:function(n){var e=a(this),t=o(e),r=i(n),s=r>=0?r:t+r;return s<0||s>=t?void 0:e[s]}}),s("at")},function(n,e,t){var r=t(29),a=t(280),o=t(19).f,i=r("unscopables"),s=Array.prototype;null==s[i]&&o(s,i,{configurable:!0,value:a(null)}),n.exports=function(n){s[i][n]=!0}},function(n,e,t){var r,a=t(25),o=t(281),i=t(51),s=t(50),l=t(283),c=t(82),p=t(84),u=p("IE_PROTO"),d=function(){},m=function(n){return"<script>"+n+"<\/script>"},g=function(n){n.write(m("")),n.close();var e=n.parentWindow.Object;return n=null,e},f=function(){try{r=new ActiveXObject("htmlfile")}catch(n){}var n,e;f="undefined"!=typeof document?document.domain&&r?g(r):((e=c("iframe")).style.display="none",l.appendChild(e),e.src=String("javascript:"),(n=e.contentWindow.document).open(),n.write(m("document.F=Object")),n.close(),n.F):g(r);for(var t=i.length;t--;)delete f.prototype[i[t]];return f()};s[u]=!0,n.exports=Object.create||function(n,e){var t;return null!==n?(d.prototype=a(n),t=new d,d.prototype=null,t[u]=n):t=f(),void 0===e?t:o.f(t,e)}},function(n,e,t){var r=t(9),a=t(83),o=t(19),i=t(25),s=t(30),l=t(282);e.f=r&&!a?Object.defineProperties:function(n,e){i(n);for(var t,r=s(e),a=l(e),c=a.length,p=0;c>p;)o.f(n,t=a[p++],r[t]);return n}},function(n,e,t){var r=t(86),a=t(51);n.exports=Object.keys||function(n){return r(n,a)}},function(n,e,t){var r=t(32);n.exports=r("document","documentElement")},function(n,e,t){"use strict";var r=t(23),a=t(7),o=t(46),i=t(34),s=t(88),l=t(6),c=a("".charAt);r({target:"String",proto:!0,forced:l((function(){return"\ud842"!=="".at(-2)}))},{at:function(n){var e=s(o(this)),t=e.length,r=i(n),a=r>=0?r:t+r;return a<0||a>=t?void 0:c(e,a)}})},function(n,e,t){"use strict";t(112)},function(n,e,t){
/*!
 * Valine v1.5.1
 * (c) 2017-2022 xCss
 * Released under the GPL-2.0 License.
 * Last Update: 2022-7-21 3:43:59 F10: PM
 */
n.exports=function(n){function e(r){if(t[r])return t[r].exports;var a=t[r]={i:r,l:!1,exports:{}};return n[r].call(a.exports,a,a.exports,e),a.l=!0,a.exports}var t={};return e.m=n,e.c=t,e.i=function(n){return n},e.d=function(n,t,r){e.o(n,t)||Object.defineProperty(n,t,{configurable:!1,enumerable:!0,get:r})},e.n=function(n){var t=n&&n.__esModule?function(){return n.default}:function(){return n};return e.d(t,"a",t),t},e.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},e.p="",e(e.s=108)}([function(n,e,t){"use strict";var r=SyntaxError,a=Function,o=TypeError,i=function(n){try{return a('"use strict"; return ('+n+").constructor;")()}catch(n){}},s=Object.getOwnPropertyDescriptor;if(s)try{s({},"")}catch(n){s=null}var l=function(){throw new o},c=s?function(){try{return l}catch(n){try{return s(arguments,"callee").get}catch(n){return l}}}():l,p=t(22)(),u=Object.getPrototypeOf||function(n){return n.__proto__},d={},m="undefined"==typeof Uint8Array?void 0:u(Uint8Array),g={"%AggregateError%":"undefined"==typeof AggregateError?void 0:AggregateError,"%Array%":Array,"%ArrayBuffer%":"undefined"==typeof ArrayBuffer?void 0:ArrayBuffer,"%ArrayIteratorPrototype%":p?u([][Symbol.iterator]()):void 0,"%AsyncFromSyncIteratorPrototype%":void 0,"%AsyncFunction%":d,"%AsyncGenerator%":d,"%AsyncGeneratorFunction%":d,"%AsyncIteratorPrototype%":d,"%Atomics%":"undefined"==typeof Atomics?void 0:Atomics,"%BigInt%":"undefined"==typeof BigInt?void 0:BigInt,"%Boolean%":Boolean,"%DataView%":"undefined"==typeof DataView?void 0:DataView,"%Date%":Date,"%decodeURI%":decodeURI,"%decodeURIComponent%":decodeURIComponent,"%encodeURI%":encodeURI,"%encodeURIComponent%":encodeURIComponent,"%Error%":Error,"%eval%":eval,"%EvalError%":EvalError,"%Float32Array%":"undefined"==typeof Float32Array?void 0:Float32Array,"%Float64Array%":"undefined"==typeof Float64Array?void 0:Float64Array,"%FinalizationRegistry%":"undefined"==typeof FinalizationRegistry?void 0:FinalizationRegistry,"%Function%":a,"%GeneratorFunction%":d,"%Int8Array%":"undefined"==typeof Int8Array?void 0:Int8Array,"%Int16Array%":"undefined"==typeof Int16Array?void 0:Int16Array,"%Int32Array%":"undefined"==typeof Int32Array?void 0:Int32Array,"%isFinite%":isFinite,"%isNaN%":isNaN,"%IteratorPrototype%":p?u(u([][Symbol.iterator]())):void 0,"%JSON%":"object"==typeof JSON?JSON:void 0,"%Map%":"undefined"==typeof Map?void 0:Map,"%MapIteratorPrototype%":"undefined"!=typeof Map&&p?u((new Map)[Symbol.iterator]()):void 0,"%Math%":Math,"%Number%":Number,"%Object%":Object,"%parseFloat%":parseFloat,"%parseInt%":parseInt,"%Promise%":"undefined"==typeof Promise?void 0:Promise,"%Proxy%":"undefined"==typeof Proxy?void 0:Proxy,"%RangeError%":RangeError,"%ReferenceError%":ReferenceError,"%Reflect%":"undefined"==typeof Reflect?void 0:Reflect,"%RegExp%":RegExp,"%Set%":"undefined"==typeof Set?void 0:Set,"%SetIteratorPrototype%":"undefined"!=typeof Set&&p?u((new Set)[Symbol.iterator]()):void 0,"%SharedArrayBuffer%":"undefined"==typeof SharedArrayBuffer?void 0:SharedArrayBuffer,"%String%":String,"%StringIteratorPrototype%":p?u(""[Symbol.iterator]()):void 0,"%Symbol%":p?Symbol:void 0,"%SyntaxError%":r,"%ThrowTypeError%":c,"%TypedArray%":m,"%TypeError%":o,"%Uint8Array%":"undefined"==typeof Uint8Array?void 0:Uint8Array,"%Uint8ClampedArray%":"undefined"==typeof Uint8ClampedArray?void 0:Uint8ClampedArray,"%Uint16Array%":"undefined"==typeof Uint16Array?void 0:Uint16Array,"%Uint32Array%":"undefined"==typeof Uint32Array?void 0:Uint32Array,"%URIError%":URIError,"%WeakMap%":"undefined"==typeof WeakMap?void 0:WeakMap,"%WeakRef%":"undefined"==typeof WeakRef?void 0:WeakRef,"%WeakSet%":"undefined"==typeof WeakSet?void 0:WeakSet},f=function n(e){var t;if("%AsyncFunction%"===e)t=i("async function () {}");else if("%GeneratorFunction%"===e)t=i("function* () {}");else if("%AsyncGeneratorFunction%"===e)t=i("async function* () {}");else if("%AsyncGenerator%"===e){var r=n("%AsyncGeneratorFunction%");r&&(t=r.prototype)}else if("%AsyncIteratorPrototype%"===e){var a=n("%AsyncGenerator%");a&&(t=u(a.prototype))}return g[e]=t,t},h={"%ArrayBufferPrototype%":["ArrayBuffer","prototype"],"%ArrayPrototype%":["Array","prototype"],"%ArrayProto_entries%":["Array","prototype","entries"],"%ArrayProto_forEach%":["Array","prototype","forEach"],"%ArrayProto_keys%":["Array","prototype","keys"],"%ArrayProto_values%":["Array","prototype","values"],"%AsyncFunctionPrototype%":["AsyncFunction","prototype"],"%AsyncGenerator%":["AsyncGeneratorFunction","prototype"],"%AsyncGeneratorPrototype%":["AsyncGeneratorFunction","prototype","prototype"],"%BooleanPrototype%":["Boolean","prototype"],"%DataViewPrototype%":["DataView","prototype"],"%DatePrototype%":["Date","prototype"],"%ErrorPrototype%":["Error","prototype"],"%EvalErrorPrototype%":["EvalError","prototype"],"%Float32ArrayPrototype%":["Float32Array","prototype"],"%Float64ArrayPrototype%":["Float64Array","prototype"],"%FunctionPrototype%":["Function","prototype"],"%Generator%":["GeneratorFunction","prototype"],"%GeneratorPrototype%":["GeneratorFunction","prototype","prototype"],"%Int8ArrayPrototype%":["Int8Array","prototype"],"%Int16ArrayPrototype%":["Int16Array","prototype"],"%Int32ArrayPrototype%":["Int32Array","prototype"],"%JSONParse%":["JSON","parse"],"%JSONStringify%":["JSON","stringify"],"%MapPrototype%":["Map","prototype"],"%NumberPrototype%":["Number","prototype"],"%ObjectPrototype%":["Object","prototype"],"%ObjProto_toString%":["Object","prototype","toString"],"%ObjProto_valueOf%":["Object","prototype","valueOf"],"%PromisePrototype%":["Promise","prototype"],"%PromiseProto_then%":["Promise","prototype","then"],"%Promise_all%":["Promise","all"],"%Promise_reject%":["Promise","reject"],"%Promise_resolve%":["Promise","resolve"],"%RangeErrorPrototype%":["RangeError","prototype"],"%ReferenceErrorPrototype%":["ReferenceError","prototype"],"%RegExpPrototype%":["RegExp","prototype"],"%SetPrototype%":["Set","prototype"],"%SharedArrayBufferPrototype%":["SharedArrayBuffer","prototype"],"%StringPrototype%":["String","prototype"],"%SymbolPrototype%":["Symbol","prototype"],"%SyntaxErrorPrototype%":["SyntaxError","prototype"],"%TypedArrayPrototype%":["TypedArray","prototype"],"%TypeErrorPrototype%":["TypeError","prototype"],"%Uint8ArrayPrototype%":["Uint8Array","prototype"],"%Uint8ClampedArrayPrototype%":["Uint8ClampedArray","prototype"],"%Uint16ArrayPrototype%":["Uint16Array","prototype"],"%Uint32ArrayPrototype%":["Uint32Array","prototype"],"%URIErrorPrototype%":["URIError","prototype"],"%WeakMapPrototype%":["WeakMap","prototype"],"%WeakSetPrototype%":["WeakSet","prototype"]},v=t(9),b=t(25),k=v.call(Function.call,Array.prototype.concat),y=v.call(Function.apply,Array.prototype.splice),S=v.call(Function.call,String.prototype.replace),x=v.call(Function.call,String.prototype.slice),w=/[^%.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|%$))/g,E=/\\(\\)?/g,D=function(n){var e=x(n,0,1),t=x(n,-1);if("%"===e&&"%"!==t)throw new r("invalid intrinsic syntax, expected closing `%`");if("%"===t&&"%"!==e)throw new r("invalid intrinsic syntax, expected opening `%`");var a=[];return S(n,w,(function(n,e,t,r){a[a.length]=t?S(r,E,"$1"):e||n})),a},C=function(n,e){var t,a=n;if(b(h,a)&&(a="%"+(t=h[a])[0]+"%"),b(g,a)){var i=g[a];if(i===d&&(i=f(a)),void 0===i&&!e)throw new o("intrinsic "+n+" exists, but is not available. Please file an issue!");return{alias:t,name:a,value:i}}throw new r("intrinsic "+n+" does not exist!")};n.exports=function(n,e){if("string"!=typeof n||0===n.length)throw new o("intrinsic name must be a non-empty string");if(arguments.length>1&&"boolean"!=typeof e)throw new o('"allowMissing" argument must be a boolean');var t=D(n),a=t.length>0?t[0]:"",i=C("%"+a+"%",e),l=i.name,c=i.value,p=!1,u=i.alias;u&&(a=u[0],y(t,k([0,1],u)));for(var d=1,m=!0;d<t.length;d+=1){var f=t[d],h=x(f,0,1),v=x(f,-1);if(('"'===h||"'"===h||"`"===h||'"'===v||"'"===v||"`"===v)&&h!==v)throw new r("property names with quotes must have matching quotes");if("constructor"!==f&&m||(p=!0),b(g,l="%"+(a+="."+f)+"%"))c=g[l];else if(null!=c){if(!(f in c)){if(!e)throw new o("base intrinsic for "+n+" exists, but the property is not available.");return}if(s&&d+1>=t.length){var S=s(c,f);c=(m=!!S)&&"get"in S&&!("originalValue"in S.get)?S.get:c[f]}else m=b(c,f),c=c[f];m&&!p&&(g[l]=c)}}return c}},function(n,e,t){"use strict";var r=t(0),a=t(4),o=a(r("String.prototype.indexOf"));n.exports=function(n,e){var t=r(n,!!e);return"function"==typeof t&&o(n,".prototype.")>-1?a(t):t}},function(n,e,t){"use strict";var r=t(88),a="function"==typeof Symbol&&"symbol"==typeof Symbol("foo"),o=Object.prototype.toString,i=Array.prototype.concat,s=Object.defineProperty,l=s&&function(){var n={};try{for(var e in s(n,"x",{enumerable:!1,value:n}),n)return!1;return n.x===n}catch(n){return!1}}(),c=function(n,e,t,r){(!(e in n)||function(n){return"function"==typeof n&&"[object Function]"===o.call(n)}(r)&&r())&&(l?s(n,e,{configurable:!0,enumerable:!1,value:t,writable:!0}):n[e]=t)},p=function(n,e){var t=arguments.length>2?arguments[2]:{},o=r(e);a&&(o=i.call(o,Object.getOwnPropertySymbols(e)));for(var s=0;s<o.length;s+=1)c(n,o[s],e[o[s]],t[o[s]])};p.supportsDescriptors=!!l,n.exports=p},function(n,e,t){"use strict";function r(n){return n&&n.__esModule?n:{default:n}}e.__esModule=!0;var a="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},o=r(t(85)),i=r(t(49)),s=r(t(46)),l=r(t(48)),c=r(t(45)),p=document,u=navigator,d=/[&<>"'`\\]/g,m=RegExp(d.source),g=/&(?:amp|lt|gt|quot|#39|#x60|#x5c);/g,f=RegExp(g.source),h={"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&quot;","'":"&#39;","`":"&#x60;","\\":"&#x5c;"},v={};for(var b in h)v[h[b]]=b;var k=null;Array.prototype.forEach||(Array.prototype.forEach=function(n,e){var t,r;if(null==this)throw new TypeError(" this is null or not defined");var a=Object(this),o=a.length>>>0;if("function"!=typeof n)throw new TypeError(n+" is not a function");for(arguments.length>1&&(t=e),r=0;r<o;){var i;r in a&&(i=a[r],n.call(t,i,r,a)),r++}}),window.NodeList&&!NodeList.prototype.forEach&&(NodeList.prototype.forEach=Array.prototype.forEach),String.prototype.trim||(String.prototype.trim=function(){return this.replace(/^[\s\uFEFF\xA0]+|[\s\uFEFF\xA0]+$/g,"")}),(0,o.default)(i.default.fn,{prepend:function(n){return n instanceof HTMLElement||(n=n[0]),this.forEach((function(e){e.insertAdjacentElement("afterBegin",n)})),this},append:function(n){return n instanceof HTMLElement||(n=n[0]),this.forEach((function(e){e.insertAdjacentElement("beforeEnd",n)})),this},remove:function(){return this.forEach((function(n){try{n.parentNode.removeChild(n)}catch(n){}})),this},find:function(n){return(0,i.default)(n,this)},show:function(){return this.forEach((function(n){n.style.display="block"})),this},hide:function(){return this.forEach((function(n){n.style.display="none"})),this},on:function(n,e,t){return i.default.fn.off(n,e,t),this.forEach((function(r){n.split(" ").forEach((function(n){r.addEventListener?r.addEventListener(n,e,t||!1):r.attachEvent?r.attachEvent("on"+n,e):r["on"+n]=e}))})),this},off:function(n,e,t){return this.forEach((function(r){n.split(" ").forEach((function(n){r.removeEventListener?r.removeEventListener(n,e,t||!1):r.detachEvent?r.detachEvent("on"+n,e):r["on"+n]=null}))})),this},html:function(n){return void 0!==n?(this.forEach((function(e){e.innerHTML=n})),this):this[0].innerHTML},text:function(n){return void 0!==n?(this.forEach((function(e){e.innerText=n})),this):this[0].innerText},empty:function(n){return n=n||0,this.forEach((function(e){setTimeout((function(n){e.innerText=""}),n)})),this},val:function(n){return void 0!==n?(this.forEach((function(e){e.value=n})),this):this[0].value||""},attr:function(){var n=arguments;if("object"==a(arguments[0])){var e=arguments[0],t=this;return Object.keys(e).forEach((function(n){t.forEach((function(t){t.setAttribute(n,e[n])}))})),this}return"string"==typeof arguments[0]&&arguments.length<2?this[0].getAttribute(arguments[0])||"":(this.forEach((function(e){e.setAttribute(n[0],n[1])})),this)},removeAttr:function(n){return this.forEach((function(e){var t,r=0,a=n&&n.match(/[^\x20\t\r\n\f\*\/\\]+/g);if(a&&1===e.nodeType)for(;t=a[r++];)e.removeAttribute(t)})),this},hasClass:function(n){return!!this[0]&&new RegExp("(\\s|^)"+n+"(\\s|$)").test(this[0].getAttribute("class"))},addClass:function(n){return this.forEach((function(e){var t=(0,i.default)(e),r=t.attr("class");t.hasClass(n)||t.attr("class",r+=" "+n)})),this},removeClass:function(n){return this.forEach((function(e){var t=(0,i.default)(e),r=t.attr("class");if(t.hasClass(n)){var a=new RegExp("(\\s|^)"+n+"(\\s|$)");t.attr("class",r.replace(a,""))}})),this}}),(0,o.default)(i.default,{extend:o.default,noop:function(){},navi:u,ua:u.userAgent,lang:u.language||u.languages[0],detect:s.default,store:l.default,escape:function(n){return n&&m.test(n)?n.replace(d,(function(n){return h[n]})):n},unescape:function(n){return n&&f.test(n)?n.replace(g,(function(n){return v[n]})):n},dynamicLoadSource:function(n,e){if((0,i.default)('script[src="'+n+'"]').length)e&&e();else{var t=p.createElement("script");t.onload=t.onreadystatechange=function(){this.onload=this.onreadystatechange=null,e&&e(),(0,i.default)(t).remove()},t.async=!0,t.setAttribute("referrerPolicy","no-referrer"),(0,i.default)("head")[0].appendChild(t),t.src=n}},sdkLoader:function(n,e,t){e in window&&window[e]?(k&&clearTimeout(k),t&&t()):i.default.dynamicLoadSource(n,(function(){k=setTimeout(i.default.sdkLoader(n,e,t),100)}))},deleteInWin:function(n,e){var t=function(e){if(n in window)try{delete window[n]}catch(e){window[n]=null}};0===e?t():setTimeout(t,e||500)},ajax:c.default}),e.default=i.default},function(n,e,t){"use strict";var r=t(9),a=t(0),o=a("%Function.prototype.apply%"),i=a("%Function.prototype.call%"),s=a("%Reflect.apply%",!0)||r.call(i,o),l=a("%Object.getOwnPropertyDescriptor%",!0),c=a("%Object.defineProperty%",!0),p=a("%Math.max%");if(c)try{c({},"a",{value:1})}catch(n){c=null}n.exports=function(n){var e=s(r,i,arguments);return l&&c&&l(e,"length").configurable&&c(e,"length",{value:1+p(0,n.length-(arguments.length-1))}),e};var u=function(){return s(r,o,arguments)};c?c(n.exports,"apply",{value:u}):n.exports.apply=u},function(n,e,t){"use strict";n.exports=t(62)},function(n,e,t){"use strict";e.__esModule=!0,e.DEFAULT_EMOJI_CDN="//img.t.sinajs.cn/t4/appstyle/expression/ext/normal/",e.DB_NAME="Comment",e.CONFIG={lang:"zh-CN",langMode:null,appId:"",appKey:"",clazzName:"Comment",meta:["nick","mail","link"],path:location.pathname,placeholder:"Just Go Go",pageSize:10,recordIP:!0,serverURLs:"",visitor:!1,mathJax:!1,emojiCDN:"",emojiMaps:void 0,enableQQ:!1,requiredFields:[]},e.defaultMeta=["nick","mail","link"],e.QQCacheKey="_v_Cache_Q",e.MetaCacheKey="_v_Cache_Meta",e.RandomStr=function(n){return(Date.now()+Math.round(1e3*Math.random())).toString(32)},e.VERSION="1.5.1"},function(n,e,t){var r=t(16),a=t(50);for(var o in(e=n.exports=function(n,e){return new a(e).process(n)}).FilterCSS=a,r)e[o]=r[o];"undefined"!=typeof window&&(window.filterCSS=n.exports)},function(n,e,t){"use strict";var r=t(66);n.exports=function(n){return"symbol"==typeof n?"Symbol":"bigint"==typeof n?"BigInt":r(n)}},function(n,e,t){"use strict";var r=t(78);n.exports=Function.prototype.bind||r},function(n,e,t){"use strict";var r,a,o=Function.prototype.toString,i="object"==typeof Reflect&&null!==Reflect&&Reflect.apply;if("function"==typeof i&&"function"==typeof Object.defineProperty)try{r=Object.defineProperty({},"length",{get:function(){throw a}}),a={},i((function(){throw 42}),null,r)}catch(n){n!==a&&(i=null)}else i=null;var s=/^\s*class\b/,l=function(n){try{var e=o.call(n);return s.test(e)}catch(n){return!1}},c=Object.prototype.toString,p="function"==typeof Symbol&&!!Symbol.toStringTag,u="object"==typeof document&&void 0===document.all&&void 0!==document.all?document.all:{};n.exports=i?function(n){if(n===u)return!0;if(!n)return!1;if("function"!=typeof n&&"object"!=typeof n)return!1;if("function"==typeof n&&!n.prototype)return!0;try{i(n,null,r)}catch(n){if(n!==a)return!1}return!l(n)}:function(n){if(n===u)return!0;if(!n)return!1;if("function"!=typeof n&&"object"!=typeof n)return!1;if("function"==typeof n&&!n.prototype)return!0;if(p)return function(n){try{return!l(n)&&(o.call(n),!0)}catch(n){return!1}}(n);if(l(n))return!1;var e=c.call(n);return"[object Function]"===e||"[object GeneratorFunction]"===e}},function(n,e){n.exports={indexOf:function(n,e){var t,r;if(Array.prototype.indexOf)return n.indexOf(e);for(t=0,r=n.length;t<r;t++)if(n[t]===e)return t;return-1},forEach:function(n,e,t){var r,a;if(Array.prototype.forEach)return n.forEach(e,t);for(r=0,a=n.length;r<a;r++)e.call(t,n[r],r,n)},trim:function(n){return String.prototype.trim?n.trim():n.replace(/(^\s*)|(\s*$)/g,"")},spaceIndex:function(n){var e=/\s|\n|\t/.exec(n);return e?e.index:-1}}},function(n,e,t){"use strict";e.__esModule=!0;var r={cdn:t(6).DEFAULT_EMOJI_CDN,maps:t(97),parse:function(n,e){return String(n).replace(new RegExp(":("+Object.keys(r.maps).join("|")+"):","ig"),(function(n,t){return r.maps[t]?r.build(t,e):n}))},build:function(n,e){var t=/^(https?:)?\/\//i,a=r.maps[n],o=t.test(a)?a:r.cdn+a,i=' <img alt="'+n+'" referrerpolicy="no-referrer" class="vemoji" src="'+o+'" />';return t.test(o)?i:""}};e.default=r},function(n,e,t){"use strict";e.__esModule=!0;var r=function(n){return n&&n.__esModule?n:{default:n}}(t(95));e.default=function(n){return(0,r.default)(n,{onTagAttr:function(n,e,t,r){return a(n,e,t,r)},onIgnoreTagAttr:function(n,e,t,r){return a(n,e,t,r)}}).replace(/\<\/?div\>/gi,"")};var a=function(n,e,t,a){if(/video|audio/i.test(n))return"";if(/code|pre|span/i.test(n)){if("style"==e){var o=t.match(/color:([#a-z0-9]{3,7}|\s+[#a-z0-9]{3,8})/gi);return o&&o.length?'style="'+o[0]+'"':""}if("class"==e)return e+"='"+r.default.escapeAttrValue(t)+"'"}return"a"===n&&"class"==e&&"at"===t?e+"='"+r.default.escapeAttrValue(t)+"'":"img"===n&&/src|class/i.test(e)?e+"='"+r.default.escapeAttrValue(t)+"' referrerPolicy='no-referrer'":void 0}},function(n,e,t){"use strict";var r=t(0),a=t(1),o=r("%TypeError%"),i=t(52),s=t(18),l=t(53),c=t(55),p=t(56),u=t(60),d=t(20),m=t(81),g=a("String.prototype.split"),f=Object("a"),h="a"!==f[0]||!(0 in f);n.exports=function(n){var e,t=u(this),r=h&&m(this)?g(this,""):t,a=p(r);if(!c(n))throw new o("Array.prototype.forEach callback must be a function");arguments.length>1&&(e=arguments[1]);for(var f=0;f<a;){var v=d(f);if(l(r,v)){var b=s(r,v);i(n,e,[b,f,r])}f+=1}}},function(n,e,t){"use strict";var r=t(75),a=t(14);n.exports=function(){var n=Array.prototype.forEach;return r(n)?n:a}},function(n,e){function t(){var n={"align-content":!1,"align-items":!1,"align-self":!1,"alignment-adjust":!1,"alignment-baseline":!1,all:!1,"anchor-point":!1,animation:!1,"animation-delay":!1,"animation-direction":!1,"animation-duration":!1,"animation-fill-mode":!1,"animation-iteration-count":!1,"animation-name":!1,"animation-play-state":!1,"animation-timing-function":!1,azimuth:!1,"backface-visibility":!1,background:!0,"background-attachment":!0,"background-clip":!0,"background-color":!0,"background-image":!0,"background-origin":!0,"background-position":!0,"background-repeat":!0,"background-size":!0,"baseline-shift":!1,binding:!1,bleed:!1,"bookmark-label":!1,"bookmark-level":!1,"bookmark-state":!1,border:!0,"border-bottom":!0,"border-bottom-color":!0,"border-bottom-left-radius":!0,"border-bottom-right-radius":!0,"border-bottom-style":!0,"border-bottom-width":!0,"border-collapse":!0,"border-color":!0,"border-image":!0,"border-image-outset":!0,"border-image-repeat":!0,"border-image-slice":!0,"border-image-source":!0,"border-image-width":!0,"border-left":!0,"border-left-color":!0,"border-left-style":!0,"border-left-width":!0,"border-radius":!0,"border-right":!0,"border-right-color":!0,"border-right-style":!0,"border-right-width":!0,"border-spacing":!0,"border-style":!0,"border-top":!0,"border-top-color":!0,"border-top-left-radius":!0,"border-top-right-radius":!0,"border-top-style":!0,"border-top-width":!0,"border-width":!0,bottom:!1,"box-decoration-break":!0,"box-shadow":!0,"box-sizing":!0,"box-snap":!0,"box-suppress":!0,"break-after":!0,"break-before":!0,"break-inside":!0,"caption-side":!1,chains:!1,clear:!0,clip:!1,"clip-path":!1,"clip-rule":!1,color:!0,"color-interpolation-filters":!0,"column-count":!1,"column-fill":!1,"column-gap":!1,"column-rule":!1,"column-rule-color":!1,"column-rule-style":!1,"column-rule-width":!1,"column-span":!1,"column-width":!1,columns:!1,contain:!1,content:!1,"counter-increment":!1,"counter-reset":!1,"counter-set":!1,crop:!1,cue:!1,"cue-after":!1,"cue-before":!1,cursor:!1,direction:!1,display:!0,"display-inside":!0,"display-list":!0,"display-outside":!0,"dominant-baseline":!1,elevation:!1,"empty-cells":!1,filter:!1,flex:!1,"flex-basis":!1,"flex-direction":!1,"flex-flow":!1,"flex-grow":!1,"flex-shrink":!1,"flex-wrap":!1,float:!1,"float-offset":!1,"flood-color":!1,"flood-opacity":!1,"flow-from":!1,"flow-into":!1,font:!0,"font-family":!0,"font-feature-settings":!0,"font-kerning":!0,"font-language-override":!0,"font-size":!0,"font-size-adjust":!0,"font-stretch":!0,"font-style":!0,"font-synthesis":!0,"font-variant":!0,"font-variant-alternates":!0,"font-variant-caps":!0,"font-variant-east-asian":!0,"font-variant-ligatures":!0,"font-variant-numeric":!0,"font-variant-position":!0,"font-weight":!0,grid:!1,"grid-area":!1,"grid-auto-columns":!1,"grid-auto-flow":!1,"grid-auto-rows":!1,"grid-column":!1,"grid-column-end":!1,"grid-column-start":!1,"grid-row":!1,"grid-row-end":!1,"grid-row-start":!1,"grid-template":!1,"grid-template-areas":!1,"grid-template-columns":!1,"grid-template-rows":!1,"hanging-punctuation":!1,height:!0,hyphens:!1,icon:!1,"image-orientation":!1,"image-resolution":!1,"ime-mode":!1,"initial-letters":!1,"inline-box-align":!1,"justify-content":!1,"justify-items":!1,"justify-self":!1,left:!1,"letter-spacing":!0,"lighting-color":!0,"line-box-contain":!1,"line-break":!1,"line-grid":!1,"line-height":!1,"line-snap":!1,"line-stacking":!1,"line-stacking-ruby":!1,"line-stacking-shift":!1,"line-stacking-strategy":!1,"list-style":!0,"list-style-image":!0,"list-style-position":!0,"list-style-type":!0,margin:!0,"margin-bottom":!0,"margin-left":!0,"margin-right":!0,"margin-top":!0,"marker-offset":!1,"marker-side":!1,marks:!1,mask:!1,"mask-box":!1,"mask-box-outset":!1,"mask-box-repeat":!1,"mask-box-slice":!1,"mask-box-source":!1,"mask-box-width":!1,"mask-clip":!1,"mask-image":!1,"mask-origin":!1,"mask-position":!1,"mask-repeat":!1,"mask-size":!1,"mask-source-type":!1,"mask-type":!1,"max-height":!0,"max-lines":!1,"max-width":!0,"min-height":!0,"min-width":!0,"move-to":!1,"nav-down":!1,"nav-index":!1,"nav-left":!1,"nav-right":!1,"nav-up":!1,"object-fit":!1,"object-position":!1,opacity:!1,order:!1,orphans:!1,outline:!1,"outline-color":!1,"outline-offset":!1,"outline-style":!1,"outline-width":!1,overflow:!1,"overflow-wrap":!1,"overflow-x":!1,"overflow-y":!1,padding:!0,"padding-bottom":!0,"padding-left":!0,"padding-right":!0,"padding-top":!0,page:!1,"page-break-after":!1,"page-break-before":!1,"page-break-inside":!1,"page-policy":!1,pause:!1,"pause-after":!1,"pause-before":!1,perspective:!1,"perspective-origin":!1,pitch:!1,"pitch-range":!1,"play-during":!1,position:!1,"presentation-level":!1,quotes:!1,"region-fragment":!1,resize:!1,rest:!1,"rest-after":!1,"rest-before":!1,richness:!1,right:!1,rotation:!1,"rotation-point":!1,"ruby-align":!1,"ruby-merge":!1,"ruby-position":!1,"shape-image-threshold":!1,"shape-outside":!1,"shape-margin":!1,size:!1,speak:!1,"speak-as":!1,"speak-header":!1,"speak-numeral":!1,"speak-punctuation":!1,"speech-rate":!1,stress:!1,"string-set":!1,"tab-size":!1,"table-layout":!1,"text-align":!0,"text-align-last":!0,"text-combine-upright":!0,"text-decoration":!0,"text-decoration-color":!0,"text-decoration-line":!0,"text-decoration-skip":!0,"text-decoration-style":!0,"text-emphasis":!0,"text-emphasis-color":!0,"text-emphasis-position":!0,"text-emphasis-style":!0,"text-height":!0,"text-indent":!0,"text-justify":!0,"text-orientation":!0,"text-overflow":!0,"text-shadow":!0,"text-space-collapse":!0,"text-transform":!0,"text-underline-position":!0,"text-wrap":!0,top:!1,transform:!1,"transform-origin":!1,"transform-style":!1,transition:!1,"transition-delay":!1,"transition-duration":!1,"transition-property":!1,"transition-timing-function":!1,"unicode-bidi":!1,"vertical-align":!1,visibility:!1,"voice-balance":!1,"voice-duration":!1,"voice-family":!1,"voice-pitch":!1,"voice-range":!1,"voice-rate":!1,"voice-stress":!1,"voice-volume":!1,volume:!1,"white-space":!1,widows:!1,width:!0,"will-change":!1,"word-break":!0,"word-spacing":!0,"word-wrap":!0,"wrap-flow":!1,"wrap-through":!1,"writing-mode":!1,"z-index":!1};return n}var r=/javascript\s*\:/gim;e.whiteList=t(),e.getDefaultWhiteList=t,e.onAttr=function(n,e,t){},e.onIgnoreAttr=function(n,e,t){},e.safeAttrValue=function(n,e){return r.test(e)?"":e}},function(n,e){n.exports={indexOf:function(n,e){var t,r;if(Array.prototype.indexOf)return n.indexOf(e);for(t=0,r=n.length;t<r;t++)if(n[t]===e)return t;return-1},forEach:function(n,e,t){var r,a;if(Array.prototype.forEach)return n.forEach(e,t);for(r=0,a=n.length;r<a;r++)e.call(t,n[r],r,n)},trim:function(n){return String.prototype.trim?n.trim():n.replace(/(^\s*)|(\s*$)/g,"")},trimRight:function(n){return String.prototype.trimRight?n.trimRight():n.replace(/(\s*$)/g,"")}}},function(n,e,t){"use strict";var r=t(0)("%TypeError%"),a=t(86),o=t(19),i=t(8);n.exports=function(n,e){if("Object"!==i(n))throw new r("Assertion failed: Type(O) is not Object");if(!o(e))throw new r("Assertion failed: IsPropertyKey(P) is not true, got "+a(e));return n[e]}},function(n,e,t){"use strict";n.exports=function(n){return"string"==typeof n||"symbol"==typeof n}},function(n,e,t){"use strict";var r=t(0),a=r("%String%"),o=r("%TypeError%");n.exports=function(n){if("symbol"==typeof n)throw new o("Cannot convert a Symbol value to a string");return a(n)}},function(n,e,t){"use strict";n.exports=function(n){return null===n||"function"!=typeof n&&"object"!=typeof n}},function(n,e,t){"use strict";var r="undefined"!=typeof Symbol&&Symbol,a=t(23);n.exports=function(){return"function"==typeof r&&"function"==typeof Symbol&&"symbol"==typeof r("foo")&&"symbol"==typeof Symbol("bar")&&a()}},function(n,e,t){"use strict";n.exports=function(){if("function"!=typeof Symbol||"function"!=typeof Object.getOwnPropertySymbols)return!1;if("symbol"==typeof Symbol.iterator)return!0;var n={},e=Symbol("test"),t=Object(e);if("string"==typeof e)return!1;if("[object Symbol]"!==Object.prototype.toString.call(e))return!1;if("[object Symbol]"!==Object.prototype.toString.call(t))return!1;for(e in n[e]=42,n)return!1;if("function"==typeof Object.keys&&0!==Object.keys(n).length)return!1;if("function"==typeof Object.getOwnPropertyNames&&0!==Object.getOwnPropertyNames(n).length)return!1;var r=Object.getOwnPropertySymbols(n);if(1!==r.length||r[0]!==e)return!1;if(!Object.prototype.propertyIsEnumerable.call(n,e))return!1;if("function"==typeof Object.getOwnPropertyDescriptor){var a=Object.getOwnPropertyDescriptor(n,e);if(42!==a.value||!0!==a.enumerable)return!1}return!0}},function(n,e,t){"use strict";var r=t(23);n.exports=function(){return r()&&!!Symbol.toStringTag}},function(n,e,t){"use strict";var r=t(9);n.exports=r.call(Function.call,Object.prototype.hasOwnProperty)},function(n,e,t){"use strict";var r=Object.prototype.toString;n.exports=function(n){var e=r.call(n),t="[object Arguments]"===e;return t||(t="[object Array]"!==e&&null!==n&&"object"==typeof n&&"number"==typeof n.length&&n.length>=0&&"[object Function]"===r.call(n.callee)),t}},function(n,e,t){"use strict";var r=t(5),a=t(1),o=a("Object.prototype.propertyIsEnumerable"),i=a("Array.prototype.push");n.exports=function(n){var e=r(n),t=[];for(var a in e)o(e,a)&&i(t,[a,e[a]]);return t}},function(n,e,t){"use strict";var r=t(27);n.exports=function(){return"function"==typeof Object.entries?Object.entries:r}},function(n,e,t){"use strict";var r=t(5),a=t(20),o=t(1)("String.prototype.replace"),i=/^[\x09\x0A\x0B\x0C\x0D\x20\xA0\u1680\u180E\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200A\u202F\u205F\u3000\u2028\u2029\uFEFF]+/,s=/[\x09\x0A\x0B\x0C\x0D\x20\xA0\u1680\u180E\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200A\u202F\u205F\u3000\u2028\u2029\uFEFF]+$/;n.exports=function(){var n=a(r(this));return o(o(n,i,""),s,"")}},function(n,e,t){"use strict";var r=t(29);n.exports=function(){return String.prototype.trim&&""==="".trim()?String.prototype.trim:r}},function(n,e,t){function r(){return{a:["target","href","title"],abbr:["title"],address:[],area:["shape","coords","href","alt"],article:[],aside:[],audio:["autoplay","controls","crossorigin","loop","muted","preload","src"],b:[],bdi:["dir"],bdo:["dir"],big:[],blockquote:["cite"],br:[],caption:[],center:[],cite:[],code:[],col:["align","valign","span","width"],colgroup:["align","valign","span","width"],dd:[],del:["datetime"],details:["open"],div:[],dl:[],dt:[],em:[],figcaption:[],figure:[],font:["color","size","face"],footer:[],h1:[],h2:[],h3:[],h4:[],h5:[],h6:[],header:[],hr:[],i:[],img:["src","alt","title","width","height"],ins:["datetime"],li:[],mark:[],nav:[],ol:[],p:[],pre:[],s:[],section:[],small:[],span:[],sub:[],summary:[],sup:[],strong:[],strike:[],table:["width","border","align","valign"],tbody:["align","valign"],td:["width","rowspan","colspan","align","valign"],tfoot:["align","valign"],th:["width","rowspan","colspan","align","valign"],thead:["align","valign"],tr:["rowspan","align","valign"],tt:[],u:[],ul:[],video:["autoplay","controls","crossorigin","loop","muted","playsinline","poster","preload","src","height","width"]}}function a(n){return n.replace(h,"&lt;").replace(v,"&gt;")}function o(n){return n.replace(b,"&quot;")}function i(n){return n.replace(k,'"')}function s(n){return n.replace(y,(function(n,e){return"x"===e[0]||"X"===e[0]?String.fromCharCode(parseInt(e.substr(1),16)):String.fromCharCode(parseInt(e,10))}))}function l(n){return n.replace(S,":").replace(x," ")}function c(n){for(var e="",t=0,r=n.length;t<r;t++)e+=n.charCodeAt(t)<32?" ":n.charAt(t);return g.trim(e)}function p(n){return c(n=l(n=s(n=i(n))))}function u(n){return a(n=o(n))}var d=t(7).FilterCSS,m=t(7).getDefaultWhiteList,g=t(11),f=new d,h=/</g,v=/>/g,b=/"/g,k=/&quot;/g,y=/&#([a-zA-Z0-9]*);?/gim,S=/&colon;?/gim,x=/&newline;?/gim,w=/((j\s*a\s*v\s*a|v\s*b|l\s*i\s*v\s*e)\s*s\s*c\s*r\s*i\s*p\s*t\s*|m\s*o\s*c\s*h\s*a)\:/gi,E=/e\s*x\s*p\s*r\s*e\s*s\s*s\s*i\s*o\s*n\s*\(.*/gi,D=/u\s*r\s*l\s*\(.*/gi;e.whiteList={a:["target","href","title"],abbr:["title"],address:[],area:["shape","coords","href","alt"],article:[],aside:[],audio:["autoplay","controls","crossorigin","loop","muted","preload","src"],b:[],bdi:["dir"],bdo:["dir"],big:[],blockquote:["cite"],br:[],caption:[],center:[],cite:[],code:[],col:["align","valign","span","width"],colgroup:["align","valign","span","width"],dd:[],del:["datetime"],details:["open"],div:[],dl:[],dt:[],em:[],figcaption:[],figure:[],font:["color","size","face"],footer:[],h1:[],h2:[],h3:[],h4:[],h5:[],h6:[],header:[],hr:[],i:[],img:["src","alt","title","width","height"],ins:["datetime"],li:[],mark:[],nav:[],ol:[],p:[],pre:[],s:[],section:[],small:[],span:[],sub:[],summary:[],sup:[],strong:[],strike:[],table:["width","border","align","valign"],tbody:["align","valign"],td:["width","rowspan","colspan","align","valign"],tfoot:["align","valign"],th:["width","rowspan","colspan","align","valign"],thead:["align","valign"],tr:["rowspan","align","valign"],tt:[],u:[],ul:[],video:["autoplay","controls","crossorigin","loop","muted","playsinline","poster","preload","src","height","width"]},e.getDefaultWhiteList=r,e.onTag=function(n,e,t){},e.onIgnoreTag=function(n,e,t){},e.onTagAttr=function(n,e,t){},e.onIgnoreTagAttr=function(n,e,t){},e.safeAttrValue=function(n,e,t,r){if(t=p(t),"href"===e||"src"===e){if("#"===(t=g.trim(t)))return"#";if("http://"!==t.substr(0,7)&&"https://"!==t.substr(0,8)&&"mailto:"!==t.substr(0,7)&&"tel:"!==t.substr(0,4)&&"data:image/"!==t.substr(0,11)&&"ftp://"!==t.substr(0,6)&&"./"!==t.substr(0,2)&&"../"!==t.substr(0,3)&&"#"!==t[0]&&"/"!==t[0])return""}else if("background"===e){if(w.lastIndex=0,w.test(t))return""}else if("style"===e){if(E.lastIndex=0,E.test(t))return"";if(D.lastIndex=0,D.test(t)&&(w.lastIndex=0,w.test(t)))return"";!1!==r&&(t=(r=r||f).process(t))}return u(t)},e.escapeHtml=a,e.escapeQuote=o,e.unescapeQuote=i,e.escapeHtmlEntities=s,e.escapeDangerHtml5Entities=l,e.clearNonPrintableCharacter=c,e.friendlyAttrValue=p,e.escapeAttrValue=u,e.onIgnoreTagStripAll=function(){return""},e.StripTagBody=function(n,e){function t(e){return!!r||-1!==g.indexOf(n,e)}"function"!=typeof e&&(e=function(){});var r=!Array.isArray(n),a=[],o=!1;return{onIgnoreTag:function(n,r,i){if(t(n)){if(i.isClosing){var s="[/removed]",l=i.position+s.length;return a.push([!1!==o?o:i.position,l]),o=!1,s}return o||(o=i.position),"[removed]"}return e(n,r,i)},remove:function(n){var e="",t=0;return g.forEach(a,(function(r){e+=n.slice(t,r[0]),t=r[1]})),e+=n.slice(t)}}},e.stripCommentTag=function(n){for(var e="",t=0;t<n.length;){var r=n.indexOf("\x3c!--",t);if(-1===r){e+=n.slice(t);break}e+=n.slice(t,r);var a=n.indexOf("--\x3e",r);if(-1===a)break;t=a+3}return e},e.stripBlankChar=function(n){var e=n.split("");return(e=e.filter((function(n){var e=n.charCodeAt(0);return!(127===e||e<=31&&10!==e&&13!==e)}))).join("")},e.cssFilter=f,e.getDefaultCSSWhiteList=m},function(n,e,t){function r(n){var e=l.spaceIndex(n);if(-1===e)var t=n.slice(1,-1);else t=n.slice(1,e+1);return"/"===(t=l.trim(t).toLowerCase()).slice(0,1)&&(t=t.slice(1)),"/"===t.slice(-1)&&(t=t.slice(0,-1)),t}function a(n){return"</"===n.slice(0,2)}function o(n,e){for(;e<n.length;e++){var t=n[e];if(" "!==t)return"="===t?e:-1}}function i(n,e){for(;e>0;e--){var t=n[e];if(" "!==t)return"="===t?e:-1}}function s(n){return function(n){return'"'===n[0]&&'"'===n[n.length-1]||"'"===n[0]&&"'"===n[n.length-1]}(n)?n.substr(1,n.length-2):n}var l=t(11),c=/[^a-zA-Z0-9_:\.\-]/gim;e.parseTag=function(n,e,t){"use strict";var o="",i=0,s=!1,l=!1,c=0,p=n.length,u="",d="";n:for(c=0;c<p;c++){var m=n.charAt(c);if(!1===s){if("<"===m){s=c;continue}}else if(!1===l){if("<"===m){o+=t(n.slice(i,c)),s=c,i=c;continue}if(">"===m){o+=t(n.slice(i,s)),u=r(d=n.slice(s,c+1)),o+=e(s,o.length,u,d,a(d)),i=c+1,s=!1;continue}if('"'===m||"'"===m)for(var g=1,f=n.charAt(c-g);""===f.trim()||"="===f;){if("="===f){l=m;continue n}f=n.charAt(c-++g)}}else if(m===l){l=!1;continue}}return i<n.length&&(o+=t(n.substr(i))),o},e.parseAttr=function(n,e){"use strict";function t(n,t){if(!((n=(n=l.trim(n)).replace(c,"").toLowerCase()).length<1)){var r=e(n,t||"");r&&a.push(r)}}for(var r=0,a=[],p=!1,u=n.length,d=0;d<u;d++){var m,g=n.charAt(d);if(!1!==p||"="!==g)if(!1===p||d!==r||'"'!==g&&"'"!==g||"="!==n.charAt(d-1)){if(/\s|\n|\t/.test(g)){if(n=n.replace(/\s|\n|\t/g," "),!1===p){if(-1===(m=o(n,d))){t(l.trim(n.slice(r,d))),p=!1,r=d+1;continue}d=m-1;continue}if(-1===(m=i(n,d-1))){t(p,s(l.trim(n.slice(r,d)))),p=!1,r=d+1;continue}}}else{if(-1===(m=n.indexOf(g,d+1)))break;t(p,l.trim(n.slice(r+1,m))),p=!1,r=(d=m)+1}else p=n.slice(r,d),r=d+1}return r<n.length&&(!1===p?t(n.slice(r)):t(p,s(l.trim(n.slice(r))))),l.trim(a.join(" "))}},function(n,e,t){var r,a,o;
/*!
	autosize 4.0.4
	license: MIT
	http://www.jacklmoore.com/autosize
*/a=[n,e],r=function(n,e){"use strict";function t(n){function e(e){var t=n.style.width;n.style.width="0px",n.offsetWidth,n.style.width=t,n.style.overflowY=e}function t(){if(0!==n.scrollHeight){var e=function(n){for(var e=[];n&&n.parentNode&&n.parentNode instanceof Element;)n.parentNode.scrollTop&&e.push({node:n.parentNode,scrollTop:n.parentNode.scrollTop}),n=n.parentNode;return e}(n),t=document.documentElement&&document.documentElement.scrollTop;n.style.height="",n.style.height=n.scrollHeight+a+"px",s=n.clientWidth,e.forEach((function(n){n.node.scrollTop=n.scrollTop})),t&&(document.documentElement.scrollTop=t)}}function r(){t();var r=Math.round(parseFloat(n.style.height)),a=window.getComputedStyle(n,null),o="content-box"===a.boxSizing?Math.round(parseFloat(a.height)):n.offsetHeight;if(o<r?"hidden"===a.overflowY&&(e("scroll"),t(),o="content-box"===a.boxSizing?Math.round(parseFloat(window.getComputedStyle(n,null).height)):n.offsetHeight):"hidden"!==a.overflowY&&(e("hidden"),t(),o="content-box"===a.boxSizing?Math.round(parseFloat(window.getComputedStyle(n,null).height)):n.offsetHeight),l!==o){l=o;var s=i("autosize:resized");try{n.dispatchEvent(s)}catch(n){}}}if(n&&n.nodeName&&"TEXTAREA"===n.nodeName&&!o.has(n)){var a=null,s=null,l=null,c=function(){n.clientWidth!==s&&r()},p=function(e){window.removeEventListener("resize",c,!1),n.removeEventListener("input",r,!1),n.removeEventListener("keyup",r,!1),n.removeEventListener("autosize:destroy",p,!1),n.removeEventListener("autosize:update",r,!1),Object.keys(e).forEach((function(t){n.style[t]=e[t]})),o.delete(n)}.bind(n,{height:n.style.height,resize:n.style.resize,overflowY:n.style.overflowY,overflowX:n.style.overflowX,wordWrap:n.style.wordWrap});n.addEventListener("autosize:destroy",p,!1),"onpropertychange"in n&&"oninput"in n&&n.addEventListener("keyup",r,!1),window.addEventListener("resize",c,!1),n.addEventListener("input",r,!1),n.addEventListener("autosize:update",r,!1),n.style.overflowX="hidden",n.style.wordWrap="break-word",o.set(n,{destroy:p,update:r}),function(){var e=window.getComputedStyle(n,null);"vertical"===e.resize?n.style.resize="none":"both"===e.resize&&(n.style.resize="horizontal"),a="content-box"===e.boxSizing?-(parseFloat(e.paddingTop)+parseFloat(e.paddingBottom)):parseFloat(e.borderTopWidth)+parseFloat(e.borderBottomWidth),isNaN(a)&&(a=0),r()}()}}function r(n){var e=o.get(n);e&&e.destroy()}function a(n){var e=o.get(n);e&&e.update()}var o="function"==typeof Map?new Map:function(){var n=[],e=[];return{has:function(e){return n.indexOf(e)>-1},get:function(t){return e[n.indexOf(t)]},set:function(t,r){-1===n.indexOf(t)&&(n.push(t),e.push(r))},delete:function(t){var r=n.indexOf(t);r>-1&&(n.splice(r,1),e.splice(r,1))}}}(),i=function(n){return new Event(n,{bubbles:!0})};try{new Event("test")}catch(n){i=function(n){var e=document.createEvent("Event");return e.initEvent(n,!0,!1),e}}var s=null;"undefined"==typeof window||"function"!=typeof window.getComputedStyle?((s=function(n){return n}).destroy=function(n){return n},s.update=function(n){return n}):((s=function(n,e){return n&&Array.prototype.forEach.call(n.length?n:[n],(function(n){return t(n)})),n}).destroy=function(n){return n&&Array.prototype.forEach.call(n.length?n:[n],r),n},s.update=function(n){return n&&Array.prototype.forEach.call(n.length?n:[n],a),n}),e.default=s,n.exports=e.default},void 0!==(o="function"==typeof r?r.apply(e,a):r)&&(n.exports=o)},function(n,e,t){"use strict";function r(n){return n&&n.__esModule?n:{default:n}}function a(n){return!!n&&this.init(n),this}function o(n){return new a(n)}var i=r(t(42)),s=r(t(33)),l=r(t(37)),c=r(t(12)),p=t(6),u=r(t(41)),d=r(t(36)),m=t(40),g=r(t(38)),f=r(t(3)),h=r(t(39)),v=r(t(13)),b=(r(t(35)),{comment:"",nick:"",mail:"",link:"",ua:f.default.ua,url:"",QQAvatar:""}),k="",y={},S={cdn:"https://gravatar.loli.net/avatar/",ds:["mp","identicon","monsterid","wavatar","robohash","retro",""],params:"",hide:!1};a.prototype.init=function(n){if("undefined"==typeof document)throw new Error("Sorry, Valine does not support Server-side rendering.");var e=this;return n&&(n=f.default.extend(p.CONFIG,n),e.i18n=(0,l.default)(n.lang||f.default.lang,n.langMode),e.cfg=n,c.default.maps=!!n.emojiMaps&&n.emojiMaps||c.default.maps,c.default.cdn=!!n.emojiCDN&&n.emojiCDN||c.default.cdn,e._init()),e},a.prototype._init=function(){var n=this;try{var e=n.cfg,t=e.avatar,r=e.avatarForce,a=e.avatar_cdn,o=e.visitor,i=e.path,s=void 0===i?location.pathname:i,l=e.pageSize,c=e.recordIP;n.cfg.path=s.replace(/index\.html?$/,"");var u=S.ds,h=r?"&q="+(0,p.RandomStr)():"";S.params="?d="+(u.indexOf(t)>-1?t:"mp")+"&v="+p.VERSION+h,S.hide="hide"===t,S.cdn=/^https?\:\/\//.test(a)?a:S.cdn,n.cfg.pageSize=isNaN(l)||l<1?10:l,c&&(0,m.recordIPFn)((function(n){return b.ip=n}));var v=n.cfg.el||null,k=(0,f.default)(v);if(v=v instanceof HTMLElement?v:k[k.length-1]||null){n.$el=(0,f.default)(v),n.$el.addClass("v").attr("data-class","v"),S.hide&&n.$el.addClass("hide-avatar"),n.cfg.meta=(n.cfg.guest_info||n.cfg.meta||p.defaultMeta).filter((function(n){return p.defaultMeta.indexOf(n)>-1})),n.cfg.requiredFields=n.cfg.requiredFields.filter((function(n){return p.defaultMeta.indexOf(n)>-1}));var y=(0==n.cfg.meta.length?p.defaultMeta:n.cfg.meta).map((function(e){var t="mail"==e?"email":"text";return p.defaultMeta.indexOf(e)>-1?'<input name="'+e+'" placeholder="'+n.i18n.t(e)+'" class="v'+e+' vinput" type="'+t+'">':""})),x='<div class="vpanel"><div class="vwrap"><p class="cancel-reply text-right" style="display:none;" title="'+n.i18n.t("cancelReply")+'"><svg class="vicon cancel-reply-btn" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4220" width="22" height="22"><path d="M796.454 985H227.545c-50.183 0-97.481-19.662-133.183-55.363-35.7-35.701-55.362-83-55.362-133.183V227.545c0-50.183 19.662-97.481 55.363-133.183 35.701-35.7 83-55.362 133.182-55.362h568.909c50.183 0 97.481 19.662 133.183 55.363 35.701 35.702 55.363 83 55.363 133.183v568.909c0 50.183-19.662 97.481-55.363 133.183S846.637 985 796.454 985zM227.545 91C152.254 91 91 152.254 91 227.545v568.909C91 871.746 152.254 933 227.545 933h568.909C871.746 933 933 871.746 933 796.454V227.545C933 152.254 871.746 91 796.454 91H227.545z" p-id="4221"></path><path d="M568.569 512l170.267-170.267c15.556-15.556 15.556-41.012 0-56.569s-41.012-15.556-56.569 0L512 455.431 341.733 285.165c-15.556-15.556-41.012-15.556-56.569 0s-15.556 41.012 0 56.569L455.431 512 285.165 682.267c-15.556 15.556-15.556 41.012 0 56.569 15.556 15.556 41.012 15.556 56.569 0L512 568.569l170.267 170.267c15.556 15.556 41.012 15.556 56.569 0 15.556-15.556 15.556-41.012 0-56.569L568.569 512z" p-id="4222" ></path></svg></p><div class="vheader item'+y.length+'">'+y.join("")+'</div><div class="vedit"><textarea id="veditor" class="veditor vinput" placeholder="'+n.cfg.placeholder+'"></textarea><div class="vrow"><div class="vcol vcol-60 status-bar"></div><div class="vcol vcol-40 vctrl text-right"><span title="'+n.i18n.t("emoji")+'"  class="vicon vemoji-btn"><svg  viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="16172" width="22" height="22" ><path d="M512 1024a512 512 0 1 1 512-512 512 512 0 0 1-512 512zM512 56.888889a455.111111 455.111111 0 1 0 455.111111 455.111111 455.111111 455.111111 0 0 0-455.111111-455.111111zM312.888889 512A85.333333 85.333333 0 1 1 398.222222 426.666667 85.333333 85.333333 0 0 1 312.888889 512z" p-id="16173"></path><path d="M512 768A142.222222 142.222222 0 0 1 369.777778 625.777778a28.444444 28.444444 0 0 1 56.888889 0 85.333333 85.333333 0 0 0 170.666666 0 28.444444 28.444444 0 0 1 56.888889 0A142.222222 142.222222 0 0 1 512 768z" p-id="16174"></path><path d="M782.222222 391.964444l-113.777778 59.733334a29.013333 29.013333 0 0 1-38.684444-10.808889 28.444444 28.444444 0 0 1 10.24-38.684445l113.777778-56.888888a28.444444 28.444444 0 0 1 38.684444 10.24 28.444444 28.444444 0 0 1-10.24 36.408888z" p-id="16175"></path><path d="M640.568889 451.697778l113.777778 56.888889a27.875556 27.875556 0 0 0 38.684444-10.24 27.875556 27.875556 0 0 0-10.24-38.684445l-113.777778-56.888889a28.444444 28.444444 0 0 0-38.684444 10.808889 28.444444 28.444444 0 0 0 10.24 38.115556z" p-id="16176"></path></svg></span><span title="'+n.i18n.t("preview")+'" class="vicon vpreview-btn"><svg  viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="17688" width="22" height="22"><path d="M502.390154 935.384615a29.538462 29.538462 0 1 1 0 59.076923H141.430154C79.911385 994.461538 29.538462 946.254769 29.538462 886.153846V137.846154C29.538462 77.745231 79.950769 29.538462 141.390769 29.538462h741.218462c61.44 0 111.852308 48.206769 111.852307 108.307692v300.268308a29.538462 29.538462 0 1 1-59.076923 0V137.846154c0-26.899692-23.355077-49.230769-52.775384-49.230769H141.390769c-29.420308 0-52.775385 22.331077-52.775384 49.230769v748.307692c0 26.899692 23.355077 49.230769 52.775384 49.230769h360.999385z" p-id="17689"></path><path d="M196.923077 216.615385m29.538461 0l374.153847 0q29.538462 0 29.538461 29.538461l0 0q0 29.538462-29.538461 29.538462l-374.153847 0q-29.538462 0-29.538461-29.538462l0 0q0-29.538462 29.538461-29.538461Z" p-id="17690"></path><path d="M649.846154 846.769231a216.615385 216.615385 0 1 0 0-433.230769 216.615385 216.615385 0 0 0 0 433.230769z m0 59.076923a275.692308 275.692308 0 1 1 0-551.384616 275.692308 275.692308 0 0 1 0 551.384616z" p-id="17691"></path><path d="M807.398383 829.479768m20.886847-20.886846l0 0q20.886846-20.886846 41.773692 0l125.321079 125.321079q20.886846 20.886846 0 41.773693l0 0q-20.886846 20.886846-41.773693 0l-125.321078-125.321079q-20.886846-20.886846 0-41.773693Z" p-id="17692"></path></svg></span></div></div></div><div class="vrow"><div class="vcol vcol-30" ><a alt="Markdown is supported" href="https://guides.github.com/features/mastering-markdown/" class="vicon" target="_blank"><svg class="markdown" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14.85 3H1.15C.52 3 0 3.52 0 4.15v7.69C0 12.48.52 13 1.15 13h13.69c.64 0 1.15-.52 1.15-1.15v-7.7C16 3.52 15.48 3 14.85 3zM9 11H7V8L5.5 9.92 4 8v3H2V5h2l1.5 2L7 5h2v6zm2.99.5L9.5 8H11V5h2v3h1.5l-2.51 3.5z"></path></svg></a></div><div class="vcol vcol-70 text-right"><button type="button"  title="Cmd|Ctrl+Enter" class="vsubmit vbtn">'+n.i18n.t("submit")+'</button></div></div><div class="vemojis" style="display:none;"></div><div class="vinput vpreview" style="display:none;"></div></div></div><div class="vcount" style="display:none;"><span class="vnum">0</span> '+n.i18n.t("comments")+'</div><div class="vload-top text-center" style="display:none;"><i class="vspinner" style="width:30px;height:30px;"></i></div><div class="vcards"></div><div class="vload-bottom text-center" style="display:none;"><i class="vspinner" style="width:30px;height:30px;"></i></div><div class="vempty" style="display:none;"></div><div class="vpage txt-center" style="display:none"><button type="button" class="vmore vbtn">'+n.i18n.t("more")+'</button></div><div class="vpower txt-right">Powered By <a href="https://valine.js.org" target="_blank">Valine</a><br>v'+p.VERSION+"</div>";n.$el.html(x),n.$el.find(".cancel-reply").on("click",(function(e){n.reset()}));var E=n.$el.find(".vempty");n.$nodata={show:function(e){return E.html(e||n.i18n.t("sofa")).show(),n},hide:function(){return E.hide(),n}};var D=n.$el.find(".vload-bottom"),C=n.$el.find(".vload-top");n.$loading={show:function(e){return e&&C.show()||D.show(),n.$nodata.hide(),n},hide:function(){return C.hide(),D.hide(),0===n.$el.find(".vcard").length&&n.$nodata.show(),n}}}(0,d.default)(n.cfg,(function(e){var t=(0,f.default)(".valine-comment-count"),r=0;!function e(t){var a=t[r++];if(a){var o=(0,f.default)(a).attr("data-xid");o&&n.Q(o).count().then((function(n){a.innerText=n,e(t)})).catch((function(n){a.innerText=0}))}}(t),o&&w.add(AV.Object.extend("Counter"),n.cfg.path),n.$el&&n.bind()}))}catch(e){(0,g.default)(n,e,"init")}};var x=function(n,e){var t=new n,r=new AV.ACL;r.setPublicReadAccess(!0),r.setPublicWriteAccess(!0),t.setACL(r),t.set("url",e.url),t.set("xid",e.xid),t.set("title",e.title),t.set("time",1),t.save().then((function(n){(0,f.default)(e.el).find(".leancloud-visitors-count").text(1)})).catch((function(n){}))},w={add:function(n,e){var t=this,r=(0,f.default)(".leancloud_visitors,.leancloud-visitors");if(1===r.length){var a=r[0],o=decodeURI((0,f.default)(a).attr("id")),i=(0,f.default)(a).attr("data-flag-title"),s=encodeURI(o),l={el:a,url:o,xid:s,title:i};if(decodeURI(o)===decodeURI(e)){var c=new AV.Query(n);c.equalTo("url",o),c.find().then((function(e){if(e.length>0){var t=e[0];t.increment("time"),t.save().then((function(n){(0,f.default)(a).find(".leancloud-visitors-count").text(n.get("time"))})).catch((function(n){}))}else x(n,l)})).catch((function(e){101==e.code?x(n,l):(0,g.default)(t,e)}))}else w.show(n,r)}else w.show(n,r)},show:function(n,e){var t=[];if(e.forEach((function(n){var e=(0,f.default)(n).find(".leancloud-visitors-count");e&&e.text("0"),t.push(/\%/.test((0,f.default)(n).attr("id"))?decodeURI((0,f.default)(n).attr("id")):(0,f.default)(n).attr("id"))})),t.length){var r=new AV.Query(n);r.containedIn("url",t),r.find().then((function(n){n.length>0&&e.forEach((function(e){n.forEach((function(n){var t=n.get("xid")||encodeURI(n.get("url")),r=n.get("time"),a=(0,f.default)(e),o=a.attr("id");if((/\%/.test(o)?o:encodeURI(o))==t){var i=a.find(".leancloud-visitors-count");i&&i.text(r)}}))}))})).catch((function(n){}))}}};a.prototype.Q=function(n){var e=this,t=arguments.length,r=e.cfg.clazzName;if(1==t){var a=new AV.Query(r);a.doesNotExist("rid");var o=new AV.Query(r);o.equalTo("rid","");var i=AV.Query.or(a,o);return"*"===n?i.exists("url"):i.equalTo("url",decodeURI(n)),i.addDescending("createdAt"),i.addDescending("insertedAt"),i}var s=JSON.stringify(arguments[1]).replace(/(\[|\])/g,""),l="select * from "+r+" where rid in ("+s+") order by -createdAt,-createdAt";return AV.Query.doCloudQuery(l)},a.prototype.installLocale=function(n,e){return this.i18n(n,e),this},a.prototype.setPath=function(n){return this.config.path=n,this},a.prototype.bind=function(){var n=this,e=n.$el.find(".vemojis"),t=n.$el.find(".vpreview"),r=n.$el.find(".vemoji-btn"),a=n.$el.find(".vpreview-btn"),o=n.$el.find(".veditor"),l=c.default.maps,d=!1;n.$emoji={show:function(){return!d&&function(n){var t=[];for(var r in l)l.hasOwnProperty(r)&&c.default.build(r)&&t.push('<i title="'+r+'" >'+c.default.build(r)+"</i>");e.html(t.join("")),d=!0,e.find("i").on("click",(function(n){n.preventDefault(),T(o[0]," :"+(0,f.default)(this).attr("title")+":")}))}(),n.$preview.hide(),e.show(),r.addClass("actived"),n.$emoji},hide:function(){return r.removeClass("actived"),e.hide(),n.$emoji}},n.$preview={show:function(){return k?(n.$emoji.hide(),a.addClass("actived"),t.html((0,h.default)(k)).show(),M()):n.$preview.hide(),n.$preview},hide:function(){return a.removeClass("actived"),t.hide().html(""),n.$preview}};var x=function(e){var r=e.val()||"";r||n.$preview.hide(),k!=r&&(k=r,a.hasClass("actived")>-1&&k!=t.html()&&t.html((0,h.default)(k)),M())};r.on("click",(function(e){r.hasClass("actived")?n.$emoji.hide():n.$emoji.show()})),a.on("click",(function(e){a.hasClass("actived")?n.$preview.hide():n.$preview.show()}));var w=n.cfg.meta,E={},D={veditor:"comment"};for(var C in w.forEach((function(n){D["v"+n]=n})),D)D.hasOwnProperty(C)&&function(){var e=D[C],t=n.$el.find("."+C);E[e]=t,t.on("input change blur propertychange",(function(r){n.cfg.enableQQ&&"blur"===r.type&&"nick"===e&&(t.val()&&!isNaN(t.val())?(0,m.fetchQQFn)(t.val(),(function(n){var e=n.nick||t.val(),r=n.qq+"@qq.com";(0,f.default)(".vnick").val(e),(0,f.default)(".vmail").val(r),b.nick=e,b.mail=r,b.QQAvatar=n.pic})):f.default.store.get(p.QQCacheKey)&&f.default.store.get(p.QQCacheKey).nick!=t.val()&&(f.default.store.remove(p.QQCacheKey),b.nick=t.val(),b.mail="",b.QQAvatar="")),"comment"===e?((0,s.default)(t[0]),I((function(n){x(t)}))()):b[e]=(0,v.default)(t.val().replace(/(^\s*)|(\s*$)/g,"").substring(0,35))}))}();var I=function(n){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:300,t=void 0;return function(){var r=this,a=arguments;t&&clearTimeout(t),t=setTimeout((function(){n.apply(r,a)}),e)}},T=function(n,e){if(document.selection)n.focus(),document.selection.createRange().text=e,n.focus();else if(n.selectionStart||"0"==n.selectionStart){var t=n.selectionStart,r=n.selectionEnd,a=n.scrollTop;n.value=n.value.substring(0,t)+e+n.value.substring(r,n.value.length),n.focus(),n.selectionStart=t+e.length,n.selectionEnd=t+e.length,n.scrollTop=a}else n.focus(),n.value+=e;I((function(e){x((0,f.default)(n))}))()},O={no:1,size:n.cfg.pageSize,skip:n.cfg.pageSize},A=n.$el.find(".vpage");A.on("click",(function(n){A.hide(),O.no++,_()}));var _=function(){var e=O.size,t=O.no,r=Number(n.$el.find(".vnum").text());n.$loading.show();var a=n.Q(n.cfg.path);a.limit(e),a.skip((t-1)*e),a.find().then((function(a){if(O.skip=O.size,a&&a.length){var o=[];a.forEach((function(e){o.push(e.id),P(e,n.$el.find(".vcards"),!0)})),n.Q(n.cfg.path,o).then((function(n){(n&&n.results||[]).forEach((function(n){P(n,(0,f.default)('.vquote[data-self-id="'+n.get("rid")+'"]'))}))})).catch((function(n){})),e*t<r?A.show():A.hide(),M()}else n.$nodata.show();n.$loading.hide()})).catch((function(e){n.$loading.hide(),(0,g.default)(n,e,"query")}))};n.Q(n.cfg.path).count().then((function(e){e>0?(n.$el.find(".vcount").show().find(".vnum").text(e),_()):n.$loading.hide()})).catch((function(e){(0,g.default)(n,e,"count")}));var R=function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"",e=/(https?|http):\/\/[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]/g,t=n.match(e)||[];return t.length>0?t[0]:""},P=function(e,t,r){var a=(0,f.default)('<div class="vcard" id="'+e.id+'"></div>'),o=(0,v.default)(e.get("ua")),s="";o&&!/ja/.test(n.cfg.lang)&&(s=(o=f.default.detect(o)).version?o.os?'<span class="vsys">'+o.browser+" "+o.version+'</span> <span class="vsys">'+o.os+" "+o.osVersion+"</span>":"":'<span class="vsys">'+o.browser+"</span>"),"*"===n.cfg.path&&(s='<a href="'+e.get("url")+'" class="vsys">'+e.get("url")+"</a>");var l=e.get("link")?/^https?\:\/\//.test(e.get("link"))?e.get("link"):"http://"+e.get("link"):"",c=f.default.escape((0,v.default)(e.get("nick").substring(0,30))),p=l?'<a class="vnick" rel="nofollow" href="'+R(l)+'" target="_blank" >'+c+"</a>":'<span class="vnick">'+c+"</span>",d=(S.hide?"":n.cfg.enableQQ&&e.get("QQAvatar")?(0,v.default)('<img class="vimg" src="'+R(e.get("QQAvatar"))+'" referrerPolicy="no-referrer"/>'):'<img class="vimg" src="'+(S.cdn+(0,i.default)(e.get("mail"))+S.params)+'">')+'<div class="vh"><div class="vhead">'+p+" "+s+'</div><div class="vmeta"><span class="vtime" >'+(0,u.default)(e.get("insertedAt"),n.i18n)+'</span><span class="vat" data-vm-id="'+(e.get("rid")||e.id)+'" data-self-id="'+e.id+'">'+n.i18n.t("reply")+'</span></div><div class="vcontent" data-expand="'+n.i18n.t("expand")+'">'+(0,h.default)(e.get("comment"))+'</div><div class="vreply-wrapper" data-self-id="'+e.id+'"></div><div class="vquote" data-self-id="'+e.id+'"></div></div>';a.html(d);var m=a.find(".vat");a.find("a:not(.at)").forEach((function(n){(0,f.default)(n).attr({target:"_blank",rel:"noopener"})})),r?t.append(a):t.prepend(a);var g=a.find(".vcontent");g&&j(g),m&&B(m,e)},F={},B=function(e,t){e.on("click",(function(r){var a=e.attr("data-vm-id"),o=e.attr("data-self-id"),i=n.$el.find(".vwrap"),s="@"+f.default.escape(t.get("nick"));(0,f.default)('.vreply-wrapper[data-self-id="'+o+'"]').append(i).find(".cancel-reply").show(),F={at:f.default.escape(s)+" ",rid:a,pid:o,rmail:t.get("mail")},E.comment.attr({placeholder:s})[0].focus()}))},M=function(){setTimeout((function(){try{n.cfg.mathjax&&"MathJax"in window&&"version"in window.MathJax&&(/^3.*/.test(window.MathJax.version)&&MathJax.typeset()||MathJax.Hub.Queue(["Typeset",MathJax.Hub,document.querySelector(".v")])),"renderMathInElement"in window&&renderMathInElement((0,f.default)(".v")[0],{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})}catch(n){}}),100)},j=function(n){setTimeout((function(){n[0].offsetHeight>200&&(n.addClass("expand"),n.on("click",(function(e){n.removeClass("expand")})))}))};!function(e){if(e=f.default.store.get(p.MetaCacheKey)||e)for(var t in w)if(w.hasOwnProperty(t)){var r=w[t];n.$el.find(".v"+r).val(f.default.unescape(e[r])),b[r]=e[r]}var a=f.default.store.get(p.QQCacheKey);b.QQAvatar=n.cfg.enableQQ&&!!a&&a.pic||""}(),n.reset=function(){b.comment="",E.comment.val(""),x(E.comment),E.comment.attr("placeholder",n.cfg.placeholder),F={},n.$preview.hide(),n.$el.find(".vpanel").append(n.$el.find(".vwrap")),n.$el.find(".cancel-reply").hide(),k="",s.default.update(E.comment[0])};var L=n.$el.find(".vsubmit"),N=function(e){if(n.cfg.requiredFields.indexOf("nick")>-1&&b.nick.length<3)return E.nick[0].focus(),void n.$el.find(".status-bar").text(""+n.i18n.t("nickFail")).empty(3e3);if(n.cfg.requiredFields.indexOf("mail")>-1&&!/[\w-\.]+@([\w-]+\.)+[a-z]{2,3}/.test(b.mail))return E.mail[0].focus(),void n.$el.find(".status-bar").text(""+n.i18n.t("mailFail")).empty(3e3);if(""!=k){for(var t in y)if(y.hasOwnProperty(t)){var r=y[t];k=k.replace(t,r),URL.revokeObjectURL(t)}y={},b.comment=(0,v.default)(k),b.nick=b.nick||"Anonymous";var a=f.default.store.get("vlx");a&&Date.now()/1e3-a/1e3<20?n.$el.find(".status-bar").text(n.i18n.t("busy")).empty(3e3):$()}else E.comment[0].focus()},$=function(){f.default.store.set("vlx",Date.now()),L.attr({disabled:!0}),n.$loading.show(!0);var e=new(AV.Object.extend(n.cfg.clazzName||"Comment"));if(b.url=decodeURI(n.cfg.path),b.insertedAt=new Date,F.rid){var t=F.pid||F.rid;e.set("rid",F.rid),e.set("pid",t),b.comment=k.replace("<p>",'<p><a class="at" href="#'+t+'">'+F.at+"</a> , ")}for(var r in b)if(b.hasOwnProperty(r)){var a=b[r];e.set(r,a)}e.setACL(function(){var n=new AV.ACL;return n.setPublicReadAccess(!0),n.setPublicWriteAccess(!1),n}()),e.save().then((function(e){"Anonymous"!=b.nick&&f.default.store.set(p.MetaCacheKey,{nick:b.nick,link:b.link,mail:b.mail});var t=n.$el.find(".vnum");try{F.rid?P(e,(0,f.default)('.vquote[data-self-id="'+F.rid+'"]'),!0):(Number(t.text())?t.text(Number(t.text())+1):n.$el.find(".vcount").show().find(".vnum").text(Number(t.text())+1),P(e,n.$el.find(".vcards")),O.skip++),L.removeAttr("disabled"),n.$loading.hide(),n.reset()}catch(e){(0,g.default)(n,e,"save")}})).catch((function(e){(0,g.default)(n,e,"commitEvt")}))};L.on("click",N),(0,f.default)(document).on("keydown",(function(n){var e=(n=window.event||n).keyCode||n.which||n.charCode;(n.ctrlKey||n.metaKey)&&13===e&&N(),9===e&&"veditor"==(document.activeElement.id||"")&&(n.preventDefault(),T(o[0],"    "))})).on("paste",(function(n){var e="clipboardData"in n?n.clipboardData:n.originalEvent&&n.originalEvent.clipboardData||window.clipboardData;e&&U(e.items,!0)})),o.on("dragenter dragleave dragover drop",(function(n){n.stopPropagation(),n.preventDefault(),"drop"===n.type&&U(n.dataTransfer.items)}));var U=function(n,e){for(var t=0,r=n.length;t<r;t++){var a=n[t];if("string"===a.kind&&a.type.match("^text/html"))!e&&a.getAsString((function(n){n&&T(o[0],n.replace(/<[^>]+>/g,""))}));else if(a.type.indexOf("image")>-1){z(a.getAsFile());continue}}},z=function(n){try{var e=URL.createObjectURL(n),t="![image]("+e+") ",r=new FileReader;T(o[0],t),r.onload=function(){y[e]=r.result},r.readAsDataURL(n)}catch(n){}}},n.exports=o,n.exports.default=o},function(n,e,t){"use strict";e.__esModule=!0;var r=function(n){return n&&n.__esModule?n:{default:n}}(t(3));e.default={getApi:function(n,e){r.default.ajax({url:"https://app-router.com/2/route",body:{appId:n}}).then((function(n){n.json().then((function(n){return e&&e("//"+n.api_server)}))}))}}},function(n,e,t){"use strict";e.__esModule=!0;var r=function(n){return n&&n.__esModule?n:{default:n}}(t(3)),a=!1;e.default=function(n,e){if("AV"in window){var t=window.AV.version||window.AV.VERSION;parseInt(t.split(".")[0])>2?a=!!AV.applicationId&&!!AV.applicationKey:r.default.deleteInWin("AV",0)}a?e&&e():r.default.sdkLoader("//unpkg.com/leancloud-storage@3/dist/av-min.js","AV",(function(t){var r,o="https://",i=n.app_id||n.appId,s=n.app_key||n.appKey;if(!n.serverURLs)switch(i.slice(-9)){case"-9Nh9j0Va":o+="tab.";break;case"-MdYXbMMI":o+="us."}r=n.serverURLs||o+"leancloud.cn",AV.init({appId:i,appKey:s,serverURLs:r}),a=!0,e&&e()}))}},function(n,e,t){"use strict";function r(n){return n&&n.__esModule?n:{default:n}}e.__esModule=!0;var a=r(t(84)),o=r(t(100)),i=r(t(101)),s=r(t(98)),l=r(t(99)),c={zh:o.default,"zh-cn":o.default,"zh-CN":o.default,"zh-TW":i.default,en:s.default,"en-US":s.default,ja:l.default,"ja-JP":l.default};e.default=function(n,e){return!c[n]&&n&&e&&(c[n]=e),new a.default({phrases:c[n||"zh"],locale:n})}},function(n,e,t){"use strict";e.__esModule=!0,e.default=function(n,e){if(n.$el&&n.$loading.hide().$nodata.hide(),"[object Error]"==={}.toString.call(e)){var t=e.code||e.message||e.error||"";if(isNaN(t))n.$el&&n.$nodata.show('<pre style="text-align:left;"> '+JSON.stringify(e)+"</pre>");else{var r=n.i18n.t("code-"+t),a=(r=="code-"+t?void 0:r)||e.message||e.error||"";101==t||-1==t?n.$nodata.show():n.$el&&n.$nodata.show('<pre style="text-align:left;">Code '+t+": "+a+"</pre>")}}else n.$el&&n.$nodata.show('<pre style="text-align:left;">'+JSON.stringify(e)+"</pre>")}},function(n,e,t){"use strict";function r(n){return n&&n.__esModule?n:{default:n}}e.__esModule=!0;var a=t(83),o=r(t(79)),i=r(t(3)),s=r(t(12)),l=r(t(13)),c=new a.marked.Renderer;c.code=function(n,e){return'<pre><code class="hljs language-'+e+'">'+(e&&hljs.getLanguage(e)?hljs.highlight(e,n).value:i.default.escape(n))+"</code></pre>"},a.marked.setOptions({renderer:"hljs"in window?c:new a.marked.Renderer,highlight:function(n,e){return"hljs"in window?e&&hljs.getLanguage(e)&&hljs.highlight(e,n,!0).value||hljs.highlightAuto(n).value:(0,o.default)(n)},gfm:!0,tables:!0,breaks:!0,pedantic:!1,sanitize:!1,smartLists:!0,smartypants:!0,headerPrefix:"v-"}),e.default=function(n){return(0,l.default)((0,a.marked)(s.default.parse(n,!0)))}},function(n,e,t){"use strict";e.__esModule=!0,e.recordIPFn=e.fetchQQFn=void 0;var r=function(n){return n&&n.__esModule?n:{default:n}}(t(3)),a=t(6);e.fetchQQFn=function(n,e){var t=r.default.store.get(a.QQCacheKey);t&&t.qq==n?e&&e(t):r.default.ajax({url:"//valine.api.ioliu.cn/getqqinfo",method:"POST",body:{qq:n}}).then((function(n){n.json().then((function(n){n.errmsg||(r.default.store.set(a.QQCacheKey,n),e&&e(n))}))}))},e.recordIPFn=function(n){r.default.ajax({url:"https://forge.speedtest.cn/api/location/info",method:"get"}).then((function(n){return n.json()})).then((function(e){n&&n(e.ip)}))}},function(n,e,t){"use strict";e.__esModule=!0,e.default=function(n,e){if(!n)return"Invalid Date.";try{var t=a(n).getTime();if(isNaN(t))return"Invalid Date.";var o=(new Date).getTime()-t,i=Math.floor(o/864e5);if(0===i){var s=o%864e5,l=Math.floor(s/36e5);if(0===l){var c=s%36e5,p=Math.floor(c/6e4);if(0===p){var u=c%6e4;return Math.round(u/1e3)+" "+e.t("seconds")}return p+" "+e.t("minutes")}return l+" "+e.t("hours")}return i<0?e.t("now"):i<8?i+" "+e.t("days"):r(n)}catch(n){}};var r=function(n){var e=o(n.getDate(),2),t=o(n.getMonth()+1,2);return o(n.getFullYear(),2)+"-"+t+"-"+e},a=function n(e){return e instanceof Date?e:!isNaN(e)||/^\d+$/.test(e)?new Date(parseInt(e)):/GMT/.test(e||"")?n(new Date(e).getTime()):(e=(e||"").replace(/(^\s*)|(\s*$)/g,"").replace(/\.\d+/,"").replace(/-/,"/").replace(/-/,"/").replace(/(\d)T(\d)/,"$1 $2").replace(/Z/," UTC").replace(/([+-]\d\d):?(\d\d)/," $1$2"),new Date(e))},o=function(n,e){for(var t=n.toString();t.length<e;)t="0"+t;return t}},function(n,e,t){var r;!function(a){"use strict";function o(n,e){var t=(65535&n)+(65535&e);return(n>>16)+(e>>16)+(t>>16)<<16|65535&t}function i(n,e,t,r,a,i){return o(function(n,e){return n<<e|n>>>32-e}(o(o(e,n),o(r,i)),a),t)}function s(n,e,t,r,a,o,s){return i(e&t|~e&r,n,e,a,o,s)}function l(n,e,t,r,a,o,s){return i(e&r|t&~r,n,e,a,o,s)}function c(n,e,t,r,a,o,s){return i(e^t^r,n,e,a,o,s)}function p(n,e,t,r,a,o,s){return i(t^(e|~r),n,e,a,o,s)}function u(n,e){n[e>>5]|=128<<e%32,n[14+(e+64>>>9<<4)]=e;var t,r,a,i,u,d=1732584193,m=-271733879,g=-1732584194,f=271733878;for(t=0;t<n.length;t+=16)r=d,a=m,i=g,u=f,d=s(d,m,g,f,n[t],7,-680876936),f=s(f,d,m,g,n[t+1],12,-389564586),g=s(g,f,d,m,n[t+2],17,606105819),m=s(m,g,f,d,n[t+3],22,-1044525330),d=s(d,m,g,f,n[t+4],7,-176418897),f=s(f,d,m,g,n[t+5],12,1200080426),g=s(g,f,d,m,n[t+6],17,-1473231341),m=s(m,g,f,d,n[t+7],22,-45705983),d=s(d,m,g,f,n[t+8],7,1770035416),f=s(f,d,m,g,n[t+9],12,-1958414417),g=s(g,f,d,m,n[t+10],17,-42063),m=s(m,g,f,d,n[t+11],22,-1990404162),d=s(d,m,g,f,n[t+12],7,1804603682),f=s(f,d,m,g,n[t+13],12,-40341101),g=s(g,f,d,m,n[t+14],17,-1502002290),d=l(d,m=s(m,g,f,d,n[t+15],22,1236535329),g,f,n[t+1],5,-165796510),f=l(f,d,m,g,n[t+6],9,-1069501632),g=l(g,f,d,m,n[t+11],14,643717713),m=l(m,g,f,d,n[t],20,-373897302),d=l(d,m,g,f,n[t+5],5,-701558691),f=l(f,d,m,g,n[t+10],9,38016083),g=l(g,f,d,m,n[t+15],14,-660478335),m=l(m,g,f,d,n[t+4],20,-405537848),d=l(d,m,g,f,n[t+9],5,568446438),f=l(f,d,m,g,n[t+14],9,-1019803690),g=l(g,f,d,m,n[t+3],14,-187363961),m=l(m,g,f,d,n[t+8],20,1163531501),d=l(d,m,g,f,n[t+13],5,-1444681467),f=l(f,d,m,g,n[t+2],9,-51403784),g=l(g,f,d,m,n[t+7],14,1735328473),d=c(d,m=l(m,g,f,d,n[t+12],20,-1926607734),g,f,n[t+5],4,-378558),f=c(f,d,m,g,n[t+8],11,-2022574463),g=c(g,f,d,m,n[t+11],16,1839030562),m=c(m,g,f,d,n[t+14],23,-35309556),d=c(d,m,g,f,n[t+1],4,-1530992060),f=c(f,d,m,g,n[t+4],11,1272893353),g=c(g,f,d,m,n[t+7],16,-155497632),m=c(m,g,f,d,n[t+10],23,-1094730640),d=c(d,m,g,f,n[t+13],4,681279174),f=c(f,d,m,g,n[t],11,-358537222),g=c(g,f,d,m,n[t+3],16,-722521979),m=c(m,g,f,d,n[t+6],23,76029189),d=c(d,m,g,f,n[t+9],4,-640364487),f=c(f,d,m,g,n[t+12],11,-421815835),g=c(g,f,d,m,n[t+15],16,530742520),d=p(d,m=c(m,g,f,d,n[t+2],23,-995338651),g,f,n[t],6,-198630844),f=p(f,d,m,g,n[t+7],10,1126891415),g=p(g,f,d,m,n[t+14],15,-1416354905),m=p(m,g,f,d,n[t+5],21,-57434055),d=p(d,m,g,f,n[t+12],6,1700485571),f=p(f,d,m,g,n[t+3],10,-1894986606),g=p(g,f,d,m,n[t+10],15,-1051523),m=p(m,g,f,d,n[t+1],21,-2054922799),d=p(d,m,g,f,n[t+8],6,1873313359),f=p(f,d,m,g,n[t+15],10,-30611744),g=p(g,f,d,m,n[t+6],15,-1560198380),m=p(m,g,f,d,n[t+13],21,1309151649),d=p(d,m,g,f,n[t+4],6,-145523070),f=p(f,d,m,g,n[t+11],10,-1120210379),g=p(g,f,d,m,n[t+2],15,718787259),m=p(m,g,f,d,n[t+9],21,-343485551),d=o(d,r),m=o(m,a),g=o(g,i),f=o(f,u);return[d,m,g,f]}function d(n){var e,t="",r=32*n.length;for(e=0;e<r;e+=8)t+=String.fromCharCode(n[e>>5]>>>e%32&255);return t}function m(n){var e,t=[];for(t[(n.length>>2)-1]=void 0,e=0;e<t.length;e+=1)t[e]=0;var r=8*n.length;for(e=0;e<r;e+=8)t[e>>5]|=(255&n.charCodeAt(e/8))<<e%32;return t}function g(n){var e,t,r="0123456789abcdef",a="";for(t=0;t<n.length;t+=1)e=n.charCodeAt(t),a+=r.charAt(e>>>4&15)+r.charAt(15&e);return a}function f(n){return unescape(encodeURIComponent(n))}function h(n){return function(n){return d(u(m(n),8*n.length))}(f(n))}function v(n,e){return function(n,e){var t,r,a=m(n),o=[],i=[];for(o[15]=i[15]=void 0,a.length>16&&(a=u(a,8*n.length)),t=0;t<16;t+=1)o[t]=909522486^a[t],i[t]=1549556828^a[t];return r=u(o.concat(m(e)),512+8*e.length),d(u(i.concat(r),640))}(f(n),f(e))}function b(n,e,t){return e?t?v(e,n):function(n,e){return g(v(n,e))}(e,n):t?h(n):function(n){return g(h(n))}(n)}void 0!==(r=function(){return b}.call(e,t,e,n))&&(n.exports=r)}()},function(n,e,t){"use strict";var r=t(2),a=t(4),o=t(1),i=t(5),s=t(14),l=t(15),c=l(),p=t(44),u=o("Array.prototype.slice"),d=a.apply(c),m=function(n,e){return i(n),d(n,u(arguments,1))};r(m,{getPolyfill:l,implementation:s,shim:p}),n.exports=m},function(n,e,t){"use strict";var r=t(2),a=t(15);n.exports=function(){var n=a();return r(Array.prototype,{forEach:n},{forEach:function(){return Array.prototype.forEach!==n}}),n}},function(n,e,t){"use strict";e.__esModule=!0;var r=function(n){return n&&n.__esModule?n:{default:n}}(t(47));e.default=function(n){return n=(0,r.default)({url:"",method:"get",body:{}},n),new Promise((function(e,t){if("jsonp"==n.method){var r="cb_"+(Date.now()+Math.round(1e3*Math.random())).toString(32),a=document,i=a.body,s=a.createElement("script");return s.async=!0,s.defer=!0,n.url.indexOf("?")>-1?n.url+="&"+o({callback:r,t:Date.now()}):n.url+="?"+o({callback:r,t:Date.now()}),s.src=n.url,window[r]=function(n){window[r]=null,i.removeChild(s),e(n)},void i.appendChild(s)}var l="XMLHttpRequest"in window?new XMLHttpRequest:new ActiveXObject("Microsoft.XMLHTTP"),c=[],p=[],u={};for(var d in o(n.body)&&(n.url=n.url+"?"+("get"==n.method?o(n.body):"")),l.open(n.method||"get",n.url),"blob"==n.dataType&&(l.responseType="blob"),l.onload=function(){l.getAllResponseHeaders().replace(/^(.*?):[^\S\n]*([\s\S]*?)$/gm,(function(n,e,t){c.push(e=e.toLowerCase()),p.push([e,t]),u[e]=u[e]?u[e]+","+t:t})),e(function n(){return{ok:2==(l.status/100|0),statusText:l.statusText,status:l.status,url:l.responseURL,text:function(){return Promise.resolve(l.responseText)},json:function(){return Promise.resolve(l.responseText).then(JSON.parse)},blob:function(){return Promise.resolve(new Blob([l.response]))},clone:n,headers:{keys:function(){return c},entries:function(){return p},get:function(n){return u[n.toLowerCase()]},has:function(n){return n.toLowerCase()in u}}}}())},l.onerror=t,l.withCredentials="include"==n.credentials,n.headers)l.setRequestHeader(d,n.headers[d]);l.send("post"==n.method?n.body:"get"==n.method?"":o(n.body))}))};var a=encodeURIComponent,o=function(n){var e=[];for(var t in n)n.hasOwnProperty(t)&&e.push(a(t)+"="+a(n[t]));return(e=e.join("&").replace(/%20/g,"+"))||""}},function(n,e,t){"use strict";e.__esModule=!0,e.default=function(n){var e={},t={Trident:(n=n||navigator.userAgent).indexOf("Trident")>-1||n.indexOf("NET CLR")>-1,Presto:n.indexOf("Presto")>-1,WebKit:n.indexOf("AppleWebKit")>-1,Gecko:n.indexOf("Gecko/")>-1,Safari:n.indexOf("Safari")>-1,Edge:n.indexOf("Edge")>-1||n.indexOf("Edg")>-1,Chrome:n.indexOf("Chrome")>-1||n.indexOf("CriOS")>-1,IE:n.indexOf("MSIE")>-1||n.indexOf("Trident")>-1,Firefox:n.indexOf("Firefox")>-1||n.indexOf("FxiOS")>-1,"Firefox Focus":n.indexOf("Focus")>-1,Chromium:n.indexOf("Chromium")>-1,Opera:n.indexOf("Opera")>-1||n.indexOf("OPR")>-1,Vivaldi:n.indexOf("Vivaldi")>-1,Yandex:n.indexOf("YaBrowser")>-1,Kindle:n.indexOf("Kindle")>-1||n.indexOf("Silk/")>-1,360:n.indexOf("360EE")>-1||n.indexOf("360SE")>-1,UC:n.indexOf("UC")>-1||n.indexOf(" UBrowser")>-1,QQBrowser:n.indexOf("QQBrowser")>-1,QQ:n.indexOf("QQ/")>-1,Baidu:n.indexOf("Baidu")>-1||n.indexOf("BIDUBrowser")>-1,Maxthon:n.indexOf("Maxthon")>-1,Sogou:n.indexOf("MetaSr")>-1||n.indexOf("Sogou")>-1,LBBROWSER:n.indexOf("LBBROWSER")>-1,"2345Explorer":n.indexOf("2345Explorer")>-1,TheWorld:n.indexOf("TheWorld")>-1,XiaoMi:n.indexOf("MiuiBrowser")>-1,Quark:n.indexOf("Quark")>-1,Qiyu:n.indexOf("Qiyu")>-1,Wechat:n.indexOf("MicroMessenger")>-1,Taobao:n.indexOf("AliApp(TB")>-1,Alipay:n.indexOf("AliApp(AP")>-1,Weibo:n.indexOf("Weibo")>-1,Douban:n.indexOf("com.douban.frodo")>-1,Suning:n.indexOf("SNEBUY-APP")>-1,iQiYi:n.indexOf("IqiyiApp")>-1,Windows:n.indexOf("Windows")>-1,Linux:n.indexOf("Linux")>-1||n.indexOf("X11")>-1,macOS:n.indexOf("Macintosh")>-1,Android:n.indexOf("Android")>-1||n.indexOf("Adr")>-1,Ubuntu:n.indexOf("Ubuntu")>-1,FreeBSD:n.indexOf("FreeBSD")>-1,Debian:n.indexOf("Debian")>-1,"Windows Phone":n.indexOf("IEMobile")>-1||n.indexOf("Windows Phone")>-1,BlackBerry:n.indexOf("BlackBerry")>-1||n.indexOf("RIM")>-1||n.indexOf("BB10")>-1,MeeGo:n.indexOf("MeeGo")>-1,Symbian:n.indexOf("Symbian")>-1,iOS:n.indexOf("like Mac OS X")>-1,"Chrome OS":n.indexOf("CrOS")>-1,WebOS:n.indexOf("hpwOS")>-1,Mobile:n.indexOf("Mobi")>-1||n.indexOf("iPh")>-1||n.indexOf("480")>-1,Tablet:n.indexOf("Tablet")>-1||n.indexOf("Pad")>-1||n.indexOf("Nexus 7")>-1};t.Mobile&&(t.Mobile=!(n.indexOf("iPad")>-1));var r={browser:["Safari","Chrome","Edge","IE","Firefox","Firefox Focus","Chromium","Opera","Vivaldi","Yandex","Kindle","360","UC","QQBrowser","QQ","Baidu","Maxthon","Sogou","LBBROWSER","2345Explorer","TheWorld","XiaoMi","Quark","Qiyu","Wechat","Taobao","Alipay","Weibo","Douban","Suning","iQiYi"],os:["Windows","Linux","Mac OS","macOS","Android","Ubuntu","FreeBSD","Debian","iOS","Windows Phone","BlackBerry","MeeGo","Symbian","Chrome OS","WebOS"]};for(var a in r)if(r.hasOwnProperty(a))for(var o=0,i=r[a].length;o<i;o++){var s=r[a][o];t[s]&&(e[a]=s)}var l={Windows:function(){return{"10.0":"11",6.4:"10",6.3:"8.1",6.2:"8",6.1:"7","6.0":"Vista",5.2:"XP",5.1:"XP","5.0":"2000"}[n.replace(/^.*Windows NT ([\d.]+).*$/,"$1")]},Android:n.replace(/^.*Android ([\d.]+);.*$/,"$1"),iOS:n.replace(/^.*OS ([\d_]+) like.*$/,"$1").replace(/_/g,"."),Debian:n.replace(/^.*Debian\/([\d.]+).*$/,"$1"),"Windows Phone":n.replace(/^.*Windows Phone( OS)? ([\d.]+);.*$/,"$2"),macOS:n.replace(/^.*Mac OS X ([\d_]+).*$/,"$1").replace(/_/g,"."),WebOS:n.replace(/^.*hpwOS\/([\d.]+);.*$/,"$1"),BlackBerry:n.replace(/^.*BB([\d.]+);*$/,"$1")};e.osVersion="";var c=l[e.os];c&&(e.osVersion="function"==typeof c?c():c==n?"":c);var p={Safari:n.replace(/^.*Version\/([\d.]+).*$/,"$1"),Chrome:n.replace(/^.*Chrome\/([\d.]+).*$/,"$1").replace(/^.*CriOS\/([\d.]+).*$/,"$1"),IE:n.replace(/^.*MSIE ([\d.]+).*$/,"$1").replace(/^.*rv:([\d.]+).*$/,"$1"),Edge:n.replace(/^.*Edge?\/([\d.]+).*$/,"$1"),Firefox:n.replace(/^.*Firefox\/([\d.]+).*$/,"$1").replace(/^.*FxiOS\/([\d.]+).*$/,"$1"),"Firefox Focus":n.replace(/^.*Focus\/([\d.]+).*$/,"$1"),Chromium:n.replace(/^.*Chromium\/([\d.]+).*$/,"$1"),Opera:n.replace(/^.*Opera\/([\d.]+).*$/,"$1").replace(/^.*OPR\/([\d.]+).*$/,"$1"),Vivaldi:n.replace(/^.*Vivaldi\/([\d.]+).*$/,"$1"),Yandex:n.replace(/^.*YaBrowser\/([\d.]+).*$/,"$1"),Kindle:n.replace(/^.*Version\/([\d.]+).*$/,"$1"),Maxthon:n.replace(/^.*Maxthon\/([\d.]+).*$/,"$1"),QQBrowser:n.replace(/^.*QQBrowser\/([\d.]+).*$/,"$1"),QQ:n.replace(/^.*QQ\/([\d.]+).*$/,"$1"),Baidu:n.replace(/^.*BIDUBrowser[\s\/]([\d.]+).*$/,"$1"),UC:n.replace(/^.*UC?Browser\/([\d.]+).*$/,"$1"),Sogou:n.replace(/^.*SE ([\d.X]+).*$/,"$1").replace(/^.*SogouMobileBrowser\/([\d.]+).*$/,"$1"),"2345Explorer":n.replace(/^.*2345Explorer\/([\d.]+).*$/,"$1"),TheWorld:n.replace(/^.*TheWorld ([\d.]+).*$/,"$1"),XiaoMi:n.replace(/^.*MiuiBrowser\/([\d.]+).*$/,"$1"),Quark:n.replace(/^.*Quark\/([\d.]+).*$/,"$1"),Qiyu:n.replace(/^.*Qiyu\/([\d.]+).*$/,"$1"),Wechat:n.replace(/^.*MicroMessenger\/([\d.]+).*$/,"$1"),Taobao:n.replace(/^.*AliApp\(TB\/([\d.]+).*$/,"$1"),Alipay:n.replace(/^.*AliApp\(AP\/([\d.]+).*$/,"$1"),Weibo:n.replace(/^.*weibo__([\d.]+).*$/,"$1"),Douban:n.replace(/^.*com.douban.frodo\/([\d.]+).*$/,"$1"),Suning:n.replace(/^.*SNEBUY-APP([\d.]+).*$/,"$1"),iQiYi:n.replace(/^.*IqiyiVersion\/([\d.]+).*$/,"$1")};e.version="";var u=p[e.browser];return u&&(e.version="function"==typeof u?u():u==n?"":u),null==e.browser&&(e.browser="Unknow App"),e}},function(n,e,t){"use strict";e.__esModule=!0,e.default=function(n){n=Object(n);for(var e=1,t=arguments.length;e<t;e++){var r=arguments[e];if(r)for(var a in r)Object.prototype.hasOwnProperty.call(r,a)&&(n[a]=r[a])}return n}},function(n,e,t){"use strict";function r(n){return/^\{[\s\S]*\}$/.test(JSON.stringify(n))}function a(n){return"[object Function]"==={}.toString.call(n)}function o(n){return"[object Array]"==={}.toString.call(n)}function i(n){if("string"==typeof n)try{return JSON.parse(n)}catch(e){return n}}function s(){if(!(this instanceof s))return new s}function l(n,e){var t=arguments,i=null;if(p||(p=s()),0===t.length)return p.get();if(1===t.length){if("string"==typeof n)return p.get(n);if(r(n))return p.set(n)}if(2===t.length&&"string"==typeof n){if(!e)return p.remove(n);if(e&&"string"==typeof e)return p.set(n,e);e&&a(e)&&(i=null,i=e(n,p.get(n)),l.set(n,i))}if(2===t.length&&o(n)&&a(e))for(var c=0,u=n.length;c<u;c++)i=e(n[c],p.get(n[c])),l.set(n[c],i);return l}e.__esModule=!0;var c=window.localStorage;c=function(n){var e="_Is_Incognit";try{n.setItem(e,"yes")}catch(e){if(["QuotaExceededError","NS_ERROR_DOM_QUOTA_REACHED"].indexOf(e.name)>-1){var t=function(){};n.__proto__={setItem:t,getItem:t,removeItem:t,clear:t}}}finally{"yes"===n.getItem(e)&&n.removeItem(e)}return n}(c),s.prototype={set:function(n,e){if(n&&!r(n))c.setItem(n,function(n){return void 0===n||"function"==typeof n?n+"":JSON.stringify(n)}(e));else if(r(n))for(var t in n)this.set(t,n[t]);return this},get:function(n){if(!n){var e={};return this.each((function(n,t){return e[n]=t})),e}if("?"===n.charAt(0))return this.has(n.substr(1));var t=arguments;if(t.length>1){for(var r={},a=0,o=t.length;a<o;a++){var s=i(c.getItem(t[a]));s&&(r[t[a]]=s)}return r}return i(c.getItem(n))},clear:function(){return c.clear(),this},remove:function(n){var e=this.get(n);return c.removeItem(n),e},has:function(n){return{}.hasOwnProperty.call(this.get(),n)},keys:function(){var n=[];return this.each((function(e){n.push(e)})),n},each:function(n){for(var e=0,t=c.length;e<t;e++){var r=c.key(e);n(r,this.get(r))}return this},search:function(n){for(var e=this.keys(),t={},r=0,a=e.length;r<a;r++)e[r].indexOf(n)>-1&&(t[e[r]]=this.get(e[r]));return t}};var p=null;for(var u in s.prototype)l[u]=s.prototype[u];e.default=l},function(n,e,t){var r,a;a=function(n,e,t){function r(e,a,o){return o=Object.create(r.fn),e&&o.push.apply(o,e.addEventListener?[e]:""+e===e?/</.test(e)?((a=n.createElement(a)).innerHTML=e,a.children):a?(a=r(a)[0])?a[t](e):o:n[t](e):e),o}return r.fn=[],r.one=function(n,e){return r(n,e)[0]||null},r}(document,0,"querySelectorAll"),void 0!==(r=function(){return a}.apply(e,[]))&&(n.exports=r)},function(n,e,t){function r(n){return null==n}function a(n){(n=function(n){var e={};for(var t in n)e[t]=n[t];return e}(n||{})).whiteList=n.whiteList||o.whiteList,n.onAttr=n.onAttr||o.onAttr,n.onIgnoreAttr=n.onIgnoreAttr||o.onIgnoreAttr,n.safeAttrValue=n.safeAttrValue||o.safeAttrValue,this.options=n}var o=t(16),i=t(51);t(17),a.prototype.process=function(n){if(!(n=(n=n||"").toString()))return"";var e=this.options,t=e.whiteList,a=e.onAttr,o=e.onIgnoreAttr,s=e.safeAttrValue;return i(n,(function(n,e,i,l,c){var p=t[i],u=!1;if(!0===p?u=p:"function"==typeof p?u=p(l):p instanceof RegExp&&(u=p.test(l)),!0!==u&&(u=!1),l=s(i,l)){var d,m={position:e,sourcePosition:n,source:c,isWhite:u};return u?r(d=a(i,l,m))?i+":"+l:d:r(d=o(i,l,m))?void 0:d}}))},n.exports=a},function(n,e,t){var r=t(17);n.exports=function(n,e){function t(){if(!o){var t=r.trim(n.slice(i,s)),a=t.indexOf(":");if(-1!==a){var c=r.trim(t.slice(0,a)),p=r.trim(t.slice(a+1));if(c){var u=e(i,l.length,c,p,t);u&&(l+=u+"; ")}}}i=s+1}";"!==(n=r.trimRight(n))[n.length-1]&&(n+=";");for(var a=n.length,o=!1,i=0,s=0,l="";s<a;s++){var c=n[s];if("/"===c&&"*"===n[s+1]){var p=n.indexOf("*/",s+2);if(-1===p)break;i=(s=p+1)+1,o=!1}else"("===c?o=!0:")"===c?o=!1:";"===c?o||t():"\n"===c&&t()}return r.trim(l)}},function(n,e,t){"use strict";var r=t(0),a=t(1),o=r("%TypeError%"),i=t(54),s=r("%Reflect.apply%",!0)||a("%Function.prototype.apply%");n.exports=function(n,e){var t=arguments.length>2?arguments[2]:[];if(!i(t))throw new o("Assertion failed: optional `argumentsList`, if provided, must be a List");return s(n,e,t)}},function(n,e,t){"use strict";var r=t(0)("%TypeError%"),a=t(19),o=t(8);n.exports=function(n,e){if("Object"!==o(n))throw new r("Assertion failed: `O` must be an Object");if(!a(e))throw new r("Assertion failed: `P` must be a Property Key");return e in n}},function(n,e,t){"use strict";var r=t(0)("%Array%"),a=!r.isArray&&t(1)("Object.prototype.toString");n.exports=r.isArray||function(n){return"[object Array]"===a(n)}},function(n,e,t){"use strict";n.exports=t(10)},function(n,e,t){"use strict";var r=t(0)("%TypeError%"),a=t(18),o=t(58),i=t(8);n.exports=function(n){if("Object"!==i(n))throw new r("Assertion failed: `obj` must be an Object");return o(a(n,"length"))}},function(n,e,t){"use strict";var r=t(63),a=t(59);n.exports=function(n){var e=a(n);return 0!==e&&(e=r(e)),0===e?0:e}},function(n,e,t){"use strict";var r=t(72),a=t(57);n.exports=function(n){var e=a(n);return e<=0?0:e>r?r:e}},function(n,e,t){"use strict";var r=t(0),a=r("%TypeError%"),o=r("%Number%"),i=r("%RegExp%"),s=r("%parseInt%"),l=t(1),c=t(73),p=t(71),u=l("String.prototype.slice"),d=c(/^0b[01]+$/i),m=c(/^0o[0-7]+$/i),g=c(/^[-+]0x[0-9a-f]+$/i),f=c(new i("["+["","",""].join("")+"]","g")),h=["\t\n\v\f\r ","\u2028","\u2029\ufeff"].join(""),v=new RegExp("(^["+h+"]+)|(["+h+"]+$)","g"),b=l("String.prototype.replace"),k=t(61);n.exports=function n(e){var t=p(e)?e:k(e,o);if("symbol"==typeof t)throw new a("Cannot convert a Symbol value to a number");if("bigint"==typeof t)throw new a("Conversion from 'BigInt' to 'number' is not allowed.");if("string"==typeof t){if(d(t))return n(s(u(t,2),2));if(m(t))return n(s(u(t,2),8));if(f(t)||g(t))return NaN;var r=function(n){return b(n,v,"")}(t);if(r!==t)return n(r)}return o(t)}},function(n,e,t){"use strict";var r=t(0)("%Object%"),a=t(5);n.exports=function(n){return a(n),r(n)}},function(n,e,t){"use strict";var r=t(76);n.exports=function(n){return arguments.length>1?r(n,arguments[1]):r(n)}},function(n,e,t){"use strict";var r=t(0)("%TypeError%");n.exports=function(n,e){if(null==n)throw new r(e||"Cannot call method on "+n);return n}},function(n,e,t){"use strict";var r=t(67),a=t(68),o=t(64),i=t(70),s=t(69),l=t(74);n.exports=function(n){var e=o(n);return i(e)?0:0!==e&&s(e)?l(e)*a(r(e)):e}},function(n,e,t){"use strict";var r=t(65);n.exports=function(n){var e=r(n,Number);if("string"!=typeof e)return+e;var t=e.replace(/^[ \t\x0b\f\xa0\ufeff\n\r\u2028\u2029\u1680\u180e\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200a\u202f\u205f\u3000\u0085]+|[ \t\x0b\f\xa0\ufeff\n\r\u2028\u2029\u1680\u180e\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200a\u202f\u205f\u3000\u0085]+$/g,"");return/^0[ob]|^[+-]0x/.test(t)?NaN:+t}},function(n,e,t){"use strict";n.exports=t(77)},function(n,e,t){"use strict";n.exports=function(n){return null===n?"Null":void 0===n?"Undefined":"function"==typeof n||"object"==typeof n?"Object":"number"==typeof n?"Number":"boolean"==typeof n?"Boolean":"string"==typeof n?"String":void 0}},function(n,e,t){"use strict";var r=t(0)("%Math.abs%");n.exports=function(n){return r(n)}},function(n,e,t){"use strict";var r=Math.floor;n.exports=function(n){return r(n)}},function(n,e,t){"use strict";var r=Number.isNaN||function(n){return n!=n};n.exports=Number.isFinite||function(n){return"number"==typeof n&&!r(n)&&n!==1/0&&n!==-1/0}},function(n,e,t){"use strict";n.exports=Number.isNaN||function(n){return n!=n}},function(n,e,t){"use strict";n.exports=function(n){return null===n||"function"!=typeof n&&"object"!=typeof n}},function(n,e,t){"use strict";var r=t(0),a=r("%Math%"),o=r("%Number%");n.exports=o.MAX_SAFE_INTEGER||a.pow(2,53)-1},function(n,e,t){"use strict";var r=t(0)("RegExp.prototype.test"),a=t(4);n.exports=function(n){return a(r,n)}},function(n,e,t){"use strict";n.exports=function(n){return n>=0?1:-1}},function(n,e){n.exports=function(n){var e=!0,t=!0,r=!1;if("function"==typeof n){try{n.call("f",(function(n,t,r){"object"!=typeof r&&(e=!1)})),n.call([null],(function(){"use strict";t="string"==typeof this}),"x")}catch(n){r=!0}return!r&&e&&t}return!1}},function(n,e,t){"use strict";var r="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator,a=t(21),o=t(10),i=t(80),s=t(82),l=function(n,e){if(null==n)throw new TypeError("Cannot call method on "+n);if("string"!=typeof e||"number"!==e&&"string"!==e)throw new TypeError('hint must be "string" or "number"');var t,r,i,s="string"===e?["toString","valueOf"]:["valueOf","toString"];for(i=0;i<s.length;++i)if(t=n[s[i]],o(t)&&(r=t.call(n),a(r)))return r;throw new TypeError("No default value")},c=function(n,e){var t=n[e];if(null!=t){if(!o(t))throw new TypeError(t+" returned for property "+e+" of object "+n+" is not a function");return t}};n.exports=function(n){if(a(n))return n;var e,t="default";if(arguments.length>1&&(arguments[1]===String?t="string":arguments[1]===Number&&(t="number")),r&&(Symbol.toPrimitive?e=c(n,Symbol.toPrimitive):s(n)&&(e=Symbol.prototype.valueOf)),void 0!==e){var o=e.call(n,t);if(a(o))return o;throw new TypeError("unable to convert exotic object to primitive")}return"default"===t&&(i(n)||s(n))&&(t="string"),l(n,"default"===t?"number":t)}},function(n,e,t){"use strict";var r=Object.prototype.toString,a=t(21),o=t(10),i=function(n){var e;if((e=arguments.length>1?arguments[1]:"[object Date]"===r.call(n)?String:Number)===String||e===Number){var t,i,s=e===String?["toString","valueOf"]:["valueOf","toString"];for(i=0;i<s.length;++i)if(o(n[s[i]])&&(t=n[s[i]](),a(t)))return t;throw new TypeError("No default value")}throw new TypeError("invalid [[DefaultValue]] hint supplied")};n.exports=function(n){return a(n)?n:arguments.length>1?i(n,arguments[1]):i(n)}},function(n,e,t){"use strict";var r=Array.prototype.slice,a=Object.prototype.toString;n.exports=function(n){var e=this;if("function"!=typeof e||"[object Function]"!==a.call(e))throw new TypeError("Function.prototype.bind called on incompatible "+e);for(var t,o=r.call(arguments,1),i=function(){if(this instanceof t){var a=e.apply(this,o.concat(r.call(arguments)));return Object(a)===a?a:this}return e.apply(n,o.concat(r.call(arguments)))},s=Math.max(0,e.length-o.length),l=[],c=0;c<s;c++)l.push("$"+c);if(t=Function("binder","return function ("+l.join(",")+"){ return binder.apply(this,arguments); }")(i),e.prototype){var p=function(){};p.prototype=e.prototype,t.prototype=new p,p.prototype=null}return t}},function(n,e,t){n.exports=function(){"use strict";var n=function(n,e){return function(n){var e=n.exports=function(){return new RegExp("(?:"+e.line().source+")|(?:"+e.block().source+")","gm")};e.line=function(){return/(?:^|\s)\/\/(.+?)$/gm},e.block=function(){return/\/\*([\S\s]*?)\*\//gm}}(e={exports:{}}),e.exports}(),e=["23AC69","91C132","F19726","E8552D","1AAB8E","E1147F","2980C1","1BA1E6","9FA0A0","F19726","E30B20","E30B20","A3338B"];return function(t,r){void 0===r&&(r={});var a=r.colors;void 0===a&&(a=e);var o=0,i={},s=new RegExp("("+/[\u4E00-\u9FFF\u3400-\u4dbf\uf900-\ufaff\u3040-\u309f\uac00-\ud7af\u0400-\u04FF]+|\w+/.source+"|"+/</.source+")|("+n().source+")","gmi");return t.replace(s,(function(n,e,t){if(t)return function(n){return'<span style="color: slategray">'+n+"</span>"}(t);if("<"===e)return"&lt;";var r;i[e]?r=i[e]:(r=a[o],i[e]=r);var s='<span style="color: #'+r+'">'+e+"</span>";return o=++o%a.length,s}))}}()},function(n,e,t){"use strict";var r=Date.prototype.getDay,a=Object.prototype.toString,o=t(24)();n.exports=function(n){return"object"==typeof n&&null!==n&&(o?function(n){try{return r.call(n),!0}catch(n){return!1}}(n):"[object Date]"===a.call(n))}},function(n,e,t){"use strict";var r=String.prototype.valueOf,a=Object.prototype.toString,o=t(24)();n.exports=function(n){return"string"==typeof n||"object"==typeof n&&(o?function(n){try{return r.call(n),!0}catch(n){return!1}}(n):"[object String]"===a.call(n))}},function(n,e,t){"use strict";var r=Object.prototype.toString;if(t(22)()){var a=Symbol.prototype.toString,o=/^Symbol\(.*\)$/;n.exports=function(n){if("symbol"==typeof n)return!0;if("[object Symbol]"!==r.call(n))return!1;try{return function(n){return"symbol"==typeof n.valueOf()&&o.test(a.call(n))}(n)}catch(n){return!1}}}else n.exports=function(n){return!1}},function(n,e,t){!function(n){"use strict";function e(n,e){for(var t=0;t<e.length;t++){var r=e[t];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(n,r.key,r)}}function t(n,t,r){return t&&e(n.prototype,t),r&&e(n,r),Object.defineProperty(n,"prototype",{writable:!1}),n}function r(n,e){if(n){if("string"==typeof n)return a(n,e);var t=Object.prototype.toString.call(n).slice(8,-1);return"Object"===t&&n.constructor&&(t=n.constructor.name),"Map"===t||"Set"===t?Array.from(n):"Arguments"===t||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t)?a(n,e):void 0}}function a(n,e){(null==e||e>n.length)&&(e=n.length);for(var t=0,r=new Array(e);t<e;t++)r[t]=n[t];return r}function o(n,e){var t="undefined"!=typeof Symbol&&n[Symbol.iterator]||n["@@iterator"];if(t)return(t=t.call(n)).next.bind(t);if(Array.isArray(n)||(t=r(n))||e&&n&&"number"==typeof n.length){t&&(n=t);var a=0;return function(){return a>=n.length?{done:!0}:{done:!1,value:n[a++]}}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}function i(){return{baseUrl:null,breaks:!1,extensions:null,gfm:!0,headerIds:!0,headerPrefix:"",highlight:null,langPrefix:"language-",mangle:!0,pedantic:!1,renderer:null,sanitize:!1,sanitizer:null,silent:!1,smartLists:!1,smartypants:!1,tokenizer:null,walkTokens:null,xhtml:!1}}function s(e){n.defaults=e}function l(n,e){if(e){if(S.test(n))return n.replace(x,C)}else if(w.test(n))return n.replace(E,C);return n}function c(n){return n.replace(I,(function(n,e){return"colon"===(e=e.toLowerCase())?":":"#"===e.charAt(0)?"x"===e.charAt(1)?String.fromCharCode(parseInt(e.substring(2),16)):String.fromCharCode(+e.substring(1)):""}))}function p(n,e){n=n.source||n,e=e||"";var t={replace:function(e,r){return r=(r=r.source||r).replace(T,"$1"),n=n.replace(e,r),t},getRegex:function(){return new RegExp(n,e)}};return t}function u(n,e,t){if(n){var r;try{r=decodeURIComponent(c(t)).replace(O,"").toLowerCase()}catch(n){return null}if(0===r.indexOf("javascript:")||0===r.indexOf("vbscript:")||0===r.indexOf("data:"))return null}e&&!A.test(t)&&(t=function(n,e){_[" "+n]||(R.test(n)?_[" "+n]=n+"/":_[" "+n]=g(n,"/",!0));var t=-1===(n=_[" "+n]).indexOf(":");return"//"===e.substring(0,2)?t?e:n.replace(P,"$1")+e:"/"===e.charAt(0)?t?e:n.replace(F,"$1")+e:n+e}(e,t));try{t=encodeURI(t).replace(/%25/g,"%")}catch(n){return null}return t}function d(n){for(var e,t,r=1;r<arguments.length;r++)for(t in e=arguments[r])Object.prototype.hasOwnProperty.call(e,t)&&(n[t]=e[t]);return n}function m(n,e){var t=n.replace(/\|/g,(function(n,e,t){for(var r=!1,a=e;--a>=0&&"\\"===t[a];)r=!r;return r?"|":" |"})).split(/ \|/),r=0;if(t[0].trim()||t.shift(),t.length>0&&!t[t.length-1].trim()&&t.pop(),t.length>e)t.splice(e);else for(;t.length<e;)t.push("");for(;r<t.length;r++)t[r]=t[r].trim().replace(/\\\|/g,"|");return t}function g(n,e,t){var r=n.length;if(0===r)return"";for(var a=0;a<r;){var o=n.charAt(r-a-1);if(o!==e||t){if(o===e||!t)break;a++}else a++}return n.substr(0,r-a)}function f(n){n&&n.sanitize&&n.silent}function h(n,e){if(e<1)return"";for(var t="";e>1;)1&e&&(t+=n),e>>=1,n+=n;return t+n}function v(n,e,t,r){var a=e.href,o=e.title?l(e.title):null,i=n[1].replace(/\\([\[\]])/g,"$1");if("!"!==n[0].charAt(0)){r.state.inLink=!0;var s={type:"link",raw:t,href:a,title:o,text:i,tokens:r.inlineTokens(i,[])};return r.state.inLink=!1,s}return{type:"image",raw:t,href:a,title:o,text:l(i)}}function b(n){return n.replace(/---/g,"").replace(/--/g,"").replace(/(^|[-\u2014/(\[{"\s])'/g,"$1").replace(/'/g,"").replace(/(^|[-\u2014/(\[{\u2018\s])"/g,"$1").replace(/"/g,"").replace(/\.{3}/g,"")}function k(n){var e,t,r="",a=n.length;for(e=0;e<a;e++)t=n.charCodeAt(e),Math.random()>.5&&(t="x"+t.toString(16)),r+="&#"+t+";";return r}function y(n,e,t){if(null==n)throw new Error("marked(): input parameter is undefined or null");if("string"!=typeof n)throw new Error("marked(): input parameter is of type "+Object.prototype.toString.call(n)+", string expected");if("function"==typeof e&&(t=e,e=null),f(e=d({},y.defaults,e||{})),t){var r,a=e.highlight;try{r=N.lex(n,e)}catch(n){return t(n)}var o=function(n){var o;if(!n)try{e.walkTokens&&y.walkTokens(r,e.walkTokens),o=H.parse(r,e)}catch(e){n=e}return e.highlight=a,n?t(n):t(null,o)};if(!a||a.length<3)return o();if(delete e.highlight,!r.length)return o();var i=0;return y.walkTokens(r,(function(n){"code"===n.type&&(i++,setTimeout((function(){a(n.text,n.lang,(function(e,t){if(e)return o(e);null!=t&&t!==n.text&&(n.text=t,n.escaped=!0),0==--i&&o()}))}),0))})),void(0===i&&o())}try{var s=N.lex(n,e);return e.walkTokens&&y.walkTokens(s,e.walkTokens),H.parse(s,e)}catch(n){if(n.message+="\nPlease report this to https://github.com/markedjs/marked.",e.silent)return"<p>An error occurred:</p><pre>"+l(n.message+"",!0)+"</pre>";throw n}}n.defaults={baseUrl:null,breaks:!1,extensions:null,gfm:!0,headerIds:!0,headerPrefix:"",highlight:null,langPrefix:"language-",mangle:!0,pedantic:!1,renderer:null,sanitize:!1,sanitizer:null,silent:!1,smartLists:!1,smartypants:!1,tokenizer:null,walkTokens:null,xhtml:!1};var S=/[&<>"']/,x=/[&<>"']/g,w=/[<>"']|&(?!#?\w+;)/,E=/[<>"']|&(?!#?\w+;)/g,D={"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&quot;","'":"&#39;"},C=function(n){return D[n]},I=/&(#(?:\d+)|(?:#x[0-9A-Fa-f]+)|(?:\w+));?/gi,T=/(^|[^\[])\^/g,O=/[^\w:]/g,A=/^$|^[a-z][a-z0-9+.-]*:|^[?#]/i,_={},R=/^[^:]+:\/*[^/]*$/,P=/^([^:]+:)[\s\S]*$/,F=/^([^:]+:\/*[^/]*)[\s\S]*$/,B={exec:function(){}},M=function(){function e(e){this.options=e||n.defaults}var t=e.prototype;return t.space=function(n){var e=this.rules.block.newline.exec(n);if(e&&e[0].length>0)return{type:"space",raw:e[0]}},t.code=function(n){var e=this.rules.block.code.exec(n);if(e){var t=e[0].replace(/^ {1,4}/gm,"");return{type:"code",raw:e[0],codeBlockStyle:"indented",text:this.options.pedantic?t:g(t,"\n")}}},t.fences=function(n){var e=this.rules.block.fences.exec(n);if(e){var t=e[0],r=function(n,e){var t=n.match(/^(\s+)(?:```)/);if(null===t)return e;var r=t[1];return e.split("\n").map((function(n){var e=n.match(/^\s+/);return null===e?n:e[0].length>=r.length?n.slice(r.length):n})).join("\n")}(t,e[3]||"");return{type:"code",raw:t,lang:e[2]?e[2].trim():e[2],text:r}}},t.heading=function(n){var e=this.rules.block.heading.exec(n);if(e){var t=e[2].trim();if(/#$/.test(t)){var r=g(t,"#");this.options.pedantic?t=r.trim():r&&!/ $/.test(r)||(t=r.trim())}var a={type:"heading",raw:e[0],depth:e[1].length,text:t,tokens:[]};return this.lexer.inline(a.text,a.tokens),a}},t.hr=function(n){var e=this.rules.block.hr.exec(n);if(e)return{type:"hr",raw:e[0]}},t.blockquote=function(n){var e=this.rules.block.blockquote.exec(n);if(e){var t=e[0].replace(/^ *> ?/gm,"");return{type:"blockquote",raw:e[0],tokens:this.lexer.blockTokens(t,[]),text:t}}},t.list=function(n){var e=this.rules.block.list.exec(n);if(e){var t,r,a,i,s,l,c,p,u,d,m,g,f=e[1].trim(),h=f.length>1,v={type:"list",raw:"",ordered:h,start:h?+f.slice(0,-1):"",loose:!1,items:[]};f=h?"\\d{1,9}\\"+f.slice(-1):"\\"+f,this.options.pedantic&&(f=h?f:"[*+-]");for(var b=new RegExp("^( {0,3}"+f+")((?: [^\\n]*)?(?:\\n|$))");n&&(g=!1,e=b.exec(n))&&!this.rules.block.hr.test(n);){if(t=e[0],n=n.substring(t.length),p=e[2].split("\n",1)[0],u=n.split("\n",1)[0],this.options.pedantic?(i=2,m=p.trimLeft()):(i=(i=e[2].search(/[^ ]/))>4?1:i,m=p.slice(i),i+=e[1].length),l=!1,!p&&/^ *$/.test(u)&&(t+=u+"\n",n=n.substring(u.length+1),g=!0),!g)for(var k=new RegExp("^ {0,"+Math.min(3,i-1)+"}(?:[*+-]|\\d{1,9}[.)])");n&&(p=d=n.split("\n",1)[0],this.options.pedantic&&(p=p.replace(/^ {1,4}(?=( {4})*[^ ])/g,"  ")),!k.test(p));){if(p.search(/[^ ]/)>=i||!p.trim())m+="\n"+p.slice(i);else{if(l)break;m+="\n"+p}l||p.trim()||(l=!0),t+=d+"\n",n=n.substring(d.length+1)}v.loose||(c?v.loose=!0:/\n *\n *$/.test(t)&&(c=!0)),this.options.gfm&&(r=/^\[[ xX]\] /.exec(m))&&(a="[ ] "!==r[0],m=m.replace(/^\[[ xX]\] +/,"")),v.items.push({type:"list_item",raw:t,task:!!r,checked:a,loose:!1,text:m}),v.raw+=t}v.items[v.items.length-1].raw=t.trimRight(),v.items[v.items.length-1].text=m.trimRight(),v.raw=v.raw.trimRight();var y=v.items.length;for(s=0;s<y;s++){this.lexer.state.top=!1,v.items[s].tokens=this.lexer.blockTokens(v.items[s].text,[]);var S=v.items[s].tokens.filter((function(n){return"space"===n.type})),x=S.every((function(n){for(var e,t=0,r=o(n.raw.split(""));!(e=r()).done;)if("\n"===e.value&&(t+=1),t>1)return!0;return!1}));!v.loose&&S.length&&x&&(v.loose=!0,v.items[s].loose=!0)}return v}},t.html=function(n){var e=this.rules.block.html.exec(n);if(e){var t={type:"html",raw:e[0],pre:!this.options.sanitizer&&("pre"===e[1]||"script"===e[1]||"style"===e[1]),text:e[0]};return this.options.sanitize&&(t.type="paragraph",t.text=this.options.sanitizer?this.options.sanitizer(e[0]):l(e[0]),t.tokens=[],this.lexer.inline(t.text,t.tokens)),t}},t.def=function(n){var e=this.rules.block.def.exec(n);if(e)return e[3]&&(e[3]=e[3].substring(1,e[3].length-1)),{type:"def",tag:e[1].toLowerCase().replace(/\s+/g," "),raw:e[0],href:e[2],title:e[3]}},t.table=function(n){var e=this.rules.block.table.exec(n);if(e){var t={type:"table",header:m(e[1]).map((function(n){return{text:n}})),align:e[2].replace(/^ *|\| *$/g,"").split(/ *\| */),rows:e[3]&&e[3].trim()?e[3].replace(/\n[ \t]*$/,"").split("\n"):[]};if(t.header.length===t.align.length){t.raw=e[0];var r,a,o,i,s=t.align.length;for(r=0;r<s;r++)/^ *-+: *$/.test(t.align[r])?t.align[r]="right":/^ *:-+: *$/.test(t.align[r])?t.align[r]="center":/^ *:-+ *$/.test(t.align[r])?t.align[r]="left":t.align[r]=null;for(s=t.rows.length,r=0;r<s;r++)t.rows[r]=m(t.rows[r],t.header.length).map((function(n){return{text:n}}));for(s=t.header.length,a=0;a<s;a++)t.header[a].tokens=[],this.lexer.inlineTokens(t.header[a].text,t.header[a].tokens);for(s=t.rows.length,a=0;a<s;a++)for(i=t.rows[a],o=0;o<i.length;o++)i[o].tokens=[],this.lexer.inlineTokens(i[o].text,i[o].tokens);return t}}},t.lheading=function(n){var e=this.rules.block.lheading.exec(n);if(e){var t={type:"heading",raw:e[0],depth:"="===e[2].charAt(0)?1:2,text:e[1],tokens:[]};return this.lexer.inline(t.text,t.tokens),t}},t.paragraph=function(n){var e=this.rules.block.paragraph.exec(n);if(e){var t={type:"paragraph",raw:e[0],text:"\n"===e[1].charAt(e[1].length-1)?e[1].slice(0,-1):e[1],tokens:[]};return this.lexer.inline(t.text,t.tokens),t}},t.text=function(n){var e=this.rules.block.text.exec(n);if(e){var t={type:"text",raw:e[0],text:e[0],tokens:[]};return this.lexer.inline(t.text,t.tokens),t}},t.escape=function(n){var e=this.rules.inline.escape.exec(n);if(e)return{type:"escape",raw:e[0],text:l(e[1])}},t.tag=function(n){var e=this.rules.inline.tag.exec(n);if(e)return!this.lexer.state.inLink&&/^<a /i.test(e[0])?this.lexer.state.inLink=!0:this.lexer.state.inLink&&/^<\/a>/i.test(e[0])&&(this.lexer.state.inLink=!1),!this.lexer.state.inRawBlock&&/^<(pre|code|kbd|script)(\s|>)/i.test(e[0])?this.lexer.state.inRawBlock=!0:this.lexer.state.inRawBlock&&/^<\/(pre|code|kbd|script)(\s|>)/i.test(e[0])&&(this.lexer.state.inRawBlock=!1),{type:this.options.sanitize?"text":"html",raw:e[0],inLink:this.lexer.state.inLink,inRawBlock:this.lexer.state.inRawBlock,text:this.options.sanitize?this.options.sanitizer?this.options.sanitizer(e[0]):l(e[0]):e[0]}},t.link=function(n){var e=this.rules.inline.link.exec(n);if(e){var t=e[2].trim();if(!this.options.pedantic&&/^</.test(t)){if(!/>$/.test(t))return;var r=g(t.slice(0,-1),"\\");if((t.length-r.length)%2==0)return}else{var a=function(n,e){if(-1===n.indexOf(e[1]))return-1;for(var t=n.length,r=0,a=0;a<t;a++)if("\\"===n[a])a++;else if(n[a]===e[0])r++;else if(n[a]===e[1]&&--r<0)return a;return-1}(e[2],"()");if(a>-1){var o=(0===e[0].indexOf("!")?5:4)+e[1].length+a;e[2]=e[2].substring(0,a),e[0]=e[0].substring(0,o).trim(),e[3]=""}}var i=e[2],s="";if(this.options.pedantic){var l=/^([^'"]*[^\s])\s+(['"])(.*)\2/.exec(i);l&&(i=l[1],s=l[3])}else s=e[3]?e[3].slice(1,-1):"";return i=i.trim(),/^</.test(i)&&(i=this.options.pedantic&&!/>$/.test(t)?i.slice(1):i.slice(1,-1)),v(e,{href:i?i.replace(this.rules.inline._escapes,"$1"):i,title:s?s.replace(this.rules.inline._escapes,"$1"):s},e[0],this.lexer)}},t.reflink=function(n,e){var t;if((t=this.rules.inline.reflink.exec(n))||(t=this.rules.inline.nolink.exec(n))){var r=(t[2]||t[1]).replace(/\s+/g," ");if(!(r=e[r.toLowerCase()])||!r.href){var a=t[0].charAt(0);return{type:"text",raw:a,text:a}}return v(t,r,t[0],this.lexer)}},t.emStrong=function(n,e,t){void 0===t&&(t="");var r=this.rules.inline.emStrong.lDelim.exec(n);if(r&&(!r[3]||!t.match(/(?:[0-9A-Za-z\xAA\xB2\xB3\xB5\xB9\xBA\xBC-\xBE\xC0-\xD6\xD8-\xF6\xF8-\u02C1\u02C6-\u02D1\u02E0-\u02E4\u02EC\u02EE\u0370-\u0374\u0376\u0377\u037A-\u037D\u037F\u0386\u0388-\u038A\u038C\u038E-\u03A1\u03A3-\u03F5\u03F7-\u0481\u048A-\u052F\u0531-\u0556\u0559\u0560-\u0588\u05D0-\u05EA\u05EF-\u05F2\u0620-\u064A\u0660-\u0669\u066E\u066F\u0671-\u06D3\u06D5\u06E5\u06E6\u06EE-\u06FC\u06FF\u0710\u0712-\u072F\u074D-\u07A5\u07B1\u07C0-\u07EA\u07F4\u07F5\u07FA\u0800-\u0815\u081A\u0824\u0828\u0840-\u0858\u0860-\u086A\u0870-\u0887\u0889-\u088E\u08A0-\u08C9\u0904-\u0939\u093D\u0950\u0958-\u0961\u0966-\u096F\u0971-\u0980\u0985-\u098C\u098F\u0990\u0993-\u09A8\u09AA-\u09B0\u09B2\u09B6-\u09B9\u09BD\u09CE\u09DC\u09DD\u09DF-\u09E1\u09E6-\u09F1\u09F4-\u09F9\u09FC\u0A05-\u0A0A\u0A0F\u0A10\u0A13-\u0A28\u0A2A-\u0A30\u0A32\u0A33\u0A35\u0A36\u0A38\u0A39\u0A59-\u0A5C\u0A5E\u0A66-\u0A6F\u0A72-\u0A74\u0A85-\u0A8D\u0A8F-\u0A91\u0A93-\u0AA8\u0AAA-\u0AB0\u0AB2\u0AB3\u0AB5-\u0AB9\u0ABD\u0AD0\u0AE0\u0AE1\u0AE6-\u0AEF\u0AF9\u0B05-\u0B0C\u0B0F\u0B10\u0B13-\u0B28\u0B2A-\u0B30\u0B32\u0B33\u0B35-\u0B39\u0B3D\u0B5C\u0B5D\u0B5F-\u0B61\u0B66-\u0B6F\u0B71-\u0B77\u0B83\u0B85-\u0B8A\u0B8E-\u0B90\u0B92-\u0B95\u0B99\u0B9A\u0B9C\u0B9E\u0B9F\u0BA3\u0BA4\u0BA8-\u0BAA\u0BAE-\u0BB9\u0BD0\u0BE6-\u0BF2\u0C05-\u0C0C\u0C0E-\u0C10\u0C12-\u0C28\u0C2A-\u0C39\u0C3D\u0C58-\u0C5A\u0C5D\u0C60\u0C61\u0C66-\u0C6F\u0C78-\u0C7E\u0C80\u0C85-\u0C8C\u0C8E-\u0C90\u0C92-\u0CA8\u0CAA-\u0CB3\u0CB5-\u0CB9\u0CBD\u0CDD\u0CDE\u0CE0\u0CE1\u0CE6-\u0CEF\u0CF1\u0CF2\u0D04-\u0D0C\u0D0E-\u0D10\u0D12-\u0D3A\u0D3D\u0D4E\u0D54-\u0D56\u0D58-\u0D61\u0D66-\u0D78\u0D7A-\u0D7F\u0D85-\u0D96\u0D9A-\u0DB1\u0DB3-\u0DBB\u0DBD\u0DC0-\u0DC6\u0DE6-\u0DEF\u0E01-\u0E30\u0E32\u0E33\u0E40-\u0E46\u0E50-\u0E59\u0E81\u0E82\u0E84\u0E86-\u0E8A\u0E8C-\u0EA3\u0EA5\u0EA7-\u0EB0\u0EB2\u0EB3\u0EBD\u0EC0-\u0EC4\u0EC6\u0ED0-\u0ED9\u0EDC-\u0EDF\u0F00\u0F20-\u0F33\u0F40-\u0F47\u0F49-\u0F6C\u0F88-\u0F8C\u1000-\u102A\u103F-\u1049\u1050-\u1055\u105A-\u105D\u1061\u1065\u1066\u106E-\u1070\u1075-\u1081\u108E\u1090-\u1099\u10A0-\u10C5\u10C7\u10CD\u10D0-\u10FA\u10FC-\u1248\u124A-\u124D\u1250-\u1256\u1258\u125A-\u125D\u1260-\u1288\u128A-\u128D\u1290-\u12B0\u12B2-\u12B5\u12B8-\u12BE\u12C0\u12C2-\u12C5\u12C8-\u12D6\u12D8-\u1310\u1312-\u1315\u1318-\u135A\u1369-\u137C\u1380-\u138F\u13A0-\u13F5\u13F8-\u13FD\u1401-\u166C\u166F-\u167F\u1681-\u169A\u16A0-\u16EA\u16EE-\u16F8\u1700-\u1711\u171F-\u1731\u1740-\u1751\u1760-\u176C\u176E-\u1770\u1780-\u17B3\u17D7\u17DC\u17E0-\u17E9\u17F0-\u17F9\u1810-\u1819\u1820-\u1878\u1880-\u1884\u1887-\u18A8\u18AA\u18B0-\u18F5\u1900-\u191E\u1946-\u196D\u1970-\u1974\u1980-\u19AB\u19B0-\u19C9\u19D0-\u19DA\u1A00-\u1A16\u1A20-\u1A54\u1A80-\u1A89\u1A90-\u1A99\u1AA7\u1B05-\u1B33\u1B45-\u1B4C\u1B50-\u1B59\u1B83-\u1BA0\u1BAE-\u1BE5\u1C00-\u1C23\u1C40-\u1C49\u1C4D-\u1C7D\u1C80-\u1C88\u1C90-\u1CBA\u1CBD-\u1CBF\u1CE9-\u1CEC\u1CEE-\u1CF3\u1CF5\u1CF6\u1CFA\u1D00-\u1DBF\u1E00-\u1F15\u1F18-\u1F1D\u1F20-\u1F45\u1F48-\u1F4D\u1F50-\u1F57\u1F59\u1F5B\u1F5D\u1F5F-\u1F7D\u1F80-\u1FB4\u1FB6-\u1FBC\u1FBE\u1FC2-\u1FC4\u1FC6-\u1FCC\u1FD0-\u1FD3\u1FD6-\u1FDB\u1FE0-\u1FEC\u1FF2-\u1FF4\u1FF6-\u1FFC\u2070\u2071\u2074-\u2079\u207F-\u2089\u2090-\u209C\u2102\u2107\u210A-\u2113\u2115\u2119-\u211D\u2124\u2126\u2128\u212A-\u212D\u212F-\u2139\u213C-\u213F\u2145-\u2149\u214E\u2150-\u2189\u2460-\u249B\u24EA-\u24FF\u2776-\u2793\u2C00-\u2CE4\u2CEB-\u2CEE\u2CF2\u2CF3\u2CFD\u2D00-\u2D25\u2D27\u2D2D\u2D30-\u2D67\u2D6F\u2D80-\u2D96\u2DA0-\u2DA6\u2DA8-\u2DAE\u2DB0-\u2DB6\u2DB8-\u2DBE\u2DC0-\u2DC6\u2DC8-\u2DCE\u2DD0-\u2DD6\u2DD8-\u2DDE\u2E2F\u3005-\u3007\u3021-\u3029\u3031-\u3035\u3038-\u303C\u3041-\u3096\u309D-\u309F\u30A1-\u30FA\u30FC-\u30FF\u3105-\u312F\u3131-\u318E\u3192-\u3195\u31A0-\u31BF\u31F0-\u31FF\u3220-\u3229\u3248-\u324F\u3251-\u325F\u3280-\u3289\u32B1-\u32BF\u3400-\u4DBF\u4E00-\uA48C\uA4D0-\uA4FD\uA500-\uA60C\uA610-\uA62B\uA640-\uA66E\uA67F-\uA69D\uA6A0-\uA6EF\uA717-\uA71F\uA722-\uA788\uA78B-\uA7CA\uA7D0\uA7D1\uA7D3\uA7D5-\uA7D9\uA7F2-\uA801\uA803-\uA805\uA807-\uA80A\uA80C-\uA822\uA830-\uA835\uA840-\uA873\uA882-\uA8B3\uA8D0-\uA8D9\uA8F2-\uA8F7\uA8FB\uA8FD\uA8FE\uA900-\uA925\uA930-\uA946\uA960-\uA97C\uA984-\uA9B2\uA9CF-\uA9D9\uA9E0-\uA9E4\uA9E6-\uA9FE\uAA00-\uAA28\uAA40-\uAA42\uAA44-\uAA4B\uAA50-\uAA59\uAA60-\uAA76\uAA7A\uAA7E-\uAAAF\uAAB1\uAAB5\uAAB6\uAAB9-\uAABD\uAAC0\uAAC2\uAADB-\uAADD\uAAE0-\uAAEA\uAAF2-\uAAF4\uAB01-\uAB06\uAB09-\uAB0E\uAB11-\uAB16\uAB20-\uAB26\uAB28-\uAB2E\uAB30-\uAB5A\uAB5C-\uAB69\uAB70-\uABE2\uABF0-\uABF9\uAC00-\uD7A3\uD7B0-\uD7C6\uD7CB-\uD7FB\uF900-\uFA6D\uFA70-\uFAD9\uFB00-\uFB06\uFB13-\uFB17\uFB1D\uFB1F-\uFB28\uFB2A-\uFB36\uFB38-\uFB3C\uFB3E\uFB40\uFB41\uFB43\uFB44\uFB46-\uFBB1\uFBD3-\uFD3D\uFD50-\uFD8F\uFD92-\uFDC7\uFDF0-\uFDFB\uFE70-\uFE74\uFE76-\uFEFC\uFF10-\uFF19\uFF21-\uFF3A\uFF41-\uFF5A\uFF66-\uFFBE\uFFC2-\uFFC7\uFFCA-\uFFCF\uFFD2-\uFFD7\uFFDA-\uFFDC]|\uD800[\uDC00-\uDC0B\uDC0D-\uDC26\uDC28-\uDC3A\uDC3C\uDC3D\uDC3F-\uDC4D\uDC50-\uDC5D\uDC80-\uDCFA\uDD07-\uDD33\uDD40-\uDD78\uDD8A\uDD8B\uDE80-\uDE9C\uDEA0-\uDED0\uDEE1-\uDEFB\uDF00-\uDF23\uDF2D-\uDF4A\uDF50-\uDF75\uDF80-\uDF9D\uDFA0-\uDFC3\uDFC8-\uDFCF\uDFD1-\uDFD5]|\uD801[\uDC00-\uDC9D\uDCA0-\uDCA9\uDCB0-\uDCD3\uDCD8-\uDCFB\uDD00-\uDD27\uDD30-\uDD63\uDD70-\uDD7A\uDD7C-\uDD8A\uDD8C-\uDD92\uDD94\uDD95\uDD97-\uDDA1\uDDA3-\uDDB1\uDDB3-\uDDB9\uDDBB\uDDBC\uDE00-\uDF36\uDF40-\uDF55\uDF60-\uDF67\uDF80-\uDF85\uDF87-\uDFB0\uDFB2-\uDFBA]|\uD802[\uDC00-\uDC05\uDC08\uDC0A-\uDC35\uDC37\uDC38\uDC3C\uDC3F-\uDC55\uDC58-\uDC76\uDC79-\uDC9E\uDCA7-\uDCAF\uDCE0-\uDCF2\uDCF4\uDCF5\uDCFB-\uDD1B\uDD20-\uDD39\uDD80-\uDDB7\uDDBC-\uDDCF\uDDD2-\uDE00\uDE10-\uDE13\uDE15-\uDE17\uDE19-\uDE35\uDE40-\uDE48\uDE60-\uDE7E\uDE80-\uDE9F\uDEC0-\uDEC7\uDEC9-\uDEE4\uDEEB-\uDEEF\uDF00-\uDF35\uDF40-\uDF55\uDF58-\uDF72\uDF78-\uDF91\uDFA9-\uDFAF]|\uD803[\uDC00-\uDC48\uDC80-\uDCB2\uDCC0-\uDCF2\uDCFA-\uDD23\uDD30-\uDD39\uDE60-\uDE7E\uDE80-\uDEA9\uDEB0\uDEB1\uDF00-\uDF27\uDF30-\uDF45\uDF51-\uDF54\uDF70-\uDF81\uDFB0-\uDFCB\uDFE0-\uDFF6]|\uD804[\uDC03-\uDC37\uDC52-\uDC6F\uDC71\uDC72\uDC75\uDC83-\uDCAF\uDCD0-\uDCE8\uDCF0-\uDCF9\uDD03-\uDD26\uDD36-\uDD3F\uDD44\uDD47\uDD50-\uDD72\uDD76\uDD83-\uDDB2\uDDC1-\uDDC4\uDDD0-\uDDDA\uDDDC\uDDE1-\uDDF4\uDE00-\uDE11\uDE13-\uDE2B\uDE80-\uDE86\uDE88\uDE8A-\uDE8D\uDE8F-\uDE9D\uDE9F-\uDEA8\uDEB0-\uDEDE\uDEF0-\uDEF9\uDF05-\uDF0C\uDF0F\uDF10\uDF13-\uDF28\uDF2A-\uDF30\uDF32\uDF33\uDF35-\uDF39\uDF3D\uDF50\uDF5D-\uDF61]|\uD805[\uDC00-\uDC34\uDC47-\uDC4A\uDC50-\uDC59\uDC5F-\uDC61\uDC80-\uDCAF\uDCC4\uDCC5\uDCC7\uDCD0-\uDCD9\uDD80-\uDDAE\uDDD8-\uDDDB\uDE00-\uDE2F\uDE44\uDE50-\uDE59\uDE80-\uDEAA\uDEB8\uDEC0-\uDEC9\uDF00-\uDF1A\uDF30-\uDF3B\uDF40-\uDF46]|\uD806[\uDC00-\uDC2B\uDCA0-\uDCF2\uDCFF-\uDD06\uDD09\uDD0C-\uDD13\uDD15\uDD16\uDD18-\uDD2F\uDD3F\uDD41\uDD50-\uDD59\uDDA0-\uDDA7\uDDAA-\uDDD0\uDDE1\uDDE3\uDE00\uDE0B-\uDE32\uDE3A\uDE50\uDE5C-\uDE89\uDE9D\uDEB0-\uDEF8]|\uD807[\uDC00-\uDC08\uDC0A-\uDC2E\uDC40\uDC50-\uDC6C\uDC72-\uDC8F\uDD00-\uDD06\uDD08\uDD09\uDD0B-\uDD30\uDD46\uDD50-\uDD59\uDD60-\uDD65\uDD67\uDD68\uDD6A-\uDD89\uDD98\uDDA0-\uDDA9\uDEE0-\uDEF2\uDFB0\uDFC0-\uDFD4]|\uD808[\uDC00-\uDF99]|\uD809[\uDC00-\uDC6E\uDC80-\uDD43]|\uD80B[\uDF90-\uDFF0]|[\uD80C\uD81C-\uD820\uD822\uD840-\uD868\uD86A-\uD86C\uD86F-\uD872\uD874-\uD879\uD880-\uD883][\uDC00-\uDFFF]|\uD80D[\uDC00-\uDC2E]|\uD811[\uDC00-\uDE46]|\uD81A[\uDC00-\uDE38\uDE40-\uDE5E\uDE60-\uDE69\uDE70-\uDEBE\uDEC0-\uDEC9\uDED0-\uDEED\uDF00-\uDF2F\uDF40-\uDF43\uDF50-\uDF59\uDF5B-\uDF61\uDF63-\uDF77\uDF7D-\uDF8F]|\uD81B[\uDE40-\uDE96\uDF00-\uDF4A\uDF50\uDF93-\uDF9F\uDFE0\uDFE1\uDFE3]|\uD821[\uDC00-\uDFF7]|\uD823[\uDC00-\uDCD5\uDD00-\uDD08]|\uD82B[\uDFF0-\uDFF3\uDFF5-\uDFFB\uDFFD\uDFFE]|\uD82C[\uDC00-\uDD22\uDD50-\uDD52\uDD64-\uDD67\uDD70-\uDEFB]|\uD82F[\uDC00-\uDC6A\uDC70-\uDC7C\uDC80-\uDC88\uDC90-\uDC99]|\uD834[\uDEE0-\uDEF3\uDF60-\uDF78]|\uD835[\uDC00-\uDC54\uDC56-\uDC9C\uDC9E\uDC9F\uDCA2\uDCA5\uDCA6\uDCA9-\uDCAC\uDCAE-\uDCB9\uDCBB\uDCBD-\uDCC3\uDCC5-\uDD05\uDD07-\uDD0A\uDD0D-\uDD14\uDD16-\uDD1C\uDD1E-\uDD39\uDD3B-\uDD3E\uDD40-\uDD44\uDD46\uDD4A-\uDD50\uDD52-\uDEA5\uDEA8-\uDEC0\uDEC2-\uDEDA\uDEDC-\uDEFA\uDEFC-\uDF14\uDF16-\uDF34\uDF36-\uDF4E\uDF50-\uDF6E\uDF70-\uDF88\uDF8A-\uDFA8\uDFAA-\uDFC2\uDFC4-\uDFCB\uDFCE-\uDFFF]|\uD837[\uDF00-\uDF1E]|\uD838[\uDD00-\uDD2C\uDD37-\uDD3D\uDD40-\uDD49\uDD4E\uDE90-\uDEAD\uDEC0-\uDEEB\uDEF0-\uDEF9]|\uD839[\uDFE0-\uDFE6\uDFE8-\uDFEB\uDFED\uDFEE\uDFF0-\uDFFE]|\uD83A[\uDC00-\uDCC4\uDCC7-\uDCCF\uDD00-\uDD43\uDD4B\uDD50-\uDD59]|\uD83B[\uDC71-\uDCAB\uDCAD-\uDCAF\uDCB1-\uDCB4\uDD01-\uDD2D\uDD2F-\uDD3D\uDE00-\uDE03\uDE05-\uDE1F\uDE21\uDE22\uDE24\uDE27\uDE29-\uDE32\uDE34-\uDE37\uDE39\uDE3B\uDE42\uDE47\uDE49\uDE4B\uDE4D-\uDE4F\uDE51\uDE52\uDE54\uDE57\uDE59\uDE5B\uDE5D\uDE5F\uDE61\uDE62\uDE64\uDE67-\uDE6A\uDE6C-\uDE72\uDE74-\uDE77\uDE79-\uDE7C\uDE7E\uDE80-\uDE89\uDE8B-\uDE9B\uDEA1-\uDEA3\uDEA5-\uDEA9\uDEAB-\uDEBB]|\uD83C[\uDD00-\uDD0C]|\uD83E[\uDFF0-\uDFF9]|\uD869[\uDC00-\uDEDF\uDF00-\uDFFF]|\uD86D[\uDC00-\uDF38\uDF40-\uDFFF]|\uD86E[\uDC00-\uDC1D\uDC20-\uDFFF]|\uD873[\uDC00-\uDEA1\uDEB0-\uDFFF]|\uD87A[\uDC00-\uDFE0]|\uD87E[\uDC00-\uDE1D]|\uD884[\uDC00-\uDF4A])/))){var a=r[1]||r[2]||"";if(!a||a&&(""===t||this.rules.inline.punctuation.exec(t))){var o,i,s=r[0].length-1,l=s,c=0,p="*"===r[0][0]?this.rules.inline.emStrong.rDelimAst:this.rules.inline.emStrong.rDelimUnd;for(p.lastIndex=0,e=e.slice(-1*n.length+s);null!=(r=p.exec(e));)if(o=r[1]||r[2]||r[3]||r[4]||r[5]||r[6])if(i=o.length,r[3]||r[4])l+=i;else if(!((r[5]||r[6])&&s%3)||(s+i)%3){if(!((l-=i)>0)){if(i=Math.min(i,i+l+c),Math.min(s,i)%2){var u=n.slice(1,s+r.index+i);return{type:"em",raw:n.slice(0,s+r.index+i+1),text:u,tokens:this.lexer.inlineTokens(u,[])}}var d=n.slice(2,s+r.index+i-1);return{type:"strong",raw:n.slice(0,s+r.index+i+1),text:d,tokens:this.lexer.inlineTokens(d,[])}}}else c+=i}}},t.codespan=function(n){var e=this.rules.inline.code.exec(n);if(e){var t=e[2].replace(/\n/g," "),r=/[^ ]/.test(t),a=/^ /.test(t)&&/ $/.test(t);return r&&a&&(t=t.substring(1,t.length-1)),t=l(t,!0),{type:"codespan",raw:e[0],text:t}}},t.br=function(n){var e=this.rules.inline.br.exec(n);if(e)return{type:"br",raw:e[0]}},t.del=function(n){var e=this.rules.inline.del.exec(n);if(e)return{type:"del",raw:e[0],text:e[2],tokens:this.lexer.inlineTokens(e[2],[])}},t.autolink=function(n,e){var t,r,a=this.rules.inline.autolink.exec(n);if(a)return r="@"===a[2]?"mailto:"+(t=l(this.options.mangle?e(a[1]):a[1])):t=l(a[1]),{type:"link",raw:a[0],text:t,href:r,tokens:[{type:"text",raw:t,text:t}]}},t.url=function(n,e){var t;if(t=this.rules.inline.url.exec(n)){var r,a;if("@"===t[2])a="mailto:"+(r=l(this.options.mangle?e(t[0]):t[0]));else{var o;do{o=t[0],t[0]=this.rules.inline._backpedal.exec(t[0])[0]}while(o!==t[0]);r=l(t[0]),a="www."===t[1]?"http://"+r:r}return{type:"link",raw:t[0],text:r,href:a,tokens:[{type:"text",raw:r,text:r}]}}},t.inlineText=function(n,e){var t,r=this.rules.inline.text.exec(n);if(r)return t=this.lexer.state.inRawBlock?this.options.sanitize?this.options.sanitizer?this.options.sanitizer(r[0]):l(r[0]):r[0]:l(this.options.smartypants?e(r[0]):r[0]),{type:"text",raw:r[0],text:t}},e}(),j={newline:/^(?: *(?:\n|$))+/,code:/^( {4}[^\n]+(?:\n(?: *(?:\n|$))*)?)+/,fences:/^ {0,3}(`{3,}(?=[^`\n]*\n)|~{3,})([^\n]*)\n(?:|([\s\S]*?)\n)(?: {0,3}\1[~`]* *(?=\n|$)|$)/,hr:/^ {0,3}((?:- *){3,}|(?:_ *){3,}|(?:\* *){3,})(?:\n+|$)/,heading:/^ {0,3}(#{1,6})(?=\s|$)(.*)(?:\n+|$)/,blockquote:/^( {0,3}> ?(paragraph|[^\n]*)(?:\n|$))+/,list:/^( {0,3}bull)( [^\n]+?)?(?:\n|$)/,html:"^ {0,3}(?:<(script|pre|style|textarea)[\\s>][\\s\\S]*?(?:</\\1>[^\\n]*\\n+|$)|comment[^\\n]*(\\n+|$)|<\\?[\\s\\S]*?(?:\\?>\\n*|$)|<![A-Z][\\s\\S]*?(?:>\\n*|$)|<!\\[CDATA\\[[\\s\\S]*?(?:\\]\\]>\\n*|$)|</?(tag)(?: +|\\n|/?>)[\\s\\S]*?(?:(?:\\n *)+\\n|$)|<(?!script|pre|style|textarea)([a-z][\\w-]*)(?:attribute)*? */?>(?=[ \\t]*(?:\\n|$))[\\s\\S]*?(?:(?:\\n *)+\\n|$)|</(?!script|pre|style|textarea)[a-z][\\w-]*\\s*>(?=[ \\t]*(?:\\n|$))[\\s\\S]*?(?:(?:\\n *)+\\n|$))",def:/^ {0,3}\[(label)\]: *(?:\n *)?<?([^\s>]+)>?(?:(?: +(?:\n *)?| *\n *)(title))? *(?:\n+|$)/,table:B,lheading:/^([^\n]+)\n {0,3}(=+|-+) *(?:\n+|$)/,_paragraph:/^([^\n]+(?:\n(?!hr|heading|lheading|blockquote|fences|list|html|table| +\n)[^\n]+)*)/,text:/^[^\n]+/,_label:/(?!\s*\])(?:\\.|[^\[\]\\])+/,_title:/(?:"(?:\\"?|[^"\\])*"|'[^'\n]*(?:\n[^'\n]+)*\n?'|\([^()]*\))/};j.def=p(j.def).replace("label",j._label).replace("title",j._title).getRegex(),j.bullet=/(?:[*+-]|\d{1,9}[.)])/,j.listItemStart=p(/^( *)(bull) */).replace("bull",j.bullet).getRegex(),j.list=p(j.list).replace(/bull/g,j.bullet).replace("hr","\\n+(?=\\1?(?:(?:- *){3,}|(?:_ *){3,}|(?:\\* *){3,})(?:\\n+|$))").replace("def","\\n+(?="+j.def.source+")").getRegex(),j._tag="address|article|aside|base|basefont|blockquote|body|caption|center|col|colgroup|dd|details|dialog|dir|div|dl|dt|fieldset|figcaption|figure|footer|form|frame|frameset|h[1-6]|head|header|hr|html|iframe|legend|li|link|main|menu|menuitem|meta|nav|noframes|ol|optgroup|option|p|param|section|source|summary|table|tbody|td|tfoot|th|thead|title|tr|track|ul",j._comment=/<!--(?!-?>)[\s\S]*?(?:-->|$)/,j.html=p(j.html,"i").replace("comment",j._comment).replace("tag",j._tag).replace("attribute",/ +[a-zA-Z:_][\w.:-]*(?: *= *"[^"\n]*"| *= *'[^'\n]*'| *= *[^\s"'=<>`]+)?/).getRegex(),j.paragraph=p(j._paragraph).replace("hr",j.hr).replace("heading"," {0,3}#{1,6} ").replace("|lheading","").replace("|table","").replace("blockquote"," {0,3}>").replace("fences"," {0,3}(?:`{3,}(?=[^`\\n]*\\n)|~{3,})[^\\n]*\\n").replace("list"," {0,3}(?:[*+-]|1[.)]) ").replace("html","</?(?:tag)(?: +|\\n|/?>)|<(?:script|pre|style|textarea|!--)").replace("tag",j._tag).getRegex(),j.blockquote=p(j.blockquote).replace("paragraph",j.paragraph).getRegex(),j.normal=d({},j),j.gfm=d({},j.normal,{table:"^ *([^\\n ].*\\|.*)\\n {0,3}(?:\\| *)?(:?-+:? *(?:\\| *:?-+:? *)*)(?:\\| *)?(?:\\n((?:(?! *\\n|hr|heading|blockquote|code|fences|list|html).*(?:\\n|$))*)\\n*|$)"}),j.gfm.table=p(j.gfm.table).replace("hr",j.hr).replace("heading"," {0,3}#{1,6} ").replace("blockquote"," {0,3}>").replace("code"," {4}[^\\n]").replace("fences"," {0,3}(?:`{3,}(?=[^`\\n]*\\n)|~{3,})[^\\n]*\\n").replace("list"," {0,3}(?:[*+-]|1[.)]) ").replace("html","</?(?:tag)(?: +|\\n|/?>)|<(?:script|pre|style|textarea|!--)").replace("tag",j._tag).getRegex(),j.gfm.paragraph=p(j._paragraph).replace("hr",j.hr).replace("heading"," {0,3}#{1,6} ").replace("|lheading","").replace("table",j.gfm.table).replace("blockquote"," {0,3}>").replace("fences"," {0,3}(?:`{3,}(?=[^`\\n]*\\n)|~{3,})[^\\n]*\\n").replace("list"," {0,3}(?:[*+-]|1[.)]) ").replace("html","</?(?:tag)(?: +|\\n|/?>)|<(?:script|pre|style|textarea|!--)").replace("tag",j._tag).getRegex(),j.pedantic=d({},j.normal,{html:p("^ *(?:comment *(?:\\n|\\s*$)|<(tag)[\\s\\S]+?</\\1> *(?:\\n{2,}|\\s*$)|<tag(?:\"[^\"]*\"|'[^']*'|\\s[^'\"/>\\s]*)*?/?> *(?:\\n{2,}|\\s*$))").replace("comment",j._comment).replace(/tag/g,"(?!(?:a|em|strong|small|s|cite|q|dfn|abbr|data|time|code|var|samp|kbd|sub|sup|i|b|u|mark|ruby|rt|rp|bdi|bdo|span|br|wbr|ins|del|img)\\b)\\w+(?!:|[^\\w\\s@]*@)\\b").getRegex(),def:/^ *\[([^\]]+)\]: *<?([^\s>]+)>?(?: +(["(][^\n]+[")]))? *(?:\n+|$)/,heading:/^(#{1,6})(.*)(?:\n+|$)/,fences:B,paragraph:p(j.normal._paragraph).replace("hr",j.hr).replace("heading"," *#{1,6} *[^\n]").replace("lheading",j.lheading).replace("blockquote"," {0,3}>").replace("|fences","").replace("|list","").replace("|html","").getRegex()});var L={escape:/^\\([!"#$%&'()*+,\-./:;<=>?@\[\]\\^_`{|}~])/,autolink:/^<(scheme:[^\s\x00-\x1f<>]*|email)>/,url:B,tag:"^comment|^</[a-zA-Z][\\w:-]*\\s*>|^<[a-zA-Z][\\w-]*(?:attribute)*?\\s*/?>|^<\\?[\\s\\S]*?\\?>|^<![a-zA-Z]+\\s[\\s\\S]*?>|^<!\\[CDATA\\[[\\s\\S]*?\\]\\]>",link:/^!?\[(label)\]\(\s*(href)(?:\s+(title))?\s*\)/,reflink:/^!?\[(label)\]\[(ref)\]/,nolink:/^!?\[(ref)\](?:\[\])?/,reflinkSearch:"reflink|nolink(?!\\()",emStrong:{lDelim:/^(?:\*+(?:([punct_])|[^\s*]))|^_+(?:([punct*])|([^\s_]))/,rDelimAst:/^[^_*]*?\_\_[^_*]*?\*[^_*]*?(?=\_\_)|[punct_](\*+)(?=[\s]|$)|[^punct*_\s](\*+)(?=[punct_\s]|$)|[punct_\s](\*+)(?=[^punct*_\s])|[\s](\*+)(?=[punct_])|[punct_](\*+)(?=[punct_])|[^punct*_\s](\*+)(?=[^punct*_\s])/,rDelimUnd:/^[^_*]*?\*\*[^_*]*?\_[^_*]*?(?=\*\*)|[punct*](\_+)(?=[\s]|$)|[^punct*_\s](\_+)(?=[punct*\s]|$)|[punct*\s](\_+)(?=[^punct*_\s])|[\s](\_+)(?=[punct*])|[punct*](\_+)(?=[punct*])/},code:/^(`+)([^`]|[^`][\s\S]*?[^`])\1(?!`)/,br:/^( {2,}|\\)\n(?!\s*$)/,del:B,text:/^(`+|[^`])(?:(?= {2,}\n)|[\s\S]*?(?:(?=[\\<!\[`*_]|\b_|$)|[^ ](?= {2,}\n)))/,punctuation:/^([\spunctuation])/,_punctuation:"!\"#$%&'()+\\-.,/:;<=>?@\\[\\]`^{|}~"};L.punctuation=p(L.punctuation).replace(/punctuation/g,L._punctuation).getRegex(),L.blockSkip=/\[[^\]]*?\]\([^\)]*?\)|`[^`]*?`|<[^>]*?>/g,L.escapedEmSt=/\\\*|\\_/g,L._comment=p(j._comment).replace("(?:--\x3e|$)","--\x3e").getRegex(),L.emStrong.lDelim=p(L.emStrong.lDelim).replace(/punct/g,L._punctuation).getRegex(),L.emStrong.rDelimAst=p(L.emStrong.rDelimAst,"g").replace(/punct/g,L._punctuation).getRegex(),L.emStrong.rDelimUnd=p(L.emStrong.rDelimUnd,"g").replace(/punct/g,L._punctuation).getRegex(),L._escapes=/\\([!"#$%&'()*+,\-./:;<=>?@\[\]\\^_`{|}~])/g,L._scheme=/[a-zA-Z][a-zA-Z0-9+.-]{1,31}/,L._email=/[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+(@)[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)+(?![-_])/,L.autolink=p(L.autolink).replace("scheme",L._scheme).replace("email",L._email).getRegex(),L._attribute=/\s+[a-zA-Z:_][\w.:-]*(?:\s*=\s*"[^"]*"|\s*=\s*'[^']*'|\s*=\s*[^\s"'=<>`]+)?/,L.tag=p(L.tag).replace("comment",L._comment).replace("attribute",L._attribute).getRegex(),L._label=/(?:\[(?:\\.|[^\[\]\\])*\]|\\.|`[^`]*`|[^\[\]\\`])*?/,L._href=/<(?:\\.|[^\n<>\\])+>|[^\s\x00-\x1f]*/,L._title=/"(?:\\"?|[^"\\])*"|'(?:\\'?|[^'\\])*'|\((?:\\\)?|[^)\\])*\)/,L.link=p(L.link).replace("label",L._label).replace("href",L._href).replace("title",L._title).getRegex(),L.reflink=p(L.reflink).replace("label",L._label).replace("ref",j._label).getRegex(),L.nolink=p(L.nolink).replace("ref",j._label).getRegex(),L.reflinkSearch=p(L.reflinkSearch,"g").replace("reflink",L.reflink).replace("nolink",L.nolink).getRegex(),L.normal=d({},L),L.pedantic=d({},L.normal,{strong:{start:/^__|\*\*/,middle:/^__(?=\S)([\s\S]*?\S)__(?!_)|^\*\*(?=\S)([\s\S]*?\S)\*\*(?!\*)/,endAst:/\*\*(?!\*)/g,endUnd:/__(?!_)/g},em:{start:/^_|\*/,middle:/^()\*(?=\S)([\s\S]*?\S)\*(?!\*)|^_(?=\S)([\s\S]*?\S)_(?!_)/,endAst:/\*(?!\*)/g,endUnd:/_(?!_)/g},link:p(/^!?\[(label)\]\((.*?)\)/).replace("label",L._label).getRegex(),reflink:p(/^!?\[(label)\]\s*\[([^\]]*)\]/).replace("label",L._label).getRegex()}),L.gfm=d({},L.normal,{escape:p(L.escape).replace("])","~|])").getRegex(),_extended_email:/[A-Za-z0-9._+-]+(@)[a-zA-Z0-9-_]+(?:\.[a-zA-Z0-9-_]*[a-zA-Z0-9])+(?![-_])/,url:/^((?:ftp|https?):\/\/|www\.)(?:[a-zA-Z0-9\-]+\.?)+[^\s<]*|^email/,_backpedal:/(?:[^?!.,:;*_~()&]+|\([^)]*\)|&(?![a-zA-Z0-9]+;$)|[?!.,:;*_~)]+(?!$))+/,del:/^(~~?)(?=[^\s~])([\s\S]*?[^\s~])\1(?=[^~]|$)/,text:/^([`~]+|[^`~])(?:(?= {2,}\n)|(?=[a-zA-Z0-9.!#$%&'*+\/=?_`{\|}~-]+@)|[\s\S]*?(?:(?=[\\<!\[`*~_]|\b_|https?:\/\/|ftp:\/\/|www\.|$)|[^ ](?= {2,}\n)|[^a-zA-Z0-9.!#$%&'*+\/=?_`{\|}~-](?=[a-zA-Z0-9.!#$%&'*+\/=?_`{\|}~-]+@)))/}),L.gfm.url=p(L.gfm.url,"i").replace("email",L.gfm._extended_email).getRegex(),L.breaks=d({},L.gfm,{br:p(L.br).replace("{2,}","*").getRegex(),text:p(L.gfm.text).replace("\\b_","\\b_| {2,}\\n").replace(/\{2,\}/g,"*").getRegex()});var N=function(){function e(e){this.tokens=[],this.tokens.links=Object.create(null),this.options=e||n.defaults,this.options.tokenizer=this.options.tokenizer||new M,this.tokenizer=this.options.tokenizer,this.tokenizer.options=this.options,this.tokenizer.lexer=this,this.inlineQueue=[],this.state={inLink:!1,inRawBlock:!1,top:!0};var t={block:j.normal,inline:L.normal};this.options.pedantic?(t.block=j.pedantic,t.inline=L.pedantic):this.options.gfm&&(t.block=j.gfm,this.options.breaks?t.inline=L.breaks:t.inline=L.gfm),this.tokenizer.rules=t}e.lex=function(n,t){return new e(t).lex(n)},e.lexInline=function(n,t){return new e(t).inlineTokens(n)};var r=e.prototype;return r.lex=function(n){n=n.replace(/\r\n|\r/g,"\n").replace(/\t/g,"    "),this.blockTokens(n,this.tokens);for(var e;e=this.inlineQueue.shift();)this.inlineTokens(e.src,e.tokens);return this.tokens},r.blockTokens=function(n,e){var t,r,a,o,i=this;for(void 0===e&&(e=[]),this.options.pedantic&&(n=n.replace(/^ +$/gm,""));n;)if(!(this.options.extensions&&this.options.extensions.block&&this.options.extensions.block.some((function(r){return!!(t=r.call({lexer:i},n,e))&&(n=n.substring(t.raw.length),e.push(t),!0)}))))if(t=this.tokenizer.space(n))n=n.substring(t.raw.length),1===t.raw.length&&e.length>0?e[e.length-1].raw+="\n":e.push(t);else if(t=this.tokenizer.code(n))n=n.substring(t.raw.length),!(r=e[e.length-1])||"paragraph"!==r.type&&"text"!==r.type?e.push(t):(r.raw+="\n"+t.raw,r.text+="\n"+t.text,this.inlineQueue[this.inlineQueue.length-1].src=r.text);else if(t=this.tokenizer.fences(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.heading(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.hr(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.blockquote(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.list(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.html(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.def(n))n=n.substring(t.raw.length),!(r=e[e.length-1])||"paragraph"!==r.type&&"text"!==r.type?this.tokens.links[t.tag]||(this.tokens.links[t.tag]={href:t.href,title:t.title}):(r.raw+="\n"+t.raw,r.text+="\n"+t.raw,this.inlineQueue[this.inlineQueue.length-1].src=r.text);else if(t=this.tokenizer.table(n))n=n.substring(t.raw.length),e.push(t);else if(t=this.tokenizer.lheading(n))n=n.substring(t.raw.length),e.push(t);else if(a=n,this.options.extensions&&this.options.extensions.startBlock&&function(){var e=1/0,t=n.slice(1),r=void 0;i.options.extensions.startBlock.forEach((function(n){"number"==typeof(r=n.call({lexer:this},t))&&r>=0&&(e=Math.min(e,r))})),e<1/0&&e>=0&&(a=n.substring(0,e+1))}(),this.state.top&&(t=this.tokenizer.paragraph(a)))r=e[e.length-1],o&&"paragraph"===r.type?(r.raw+="\n"+t.raw,r.text+="\n"+t.text,this.inlineQueue.pop(),this.inlineQueue[this.inlineQueue.length-1].src=r.text):e.push(t),o=a.length!==n.length,n=n.substring(t.raw.length);else if(t=this.tokenizer.text(n))n=n.substring(t.raw.length),(r=e[e.length-1])&&"text"===r.type?(r.raw+="\n"+t.raw,r.text+="\n"+t.text,this.inlineQueue.pop(),this.inlineQueue[this.inlineQueue.length-1].src=r.text):e.push(t);else if(n){var s="Infinite loop on byte: "+n.charCodeAt(0);if(this.options.silent)break;throw new Error(s)}return this.state.top=!0,e},r.inline=function(n,e){this.inlineQueue.push({src:n,tokens:e})},r.inlineTokens=function(n,e){var t=this;void 0===e&&(e=[]);var r,a,o,i,s,l,c=n;if(this.tokens.links){var p=Object.keys(this.tokens.links);if(p.length>0)for(;null!=(i=this.tokenizer.rules.inline.reflinkSearch.exec(c));)p.includes(i[0].slice(i[0].lastIndexOf("[")+1,-1))&&(c=c.slice(0,i.index)+"["+h("a",i[0].length-2)+"]"+c.slice(this.tokenizer.rules.inline.reflinkSearch.lastIndex))}for(;null!=(i=this.tokenizer.rules.inline.blockSkip.exec(c));)c=c.slice(0,i.index)+"["+h("a",i[0].length-2)+"]"+c.slice(this.tokenizer.rules.inline.blockSkip.lastIndex);for(;null!=(i=this.tokenizer.rules.inline.escapedEmSt.exec(c));)c=c.slice(0,i.index)+"++"+c.slice(this.tokenizer.rules.inline.escapedEmSt.lastIndex);for(;n;)if(s||(l=""),s=!1,!(this.options.extensions&&this.options.extensions.inline&&this.options.extensions.inline.some((function(a){return!!(r=a.call({lexer:t},n,e))&&(n=n.substring(r.raw.length),e.push(r),!0)}))))if(r=this.tokenizer.escape(n))n=n.substring(r.raw.length),e.push(r);else if(r=this.tokenizer.tag(n))n=n.substring(r.raw.length),(a=e[e.length-1])&&"text"===r.type&&"text"===a.type?(a.raw+=r.raw,a.text+=r.text):e.push(r);else if(r=this.tokenizer.link(n))n=n.substring(r.raw.length),e.push(r);else if(r=this.tokenizer.reflink(n,this.tokens.links))n=n.substring(r.raw.length),(a=e[e.length-1])&&"text"===r.type&&"text"===a.type?(a.raw+=r.raw,a.text+=r.text):e.push(r);else if(r=this.tokenizer.emStrong(n,c,l))n=n.substring(r.raw.length),e.push(r);else if(r=this.tokenizer.codespan(n))n=n.substring(r.raw.length),e.push(r);else if(r=this.tokenizer.br(n))n=n.substring(r.raw.length),e.push(r);else if(r=this.tokenizer.del(n))n=n.substring(r.raw.length),e.push(r);else if(r=this.tokenizer.autolink(n,k))n=n.substring(r.raw.length),e.push(r);else if(this.state.inLink||!(r=this.tokenizer.url(n,k))){if(o=n,this.options.extensions&&this.options.extensions.startInline&&function(){var e=1/0,r=n.slice(1),a=void 0;t.options.extensions.startInline.forEach((function(n){"number"==typeof(a=n.call({lexer:this},r))&&a>=0&&(e=Math.min(e,a))})),e<1/0&&e>=0&&(o=n.substring(0,e+1))}(),r=this.tokenizer.inlineText(o,b))n=n.substring(r.raw.length),"_"!==r.raw.slice(-1)&&(l=r.raw.slice(-1)),s=!0,(a=e[e.length-1])&&"text"===a.type?(a.raw+=r.raw,a.text+=r.text):e.push(r);else if(n){var u="Infinite loop on byte: "+n.charCodeAt(0);if(this.options.silent)break;throw new Error(u)}}else n=n.substring(r.raw.length),e.push(r);return e},t(e,null,[{key:"rules",get:function(){return{block:j,inline:L}}}]),e}(),$=function(){function e(e){this.options=e||n.defaults}var t=e.prototype;return t.code=function(n,e,t){var r=(e||"").match(/\S*/)[0];if(this.options.highlight){var a=this.options.highlight(n,r);null!=a&&a!==n&&(t=!0,n=a)}return n=n.replace(/\n$/,"")+"\n",r?'<pre><code class="'+this.options.langPrefix+l(r,!0)+'">'+(t?n:l(n,!0))+"</code></pre>\n":"<pre><code>"+(t?n:l(n,!0))+"</code></pre>\n"},t.blockquote=function(n){return"<blockquote>\n"+n+"</blockquote>\n"},t.html=function(n){return n},t.heading=function(n,e,t,r){return this.options.headerIds?"<h"+e+' id="'+this.options.headerPrefix+r.slug(t)+'">'+n+"</h"+e+">\n":"<h"+e+">"+n+"</h"+e+">\n"},t.hr=function(){return this.options.xhtml?"<hr/>\n":"<hr>\n"},t.list=function(n,e,t){var r=e?"ol":"ul";return"<"+r+(e&&1!==t?' start="'+t+'"':"")+">\n"+n+"</"+r+">\n"},t.listitem=function(n){return"<li>"+n+"</li>\n"},t.checkbox=function(n){return"<input "+(n?'checked="" ':"")+'disabled="" type="checkbox"'+(this.options.xhtml?" /":"")+"> "},t.paragraph=function(n){return"<p>"+n+"</p>\n"},t.table=function(n,e){return e&&(e="<tbody>"+e+"</tbody>"),"<table>\n<thead>\n"+n+"</thead>\n"+e+"</table>\n"},t.tablerow=function(n){return"<tr>\n"+n+"</tr>\n"},t.tablecell=function(n,e){var t=e.header?"th":"td";return(e.align?"<"+t+' align="'+e.align+'">':"<"+t+">")+n+"</"+t+">\n"},t.strong=function(n){return"<strong>"+n+"</strong>"},t.em=function(n){return"<em>"+n+"</em>"},t.codespan=function(n){return"<code>"+n+"</code>"},t.br=function(){return this.options.xhtml?"<br/>":"<br>"},t.del=function(n){return"<del>"+n+"</del>"},t.link=function(n,e,t){if(null===(n=u(this.options.sanitize,this.options.baseUrl,n)))return t;var r='<a href="'+l(n)+'"';return e&&(r+=' title="'+e+'"'),r+">"+t+"</a>"},t.image=function(n,e,t){if(null===(n=u(this.options.sanitize,this.options.baseUrl,n)))return t;var r='<img src="'+n+'" alt="'+t+'"';return e&&(r+=' title="'+e+'"'),r+(this.options.xhtml?"/>":">")},t.text=function(n){return n},e}(),U=function(){function n(){}var e=n.prototype;return e.strong=function(n){return n},e.em=function(n){return n},e.codespan=function(n){return n},e.del=function(n){return n},e.html=function(n){return n},e.text=function(n){return n},e.link=function(n,e,t){return""+t},e.image=function(n,e,t){return""+t},e.br=function(){return""},n}(),z=function(){function n(){this.seen={}}var e=n.prototype;return e.serialize=function(n){return n.toLowerCase().trim().replace(/<[!\/a-z].*?>/gi,"").replace(/[\u2000-\u206F\u2E00-\u2E7F\\'!"#$%&()*+,./:;<=>?@[\]^`{|}~]/g,"").replace(/\s/g,"-")},e.getNextSafeSlug=function(n,e){var t=n,r=0;if(this.seen.hasOwnProperty(t)){r=this.seen[n];do{t=n+"-"+ ++r}while(this.seen.hasOwnProperty(t))}return e||(this.seen[n]=r,this.seen[t]=0),t},e.slug=function(n,e){void 0===e&&(e={});var t=this.serialize(n);return this.getNextSafeSlug(t,e.dryrun)},n}(),H=function(){function e(e){this.options=e||n.defaults,this.options.renderer=this.options.renderer||new $,this.renderer=this.options.renderer,this.renderer.options=this.options,this.textRenderer=new U,this.slugger=new z}e.parse=function(n,t){return new e(t).parse(n)},e.parseInline=function(n,t){return new e(t).parseInline(n)};var t=e.prototype;return t.parse=function(n,e){void 0===e&&(e=!0);var t,r,a,o,i,s,l,p,u,d,m,g,f,h,v,b,k,y,S,x="",w=n.length;for(t=0;t<w;t++)if(d=n[t],!(this.options.extensions&&this.options.extensions.renderers&&this.options.extensions.renderers[d.type])||!1===(S=this.options.extensions.renderers[d.type].call({parser:this},d))&&["space","hr","heading","code","table","blockquote","list","html","paragraph","text"].includes(d.type))switch(d.type){case"space":continue;case"hr":x+=this.renderer.hr();continue;case"heading":x+=this.renderer.heading(this.parseInline(d.tokens),d.depth,c(this.parseInline(d.tokens,this.textRenderer)),this.slugger);continue;case"code":x+=this.renderer.code(d.text,d.lang,d.escaped);continue;case"table":for(p="",l="",o=d.header.length,r=0;r<o;r++)l+=this.renderer.tablecell(this.parseInline(d.header[r].tokens),{header:!0,align:d.align[r]});for(p+=this.renderer.tablerow(l),u="",o=d.rows.length,r=0;r<o;r++){for(l="",i=(s=d.rows[r]).length,a=0;a<i;a++)l+=this.renderer.tablecell(this.parseInline(s[a].tokens),{header:!1,align:d.align[a]});u+=this.renderer.tablerow(l)}x+=this.renderer.table(p,u);continue;case"blockquote":u=this.parse(d.tokens),x+=this.renderer.blockquote(u);continue;case"list":for(m=d.ordered,g=d.start,f=d.loose,o=d.items.length,u="",r=0;r<o;r++)b=(v=d.items[r]).checked,k=v.task,h="",v.task&&(y=this.renderer.checkbox(b),f?v.tokens.length>0&&"paragraph"===v.tokens[0].type?(v.tokens[0].text=y+" "+v.tokens[0].text,v.tokens[0].tokens&&v.tokens[0].tokens.length>0&&"text"===v.tokens[0].tokens[0].type&&(v.tokens[0].tokens[0].text=y+" "+v.tokens[0].tokens[0].text)):v.tokens.unshift({type:"text",text:y}):h+=y),h+=this.parse(v.tokens,f),u+=this.renderer.listitem(h,k,b);x+=this.renderer.list(u,m,g);continue;case"html":x+=this.renderer.html(d.text);continue;case"paragraph":x+=this.renderer.paragraph(this.parseInline(d.tokens));continue;case"text":for(u=d.tokens?this.parseInline(d.tokens):d.text;t+1<w&&"text"===n[t+1].type;)u+="\n"+((d=n[++t]).tokens?this.parseInline(d.tokens):d.text);x+=e?this.renderer.paragraph(u):u;continue;default:var E='Token with "'+d.type+'" type was not found.';if(this.options.silent)return;throw new Error(E)}else x+=S||"";return x},t.parseInline=function(n,e){e=e||this.renderer;var t,r,a,o="",i=n.length;for(t=0;t<i;t++)if(r=n[t],!(this.options.extensions&&this.options.extensions.renderers&&this.options.extensions.renderers[r.type])||!1===(a=this.options.extensions.renderers[r.type].call({parser:this},r))&&["escape","html","link","image","strong","em","codespan","br","del","text"].includes(r.type))switch(r.type){case"escape":o+=e.text(r.text);break;case"html":o+=e.html(r.text);break;case"link":o+=e.link(r.href,r.title,this.parseInline(r.tokens,e));break;case"image":o+=e.image(r.href,r.title,r.text);break;case"strong":o+=e.strong(this.parseInline(r.tokens,e));break;case"em":o+=e.em(this.parseInline(r.tokens,e));break;case"codespan":o+=e.codespan(r.text);break;case"br":o+=e.br();break;case"del":o+=e.del(this.parseInline(r.tokens,e));break;case"text":o+=e.text(r.text);break;default:var s='Token with "'+r.type+'" type was not found.';if(this.options.silent)return;throw new Error(s)}else o+=a||"";return o},e}();y.options=y.setOptions=function(n){return d(y.defaults,n),s(y.defaults),y},y.getDefaults=i,y.defaults=n.defaults,y.use=function(){for(var n=arguments.length,e=new Array(n),t=0;t<n;t++)e[t]=arguments[t];var r,a=d.apply(void 0,[{}].concat(e)),o=y.defaults.extensions||{renderers:{},childTokens:{}};e.forEach((function(n){if(n.extensions&&(r=!0,n.extensions.forEach((function(n){if(!n.name)throw new Error("extension name required");if(n.renderer){var e=o.renderers?o.renderers[n.name]:null;o.renderers[n.name]=e?function(){for(var t=arguments.length,r=new Array(t),a=0;a<t;a++)r[a]=arguments[a];var o=n.renderer.apply(this,r);return!1===o&&(o=e.apply(this,r)),o}:n.renderer}if(n.tokenizer){if(!n.level||"block"!==n.level&&"inline"!==n.level)throw new Error("extension level must be 'block' or 'inline'");o[n.level]?o[n.level].unshift(n.tokenizer):o[n.level]=[n.tokenizer],n.start&&("block"===n.level?o.startBlock?o.startBlock.push(n.start):o.startBlock=[n.start]:"inline"===n.level&&(o.startInline?o.startInline.push(n.start):o.startInline=[n.start]))}n.childTokens&&(o.childTokens[n.name]=n.childTokens)}))),n.renderer&&function(){var e=y.defaults.renderer||new $;for(var t in n.renderer)!function(t){var r=e[t];e[t]=function(){for(var a=arguments.length,o=new Array(a),i=0;i<a;i++)o[i]=arguments[i];var s=n.renderer[t].apply(e,o);return!1===s&&(s=r.apply(e,o)),s}}(t);a.renderer=e}(),n.tokenizer&&function(){var e=y.defaults.tokenizer||new M;for(var t in n.tokenizer)!function(t){var r=e[t];e[t]=function(){for(var a=arguments.length,o=new Array(a),i=0;i<a;i++)o[i]=arguments[i];var s=n.tokenizer[t].apply(e,o);return!1===s&&(s=r.apply(e,o)),s}}(t);a.tokenizer=e}(),n.walkTokens){var e=y.defaults.walkTokens;a.walkTokens=function(t){n.walkTokens.call(this,t),e&&e.call(this,t)}}r&&(a.extensions=o),y.setOptions(a)}))},y.walkTokens=function(n,e){for(var t,r=o(n);!(t=r()).done;)!function(){var n=t.value;switch(e.call(y,n),n.type){case"table":for(var r,a=o(n.header);!(r=a()).done;){var i=r.value;y.walkTokens(i.tokens,e)}for(var s,l=o(n.rows);!(s=l()).done;)for(var c,p=o(s.value);!(c=p()).done;){var u=c.value;y.walkTokens(u.tokens,e)}break;case"list":y.walkTokens(n.items,e);break;default:y.defaults.extensions&&y.defaults.extensions.childTokens&&y.defaults.extensions.childTokens[n.type]?y.defaults.extensions.childTokens[n.type].forEach((function(t){y.walkTokens(n[t],e)})):n.tokens&&y.walkTokens(n.tokens,e)}}()},y.parseInline=function(n,e){if(null==n)throw new Error("marked.parseInline(): input parameter is undefined or null");if("string"!=typeof n)throw new Error("marked.parseInline(): input parameter is of type "+Object.prototype.toString.call(n)+", string expected");f(e=d({},y.defaults,e||{}));try{var t=N.lexInline(n,e);return e.walkTokens&&y.walkTokens(t,e.walkTokens),H.parseInline(t,e)}catch(n){if(n.message+="\nPlease report this to https://github.com/markedjs/marked.",e.silent)return"<p>An error occurred:</p><pre>"+l(n.message+"",!0)+"</pre>";throw n}},y.Parser=H,y.parser=H.parse,y.Renderer=$,y.TextRenderer=U,y.Lexer=N,y.lexer=N.lex,y.Tokenizer=M,y.Slugger=z,y.parse=y;var q=y.options,V=y.setOptions,K=y.use,W=y.walkTokens,G=y.parseInline,J=y,Y=H.parse,Q=N.lex;n.Lexer=N,n.Parser=H,n.Renderer=$,n.Slugger=z,n.TextRenderer=U,n.Tokenizer=M,n.getDefaults=i,n.lexer=Q,n.marked=y,n.options=q,n.parse=J,n.parseInline=G,n.parser=Y,n.setOptions=V,n.use=K,n.walkTokens=W,Object.defineProperty(n,"__esModule",{value:!0})}(e)},function(n,e,t){"use strict";function r(n,e){var t=function(n){var e={};return s(l(n),(function(n){var t=n[0],r=n[1];s(r,(function(n){e[n]=t}))})),e}(n.pluralTypeToLanguages);return t[e]||t[g.call(e,/-/,1)[0]]||t.en}function a(n){return n.replace(/[.*+?^${}()|[\]\\]/g,"\\$&")}function o(n,e,t,r,a){if("string"!=typeof n)throw new TypeError("Polyglot.transformPhrase expects argument #1 to be string");if(null==e)return n;var o=n,i=r||k,s="number"==typeof e?{smart_count:e}:e;if(null!=s.smart_count&&n){var l=a||v,c=g.call(n,f),d=function(n,e,t){return n.pluralTypes[e](t)}(l,b(l,t||"en"),s.smart_count);o=u(c[d]||c[0])}return m.call(o,i,(function(n,e){return p(s,e)&&null!=s[e]?s[e]:n}))}function i(n){var e=n||{};this.phrases={},this.extend(e.phrases||{}),this.currentLocale=e.locale||"en";var t=e.allowMissing?o:null;this.onMissingKey="function"==typeof e.onMissingKey?e.onMissingKey:t,this.warn=e.warn||d,this.tokenRegex=function(n){var e=n&&n.prefix||"%{",t=n&&n.suffix||"}";if(e===f||t===f)throw new RangeError('"'+f+'" token is reserved for pluralization');return new RegExp(a(e)+"(.*?)"+a(t),"g")}(e.interpolation),this.pluralRules=e.pluralRules||v}var s=t(43),l=t(89),c=t(94),p=t(25),u=t(92),d=function(n){c(!1,n)},m=String.prototype.replace,g=String.prototype.split,f="||||",h=function(n){var e=n%100,t=e%10;return 11!==e&&1===t?0:2<=t&&t<=4&&!(e>=12&&e<=14)?1:2},v={pluralTypes:{arabic:function(n){if(n<3)return n;var e=n%100;return e>=3&&e<=10?3:e>=11?4:5},bosnian_serbian:h,chinese:function(){return 0},croatian:h,french:function(n){return n>=2?1:0},german:function(n){return 1!==n?1:0},russian:h,lithuanian:function(n){return n%10==1&&n%100!=11?0:n%10>=2&&n%10<=9&&(n%100<11||n%100>19)?1:2},czech:function(n){return 1===n?0:n>=2&&n<=4?1:2},polish:function(n){if(1===n)return 0;var e=n%10;return 2<=e&&e<=4&&(n%100<10||n%100>=20)?1:2},icelandic:function(n){return n%10!=1||n%100==11?1:0},slovenian:function(n){var e=n%100;return 1===e?0:2===e?1:3===e||4===e?2:3}},pluralTypeToLanguages:{arabic:["ar"],bosnian_serbian:["bs-Latn-BA","bs-Cyrl-BA","srl-RS","sr-RS"],chinese:["id","id-ID","ja","ko","ko-KR","lo","ms","th","th-TH","zh"],croatian:["hr","hr-HR"],german:["fa","da","de","en","es","fi","el","he","hi-IN","hu","hu-HU","it","nl","no","pt","sv","tr"],french:["fr","tl","pt-br"],russian:["ru","ru-RU"],lithuanian:["lt"],czech:["cs","cs-CZ","sk"],polish:["pl"],icelandic:["is"],slovenian:["sl-SL"]}},b=function(){var n={};return function(e,t){var a=n[t];return a&&!e.pluralTypes[a]&&(a=null,n[t]=a),a||(a=r(e,t))&&(n[t]=a),a}}(),k=/%\{(.*?)\}/g;i.prototype.locale=function(n){return n&&(this.currentLocale=n),this.currentLocale},i.prototype.extend=function(n,e){s(l(n||{}),(function(n){var t=n[0],r=n[1],a=e?e+"."+t:t;"object"==typeof r?this.extend(r,a):this.phrases[a]=r}),this)},i.prototype.unset=function(n,e){"string"==typeof n?delete this.phrases[n]:s(l(n||{}),(function(n){var t=n[0],r=n[1],a=e?e+"."+t:t;"object"==typeof r?this.unset(r,a):delete this.phrases[a]}),this)},i.prototype.clear=function(){this.phrases={}},i.prototype.replace=function(n){this.clear(),this.extend(n)},i.prototype.t=function(n,e){var t,r,a=null==e?{}:e;return"string"==typeof this.phrases[n]?t=this.phrases[n]:"string"==typeof a._?t=a._:this.onMissingKey?r=(0,this.onMissingKey)(n,a,this.currentLocale,this.tokenRegex,this.pluralRules):(this.warn('Missing translation for key: "'+n+'"'),r=n),"string"==typeof t&&(r=o(t,a,this.currentLocale,this.tokenRegex,this.pluralRules)),r},i.prototype.has=function(n){return p(this.phrases,n)},i.transformPhrase=function(n,e,t){return o(n,e,t)},n.exports=i},function(n,e,t){"use strict";function r(n){if(null==n)throw new TypeError("Object.assign cannot be called with null or undefined");return Object(n)}
/*
object-assign
(c) Sindre Sorhus
@license MIT
*/var a=Object.getOwnPropertySymbols,o=Object.prototype.hasOwnProperty,i=Object.prototype.propertyIsEnumerable;n.exports=function(){try{if(!Object.assign)return!1;var n=new String("abc");if(n[5]="de","5"===Object.getOwnPropertyNames(n)[0])return!1;for(var e={},t=0;t<10;t++)e["_"+String.fromCharCode(t)]=t;if("0123456789"!==Object.getOwnPropertyNames(e).map((function(n){return e[n]})).join(""))return!1;var r={};return"abcdefghijklmnopqrst".split("").forEach((function(n){r[n]=n})),"abcdefghijklmnopqrst"===Object.keys(Object.assign({},r)).join("")}catch(n){return!1}}()?Object.assign:function(n,e){for(var t,s,l=r(n),c=1;c<arguments.length;c++){for(var p in t=Object(arguments[c]))o.call(t,p)&&(l[p]=t[p]);if(a){s=a(t);for(var u=0;u<s.length;u++)i.call(t,s[u])&&(l[s[u]]=t[s[u]])}}return l}},function(n,e,t){function r(n,e){if(n===1/0||n===-1/0||n!=n||n&&n>-1e3&&n<1e3||U.call(/e/,e))return e;var t=/[0-9](?=(?:[0-9]{3})+(?![0-9]))/g;if("number"==typeof n){var r=n<0?-V(-n):V(n);if(r!==n){var a=String(r),o=j.call(e,a.length+1);return L.call(a,t,"$&_")+"."+L.call(L.call(o,/([0-9]{3})/g,"$&_"),/_$/,"")}}return L.call(e,t,"$&_")}function a(n,e,t){var r="double"===(t.quoteStyle||e)?'"':"'";return r+n+r}function o(n){return L.call(String(n),/"/g,"&quot;")}function i(n){return!("[object Array]"!==c(n)||Y&&"object"==typeof n&&Y in n)}function s(n){if(J)return n&&"object"==typeof n&&n instanceof Symbol;if("symbol"==typeof n)return!0;if(!n||"object"!=typeof n||!G)return!1;try{return G.call(n),!0}catch(n){}return!1}function l(n,e){return en.call(n,e)}function c(n){return F.call(n)}function p(n){if(n.name)return n.name;var e=M.call(B.call(n),/^function\s*([\w$]+)/);return e?e[1]:null}function u(n,e){if(n.indexOf)return n.indexOf(e);for(var t=0,r=n.length;t<r;t++)if(n[t]===e)return t;return-1}function d(n){if(!A||!n||"object"!=typeof n)return!1;try{A.call(n,A);try{_.call(n,_)}catch(n){return!0}return n instanceof WeakMap}catch(n){}return!1}function m(n){if(!R||!n||"object"!=typeof n)return!1;try{return R.call(n),!0}catch(n){}return!1}function g(n){if(!_||!n||"object"!=typeof n)return!1;try{_.call(n,_);try{A.call(n,A)}catch(n){return!0}return n instanceof WeakSet}catch(n){}return!1}function f(n,e){if(n.length>e.maxStringLength){var t=n.length-e.maxStringLength,r="... "+t+" more character"+(t>1?"s":"");return f(j.call(n,0,e.maxStringLength),e)+r}return a(L.call(L.call(n,/(['\\])/g,"\\$1"),/[\x00-\x1f]/g,h),"single",e)}function h(n){var e=n.charCodeAt(0),t={8:"b",9:"t",10:"n",12:"f",13:"r"}[e];return t?"\\"+t:"\\x"+(e<16?"0":"")+N.call(e.toString(16))}function v(n){return"Object("+n+")"}function b(n){return n+" { ? }"}function k(n,e,t,r){return n+" ("+e+") {"+(r?y(t,r):H.call(t,", "))+"}"}function y(n,e){if(0===n.length)return"";var t="\n"+e.prev+e.base;return t+H.call(n,","+t)+"\n"+e.prev}function S(n,e){var t=i(n),r=[];if(t){r.length=n.length;for(var a=0;a<n.length;a++)r[a]=l(n,a)?e(n[a],n):""}var o,s="function"==typeof W?W(n):[];if(J){o={};for(var c=0;c<s.length;c++)o["$"+s[c]]=s[c]}for(var p in n)l(n,p)&&(t&&String(Number(p))===p&&p<n.length||J&&o["$"+p]instanceof Symbol||(U.call(/[^\w$]/,p)?r.push(e(p,n)+": "+e(n[p],n)):r.push(p+": "+e(n[p],n))));if("function"==typeof W)for(var u=0;u<s.length;u++)Q.call(n,s[u])&&r.push("["+e(s[u])+"]: "+e(n[s[u]],n));return r}var x="function"==typeof Map&&Map.prototype,w=Object.getOwnPropertyDescriptor&&x?Object.getOwnPropertyDescriptor(Map.prototype,"size"):null,E=x&&w&&"function"==typeof w.get?w.get:null,D=x&&Map.prototype.forEach,C="function"==typeof Set&&Set.prototype,I=Object.getOwnPropertyDescriptor&&C?Object.getOwnPropertyDescriptor(Set.prototype,"size"):null,T=C&&I&&"function"==typeof I.get?I.get:null,O=C&&Set.prototype.forEach,A="function"==typeof WeakMap&&WeakMap.prototype?WeakMap.prototype.has:null,_="function"==typeof WeakSet&&WeakSet.prototype?WeakSet.prototype.has:null,R="function"==typeof WeakRef&&WeakRef.prototype?WeakRef.prototype.deref:null,P=Boolean.prototype.valueOf,F=Object.prototype.toString,B=Function.prototype.toString,M=String.prototype.match,j=String.prototype.slice,L=String.prototype.replace,N=String.prototype.toUpperCase,$=String.prototype.toLowerCase,U=RegExp.prototype.test,z=Array.prototype.concat,H=Array.prototype.join,q=Array.prototype.slice,V=Math.floor,K="function"==typeof BigInt?BigInt.prototype.valueOf:null,W=Object.getOwnPropertySymbols,G="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?Symbol.prototype.toString:null,J="function"==typeof Symbol&&"object"==typeof Symbol.iterator,Y="function"==typeof Symbol&&Symbol.toStringTag&&(Symbol.toStringTag,1)?Symbol.toStringTag:null,Q=Object.prototype.propertyIsEnumerable,X=("function"==typeof Reflect?Reflect.getPrototypeOf:Object.getPrototypeOf)||([].__proto__===Array.prototype?function(n){return n.__proto__}:null),Z=t(102).custom,nn=Z&&s(Z)?Z:null;n.exports=function n(e,t,h,x){function w(e,t,r){if(t&&(x=q.call(x)).push(t),r){var a={depth:C.depth};return l(C,"quoteStyle")&&(a.quoteStyle=C.quoteStyle),n(e,a,h+1,x)}return n(e,C,h+1,x)}var C=t||{};if(l(C,"quoteStyle")&&"single"!==C.quoteStyle&&"double"!==C.quoteStyle)throw new TypeError('option "quoteStyle" must be "single" or "double"');if(l(C,"maxStringLength")&&("number"==typeof C.maxStringLength?C.maxStringLength<0&&C.maxStringLength!==1/0:null!==C.maxStringLength))throw new TypeError('option "maxStringLength", if provided, must be a positive integer, Infinity, or `null`');var I=!l(C,"customInspect")||C.customInspect;if("boolean"!=typeof I&&"symbol"!==I)throw new TypeError("option \"customInspect\", if provided, must be `true`, `false`, or `'symbol'`");if(l(C,"indent")&&null!==C.indent&&"\t"!==C.indent&&!(parseInt(C.indent,10)===C.indent&&C.indent>0))throw new TypeError('option "indent" must be "\\t", an integer > 0, or `null`');if(l(C,"numericSeparator")&&"boolean"!=typeof C.numericSeparator)throw new TypeError('option "numericSeparator", if provided, must be `true` or `false`');var A=C.numericSeparator;if(void 0===e)return"undefined";if(null===e)return"null";if("boolean"==typeof e)return e?"true":"false";if("string"==typeof e)return f(e,C);if("number"==typeof e){if(0===e)return 1/0/e>0?"0":"-0";var _=String(e);return A?r(e,_):_}if("bigint"==typeof e){var R=String(e)+"n";return A?r(e,R):R}var F=void 0===C.depth?5:C.depth;if(void 0===h&&(h=0),h>=F&&F>0&&"object"==typeof e)return i(e)?"[Array]":"[Object]";var B=function(n,e){var t;if("\t"===n.indent)t="\t";else{if(!("number"==typeof n.indent&&n.indent>0))return null;t=H.call(Array(n.indent+1)," ")}return{base:t,prev:H.call(Array(e+1),t)}}(C,h);if(void 0===x)x=[];else if(u(x,e)>=0)return"[Circular]";if("function"==typeof e){var M=p(e),N=S(e,w);return"[Function"+(M?": "+M:" (anonymous)")+"]"+(N.length>0?" { "+H.call(N,", ")+" }":"")}if(s(e)){var U=J?L.call(String(e),/^(Symbol\(.*\))_[^)]*$/,"$1"):G.call(e);return"object"!=typeof e||J?U:v(U)}if(function(n){return!(!n||"object"!=typeof n)&&("undefined"!=typeof HTMLElement&&n instanceof HTMLElement||"string"==typeof n.nodeName&&"function"==typeof n.getAttribute)}(e)){for(var V="<"+$.call(String(e.nodeName)),W=e.attributes||[],Z=0;Z<W.length;Z++)V+=" "+W[Z].name+"="+a(o(W[Z].value),"double",C);return V+=">",e.childNodes&&e.childNodes.length&&(V+="..."),V+"</"+$.call(String(e.nodeName))+">"}if(i(e)){if(0===e.length)return"[]";var en=S(e,w);return B&&!function(n){for(var e=0;e<n.length;e++)if(u(n[e],"\n")>=0)return!1;return!0}(en)?"["+y(en,B)+"]":"[ "+H.call(en,", ")+" ]"}if(function(n){return!("[object Error]"!==c(n)||Y&&"object"==typeof n&&Y in n)}(e)){var tn=S(e,w);return"cause"in e&&!Q.call(e,"cause")?"{ ["+String(e)+"] "+H.call(z.call("[cause]: "+w(e.cause),tn),", ")+" }":0===tn.length?"["+String(e)+"]":"{ ["+String(e)+"] "+H.call(tn,", ")+" }"}if("object"==typeof e&&I){if(nn&&"function"==typeof e[nn])return e[nn]();if("symbol"!==I&&"function"==typeof e.inspect)return e.inspect()}if(function(n){if(!E||!n||"object"!=typeof n)return!1;try{E.call(n);try{T.call(n)}catch(n){return!0}return n instanceof Map}catch(n){}return!1}(e)){var rn=[];return D.call(e,(function(n,t){rn.push(w(t,e,!0)+" => "+w(n,e))})),k("Map",E.call(e),rn,B)}if(function(n){if(!T||!n||"object"!=typeof n)return!1;try{T.call(n);try{E.call(n)}catch(n){return!0}return n instanceof Set}catch(n){}return!1}(e)){var an=[];return O.call(e,(function(n){an.push(w(n,e))})),k("Set",T.call(e),an,B)}if(d(e))return b("WeakMap");if(g(e))return b("WeakSet");if(m(e))return b("WeakRef");if(function(n){return!("[object Number]"!==c(n)||Y&&"object"==typeof n&&Y in n)}(e))return v(w(Number(e)));if(function(n){if(!n||"object"!=typeof n||!K)return!1;try{return K.call(n),!0}catch(n){}return!1}(e))return v(w(K.call(e)));if(function(n){return!("[object Boolean]"!==c(n)||Y&&"object"==typeof n&&Y in n)}(e))return v(P.call(e));if(function(n){return!("[object String]"!==c(n)||Y&&"object"==typeof n&&Y in n)}(e))return v(w(String(e)));if(!function(n){return!("[object Date]"!==c(n)||Y&&"object"==typeof n&&Y in n)}(e)&&!function(n){return!("[object RegExp]"!==c(n)||Y&&"object"==typeof n&&Y in n)}(e)){var on=S(e,w),sn=X?X(e)===Object.prototype:e instanceof Object||e.constructor===Object,ln=e instanceof Object?"":"null prototype",cn=!sn&&Y&&Object(e)===e&&Y in e?j.call(c(e),8,-1):ln?"Object":"",pn=(sn||"function"!=typeof e.constructor?"":e.constructor.name?e.constructor.name+" ":"")+(cn||ln?"["+H.call(z.call([],cn||[],ln||[]),": ")+"] ":"");return 0===on.length?pn+"{}":B?pn+"{"+y(on,B)+"}":pn+"{ "+H.call(on,", ")+" }"}return String(e)};var en=Object.prototype.hasOwnProperty||function(n){return n in this}},function(n,e,t){"use strict";var r;if(!Object.keys){var a=Object.prototype.hasOwnProperty,o=Object.prototype.toString,i=t(26),s=Object.prototype.propertyIsEnumerable,l=!s.call({toString:null},"toString"),c=s.call((function(){}),"prototype"),p=["toString","toLocaleString","valueOf","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","constructor"],u=function(n){var e=n.constructor;return e&&e.prototype===n},d={$applicationCache:!0,$console:!0,$external:!0,$frame:!0,$frameElement:!0,$frames:!0,$innerHeight:!0,$innerWidth:!0,$onmozfullscreenchange:!0,$onmozfullscreenerror:!0,$outerHeight:!0,$outerWidth:!0,$pageXOffset:!0,$pageYOffset:!0,$parent:!0,$scrollLeft:!0,$scrollTop:!0,$scrollX:!0,$scrollY:!0,$self:!0,$webkitIndexedDB:!0,$webkitStorageInfo:!0,$window:!0},m=function(){if("undefined"==typeof window)return!1;for(var n in window)try{if(!d["$"+n]&&a.call(window,n)&&null!==window[n]&&"object"==typeof window[n])try{u(window[n])}catch(n){return!0}}catch(n){return!0}return!1}(),g=function(n){if("undefined"==typeof window||!m)return u(n);try{return u(n)}catch(n){return!1}};r=function(n){var e=null!==n&&"object"==typeof n,t="[object Function]"===o.call(n),r=i(n),s=e&&"[object String]"===o.call(n),u=[];if(!e&&!t&&!r)throw new TypeError("Object.keys called on a non-object");var d=c&&t;if(s&&n.length>0&&!a.call(n,0))for(var m=0;m<n.length;++m)u.push(String(m));if(r&&n.length>0)for(var f=0;f<n.length;++f)u.push(String(f));else for(var h in n)d&&"prototype"===h||!a.call(n,h)||u.push(String(h));if(l)for(var v=g(n),b=0;b<p.length;++b)v&&"constructor"===p[b]||!a.call(n,p[b])||u.push(p[b]);return u}}n.exports=r},function(n,e,t){"use strict";var r=Array.prototype.slice,a=t(26),o=Object.keys,i=o?function(n){return o(n)}:t(87),s=Object.keys;i.shim=function(){return Object.keys?function(){var n=Object.keys(arguments);return n&&n.length===arguments.length}(1,2)||(Object.keys=function(n){return s(a(n)?r.call(n):n)}):Object.keys=i,Object.keys||i},n.exports=i},function(n,e,t){"use strict";var r=t(2),a=t(4),o=t(27),i=t(28),s=t(90),l=a(i(),Object);r(l,{getPolyfill:i,implementation:o,shim:s}),n.exports=l},function(n,e,t){"use strict";var r=t(28),a=t(2);n.exports=function(){var n=r();return a(Object,{entries:n},{entries:function(){return Object.entries!==n}}),n}},function(n,e){function t(){throw new Error("setTimeout has not been defined")}function r(){throw new Error("clearTimeout has not been defined")}function a(n){if(c===setTimeout)return setTimeout(n,0);if((c===t||!c)&&setTimeout)return c=setTimeout,setTimeout(n,0);try{return c(n,0)}catch(e){try{return c.call(null,n,0)}catch(e){return c.call(this,n,0)}}}function o(){g&&d&&(g=!1,d.length?m=d.concat(m):f=-1,m.length&&i())}function i(){if(!g){var n=a(o);g=!0;for(var e=m.length;e;){for(d=m,m=[];++f<e;)d&&d[f].run();f=-1,e=m.length}d=null,g=!1,function(n){if(p===clearTimeout)return clearTimeout(n);if((p===r||!p)&&clearTimeout)return p=clearTimeout,clearTimeout(n);try{p(n)}catch(e){try{return p.call(null,n)}catch(e){return p.call(this,n)}}}(n)}}function s(n,e){this.fun=n,this.array=e}function l(){}var c,p,u=n.exports={};!function(){try{c="function"==typeof setTimeout?setTimeout:t}catch(n){c=t}try{p="function"==typeof clearTimeout?clearTimeout:r}catch(n){p=r}}();var d,m=[],g=!1,f=-1;u.nextTick=function(n){var e=new Array(arguments.length-1);if(arguments.length>1)for(var t=1;t<arguments.length;t++)e[t-1]=arguments[t];m.push(new s(n,e)),1!==m.length||g||a(i)},s.prototype.run=function(){this.fun.apply(null,this.array)},u.title="browser",u.browser=!0,u.env={},u.argv=[],u.version="",u.versions={},u.on=l,u.addListener=l,u.once=l,u.off=l,u.removeListener=l,u.removeAllListeners=l,u.emit=l,u.prependListener=l,u.prependOnceListener=l,u.listeners=function(n){return[]},u.binding=function(n){throw new Error("process.binding is not supported")},u.cwd=function(){return"/"},u.chdir=function(n){throw new Error("process.chdir is not supported")},u.umask=function(){return 0}},function(n,e,t){"use strict";var r=t(4),a=t(2),o=t(29),i=t(30),s=t(93),l=r(i());a(l,{getPolyfill:i,implementation:o,shim:s}),n.exports=l},function(n,e,t){"use strict";var r=t(2),a=t(30);n.exports=function(){var n=a();return r(String.prototype,{trim:n},{trim:function(){return String.prototype.trim!==n}}),n}},function(n,e,t){"use strict";(function(e){var t=function(){};if("production"!==e.env.NODE_ENV){var r=function(n,e){var t=arguments.length;e=new Array(t>1?t-1:0);for(var r=1;r<t;r++)e[r-1]=arguments[r];var a=0,o="Warning: "+n.replace(/%s/g,(function(){return e[a++]}));try{throw new Error(o)}catch(n){}};t=function(n,e,t){var a=arguments.length;t=new Array(a>2?a-2:0);for(var o=2;o<a;o++)t[o-2]=arguments[o];if(void 0===e)throw new Error("`warning(condition, format, ...args)` requires a warning message argument");n||r.apply(null,[e].concat(t))}}n.exports=t}).call(e,t(91))},function(n,e,t){function r(n,e){return new i(e).process(n)}var a=t(31),o=t(32),i=t(96);for(var s in(e=n.exports=r).filterXSS=r,e.FilterXSS=i,a)e[s]=a[s];for(var s in o)e[s]=o[s];"undefined"!=typeof window&&(window.filterXSS=n.exports),"undefined"!=typeof self&&"undefined"!=typeof DedicatedWorkerGlobalScope&&self instanceof DedicatedWorkerGlobalScope&&(self.filterXSS=n.exports)},function(n,e,t){function r(n){return null==n}function a(n){(n=function(n){var e={};for(var t in n)e[t]=n[t];return e}(n||{})).stripIgnoreTag&&(n.onIgnoreTag,n.onIgnoreTag=i.onIgnoreTagStripAll),n.whiteList=n.whiteList||n.allowList||i.whiteList,n.onTag=n.onTag||i.onTag,n.onTagAttr=n.onTagAttr||i.onTagAttr,n.onIgnoreTag=n.onIgnoreTag||i.onIgnoreTag,n.onIgnoreTagAttr=n.onIgnoreTagAttr||i.onIgnoreTagAttr,n.safeAttrValue=n.safeAttrValue||i.safeAttrValue,n.escapeHtml=n.escapeHtml||i.escapeHtml,this.options=n,!1===n.css?this.cssFilter=!1:(n.css=n.css||{},this.cssFilter=new o(n.css))}var o=t(7).FilterCSS,i=t(31),s=t(32),l=s.parseTag,c=s.parseAttr,p=t(11);a.prototype.process=function(n){if(!(n=(n=n||"").toString()))return"";var e=this.options,t=e.whiteList,a=e.onTag,o=e.onIgnoreTag,s=e.onTagAttr,u=e.onIgnoreTagAttr,d=e.safeAttrValue,m=e.escapeHtml,g=this.cssFilter;e.stripBlankChar&&(n=i.stripBlankChar(n)),e.allowCommentTag||(n=i.stripCommentTag(n));var f=!1;e.stripIgnoreTagBody&&(f=i.StripTagBody(e.stripIgnoreTagBody,o),o=f.onIgnoreTag);var h=l(n,(function(n,e,i,l,f){var h,v={sourcePosition:n,position:e,isClosing:f,isWhite:t.hasOwnProperty(i)};if(!r(h=a(i,l,v)))return h;if(v.isWhite){if(v.isClosing)return"</"+i+">";var b=function(n){var e=p.spaceIndex(n);if(-1===e)return{html:"",closing:"/"===n[n.length-2]};var t="/"===(n=p.trim(n.slice(e+1,-1)))[n.length-1];return t&&(n=p.trim(n.slice(0,-1))),{html:n,closing:t}}(l),k=t[i],y=c(b.html,(function(n,e){var t,a=-1!==p.indexOf(k,n);return r(t=s(i,n,e,a))?a?(e=d(i,n,e,g))?n+'="'+e+'"':n:r(t=u(i,n,e,a))?void 0:t:t}));return l="<"+i,y&&(l+=" "+y),b.closing&&(l+=" /"),l+">"}return r(h=o(i,l,v))?m(l):h}),m);return f&&(h=f.remove(h)),h},n.exports=a},function(n,e){n.exports={smile:"e3/2018new_weixioa02_org.png",lovely:"09/2018new_keai_org.png",happy:"1e/2018new_taikaixin_org.png",clap:"6e/2018new_guzhang_thumb.png",whee:"33/2018new_xixi_thumb.png",haha:"8f/2018new_haha_thumb.png","laugh and cry":"4a/2018new_xiaoku_thumb.png",wink:"43/2018new_jiyan_org.png",greddy:"fa/2018new_chanzui_org.png",awkward:"a3/2018new_heixian_thumb.png",sweat:"28/2018new_han_org.png","pick nose":"9a/2018new_wabi_thumb.png",hum:"7c/2018new_heng_thumb.png",angry:"f6/2018new_nu_thumb.png",grievance:"a5/2018new_weiqu_thumb.png",poor:"96/2018new_kelian_org.png",disappoint:"aa/2018new_shiwang_thumb.png",sad:"ee/2018new_beishang_org.png",tear:"6e/2018new_leimu_org.png","no way":"83/2018new_kuxiao_org.png",shy:"c1/2018new_haixiu_org.png",dirt:"10/2018new_wu_thumb.png","love you":"f6/2018new_aini_org.png",kiss:"2c/2018new_qinqin_thumb.png",amorousness:"9d/2018new_huaxin_org.png",longing:"c9/2018new_chongjing_org.png",desire:"3e/2018new_tianping_thumb.png","bad laugh":"4d/2018new_huaixiao_org.png",blackness:"9e/2018new_yinxian_org.png","laugh without word":"2d/2018new_xiaoerbuyu_org.png",titter:"71/2018new_touxiao_org.png",cool:"c4/2018new_ku_org.png","not easy":"aa/2018new_bingbujiandan_thumb.png",think:"30/2018new_sikao_org.png",question:"b8/2018new_ningwen_org.png","no idea":"2a/2018new_wenhao_thumb.png",dizzy:"07/2018new_yun_thumb.png",bomb:"a2/2018new_shuai_thumb.png",bone:"a1/2018new_kulou_thumb.png","be quiet":"b0/2018new_xu_org.png","shut up":"62/2018new_bizui_org.png",stupid:"dd/2018new_shayan_org.png","surprise ":"49/2018new_chijing_org.png",vomit:"08/2018new_tu_org.png",cold:"40/2018new_kouzhao_thumb.png",sick:"3b/2018new_shengbing_thumb.png",bye:"fd/2018new_baibai_thumb.png","look down on":"da/2018new_bishi_org.png","white eye":"ef/2018new_landelini_org.png","left hum":"43/2018new_zuohengheng_thumb.png","right hum":"c1/2018new_youhengheng_thumb.png",crazy:"17/2018new_zhuakuang_org.png","scold ":"87/2018new_zhouma_thumb.png","hit on face":"cb/2018new_dalian_org.png",wow:"ae/2018new_ding_org.png",fan:"86/2018new_hufen02_org.png",money:"a2/2018new_qian_thumb.png",yawn:"55/2018new_dahaqian_org.png",sleepy:"3c/2018new_kun_thumb.png",sleep:"e2/2018new_shuijiao_thumb.png","watermelon ":"01/2018new_chigua_thumb.png",doge:"a1/2018new_doge02_org.png",dog:"22/2018new_erha_org.png",cat:"7b/2018new_miaomiao_thumb.png",thumb:"e6/2018new_zan_org.png",good:"8a/2018new_good_org.png",ok:"45/2018new_ok_org.png",yeah:"29/2018new_ye_thumb.png","shack hand":"e9/2018new_woshou_thumb.png",bow:"e7/2018new_zuoyi_org.png",come:"42/2018new_guolai_thumb.png",punch:"86/2018new_quantou_thumb.png"}},function(n,e){n.exports={nick:"NickName",mail:"E-Mail",link:"Website(http://)",nickFail:"NickName cannot be less than 3 bytes.",mailFail:"Please confirm your email address.",sofa:"No comment yet.",submit:"Submit",reply:"Reply",cancelReply:"Cancel reply",comments:"Comments",cancel:"Cancel",confirm:"Confirm",continue:"Continue",more:"Load More...",preview:"Preview",emoji:"Emoji",expand:"See more....",seconds:"seconds ago",minutes:"minutes ago",hours:"hours ago",days:"days ago",now:"just now",uploading:"Uploading ...",uploadDone:"Upload completed!",busy:"Submit is busy, please wait...","code-98":"Valine initialization failed, please check your version of av-min.js.","code-99":"Valine initialization failed, Please check the `el` element in the init method.","code-100":"Valine initialization failed, Please check your appId and appKey.","code-140":"The total number of API calls today has exceeded the development version limit.","code-401":"Unauthorized operation, Please check your appId and appKey.","code-403":"Access denied by API domain white list, Please check your security domain."}},function(n,e){n.exports={nick:"",mail:"",link:"(http://)",nickFail:"3.",mailFail:".",sofa:"~",submit:"",reply:"",cancelReply:"",comments:"",cancel:"",confirm:"",continue:"",more:"...",preview:"",emoji:"",expand:"",seconds:"",minutes:"",hours:"",days:"",now:"",uploading:"...",uploadDone:"!",busy:"20     ...","code-98":"av-min.js .","code-99":"init`el`.","code-100":"AppIdAppKey.","code-140":"API.","code-401":"AppIdAppKey.","code-403":"API"}},function(n,e){n.exports={nick:"",mail:"",link:"(http://)",nickFail:"3",mailFail:"",sofa:"~",submit:"",reply:"",cancelReply:"",comments:"",cancel:"",confirm:"",continue:"",more:"...",preview:"",emoji:"",expand:"...",seconds:"",minutes:"",hours:"",days:"",now:"",uploading:"...",uploadDone:"!",busy:"...","code-98":"Valine  av-min.js ","code-99":"Valine init`el`.","code-100":"Valine AppIdAppKey.","code-140":" API .","code-401":"AppIdAppKey.","code-403":"API."}},function(n,e){n.exports={nick:"",mail:"",link:"(http://)",nickFail:"3",mailFail:"",sofa:"~",submit:"",reply:"",cancelReply:"",comments:"",cancel:"",confirm:"",continue:"",more:"...",preview:"",emoji:"",expand:"...",seconds:"",minutes:"",hours:"",days:"",now:"",uploading:"...",uploadDone:"!",busy:"...","code-98":"Valine  av-min.js ","code-99":"Valine init`el`.","code-100":"Valine AppIdAppKey.","code-140":" API .","code-401":"AppIdAppKey.","code-403":"API."}},function(n,e){},function(n,e,t){var r=t(104);"string"==typeof r&&(r=[[n.i,r,""]]);var a={transform:void 0};t(106)(r,a),r.locals&&(n.exports=r.locals)},function(n,e,t){(e=t(105)(!1)).push([n.i,'.v[data-class=v]{font-size:16px;text-align:left}.v[data-class=v] *{-webkit-box-sizing:border-box;box-sizing:border-box;line-height:1.75}.v[data-class=v] .vinput,.v[data-class=v] .veditor,.v[data-class=v] p,.v[data-class=v] pre code,.v[data-class=v] .status-bar{color:#555}.v[data-class=v] .vtime,.v[data-class=v] .vsys{color:#b3b3b3}.v[data-class=v] .text-right{text-align:right}.v[data-class=v] .text-center{text-align:center}.v[data-class=v] img{max-width:100%;border:none}.v[data-class=v] hr{margin:.825em 0;border-color:#f6f6f6;border-style:dashed}.v[data-class=v].hide-avatar .vimg{display:none}.v[data-class=v] a{position:relative;cursor:pointer;color:#1abc9c;text-decoration:none;display:inline-block}.v[data-class=v] a:hover{color:#d7191a}.v[data-class=v] pre,.v[data-class=v] code{background-color:#f8f8f8;padding:.2em .4em;border-radius:3px;font-size:85%;margin:0}.v[data-class=v] pre{padding:10px;overflow:auto;line-height:1.45}.v[data-class=v] pre code{padding:0;background:transparent;white-space:pre-wrap;word-break:keep-all}.v[data-class=v] blockquote{color:#666;margin:.5em 0;padding:0 0 0 1em;border-left:8px solid rgba(238,238,238,.5)}.v[data-class=v] .vinput{border:none;resize:none;outline:none;padding:10px 5px;max-width:100%;font-size:.775em;-webkit-box-sizing:border-box;box-sizing:border-box}.v[data-class=v] input[type=checkbox],.v[data-class=v] input[type=radio]{display:inline-block;vertical-align:middle;margin-top:-2px}.v[data-class=v] .vicon{cursor:pointer;display:inline-block;overflow:hidden;fill:#555;vertical-align:middle}.v[data-class=v] .vicon+.vicon{margin-left:10px}.v[data-class=v] .vicon.actived{fill:#66b1ff}.v[data-class=v] .vrow{font-size:0;padding:10px 0}.v[data-class=v] .vrow .vcol{display:inline-block;vertical-align:middle;font-size:14px}.v[data-class=v] .vrow .vcol.vcol-20{width:20%}.v[data-class=v] .vrow .vcol.vcol-30{width:30%}.v[data-class=v] .vrow .vcol.vcol-40{width:40%}.v[data-class=v] .vrow .vcol.vcol-50{width:50%}.v[data-class=v] .vrow .vcol.vcol-60{width:60%}.v[data-class=v] .vrow .vcol.vcol-70{width:70%}.v[data-class=v] .vrow .vcol.vcol-80{width:80%}.v[data-class=v] .vrow .vcol.vctrl{font-size:12px}.v[data-class=v] .vemoji,.v[data-class=v] .emoji{width:26px;height:26px;overflow:hidden;vertical-align:middle;margin:0 1px;display:inline-block}.v[data-class=v] .vwrap{border:1px solid #f0f0f0;border-radius:4px;margin-bottom:10px;overflow:hidden;position:relative;padding:10px}.v[data-class=v] .vwrap input{background:transparent}.v[data-class=v] .vwrap .vedit{position:relative;padding-top:10px}.v[data-class=v] .vwrap .cancel-reply-btn{position:absolute;right:5px;top:5px;cursor:pointer}.v[data-class=v] .vwrap .vemojis{display:none;font-size:18px;max-height:145px;overflow:auto;padding-bottom:10px;-webkit-box-shadow:0px 0 1px #f0f0f0;box-shadow:0px 0 1px #f0f0f0}.v[data-class=v] .vwrap .vemojis i{font-style:normal;padding-top:7px;width:36px;cursor:pointer;text-align:center;display:inline-block;vertical-align:middle}.v[data-class=v] .vwrap .vpreview{padding:7px;-webkit-box-shadow:0px 0 1px #f0f0f0;box-shadow:0px 0 1px #f0f0f0}.v[data-class=v] .vwrap .vheader .vinput{width:33.33%;border-bottom:1px #dedede dashed}.v[data-class=v] .vwrap .vheader.item2 .vinput{width:50%}.v[data-class=v] .vwrap .vheader.item1 .vinput{width:100%}.v[data-class=v] .vwrap .vheader .vinput:focus{border-bottom-color:#eb5055}@media screen and (max-width: 520px){.v[data-class=v] .vwrap .vheader .vinput{width:100%}.v[data-class=v] .vwrap .vheader.item2 .vinput{width:100%}}.v[data-class=v] .vpower{color:#999;font-size:.75em;padding:.5em 0}.v[data-class=v] .vpower a{font-size:.75em}.v[data-class=v] .vcount{padding:5px;font-weight:600;font-size:1.25em}.v[data-class=v] ul,.v[data-class=v] ol{padding:0;margin-left:1.25em}.v[data-class=v] .txt-center{text-align:center}.v[data-class=v] .txt-right{text-align:right}.v[data-class=v] .veditor{width:100%;min-height:8.75em;font-size:.875em;background:transparent;resize:vertical;-webkit-transition:all .25s ease;transition:all .25s ease}.v[data-class=v] .vbtn{-webkit-transition-duration:.4s;transition-duration:.4s;text-align:center;color:#555;border:1px solid #ededed;border-radius:.3em;display:inline-block;background:transparent;margin-bottom:0;font-weight:400;vertical-align:middle;-ms-touch-action:manipulation;touch-action:manipulation;cursor:pointer;white-space:nowrap;padding:.5em 1.25em;font-size:.875em;line-height:1.42857143;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;outline:none}.v[data-class=v] .vbtn+.vbtn{margin-left:1.25em}.v[data-class=v] .vbtn:active,.v[data-class=v] .vbtn:hover{color:#3090e4;border-color:#3090e4}.v[data-class=v] .vbtn:disabled{border-color:#e1e1e1;color:#e1e1e1;background-color:#fdfafa;cursor:not-allowed}.v[data-class=v] .vempty{padding:1.25em;text-align:center;color:#555;overflow:auto}.v[data-class=v] .vsys{display:inline-block;padding:.2em .5em;font-size:.75em;border-radius:.2em;margin-right:.3em}@media screen and (max-width: 520px){.v[data-class=v] .vsys{display:none}}.v[data-class=v] .vcards{width:100%}.v[data-class=v] .vcards .vcard{padding-top:1.25em;position:relative;display:block}.v[data-class=v] .vcards .vcard:after{content:"";clear:both;display:block}.v[data-class=v] .vcards .vcard .vimg{width:3.125em;height:3.125em;float:left;border-radius:50%;margin-right:.7525em;border:1px solid #f5f5f5;padding:.125em}@media screen and (max-width: 720px){.v[data-class=v] .vcards .vcard .vimg{width:2.5em;height:2.5em}}.v[data-class=v] .vcards .vcard .vhead{line-height:1.5;margin-top:0}.v[data-class=v] .vcards .vcard .vhead .vnick{position:relative;font-size:.875em;font-weight:500;margin-right:.875em;cursor:pointer;text-decoration:none;display:inline-block}.v[data-class=v] .vcards .vcard .vhead .vnick:hover{color:#d7191a}.v[data-class=v] .vcards .vcard .vh{overflow:hidden;padding-bottom:.5em;border-bottom:1px dashed #f5f5f5}.v[data-class=v] .vcards .vcard .vh .vtime{font-size:.75em;margin-right:.875em}.v[data-class=v] .vcards .vcard .vh .vmeta{line-height:1;position:relative}.v[data-class=v] .vcards .vcard .vh .vmeta .vat{font-size:.8125em;color:#ef2f11;cursor:pointer;float:right}.v[data-class=v] .vcards .vcard:last-child .vh{border-bottom:none}.v[data-class=v] .vcards .vcard .vcontent{word-wrap:break-word;word-break:break-all;font-size:.875em;line-height:2;position:relative;margin-bottom:.75em;padding-top:.625em}.v[data-class=v] .vcards .vcard .vcontent.expand{cursor:pointer;max-height:8em;overflow:hidden}.v[data-class=v] .vcards .vcard .vcontent.expand::before{display:block;content:"";position:absolute;width:100%;left:0;top:0;bottom:3.15em;background:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0)), to(rgba(255, 255, 255, 0.9)));background:linear-gradient(180deg, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.9));z-index:999}.v[data-class=v] .vcards .vcard .vcontent.expand::after{display:block;content:attr(data-expand);text-align:center;color:#828586;position:absolute;width:100%;height:3.15em;line-height:3.15em;left:0;bottom:0;z-index:999;background:rgba(255,255,255,.9)}.v[data-class=v] .vcards .vcard .vquote{padding-left:1em;border-left:1px dashed rgba(238,238,238,.5)}.v[data-class=v] .vcards .vcard .vquote .vimg{width:2.225em;height:2.225em}.v[data-class=v] .vpage .vmore{margin:1em 0}.v[data-class=v] .clear{content:"";display:block;clear:both}@-webkit-keyframes spin{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes spin{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@-webkit-keyframes pulse{50%{background:#dcdcdc}}@keyframes pulse{50%{background:#dcdcdc}}.v[data-class=v] .vspinner{width:22px;height:22px;display:inline-block;border:6px double #a0a0a0;border-top-color:transparent;border-bottom-color:transparent;border-radius:50%;-webkit-animation:spin 1s infinite linear;animation:spin 1s infinite linear;position:relative;vertical-align:middle;margin:0 5px}[data-theme=dark] .v[data-class=v] .vinput,[data-theme=dark] .v[data-class=v] .veditor,[data-theme=dark] .v[data-class=v] p,[data-theme=dark] .v[data-class=v] pre code,[data-theme=dark] .v[data-class=v] .status-bar,.dark .v[data-class=v] .vinput,.dark .v[data-class=v] .veditor,.dark .v[data-class=v] p,.dark .v[data-class=v] pre code,.dark .v[data-class=v] .status-bar,.theme__dark .v[data-class=v] .vinput,.theme__dark .v[data-class=v] .veditor,.theme__dark .v[data-class=v] p,.theme__dark .v[data-class=v] pre code,.theme__dark .v[data-class=v] .status-bar,.night .v[data-class=v] .vinput,.night .v[data-class=v] .veditor,.night .v[data-class=v] p,.night .v[data-class=v] pre code,.night .v[data-class=v] .status-bar{color:#b2b2b5}[data-theme=dark] .v[data-class=v] .vtime,[data-theme=dark] .v[data-class=v] .vsys,.dark .v[data-class=v] .vtime,.dark .v[data-class=v] .vsys,.theme__dark .v[data-class=v] .vtime,.theme__dark .v[data-class=v] .vsys,.night .v[data-class=v] .vtime,.night .v[data-class=v] .vsys{color:#929298}[data-theme=dark] .v[data-class=v] pre,[data-theme=dark] .v[data-class=v] code,[data-theme=dark] .v[data-class=v] pre code,.dark .v[data-class=v] pre,.dark .v[data-class=v] code,.dark .v[data-class=v] pre code,.theme__dark .v[data-class=v] pre,.theme__dark .v[data-class=v] code,.theme__dark .v[data-class=v] pre code,.night .v[data-class=v] pre,.night .v[data-class=v] code,.night .v[data-class=v] pre code{color:#929298;background-color:#151414}[data-theme=dark] .v[data-class=v] .vwrap,.dark .v[data-class=v] .vwrap,.theme__dark .v[data-class=v] .vwrap,.night .v[data-class=v] .vwrap{border-color:#b2b2b5}[data-theme=dark] .v[data-class=v] .vicon,.dark .v[data-class=v] .vicon,.theme__dark .v[data-class=v] .vicon,.night .v[data-class=v] .vicon{fill:#b2b2b5}[data-theme=dark] .v[data-class=v] .vicon.actived,.dark .v[data-class=v] .vicon.actived,.theme__dark .v[data-class=v] .vicon.actived,.night .v[data-class=v] .vicon.actived{fill:#66b1ff}[data-theme=dark] .v[data-class=v] .vbtn,.dark .v[data-class=v] .vbtn,.theme__dark .v[data-class=v] .vbtn,.night .v[data-class=v] .vbtn{color:#b2b2b5;border-color:#b2b2b5}[data-theme=dark] .v[data-class=v] .vbtn:hover,.dark .v[data-class=v] .vbtn:hover,.theme__dark .v[data-class=v] .vbtn:hover,.night .v[data-class=v] .vbtn:hover{color:#66b1ff;border-color:#66b1ff}[data-theme=dark] .v[data-class=v] a:hover,.dark .v[data-class=v] a:hover,.theme__dark .v[data-class=v] a:hover,.night .v[data-class=v] a:hover{color:#d7191a}[data-theme=dark] .v[data-class=v] .vcards .vcard .vcontent.expand::before,.dark .v[data-class=v] .vcards .vcard .vcontent.expand::before,.theme__dark .v[data-class=v] .vcards .vcard .vcontent.expand::before,.night .v[data-class=v] .vcards .vcard .vcontent.expand::before{background:-webkit-gradient(linear, left top, left bottom, from(rgba(0, 0, 0, 0.3)), to(rgba(0, 0, 0, 0.7)));background:linear-gradient(180deg, rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.7))}[data-theme=dark] .v[data-class=v] .vcards .vcard .vcontent.expand::after,.dark .v[data-class=v] .vcards .vcard .vcontent.expand::after,.theme__dark .v[data-class=v] .vcards .vcard .vcontent.expand::after,.night .v[data-class=v] .vcards .vcard .vcontent.expand::after{background:rgba(0,0,0,.7)}@media(prefers-color-scheme: dark){.v[data-class=v] .vinput,.v[data-class=v] .veditor,.v[data-class=v] p,.v[data-class=v] pre code,.v[data-class=v] .status-bar{color:#b2b2b5}.v[data-class=v] .vtime,.v[data-class=v] .vsys{color:#929298}.v[data-class=v] pre,.v[data-class=v] code,.v[data-class=v] pre code{color:#929298;background-color:#151414}.v[data-class=v] .vwrap{border-color:#b2b2b5}.v[data-class=v] .vicon{fill:#b2b2b5}.v[data-class=v] .vicon.actived{fill:#66b1ff}.v[data-class=v] .vbtn{color:#b2b2b5;border-color:#b2b2b5}.v[data-class=v] .vbtn:hover{color:#66b1ff;border-color:#66b1ff}.v[data-class=v] a:hover{color:#d7191a}.v[data-class=v] .vcards .vcard .vcontent.expand::before{background:-webkit-gradient(linear, left top, left bottom, from(rgba(0, 0, 0, 0.3)), to(rgba(0, 0, 0, 0.7)));background:linear-gradient(180deg, rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.7))}.v[data-class=v] .vcards .vcard .vcontent.expand::after{background:rgba(0,0,0,.7)}}',""]),n.exports=e},function(n,e,t){"use strict";function r(n,e){var t=n[1]||"",r=n[3];if(!r)return t;if(e&&"function"==typeof btoa){var a=function(n){return"/*# ".concat("sourceMappingURL=data:application/json;charset=utf-8;base64,".concat(btoa(unescape(encodeURIComponent(JSON.stringify(n)))))," */")}(r);return[t].concat(r.sources.map((function(n){return"/*# sourceURL=".concat(r.sourceRoot||"").concat(n," */")}))).concat([a]).join("\n")}return[t].join("\n")}n.exports=function(n){var e=[];return e.toString=function(){return this.map((function(e){var t=r(e,n);return e[2]?"@media ".concat(e[2]," {").concat(t,"}"):t})).join("")},e.i=function(n,t,r){"string"==typeof n&&(n=[[null,n,""]]);var a={};if(r)for(var o=0;o<this.length;o++){var i=this[o][0];null!=i&&(a[i]=!0)}for(var s=0;s<n.length;s++){var l=[].concat(n[s]);r&&a[l[0]]||(t&&(l[2]?l[2]="".concat(t," and ").concat(l[2]):l[2]=t),e.push(l))}},e}},function(n,e,t){function r(n,e){for(var t=0;t<n.length;t++){var r=n[t],a=g[r.id];if(a){a.refs++;for(var o=0;o<a.parts.length;o++)a.parts[o](r.parts[o]);for(;o<r.parts.length;o++)a.parts.push(p(r.parts[o],e))}else{var i=[];for(o=0;o<r.parts.length;o++)i.push(p(r.parts[o],e));g[r.id]={id:r.id,refs:1,parts:i}}}}function a(n,e){for(var t=[],r={},a=0;a<n.length;a++){var o=n[a],i=e.base?o[0]+e.base:o[0],s={css:o[1],media:o[2],sourceMap:o[3]};r[i]?r[i].parts.push(s):t.push(r[i]={id:i,parts:[s]})}return t}function o(n,e){var t=h(n.insertInto);if(!t)throw new Error("Couldn't find a style target. This probably means that the value for the 'insertInto' parameter is invalid.");var r=k[k.length-1];if("top"===n.insertAt)r?r.nextSibling?t.insertBefore(e,r.nextSibling):t.appendChild(e):t.insertBefore(e,t.firstChild),k.push(e);else{if("bottom"!==n.insertAt)throw new Error("Invalid value for parameter 'insertAt'. Must be 'top' or 'bottom'.");t.appendChild(e)}}function i(n){if(null===n.parentNode)return!1;n.parentNode.removeChild(n);var e=k.indexOf(n);e>=0&&k.splice(e,1)}function s(n){var e=document.createElement("style");return n.attrs.type="text/css",c(e,n.attrs),o(n,e),e}function l(n){var e=document.createElement("link");return n.attrs.type="text/css",n.attrs.rel="stylesheet",c(e,n.attrs),o(n,e),e}function c(n,e){Object.keys(e).forEach((function(t){n.setAttribute(t,e[t])}))}function p(n,e){var t,r,a,o;if(e.transform&&n.css){if(!(o=e.transform(n.css)))return function(){};n.css=o}if(e.singleton){var c=b++;t=v||(v=s(e)),r=u.bind(null,t,c,!1),a=u.bind(null,t,c,!0)}else n.sourceMap&&"function"==typeof URL&&"function"==typeof URL.createObjectURL&&"function"==typeof URL.revokeObjectURL&&"function"==typeof Blob&&"function"==typeof btoa?(t=l(e),r=m.bind(null,t,e),a=function(){i(t),t.href&&URL.revokeObjectURL(t.href)}):(t=s(e),r=d.bind(null,t),a=function(){i(t)});return r(n),function(e){if(e){if(e.css===n.css&&e.media===n.media&&e.sourceMap===n.sourceMap)return;r(n=e)}else a()}}function u(n,e,t,r){var a=t?"":r.css;if(n.styleSheet)n.styleSheet.cssText=S(e,a);else{var o=document.createTextNode(a),i=n.childNodes;i[e]&&n.removeChild(i[e]),i.length?n.insertBefore(o,i[e]):n.appendChild(o)}}function d(n,e){var t=e.css,r=e.media;if(r&&n.setAttribute("media",r),n.styleSheet)n.styleSheet.cssText=t;else{for(;n.firstChild;)n.removeChild(n.firstChild);n.appendChild(document.createTextNode(t))}}function m(n,e,t){var r=t.css,a=t.sourceMap,o=void 0===e.convertToAbsoluteUrls&&a;(e.convertToAbsoluteUrls||o)&&(r=y(r)),a&&(r+="\n/*# sourceMappingURL=data:application/json;base64,"+btoa(unescape(encodeURIComponent(JSON.stringify(a))))+" */");var i=new Blob([r],{type:"text/css"}),s=n.href;n.href=URL.createObjectURL(i),s&&URL.revokeObjectURL(s)}var g={},f=function(n){var e;return function(){return void 0===e&&(e=n.apply(this,arguments)),e}}((function(){return window&&document&&document.all&&!window.atob})),h=function(n){var e={};return function(t){return void 0===e[t]&&(e[t]=n.call(this,t)),e[t]}}((function(n){return document.querySelector(n)})),v=null,b=0,k=[],y=t(107);n.exports=function(n,e){if("undefined"!=typeof DEBUG&&DEBUG&&"object"!=typeof document)throw new Error("The style-loader cannot be used in a non-browser environment");(e=e||{}).attrs="object"==typeof e.attrs?e.attrs:{},e.singleton||(e.singleton=f()),e.insertInto||(e.insertInto="head"),e.insertAt||(e.insertAt="bottom");var t=a(n,e);return r(t,e),function(n){for(var o=[],i=0;i<t.length;i++){var s=t[i];(l=g[s.id]).refs--,o.push(l)}for(n&&r(a(n,e),e),i=0;i<o.length;i++){var l;if(0===(l=o[i]).refs){for(var c=0;c<l.parts.length;c++)l.parts[c]();delete g[l.id]}}}};var S=function(){var n=[];return function(e,t){return n[e]=t,n.filter(Boolean).join("\n")}}()},function(n,e){n.exports=function(n){var e="undefined"!=typeof window&&window.location;if(!e)throw new Error("fixUrls requires window.location");if(!n||"string"!=typeof n)return n;var t=e.protocol+"//"+e.host,r=t+e.pathname.replace(/\/[^\/]*$/,"/");return n.replace(/url\s*\(((?:[^)(]|\((?:[^)(]+|\([^)(]*\))*\))*)\)/gi,(function(n,e){var a,o=e.trim().replace(/^"(.*)"$/,(function(n,e){return e})).replace(/^'(.*)'$/,(function(n,e){return e}));return/^(#|data:|http:\/\/|https:\/\/|file:\/\/\/)/i.test(o)?n:(a=0===o.indexOf("//")?o:0===o.indexOf("/")?t+o:r+o.replace(/^\.\//,""),"url("+JSON.stringify(a)+")")}))}},function(n,e,t){t(103),n.exports=t(34)}])},function(n,e,t){"use strict";t(113)},function(n,e,t){"use strict";var r=t(289),a=t(114),o=t(63),i=Object.prototype.hasOwnProperty,s={brackets:function(n){return n+"[]"},comma:"comma",indices:function(n,e){return n+"["+e+"]"},repeat:function(n){return n}},l=Array.isArray,c=Array.prototype.push,p=function(n,e){c.apply(n,l(e)?e:[e])},u=Date.prototype.toISOString,d=o.default,m={addQueryPrefix:!1,allowDots:!1,charset:"utf-8",charsetSentinel:!1,delimiter:"&",encode:!0,encoder:a.encode,encodeValuesOnly:!1,format:d,formatter:o.formatters[d],indices:!1,serializeDate:function(n){return u.call(n)},skipNulls:!1,strictNullHandling:!1},g={},f=function n(e,t,o,i,s,c,u,d,f,h,v,b,k,y,S,x){for(var w,E=e,D=x,C=0,I=!1;void 0!==(D=D.get(g))&&!I;){var T=D.get(e);if(C+=1,void 0!==T){if(T===C)throw new RangeError("Cyclic object value");I=!0}void 0===D.get(g)&&(C=0)}if("function"==typeof d?E=d(t,E):E instanceof Date?E=v(E):"comma"===o&&l(E)&&(E=a.maybeMap(E,(function(n){return n instanceof Date?v(n):n}))),null===E){if(s)return u&&!y?u(t,m.encoder,S,"key",b):t;E=""}if("string"==typeof(w=E)||"number"==typeof w||"boolean"==typeof w||"symbol"==typeof w||"bigint"==typeof w||a.isBuffer(E))return u?[k(y?t:u(t,m.encoder,S,"key",b))+"="+k(u(E,m.encoder,S,"value",b))]:[k(t)+"="+k(String(E))];var O,A=[];if(void 0===E)return A;if("comma"===o&&l(E))y&&u&&(E=a.maybeMap(E,u)),O=[{value:E.length>0?E.join(",")||null:void 0}];else if(l(d))O=d;else{var _=Object.keys(E);O=f?_.sort(f):_}for(var R=i&&l(E)&&1===E.length?t+"[]":t,P=0;P<O.length;++P){var F=O[P],B="object"==typeof F&&void 0!==F.value?F.value:E[F];if(!c||null!==B){var M=l(E)?"function"==typeof o?o(R,F):R:R+(h?"."+F:"["+F+"]");x.set(e,C);var j=r();j.set(g,x),p(A,n(B,M,o,i,s,c,"comma"===o&&y&&l(E)?null:u,d,f,h,v,b,k,y,S,j))}}return A};n.exports=function(n,e){var t,a=n,c=function(n){if(!n)return m;if(null!==n.encoder&&void 0!==n.encoder&&"function"!=typeof n.encoder)throw new TypeError("Encoder has to be a function.");var e=n.charset||m.charset;if(void 0!==n.charset&&"utf-8"!==n.charset&&"iso-8859-1"!==n.charset)throw new TypeError("The charset option must be either utf-8, iso-8859-1, or undefined");var t=o.default;if(void 0!==n.format){if(!i.call(o.formatters,n.format))throw new TypeError("Unknown format option provided.");t=n.format}var r=o.formatters[t],a=m.filter;return("function"==typeof n.filter||l(n.filter))&&(a=n.filter),{addQueryPrefix:"boolean"==typeof n.addQueryPrefix?n.addQueryPrefix:m.addQueryPrefix,allowDots:void 0===n.allowDots?m.allowDots:!!n.allowDots,charset:e,charsetSentinel:"boolean"==typeof n.charsetSentinel?n.charsetSentinel:m.charsetSentinel,delimiter:void 0===n.delimiter?m.delimiter:n.delimiter,encode:"boolean"==typeof n.encode?n.encode:m.encode,encoder:"function"==typeof n.encoder?n.encoder:m.encoder,encodeValuesOnly:"boolean"==typeof n.encodeValuesOnly?n.encodeValuesOnly:m.encodeValuesOnly,filter:a,format:t,formatter:r,serializeDate:"function"==typeof n.serializeDate?n.serializeDate:m.serializeDate,skipNulls:"boolean"==typeof n.skipNulls?n.skipNulls:m.skipNulls,sort:"function"==typeof n.sort?n.sort:null,strictNullHandling:"boolean"==typeof n.strictNullHandling?n.strictNullHandling:m.strictNullHandling}}(e);"function"==typeof c.filter?a=(0,c.filter)("",a):l(c.filter)&&(t=c.filter);var u,d=[];if("object"!=typeof a||null===a)return"";u=e&&e.arrayFormat in s?e.arrayFormat:e&&"indices"in e?e.indices?"indices":"repeat":"indices";var g=s[u];if(e&&"commaRoundTrip"in e&&"boolean"!=typeof e.commaRoundTrip)throw new TypeError("`commaRoundTrip` must be a boolean, or absent");var h="comma"===g&&e&&e.commaRoundTrip;t||(t=Object.keys(a)),c.sort&&t.sort(c.sort);for(var v=r(),b=0;b<t.length;++b){var k=t[b];c.skipNulls&&null===a[k]||p(d,f(a[k],k,g,h,c.strictNullHandling,c.skipNulls,c.encode?c.encoder:null,c.filter,c.sort,c.allowDots,c.serializeDate,c.format,c.formatter,c.encodeValuesOnly,c.charset,v))}var y=d.join(c.delimiter),S=!0===c.addQueryPrefix?"?":"";return c.charsetSentinel&&("iso-8859-1"===c.charset?S+="utf8=%26%2310003%3B&":S+="utf8=%E2%9C%93&"),y.length>0?S+y:""}},function(n,e,t){"use strict";var r=t(61),a=t(295),o=t(297),i=r("%TypeError%"),s=r("%WeakMap%",!0),l=r("%Map%",!0),c=a("WeakMap.prototype.get",!0),p=a("WeakMap.prototype.set",!0),u=a("WeakMap.prototype.has",!0),d=a("Map.prototype.get",!0),m=a("Map.prototype.set",!0),g=a("Map.prototype.has",!0),f=function(n,e){for(var t,r=n;null!==(t=r.next);r=t)if(t.key===e)return r.next=t.next,t.next=n.next,n.next=t,t};n.exports=function(){var n,e,t,r={assert:function(n){if(!r.has(n))throw new i("Side channel does not contain "+o(n))},get:function(r){if(s&&r&&("object"==typeof r||"function"==typeof r)){if(n)return c(n,r)}else if(l){if(e)return d(e,r)}else if(t)return function(n,e){var t=f(n,e);return t&&t.value}(t,r)},has:function(r){if(s&&r&&("object"==typeof r||"function"==typeof r)){if(n)return u(n,r)}else if(l){if(e)return g(e,r)}else if(t)return function(n,e){return!!f(n,e)}(t,r);return!1},set:function(r,a){s&&r&&("object"==typeof r||"function"==typeof r)?(n||(n=new s),p(n,r,a)):l?(e||(e=new l),m(e,r,a)):(t||(t={key:{},next:null}),function(n,e,t){var r=f(n,e);r?r.value=t:n.next={key:e,next:n.next,value:t}}(t,r,a))}};return r}},function(n,e,t){"use strict";var r="undefined"!=typeof Symbol&&Symbol,a=t(291);n.exports=function(){return"function"==typeof r&&("function"==typeof Symbol&&("symbol"==typeof r("foo")&&("symbol"==typeof Symbol("bar")&&a())))}},function(n,e,t){"use strict";n.exports=function(){if("function"!=typeof Symbol||"function"!=typeof Object.getOwnPropertySymbols)return!1;if("symbol"==typeof Symbol.iterator)return!0;var n={},e=Symbol("test"),t=Object(e);if("string"==typeof e)return!1;if("[object Symbol]"!==Object.prototype.toString.call(e))return!1;if("[object Symbol]"!==Object.prototype.toString.call(t))return!1;for(e in n[e]=42,n)return!1;if("function"==typeof Object.keys&&0!==Object.keys(n).length)return!1;if("function"==typeof Object.getOwnPropertyNames&&0!==Object.getOwnPropertyNames(n).length)return!1;var r=Object.getOwnPropertySymbols(n);if(1!==r.length||r[0]!==e)return!1;if(!Object.prototype.propertyIsEnumerable.call(n,e))return!1;if("function"==typeof Object.getOwnPropertyDescriptor){var a=Object.getOwnPropertyDescriptor(n,e);if(42!==a.value||!0!==a.enumerable)return!1}return!0}},function(n,e,t){"use strict";var r={foo:{}},a=Object;n.exports=function(){return{__proto__:r}.foo===r.foo&&!({__proto__:null}instanceof a)}},function(n,e,t){"use strict";var r="Function.prototype.bind called on incompatible ",a=Array.prototype.slice,o=Object.prototype.toString;n.exports=function(n){var e=this;if("function"!=typeof e||"[object Function]"!==o.call(e))throw new TypeError(r+e);for(var t,i=a.call(arguments,1),s=function(){if(this instanceof t){var r=e.apply(this,i.concat(a.call(arguments)));return Object(r)===r?r:this}return e.apply(n,i.concat(a.call(arguments)))},l=Math.max(0,e.length-i.length),c=[],p=0;p<l;p++)c.push("$"+p);if(t=Function("binder","return function ("+c.join(",")+"){ return binder.apply(this,arguments); }")(s),e.prototype){var u=function(){};u.prototype=e.prototype,t.prototype=new u,u.prototype=null}return t}},function(n,e,t){"use strict";var r=t(62);n.exports=r.call(Function.call,Object.prototype.hasOwnProperty)},function(n,e,t){"use strict";var r=t(61),a=t(296),o=a(r("String.prototype.indexOf"));n.exports=function(n,e){var t=r(n,!!e);return"function"==typeof t&&o(n,".prototype.")>-1?a(t):t}},function(n,e,t){"use strict";var r=t(62),a=t(61),o=a("%Function.prototype.apply%"),i=a("%Function.prototype.call%"),s=a("%Reflect.apply%",!0)||r.call(i,o),l=a("%Object.getOwnPropertyDescriptor%",!0),c=a("%Object.defineProperty%",!0),p=a("%Math.max%");if(c)try{c({},"a",{value:1})}catch(n){c=null}n.exports=function(n){var e=s(r,i,arguments);if(l&&c){var t=l(e,"length");t.configurable&&c(e,"length",{value:1+p(0,n.length-(arguments.length-1))})}return e};var u=function(){return s(r,o,arguments)};c?c(n.exports,"apply",{value:u}):n.exports.apply=u},function(n,e,t){var r="function"==typeof Map&&Map.prototype,a=Object.getOwnPropertyDescriptor&&r?Object.getOwnPropertyDescriptor(Map.prototype,"size"):null,o=r&&a&&"function"==typeof a.get?a.get:null,i=r&&Map.prototype.forEach,s="function"==typeof Set&&Set.prototype,l=Object.getOwnPropertyDescriptor&&s?Object.getOwnPropertyDescriptor(Set.prototype,"size"):null,c=s&&l&&"function"==typeof l.get?l.get:null,p=s&&Set.prototype.forEach,u="function"==typeof WeakMap&&WeakMap.prototype?WeakMap.prototype.has:null,d="function"==typeof WeakSet&&WeakSet.prototype?WeakSet.prototype.has:null,m="function"==typeof WeakRef&&WeakRef.prototype?WeakRef.prototype.deref:null,g=Boolean.prototype.valueOf,f=Object.prototype.toString,h=Function.prototype.toString,v=String.prototype.match,b=String.prototype.slice,k=String.prototype.replace,y=String.prototype.toUpperCase,S=String.prototype.toLowerCase,x=RegExp.prototype.test,w=Array.prototype.concat,E=Array.prototype.join,D=Array.prototype.slice,C=Math.floor,I="function"==typeof BigInt?BigInt.prototype.valueOf:null,T=Object.getOwnPropertySymbols,O="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?Symbol.prototype.toString:null,A="function"==typeof Symbol&&"object"==typeof Symbol.iterator,_="function"==typeof Symbol&&Symbol.toStringTag&&(typeof Symbol.toStringTag===A||"symbol")?Symbol.toStringTag:null,R=Object.prototype.propertyIsEnumerable,P=("function"==typeof Reflect?Reflect.getPrototypeOf:Object.getPrototypeOf)||([].__proto__===Array.prototype?function(n){return n.__proto__}:null);function F(n,e){if(n===1/0||n===-1/0||n!=n||n&&n>-1e3&&n<1e3||x.call(/e/,e))return e;var t=/[0-9](?=(?:[0-9]{3})+(?![0-9]))/g;if("number"==typeof n){var r=n<0?-C(-n):C(n);if(r!==n){var a=String(r),o=b.call(e,a.length+1);return k.call(a,t,"$&_")+"."+k.call(k.call(o,/([0-9]{3})/g,"$&_"),/_$/,"")}}return k.call(e,t,"$&_")}var B=t(298),M=B.custom,j=z(M)?M:null;function L(n,e,t){var r="double"===(t.quoteStyle||e)?'"':"'";return r+n+r}function N(n){return k.call(String(n),/"/g,"&quot;")}function $(n){return!("[object Array]"!==V(n)||_&&"object"==typeof n&&_ in n)}function U(n){return!("[object RegExp]"!==V(n)||_&&"object"==typeof n&&_ in n)}function z(n){if(A)return n&&"object"==typeof n&&n instanceof Symbol;if("symbol"==typeof n)return!0;if(!n||"object"!=typeof n||!O)return!1;try{return O.call(n),!0}catch(n){}return!1}n.exports=function n(e,t,r,a){var s=t||{};if(q(s,"quoteStyle")&&"single"!==s.quoteStyle&&"double"!==s.quoteStyle)throw new TypeError('option "quoteStyle" must be "single" or "double"');if(q(s,"maxStringLength")&&("number"==typeof s.maxStringLength?s.maxStringLength<0&&s.maxStringLength!==1/0:null!==s.maxStringLength))throw new TypeError('option "maxStringLength", if provided, must be a positive integer, Infinity, or `null`');var l=!q(s,"customInspect")||s.customInspect;if("boolean"!=typeof l&&"symbol"!==l)throw new TypeError("option \"customInspect\", if provided, must be `true`, `false`, or `'symbol'`");if(q(s,"indent")&&null!==s.indent&&"\t"!==s.indent&&!(parseInt(s.indent,10)===s.indent&&s.indent>0))throw new TypeError('option "indent" must be "\\t", an integer > 0, or `null`');if(q(s,"numericSeparator")&&"boolean"!=typeof s.numericSeparator)throw new TypeError('option "numericSeparator", if provided, must be `true` or `false`');var f=s.numericSeparator;if(void 0===e)return"undefined";if(null===e)return"null";if("boolean"==typeof e)return e?"true":"false";if("string"==typeof e)return function n(e,t){if(e.length>t.maxStringLength){var r=e.length-t.maxStringLength,a="... "+r+" more character"+(r>1?"s":"");return n(b.call(e,0,t.maxStringLength),t)+a}return L(k.call(k.call(e,/(['\\])/g,"\\$1"),/[\x00-\x1f]/g,W),"single",t)}(e,s);if("number"==typeof e){if(0===e)return 1/0/e>0?"0":"-0";var y=String(e);return f?F(e,y):y}if("bigint"==typeof e){var x=String(e)+"n";return f?F(e,x):x}var C=void 0===s.depth?5:s.depth;if(void 0===r&&(r=0),r>=C&&C>0&&"object"==typeof e)return $(e)?"[Array]":"[Object]";var T=function(n,e){var t;if("\t"===n.indent)t="\t";else{if(!("number"==typeof n.indent&&n.indent>0))return null;t=E.call(Array(n.indent+1)," ")}return{base:t,prev:E.call(Array(e+1),t)}}(s,r);if(void 0===a)a=[];else if(K(a,e)>=0)return"[Circular]";function M(e,t,o){if(t&&(a=D.call(a)).push(t),o){var i={depth:s.depth};return q(s,"quoteStyle")&&(i.quoteStyle=s.quoteStyle),n(e,i,r+1,a)}return n(e,s,r+1,a)}if("function"==typeof e&&!U(e)){var H=function(n){if(n.name)return n.name;var e=v.call(h.call(n),/^function\s*([\w$]+)/);if(e)return e[1];return null}(e),Z=X(e,M);return"[Function"+(H?": "+H:" (anonymous)")+"]"+(Z.length>0?" { "+E.call(Z,", ")+" }":"")}if(z(e)){var nn=A?k.call(String(e),/^(Symbol\(.*\))_[^)]*$/,"$1"):O.call(e);return"object"!=typeof e||A?nn:G(nn)}if(function(n){if(!n||"object"!=typeof n)return!1;if("undefined"!=typeof HTMLElement&&n instanceof HTMLElement)return!0;return"string"==typeof n.nodeName&&"function"==typeof n.getAttribute}(e)){for(var en="<"+S.call(String(e.nodeName)),tn=e.attributes||[],rn=0;rn<tn.length;rn++)en+=" "+tn[rn].name+"="+L(N(tn[rn].value),"double",s);return en+=">",e.childNodes&&e.childNodes.length&&(en+="..."),en+="</"+S.call(String(e.nodeName))+">"}if($(e)){if(0===e.length)return"[]";var an=X(e,M);return T&&!function(n){for(var e=0;e<n.length;e++)if(K(n[e],"\n")>=0)return!1;return!0}(an)?"["+Q(an,T)+"]":"[ "+E.call(an,", ")+" ]"}if(function(n){return!("[object Error]"!==V(n)||_&&"object"==typeof n&&_ in n)}(e)){var on=X(e,M);return"cause"in Error.prototype||!("cause"in e)||R.call(e,"cause")?0===on.length?"["+String(e)+"]":"{ ["+String(e)+"] "+E.call(on,", ")+" }":"{ ["+String(e)+"] "+E.call(w.call("[cause]: "+M(e.cause),on),", ")+" }"}if("object"==typeof e&&l){if(j&&"function"==typeof e[j]&&B)return B(e,{depth:C-r});if("symbol"!==l&&"function"==typeof e.inspect)return e.inspect()}if(function(n){if(!o||!n||"object"!=typeof n)return!1;try{o.call(n);try{c.call(n)}catch(n){return!0}return n instanceof Map}catch(n){}return!1}(e)){var sn=[];return i&&i.call(e,(function(n,t){sn.push(M(t,e,!0)+" => "+M(n,e))})),Y("Map",o.call(e),sn,T)}if(function(n){if(!c||!n||"object"!=typeof n)return!1;try{c.call(n);try{o.call(n)}catch(n){return!0}return n instanceof Set}catch(n){}return!1}(e)){var ln=[];return p&&p.call(e,(function(n){ln.push(M(n,e))})),Y("Set",c.call(e),ln,T)}if(function(n){if(!u||!n||"object"!=typeof n)return!1;try{u.call(n,u);try{d.call(n,d)}catch(n){return!0}return n instanceof WeakMap}catch(n){}return!1}(e))return J("WeakMap");if(function(n){if(!d||!n||"object"!=typeof n)return!1;try{d.call(n,d);try{u.call(n,u)}catch(n){return!0}return n instanceof WeakSet}catch(n){}return!1}(e))return J("WeakSet");if(function(n){if(!m||!n||"object"!=typeof n)return!1;try{return m.call(n),!0}catch(n){}return!1}(e))return J("WeakRef");if(function(n){return!("[object Number]"!==V(n)||_&&"object"==typeof n&&_ in n)}(e))return G(M(Number(e)));if(function(n){if(!n||"object"!=typeof n||!I)return!1;try{return I.call(n),!0}catch(n){}return!1}(e))return G(M(I.call(e)));if(function(n){return!("[object Boolean]"!==V(n)||_&&"object"==typeof n&&_ in n)}(e))return G(g.call(e));if(function(n){return!("[object String]"!==V(n)||_&&"object"==typeof n&&_ in n)}(e))return G(M(String(e)));if(!function(n){return!("[object Date]"!==V(n)||_&&"object"==typeof n&&_ in n)}(e)&&!U(e)){var cn=X(e,M),pn=P?P(e)===Object.prototype:e instanceof Object||e.constructor===Object,un=e instanceof Object?"":"null prototype",dn=!pn&&_&&Object(e)===e&&_ in e?b.call(V(e),8,-1):un?"Object":"",mn=(pn||"function"!=typeof e.constructor?"":e.constructor.name?e.constructor.name+" ":"")+(dn||un?"["+E.call(w.call([],dn||[],un||[]),": ")+"] ":"");return 0===cn.length?mn+"{}":T?mn+"{"+Q(cn,T)+"}":mn+"{ "+E.call(cn,", ")+" }"}return String(e)};var H=Object.prototype.hasOwnProperty||function(n){return n in this};function q(n,e){return H.call(n,e)}function V(n){return f.call(n)}function K(n,e){if(n.indexOf)return n.indexOf(e);for(var t=0,r=n.length;t<r;t++)if(n[t]===e)return t;return-1}function W(n){var e=n.charCodeAt(0),t={8:"b",9:"t",10:"n",12:"f",13:"r"}[e];return t?"\\"+t:"\\x"+(e<16?"0":"")+y.call(e.toString(16))}function G(n){return"Object("+n+")"}function J(n){return n+" { ? }"}function Y(n,e,t,r){return n+" ("+e+") {"+(r?Q(t,r):E.call(t,", "))+"}"}function Q(n,e){if(0===n.length)return"";var t="\n"+e.prev+e.base;return t+E.call(n,","+t)+"\n"+e.prev}function X(n,e){var t=$(n),r=[];if(t){r.length=n.length;for(var a=0;a<n.length;a++)r[a]=q(n,a)?e(n[a],n):""}var o,i="function"==typeof T?T(n):[];if(A){o={};for(var s=0;s<i.length;s++)o["$"+i[s]]=i[s]}for(var l in n)q(n,l)&&(t&&String(Number(l))===l&&l<n.length||A&&o["$"+l]instanceof Symbol||(x.call(/[^\w$]/,l)?r.push(e(l,n)+": "+e(n[l],n)):r.push(l+": "+e(n[l],n))));if("function"==typeof T)for(var c=0;c<i.length;c++)R.call(n,i[c])&&r.push("["+e(i[c])+"]: "+e(n[i[c]],n));return r}},function(n,e){},function(n,e,t){"use strict";var r=t(114),a=Object.prototype.hasOwnProperty,o=Array.isArray,i={allowDots:!1,allowPrototypes:!1,allowSparse:!1,arrayLimit:20,charset:"utf-8",charsetSentinel:!1,comma:!1,decoder:r.decode,delimiter:"&",depth:5,ignoreQueryPrefix:!1,interpretNumericEntities:!1,parameterLimit:1e3,parseArrays:!0,plainObjects:!1,strictNullHandling:!1},s=function(n){return n.replace(/&#(\d+);/g,(function(n,e){return String.fromCharCode(parseInt(e,10))}))},l=function(n,e){return n&&"string"==typeof n&&e.comma&&n.indexOf(",")>-1?n.split(","):n},c=function(n,e,t,r){if(n){var o=t.allowDots?n.replace(/\.([^.[]+)/g,"[$1]"):n,i=/(\[[^[\]]*])/g,s=t.depth>0&&/(\[[^[\]]*])/.exec(o),c=s?o.slice(0,s.index):o,p=[];if(c){if(!t.plainObjects&&a.call(Object.prototype,c)&&!t.allowPrototypes)return;p.push(c)}for(var u=0;t.depth>0&&null!==(s=i.exec(o))&&u<t.depth;){if(u+=1,!t.plainObjects&&a.call(Object.prototype,s[1].slice(1,-1))&&!t.allowPrototypes)return;p.push(s[1])}return s&&p.push("["+o.slice(s.index)+"]"),function(n,e,t,r){for(var a=r?e:l(e,t),o=n.length-1;o>=0;--o){var i,s=n[o];if("[]"===s&&t.parseArrays)i=[].concat(a);else{i=t.plainObjects?Object.create(null):{};var c="["===s.charAt(0)&&"]"===s.charAt(s.length-1)?s.slice(1,-1):s,p=parseInt(c,10);t.parseArrays||""!==c?!isNaN(p)&&s!==c&&String(p)===c&&p>=0&&t.parseArrays&&p<=t.arrayLimit?(i=[])[p]=a:"__proto__"!==c&&(i[c]=a):i={0:a}}a=i}return a}(p,e,t,r)}};n.exports=function(n,e){var t=function(n){if(!n)return i;if(null!==n.decoder&&void 0!==n.decoder&&"function"!=typeof n.decoder)throw new TypeError("Decoder has to be a function.");if(void 0!==n.charset&&"utf-8"!==n.charset&&"iso-8859-1"!==n.charset)throw new TypeError("The charset option must be either utf-8, iso-8859-1, or undefined");var e=void 0===n.charset?i.charset:n.charset;return{allowDots:void 0===n.allowDots?i.allowDots:!!n.allowDots,allowPrototypes:"boolean"==typeof n.allowPrototypes?n.allowPrototypes:i.allowPrototypes,allowSparse:"boolean"==typeof n.allowSparse?n.allowSparse:i.allowSparse,arrayLimit:"number"==typeof n.arrayLimit?n.arrayLimit:i.arrayLimit,charset:e,charsetSentinel:"boolean"==typeof n.charsetSentinel?n.charsetSentinel:i.charsetSentinel,comma:"boolean"==typeof n.comma?n.comma:i.comma,decoder:"function"==typeof n.decoder?n.decoder:i.decoder,delimiter:"string"==typeof n.delimiter||r.isRegExp(n.delimiter)?n.delimiter:i.delimiter,depth:"number"==typeof n.depth||!1===n.depth?+n.depth:i.depth,ignoreQueryPrefix:!0===n.ignoreQueryPrefix,interpretNumericEntities:"boolean"==typeof n.interpretNumericEntities?n.interpretNumericEntities:i.interpretNumericEntities,parameterLimit:"number"==typeof n.parameterLimit?n.parameterLimit:i.parameterLimit,parseArrays:!1!==n.parseArrays,plainObjects:"boolean"==typeof n.plainObjects?n.plainObjects:i.plainObjects,strictNullHandling:"boolean"==typeof n.strictNullHandling?n.strictNullHandling:i.strictNullHandling}}(e);if(""===n||null==n)return t.plainObjects?Object.create(null):{};for(var p="string"==typeof n?function(n,e){var t,c={__proto__:null},p=e.ignoreQueryPrefix?n.replace(/^\?/,""):n,u=e.parameterLimit===1/0?void 0:e.parameterLimit,d=p.split(e.delimiter,u),m=-1,g=e.charset;if(e.charsetSentinel)for(t=0;t<d.length;++t)0===d[t].indexOf("utf8=")&&("utf8=%E2%9C%93"===d[t]?g="utf-8":"utf8=%26%2310003%3B"===d[t]&&(g="iso-8859-1"),m=t,t=d.length);for(t=0;t<d.length;++t)if(t!==m){var f,h,v=d[t],b=v.indexOf("]="),k=-1===b?v.indexOf("="):b+1;-1===k?(f=e.decoder(v,i.decoder,g,"key"),h=e.strictNullHandling?null:""):(f=e.decoder(v.slice(0,k),i.decoder,g,"key"),h=r.maybeMap(l(v.slice(k+1),e),(function(n){return e.decoder(n,i.decoder,g,"value")}))),h&&e.interpretNumericEntities&&"iso-8859-1"===g&&(h=s(h)),v.indexOf("[]=")>-1&&(h=o(h)?[h]:h),a.call(c,f)?c[f]=r.combine(c[f],h):c[f]=h}return c}(n,t):n,u=t.plainObjects?Object.create(null):{},d=Object.keys(p),m=0;m<d.length;++m){var g=d[m],f=c(g,p[g],t,"string"==typeof n);u=r.merge(u,f,t)}return!0===t.allowSparse?u:r.compact(u)}},function(n,e,t){var r=t(16),a=t(302),o=t(303);n.exports=function(n){var e=r(n);return o(e,a(e))+1}},function(n,e){n.exports=function(n){var e=new Date(n.getTime()),t=e.getTimezoneOffset();return e.setSeconds(0,0),6e4*t+e.getTime()%6e4}},function(n,e,t){var r=t(16);n.exports=function(n){var e=r(n),t=new Date(0);return t.setFullYear(e.getFullYear(),0,1),t.setHours(0,0,0,0),t}},function(n,e,t){var r=t(304);n.exports=function(n,e){var t=r(n),a=r(e),o=t.getTime()-6e4*t.getTimezoneOffset(),i=a.getTime()-6e4*a.getTimezoneOffset();return Math.round((o-i)/864e5)}},function(n,e,t){var r=t(16);n.exports=function(n){var e=r(n);return e.setHours(0,0,0,0),e}},function(n,e,t){var r=t(16),a=t(64),o=t(307);n.exports=function(n){var e=r(n),t=a(e).getTime()-o(e).getTime();return Math.round(t/6048e5)+1}},function(n,e,t){var r=t(16);n.exports=function(n,e){var t=e&&Number(e.weekStartsOn)||0,a=r(n),o=a.getDay(),i=(o<t?7:0)+o-t;return a.setDate(a.getDate()-i),a.setHours(0,0,0,0),a}},function(n,e,t){var r=t(116),a=t(64);n.exports=function(n){var e=r(n),t=new Date(0);return t.setFullYear(e,0,4),t.setHours(0,0,0,0),a(t)}},function(n,e,t){var r=t(115);n.exports=function(n){if(r(n))return!isNaN(n);throw new TypeError(toString.call(n)+" is not an instance of Date")}},function(n,e,t){var r=t(310),a=t(311);n.exports={distanceInWords:r(),format:a()}},function(n,e){n.exports=function(){var n={lessThanXSeconds:{one:"less than a second",other:"less than {{count}} seconds"},xSeconds:{one:"1 second",other:"{{count}} seconds"},halfAMinute:"half a minute",lessThanXMinutes:{one:"less than a minute",other:"less than {{count}} minutes"},xMinutes:{one:"1 minute",other:"{{count}} minutes"},aboutXHours:{one:"about 1 hour",other:"about {{count}} hours"},xHours:{one:"1 hour",other:"{{count}} hours"},xDays:{one:"1 day",other:"{{count}} days"},aboutXMonths:{one:"about 1 month",other:"about {{count}} months"},xMonths:{one:"1 month",other:"{{count}} months"},aboutXYears:{one:"about 1 year",other:"about {{count}} years"},xYears:{one:"1 year",other:"{{count}} years"},overXYears:{one:"over 1 year",other:"over {{count}} years"},almostXYears:{one:"almost 1 year",other:"almost {{count}} years"}};return{localize:function(e,t,r){var a;return r=r||{},a="string"==typeof n[e]?n[e]:1===t?n[e].one:n[e].other.replace("{{count}}",t),r.addSuffix?r.comparison>0?"in "+a:a+" ago":a}}}},function(n,e,t){var r=t(312);n.exports=function(){var n=["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],e=["January","February","March","April","May","June","July","August","September","October","November","December"],t=["Su","Mo","Tu","We","Th","Fr","Sa"],a=["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],o=["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],i=["AM","PM"],s=["am","pm"],l=["a.m.","p.m."],c={MMM:function(e){return n[e.getMonth()]},MMMM:function(n){return e[n.getMonth()]},dd:function(n){return t[n.getDay()]},ddd:function(n){return a[n.getDay()]},dddd:function(n){return o[n.getDay()]},A:function(n){return n.getHours()/12>=1?i[1]:i[0]},a:function(n){return n.getHours()/12>=1?s[1]:s[0]},aa:function(n){return n.getHours()/12>=1?l[1]:l[0]}};return["M","D","DDD","d","Q","W"].forEach((function(n){c[n+"o"]=function(e,t){return function(n){var e=n%100;if(e>20||e<10)switch(e%10){case 1:return n+"st";case 2:return n+"nd";case 3:return n+"rd"}return n+"th"}(t[n](e))}})),{formatters:c,formattingTokensRegExp:r(c)}}},function(n,e){var t=["M","MM","Q","D","DD","DDD","DDDD","d","E","W","WW","YY","YYYY","GG","GGGG","H","HH","h","hh","m","mm","s","ss","S","SS","SSS","Z","ZZ","X","x"];n.exports=function(n){var e=[];for(var r in n)n.hasOwnProperty(r)&&e.push(r);var a=t.concat(e).sort().reverse();return new RegExp("(\\[[^\\[]*\\])|(\\\\)?("+a.join("|")+"|.)","g")}},function(n,e,t){"use strict";var r=t(5),a=t(117),o=t(314),i=t(123);function s(n){var e=new o(n),t=a(o.prototype.request,e);return r.extend(t,o.prototype,e),r.extend(t,e),t}var l=s(t(65));l.Axios=o,l.create=function(n){return s(i(l.defaults,n))},l.Cancel=t(124),l.CancelToken=t(328),l.isCancel=t(122),l.all=function(n){return Promise.all(n)},l.spread=t(329),l.isAxiosError=t(330),n.exports=l,n.exports.default=l},function(n,e,t){"use strict";var r=t(5),a=t(118),o=t(315),i=t(316),s=t(123),l=t(326),c=l.validators;function p(n){this.defaults=n,this.interceptors={request:new o,response:new o}}p.prototype.request=function(n){"string"==typeof n?(n=arguments[1]||{}).url=arguments[0]:n=n||{},(n=s(this.defaults,n)).method?n.method=n.method.toLowerCase():this.defaults.method?n.method=this.defaults.method.toLowerCase():n.method="get";var e=n.transitional;void 0!==e&&l.assertOptions(e,{silentJSONParsing:c.transitional(c.boolean,"1.0.0"),forcedJSONParsing:c.transitional(c.boolean,"1.0.0"),clarifyTimeoutError:c.transitional(c.boolean,"1.0.0")},!1);var t=[],r=!0;this.interceptors.request.forEach((function(e){"function"==typeof e.runWhen&&!1===e.runWhen(n)||(r=r&&e.synchronous,t.unshift(e.fulfilled,e.rejected))}));var a,o=[];if(this.interceptors.response.forEach((function(n){o.push(n.fulfilled,n.rejected)})),!r){var p=[i,void 0];for(Array.prototype.unshift.apply(p,t),p=p.concat(o),a=Promise.resolve(n);p.length;)a=a.then(p.shift(),p.shift());return a}for(var u=n;t.length;){var d=t.shift(),m=t.shift();try{u=d(u)}catch(n){m(n);break}}try{a=i(u)}catch(n){return Promise.reject(n)}for(;o.length;)a=a.then(o.shift(),o.shift());return a},p.prototype.getUri=function(n){return n=s(this.defaults,n),a(n.url,n.params,n.paramsSerializer).replace(/^\?/,"")},r.forEach(["delete","get","head","options"],(function(n){p.prototype[n]=function(e,t){return this.request(s(t||{},{method:n,url:e,data:(t||{}).data}))}})),r.forEach(["post","put","patch"],(function(n){p.prototype[n]=function(e,t,r){return this.request(s(r||{},{method:n,url:e,data:t}))}})),n.exports=p},function(n,e,t){"use strict";var r=t(5);function a(){this.handlers=[]}a.prototype.use=function(n,e,t){return this.handlers.push({fulfilled:n,rejected:e,synchronous:!!t&&t.synchronous,runWhen:t?t.runWhen:null}),this.handlers.length-1},a.prototype.eject=function(n){this.handlers[n]&&(this.handlers[n]=null)},a.prototype.forEach=function(n){r.forEach(this.handlers,(function(e){null!==e&&n(e)}))},n.exports=a},function(n,e,t){"use strict";var r=t(5),a=t(317),o=t(122),i=t(65);function s(n){n.cancelToken&&n.cancelToken.throwIfRequested()}n.exports=function(n){return s(n),n.headers=n.headers||{},n.data=a.call(n,n.data,n.headers,n.transformRequest),n.headers=r.merge(n.headers.common||{},n.headers[n.method]||{},n.headers),r.forEach(["delete","get","head","post","put","patch","common"],(function(e){delete n.headers[e]})),(n.adapter||i.adapter)(n).then((function(e){return s(n),e.data=a.call(n,e.data,e.headers,n.transformResponse),e}),(function(e){return o(e)||(s(n),e&&e.response&&(e.response.data=a.call(n,e.response.data,e.response.headers,n.transformResponse))),Promise.reject(e)}))}},function(n,e,t){"use strict";var r=t(5),a=t(65);n.exports=function(n,e,t){var o=this||a;return r.forEach(t,(function(t){n=t.call(o,n,e)})),n}},function(n,e,t){"use strict";var r=t(5);n.exports=function(n,e){r.forEach(n,(function(t,r){r!==e&&r.toUpperCase()===e.toUpperCase()&&(n[e]=t,delete n[r])}))}},function(n,e,t){"use strict";var r=t(121);n.exports=function(n,e,t){var a=t.config.validateStatus;t.status&&a&&!a(t.status)?e(r("Request failed with status code "+t.status,t.config,null,t.request,t)):n(t)}},function(n,e,t){"use strict";var r=t(5);n.exports=r.isStandardBrowserEnv()?{write:function(n,e,t,a,o,i){var s=[];s.push(n+"="+encodeURIComponent(e)),r.isNumber(t)&&s.push("expires="+new Date(t).toGMTString()),r.isString(a)&&s.push("path="+a),r.isString(o)&&s.push("domain="+o),!0===i&&s.push("secure"),document.cookie=s.join("; ")},read:function(n){var e=document.cookie.match(new RegExp("(^|;\\s*)("+n+")=([^;]*)"));return e?decodeURIComponent(e[3]):null},remove:function(n){this.write(n,"",Date.now()-864e5)}}:{write:function(){},read:function(){return null},remove:function(){}}},function(n,e,t){"use strict";var r=t(322),a=t(323);n.exports=function(n,e){return n&&!r(e)?a(n,e):e}},function(n,e,t){"use strict";n.exports=function(n){return/^([a-z][a-z\d\+\-\.]*:)?\/\//i.test(n)}},function(n,e,t){"use strict";n.exports=function(n,e){return e?n.replace(/\/+$/,"")+"/"+e.replace(/^\/+/,""):n}},function(n,e,t){"use strict";var r=t(5),a=["age","authorization","content-length","content-type","etag","expires","from","host","if-modified-since","if-unmodified-since","last-modified","location","max-forwards","proxy-authorization","referer","retry-after","user-agent"];n.exports=function(n){var e,t,o,i={};return n?(r.forEach(n.split("\n"),(function(n){if(o=n.indexOf(":"),e=r.trim(n.substr(0,o)).toLowerCase(),t=r.trim(n.substr(o+1)),e){if(i[e]&&a.indexOf(e)>=0)return;i[e]="set-cookie"===e?(i[e]?i[e]:[]).concat([t]):i[e]?i[e]+", "+t:t}})),i):i}},function(n,e,t){"use strict";var r=t(5);n.exports=r.isStandardBrowserEnv()?function(){var n,e=/(msie|trident)/i.test(navigator.userAgent),t=document.createElement("a");function a(n){var r=n;return e&&(t.setAttribute("href",r),r=t.href),t.setAttribute("href",r),{href:t.href,protocol:t.protocol?t.protocol.replace(/:$/,""):"",host:t.host,search:t.search?t.search.replace(/^\?/,""):"",hash:t.hash?t.hash.replace(/^#/,""):"",hostname:t.hostname,port:t.port,pathname:"/"===t.pathname.charAt(0)?t.pathname:"/"+t.pathname}}return n=a(window.location.href),function(e){var t=r.isString(e)?a(e):e;return t.protocol===n.protocol&&t.host===n.host}}():function(){return!0}},function(n,e,t){"use strict";var r=t(327),a={};["object","boolean","number","function","string","symbol"].forEach((function(n,e){a[n]=function(t){return typeof t===n||"a"+(e<1?"n ":" ")+n}}));var o={},i=r.version.split(".");function s(n,e){for(var t=e?e.split("."):i,r=n.split("."),a=0;a<3;a++){if(t[a]>r[a])return!0;if(t[a]<r[a])return!1}return!1}a.transitional=function(n,e,t){var a=e&&s(e);function i(n,e){return"[Axios v"+r.version+"] Transitional option '"+n+"'"+e+(t?". "+t:"")}return function(t,r,s){if(!1===n)throw new Error(i(r," has been removed in "+e));return a&&!o[r]&&(o[r]=!0,console.warn(i(r," has been deprecated since v"+e+" and will be removed in the near future"))),!n||n(t,r,s)}},n.exports={isOlderVersion:s,assertOptions:function(n,e,t){if("object"!=typeof n)throw new TypeError("options must be an object");for(var r=Object.keys(n),a=r.length;a-- >0;){var o=r[a],i=e[o];if(i){var s=n[o],l=void 0===s||i(s,o,n);if(!0!==l)throw new TypeError("option "+o+" must be "+l)}else if(!0!==t)throw Error("Unknown option "+o)}},validators:a}},function(n){n.exports=JSON.parse('{"_args":[["axios@0.21.4","E:\\\\BigData\\\\blog"]],"_development":true,"_from":"axios@0.21.4","_id":"axios@0.21.4","_inBundle":false,"_integrity":"sha512-ut5vewkiu8jjGBdqpM44XxjuCjq9LAKeHVmoVfHVzy8eHgxxq8SbAVQNovDA8mVi05kP0Ea/n/UzcSHcTJQfNg==","_location":"/axios","_phantomChildren":{},"_requested":{"type":"version","registry":true,"raw":"axios@0.21.4","name":"axios","escapedName":"axios","rawSpec":"0.21.4","saveSpec":null,"fetchSpec":"0.21.4"},"_requiredBy":["/@vssue/api-bitbucket-v2","/@vssue/api-gitee-v5","/@vssue/api-github-v3","/@vssue/api-github-v4","/@vssue/api-gitlab-v4"],"_resolved":"https://registry.npmmirror.com/axios/-/axios-0.21.4.tgz","_spec":"0.21.4","_where":"E:\\\\BigData\\\\blog","author":{"name":"Matt Zabriskie"},"browser":{"./lib/adapters/http.js":"./lib/adapters/xhr.js"},"bugs":{"url":"https://github.com/axios/axios/issues"},"bundlesize":[{"path":"./dist/axios.min.js","threshold":"5kB"}],"dependencies":{"follow-redirects":"^1.14.0"},"description":"Promise based HTTP client for the browser and node.js","devDependencies":{"coveralls":"^3.0.0","es6-promise":"^4.2.4","grunt":"^1.3.0","grunt-banner":"^0.6.0","grunt-cli":"^1.2.0","grunt-contrib-clean":"^1.1.0","grunt-contrib-watch":"^1.0.0","grunt-eslint":"^23.0.0","grunt-karma":"^4.0.0","grunt-mocha-test":"^0.13.3","grunt-ts":"^6.0.0-beta.19","grunt-webpack":"^4.0.2","istanbul-instrumenter-loader":"^1.0.0","jasmine-core":"^2.4.1","karma":"^6.3.2","karma-chrome-launcher":"^3.1.0","karma-firefox-launcher":"^2.1.0","karma-jasmine":"^1.1.1","karma-jasmine-ajax":"^0.1.13","karma-safari-launcher":"^1.0.0","karma-sauce-launcher":"^4.3.6","karma-sinon":"^1.0.5","karma-sourcemap-loader":"^0.3.8","karma-webpack":"^4.0.2","load-grunt-tasks":"^3.5.2","minimist":"^1.2.0","mocha":"^8.2.1","sinon":"^4.5.0","terser-webpack-plugin":"^4.2.3","typescript":"^4.0.5","url-search-params":"^0.10.0","webpack":"^4.44.2","webpack-dev-server":"^3.11.0"},"homepage":"https://axios-http.com","jsdelivr":"dist/axios.min.js","keywords":["xhr","http","ajax","promise","node"],"license":"MIT","main":"index.js","name":"axios","repository":{"type":"git","url":"git+https://github.com/axios/axios.git"},"scripts":{"build":"NODE_ENV=production grunt build","coveralls":"cat coverage/lcov.info | ./node_modules/coveralls/bin/coveralls.js","examples":"node ./examples/server.js","fix":"eslint --fix lib/**/*.js","postversion":"git push && git push --tags","preversion":"npm test","start":"node ./sandbox/server.js","test":"grunt test","version":"npm run build && grunt version && git add -A dist && git add CHANGELOG.md bower.json package.json"},"typings":"./index.d.ts","unpkg":"dist/axios.min.js","version":"0.21.4"}')},function(n,e,t){"use strict";var r=t(124);function a(n){if("function"!=typeof n)throw new TypeError("executor must be a function.");var e;this.promise=new Promise((function(n){e=n}));var t=this;n((function(n){t.reason||(t.reason=new r(n),e(t.reason))}))}a.prototype.throwIfRequested=function(){if(this.reason)throw this.reason},a.source=function(){var n;return{token:new a((function(e){n=e})),cancel:n}},n.exports=a},function(n,e,t){"use strict";n.exports=function(n){return function(e){return n.apply(null,e)}}},function(n,e,t){"use strict";n.exports=function(n){return"object"==typeof n&&!0===n.isAxiosError}},function(n,e,t){},function(n,e,t){"use strict";t(125)},function(n,e,t){},function(n,e,t){var r;function a(n){function t(){if(t.enabled){var n=t,a=+new Date,o=a-(r||a);n.diff=o,n.prev=r,n.curr=a,r=a;for(var i=new Array(arguments.length),s=0;s<i.length;s++)i[s]=arguments[s];i[0]=e.coerce(i[0]),"string"!=typeof i[0]&&i.unshift("%O");var l=0;i[0]=i[0].replace(/%([a-zA-Z%])/g,(function(t,r){if("%%"===t)return t;l++;var a=e.formatters[r];if("function"==typeof a){var o=i[l];t=a.call(n,o),i.splice(l,1),l--}return t})),e.formatArgs.call(n,i);var c=t.log||e.log||console.log.bind(console);c.apply(n,i)}}return t.namespace=n,t.enabled=e.enabled(n),t.useColors=e.useColors(),t.color=function(n){var t,r=0;for(t in n)r=(r<<5)-r+n.charCodeAt(t),r|=0;return e.colors[Math.abs(r)%e.colors.length]}(n),"function"==typeof e.init&&e.init(t),t}(e=n.exports=a.debug=a.default=a).coerce=function(n){return n instanceof Error?n.stack||n.message:n},e.disable=function(){e.enable("")},e.enable=function(n){e.save(n),e.names=[],e.skips=[];for(var t=("string"==typeof n?n:"").split(/[\s,]+/),r=t.length,a=0;a<r;a++)t[a]&&("-"===(n=t[a].replace(/\*/g,".*?"))[0]?e.skips.push(new RegExp("^"+n.substr(1)+"$")):e.names.push(new RegExp("^"+n+"$")))},e.enabled=function(n){var t,r;for(t=0,r=e.skips.length;t<r;t++)if(e.skips[t].test(n))return!1;for(t=0,r=e.names.length;t<r;t++)if(e.names[t].test(n))return!0;return!1},e.humanize=t(335),e.names=[],e.skips=[],e.formatters={}},function(n,e){var t=1e3,r=6e4,a=60*r,o=24*a;function i(n,e,t){if(!(n<e))return n<1.5*e?Math.floor(n/e)+" "+t:Math.ceil(n/e)+" "+t+"s"}n.exports=function(n,e){e=e||{};var s,l=typeof n;if("string"===l&&n.length>0)return function(n){if((n=String(n)).length>100)return;var e=/^((?:\d+)?\.?\d+) *(milliseconds?|msecs?|ms|seconds?|secs?|s|minutes?|mins?|m|hours?|hrs?|h|days?|d|years?|yrs?|y)?$/i.exec(n);if(!e)return;var i=parseFloat(e[1]);switch((e[2]||"ms").toLowerCase()){case"years":case"year":case"yrs":case"yr":case"y":return 315576e5*i;case"days":case"day":case"d":return i*o;case"hours":case"hour":case"hrs":case"hr":case"h":return i*a;case"minutes":case"minute":case"mins":case"min":case"m":return i*r;case"seconds":case"second":case"secs":case"sec":case"s":return i*t;case"milliseconds":case"millisecond":case"msecs":case"msec":case"ms":return i;default:return}}(n);if("number"===l&&!1===isNaN(n))return e.long?i(s=n,o,"day")||i(s,a,"hour")||i(s,r,"minute")||i(s,t,"second")||s+" ms":function(n){if(n>=o)return Math.round(n/o)+"d";if(n>=a)return Math.round(n/a)+"h";if(n>=r)return Math.round(n/r)+"m";if(n>=t)return Math.round(n/t)+"s";return n+"ms"}(n);throw new Error("val is not a non-empty string or a valid number. val="+JSON.stringify(n))}},function(n,e,t){},function(n,e,t){"use strict";t(127)},function(n,e,t){"use strict";t(128)},function(n,e,t){"use strict";t.r(e);var r=t(1);
/*!
  * vue-router v3.6.5
  * (c) 2022 Evan You
  * @license MIT
  */function a(n,e){for(var t in e)n[t]=e[t];return n}var o=/[!'()*]/g,i=function(n){return"%"+n.charCodeAt(0).toString(16)},s=/%2C/g,l=function(n){return encodeURIComponent(n).replace(o,i).replace(s,",")};function c(n){try{return decodeURIComponent(n)}catch(n){0}return n}var p=function(n){return null==n||"object"==typeof n?n:String(n)};function u(n){var e={};return(n=n.trim().replace(/^(\?|#|&)/,""))?(n.split("&").forEach((function(n){var t=n.replace(/\+/g," ").split("="),r=c(t.shift()),a=t.length>0?c(t.join("=")):null;void 0===e[r]?e[r]=a:Array.isArray(e[r])?e[r].push(a):e[r]=[e[r],a]})),e):e}function d(n){var e=n?Object.keys(n).map((function(e){var t=n[e];if(void 0===t)return"";if(null===t)return l(e);if(Array.isArray(t)){var r=[];return t.forEach((function(n){void 0!==n&&(null===n?r.push(l(e)):r.push(l(e)+"="+l(n)))})),r.join("&")}return l(e)+"="+l(t)})).filter((function(n){return n.length>0})).join("&"):null;return e?"?"+e:""}var m=/\/?$/;function g(n,e,t,r){var a=r&&r.options.stringifyQuery,o=e.query||{};try{o=f(o)}catch(n){}var i={name:e.name||n&&n.name,meta:n&&n.meta||{},path:e.path||"/",hash:e.hash||"",query:o,params:e.params||{},fullPath:b(e,a),matched:n?v(n):[]};return t&&(i.redirectedFrom=b(t,a)),Object.freeze(i)}function f(n){if(Array.isArray(n))return n.map(f);if(n&&"object"==typeof n){var e={};for(var t in n)e[t]=f(n[t]);return e}return n}var h=g(null,{path:"/"});function v(n){for(var e=[];n;)e.unshift(n),n=n.parent;return e}function b(n,e){var t=n.path,r=n.query;void 0===r&&(r={});var a=n.hash;return void 0===a&&(a=""),(t||"/")+(e||d)(r)+a}function k(n,e,t){return e===h?n===e:!!e&&(n.path&&e.path?n.path.replace(m,"")===e.path.replace(m,"")&&(t||n.hash===e.hash&&y(n.query,e.query)):!(!n.name||!e.name)&&(n.name===e.name&&(t||n.hash===e.hash&&y(n.query,e.query)&&y(n.params,e.params))))}function y(n,e){if(void 0===n&&(n={}),void 0===e&&(e={}),!n||!e)return n===e;var t=Object.keys(n).sort(),r=Object.keys(e).sort();return t.length===r.length&&t.every((function(t,a){var o=n[t];if(r[a]!==t)return!1;var i=e[t];return null==o||null==i?o===i:"object"==typeof o&&"object"==typeof i?y(o,i):String(o)===String(i)}))}function S(n){for(var e=0;e<n.matched.length;e++){var t=n.matched[e];for(var r in t.instances){var a=t.instances[r],o=t.enteredCbs[r];if(a&&o){delete t.enteredCbs[r];for(var i=0;i<o.length;i++)a._isBeingDestroyed||o[i](a)}}}}var x={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(n,e){var t=e.props,r=e.children,o=e.parent,i=e.data;i.routerView=!0;for(var s=o.$createElement,l=t.name,c=o.$route,p=o._routerViewCache||(o._routerViewCache={}),u=0,d=!1;o&&o._routerRoot!==o;){var m=o.$vnode?o.$vnode.data:{};m.routerView&&u++,m.keepAlive&&o._directInactive&&o._inactive&&(d=!0),o=o.$parent}if(i.routerViewDepth=u,d){var g=p[l],f=g&&g.component;return f?(g.configProps&&w(f,i,g.route,g.configProps),s(f,i,r)):s()}var h=c.matched[u],v=h&&h.components[l];if(!h||!v)return p[l]=null,s();p[l]={component:v},i.registerRouteInstance=function(n,e){var t=h.instances[l];(e&&t!==n||!e&&t===n)&&(h.instances[l]=e)},(i.hook||(i.hook={})).prepatch=function(n,e){h.instances[l]=e.componentInstance},i.hook.init=function(n){n.data.keepAlive&&n.componentInstance&&n.componentInstance!==h.instances[l]&&(h.instances[l]=n.componentInstance),S(c)};var b=h.props&&h.props[l];return b&&(a(p[l],{route:c,configProps:b}),w(v,i,c,b)),s(v,i,r)}};function w(n,e,t,r){var o=e.props=function(n,e){switch(typeof e){case"undefined":return;case"object":return e;case"function":return e(n);case"boolean":return e?n.params:void 0;default:0}}(t,r);if(o){o=e.props=a({},o);var i=e.attrs=e.attrs||{};for(var s in o)n.props&&s in n.props||(i[s]=o[s],delete o[s])}}function E(n,e,t){var r=n.charAt(0);if("/"===r)return n;if("?"===r||"#"===r)return e+n;var a=e.split("/");t&&a[a.length-1]||a.pop();for(var o=n.replace(/^\//,"").split("/"),i=0;i<o.length;i++){var s=o[i];".."===s?a.pop():"."!==s&&a.push(s)}return""!==a[0]&&a.unshift(""),a.join("/")}function D(n){return n.replace(/\/(?:\s*\/)+/g,"/")}var C=Array.isArray||function(n){return"[object Array]"==Object.prototype.toString.call(n)},I=U,T=P,O=function(n,e){return B(P(n,e),e)},A=B,_=$,R=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function P(n,e){for(var t,r=[],a=0,o=0,i="",s=e&&e.delimiter||"/";null!=(t=R.exec(n));){var l=t[0],c=t[1],p=t.index;if(i+=n.slice(o,p),o=p+l.length,c)i+=c[1];else{var u=n[o],d=t[2],m=t[3],g=t[4],f=t[5],h=t[6],v=t[7];i&&(r.push(i),i="");var b=null!=d&&null!=u&&u!==d,k="+"===h||"*"===h,y="?"===h||"*"===h,S=t[2]||s,x=g||f;r.push({name:m||a++,prefix:d||"",delimiter:S,optional:y,repeat:k,partial:b,asterisk:!!v,pattern:x?j(x):v?".*":"[^"+M(S)+"]+?"})}}return o<n.length&&(i+=n.substr(o)),i&&r.push(i),r}function F(n){return encodeURI(n).replace(/[\/?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()}))}function B(n,e){for(var t=new Array(n.length),r=0;r<n.length;r++)"object"==typeof n[r]&&(t[r]=new RegExp("^(?:"+n[r].pattern+")$",N(e)));return function(e,r){for(var a="",o=e||{},i=(r||{}).pretty?F:encodeURIComponent,s=0;s<n.length;s++){var l=n[s];if("string"!=typeof l){var c,p=o[l.name];if(null==p){if(l.optional){l.partial&&(a+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(C(p)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(p)+"`");if(0===p.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var u=0;u<p.length;u++){if(c=i(p[u]),!t[s].test(c))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(c)+"`");a+=(0===u?l.prefix:l.delimiter)+c}}else{if(c=l.asterisk?encodeURI(p).replace(/[?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()})):i(p),!t[s].test(c))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+c+'"');a+=l.prefix+c}}else a+=l}return a}}function M(n){return n.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function j(n){return n.replace(/([=!:$\/()])/g,"\\$1")}function L(n,e){return n.keys=e,n}function N(n){return n&&n.sensitive?"":"i"}function $(n,e,t){C(e)||(t=e||t,e=[]);for(var r=(t=t||{}).strict,a=!1!==t.end,o="",i=0;i<n.length;i++){var s=n[i];if("string"==typeof s)o+=M(s);else{var l=M(s.prefix),c="(?:"+s.pattern+")";e.push(s),s.repeat&&(c+="(?:"+l+c+")*"),o+=c=s.optional?s.partial?l+"("+c+")?":"(?:"+l+"("+c+"))?":l+"("+c+")"}}var p=M(t.delimiter||"/"),u=o.slice(-p.length)===p;return r||(o=(u?o.slice(0,-p.length):o)+"(?:"+p+"(?=$))?"),o+=a?"$":r&&u?"":"(?="+p+"|$)",L(new RegExp("^"+o,N(t)),e)}function U(n,e,t){return C(e)||(t=e||t,e=[]),t=t||{},n instanceof RegExp?function(n,e){var t=n.source.match(/\((?!\?)/g);if(t)for(var r=0;r<t.length;r++)e.push({name:r,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return L(n,e)}(n,e):C(n)?function(n,e,t){for(var r=[],a=0;a<n.length;a++)r.push(U(n[a],e,t).source);return L(new RegExp("(?:"+r.join("|")+")",N(t)),e)}(n,e,t):function(n,e,t){return $(P(n,t),e,t)}(n,e,t)}I.parse=T,I.compile=O,I.tokensToFunction=A,I.tokensToRegExp=_;var z=Object.create(null);function H(n,e,t){e=e||{};try{var r=z[n]||(z[n]=I.compile(n));return"string"==typeof e.pathMatch&&(e[0]=e.pathMatch),r(e,{pretty:!0})}catch(n){return""}finally{delete e[0]}}function q(n,e,t,r){var o="string"==typeof n?{path:n}:n;if(o._normalized)return o;if(o.name){var i=(o=a({},n)).params;return i&&"object"==typeof i&&(o.params=a({},i)),o}if(!o.path&&o.params&&e){(o=a({},o))._normalized=!0;var s=a(a({},e.params),o.params);if(e.name)o.name=e.name,o.params=s;else if(e.matched.length){var l=e.matched[e.matched.length-1].path;o.path=H(l,s,e.path)}else 0;return o}var c=function(n){var e="",t="",r=n.indexOf("#");r>=0&&(e=n.slice(r),n=n.slice(0,r));var a=n.indexOf("?");return a>=0&&(t=n.slice(a+1),n=n.slice(0,a)),{path:n,query:t,hash:e}}(o.path||""),d=e&&e.path||"/",m=c.path?E(c.path,d,t||o.append):d,g=function(n,e,t){void 0===e&&(e={});var r,a=t||u;try{r=a(n||"")}catch(n){r={}}for(var o in e){var i=e[o];r[o]=Array.isArray(i)?i.map(p):p(i)}return r}(c.query,o.query,r&&r.options.parseQuery),f=o.hash||c.hash;return f&&"#"!==f.charAt(0)&&(f="#"+f),{_normalized:!0,path:m,query:g,hash:f}}var V,K=function(){},W={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(n){var e=this,t=this.$router,r=this.$route,o=t.resolve(this.to,r,this.append),i=o.location,s=o.route,l=o.href,c={},p=t.options.linkActiveClass,u=t.options.linkExactActiveClass,d=null==p?"router-link-active":p,f=null==u?"router-link-exact-active":u,h=null==this.activeClass?d:this.activeClass,v=null==this.exactActiveClass?f:this.exactActiveClass,b=s.redirectedFrom?g(null,q(s.redirectedFrom),null,t):s;c[v]=k(r,b,this.exactPath),c[h]=this.exact||this.exactPath?c[v]:function(n,e){return 0===n.path.replace(m,"/").indexOf(e.path.replace(m,"/"))&&(!e.hash||n.hash===e.hash)&&function(n,e){for(var t in e)if(!(t in n))return!1;return!0}(n.query,e.query)}(r,b);var y=c[v]?this.ariaCurrentValue:null,S=function(n){G(n)&&(e.replace?t.replace(i,K):t.push(i,K))},x={click:G};Array.isArray(this.event)?this.event.forEach((function(n){x[n]=S})):x[this.event]=S;var w={class:c},E=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:l,route:s,navigate:S,isActive:c[h],isExactActive:c[v]});if(E){if(1===E.length)return E[0];if(E.length>1||!E.length)return 0===E.length?n():n("span",{},E)}if("a"===this.tag)w.on=x,w.attrs={href:l,"aria-current":y};else{var D=function n(e){var t;if(e)for(var r=0;r<e.length;r++){if("a"===(t=e[r]).tag)return t;if(t.children&&(t=n(t.children)))return t}}(this.$slots.default);if(D){D.isStatic=!1;var C=D.data=a({},D.data);for(var I in C.on=C.on||{},C.on){var T=C.on[I];I in x&&(C.on[I]=Array.isArray(T)?T:[T])}for(var O in x)O in C.on?C.on[O].push(x[O]):C.on[O]=S;var A=D.data.attrs=a({},D.data.attrs);A.href=l,A["aria-current"]=y}else w.on=x}return n(this.tag,w,this.$slots.default)}};function G(n){if(!(n.metaKey||n.altKey||n.ctrlKey||n.shiftKey||n.defaultPrevented||void 0!==n.button&&0!==n.button)){if(n.currentTarget&&n.currentTarget.getAttribute){var e=n.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(e))return}return n.preventDefault&&n.preventDefault(),!0}}var J="undefined"!=typeof window;function Y(n,e,t,r,a){var o=e||[],i=t||Object.create(null),s=r||Object.create(null);n.forEach((function(n){!function n(e,t,r,a,o,i){var s=a.path,l=a.name;0;var c=a.pathToRegexpOptions||{},p=function(n,e,t){t||(n=n.replace(/\/$/,""));if("/"===n[0])return n;if(null==e)return n;return D(e.path+"/"+n)}(s,o,c.strict);"boolean"==typeof a.caseSensitive&&(c.sensitive=a.caseSensitive);var u={path:p,regex:Q(p,c),components:a.components||{default:a.component},alias:a.alias?"string"==typeof a.alias?[a.alias]:a.alias:[],instances:{},enteredCbs:{},name:l,parent:o,matchAs:i,redirect:a.redirect,beforeEnter:a.beforeEnter,meta:a.meta||{},props:null==a.props?{}:a.components?a.props:{default:a.props}};a.children&&a.children.forEach((function(a){var o=i?D(i+"/"+a.path):void 0;n(e,t,r,a,u,o)}));t[u.path]||(e.push(u.path),t[u.path]=u);if(void 0!==a.alias)for(var d=Array.isArray(a.alias)?a.alias:[a.alias],m=0;m<d.length;++m){0;var g={path:d[m],children:a.children};n(e,t,r,g,o,u.path||"/")}l&&(r[l]||(r[l]=u))}(o,i,s,n,a)}));for(var l=0,c=o.length;l<c;l++)"*"===o[l]&&(o.push(o.splice(l,1)[0]),c--,l--);return{pathList:o,pathMap:i,nameMap:s}}function Q(n,e){return I(n,[],e)}function X(n,e){var t=Y(n),r=t.pathList,a=t.pathMap,o=t.nameMap;function i(n,t,i){var s=q(n,t,!1,e),c=s.name;if(c){var p=o[c];if(!p)return l(null,s);var u=p.regex.keys.filter((function(n){return!n.optional})).map((function(n){return n.name}));if("object"!=typeof s.params&&(s.params={}),t&&"object"==typeof t.params)for(var d in t.params)!(d in s.params)&&u.indexOf(d)>-1&&(s.params[d]=t.params[d]);return s.path=H(p.path,s.params),l(p,s,i)}if(s.path){s.params={};for(var m=0;m<r.length;m++){var g=r[m],f=a[g];if(Z(f.regex,s.path,s.params))return l(f,s,i)}}return l(null,s)}function s(n,t){var r=n.redirect,a="function"==typeof r?r(g(n,t,null,e)):r;if("string"==typeof a&&(a={path:a}),!a||"object"!=typeof a)return l(null,t);var s=a,c=s.name,p=s.path,u=t.query,d=t.hash,m=t.params;if(u=s.hasOwnProperty("query")?s.query:u,d=s.hasOwnProperty("hash")?s.hash:d,m=s.hasOwnProperty("params")?s.params:m,c){o[c];return i({_normalized:!0,name:c,query:u,hash:d,params:m},void 0,t)}if(p){var f=function(n,e){return E(n,e.parent?e.parent.path:"/",!0)}(p,n);return i({_normalized:!0,path:H(f,m),query:u,hash:d},void 0,t)}return l(null,t)}function l(n,t,r){return n&&n.redirect?s(n,r||t):n&&n.matchAs?function(n,e,t){var r=i({_normalized:!0,path:H(t,e.params)});if(r){var a=r.matched,o=a[a.length-1];return e.params=r.params,l(o,e)}return l(null,e)}(0,t,n.matchAs):g(n,t,r,e)}return{match:i,addRoute:function(n,e){var t="object"!=typeof n?o[n]:void 0;Y([e||n],r,a,o,t),t&&t.alias.length&&Y(t.alias.map((function(n){return{path:n,children:[e]}})),r,a,o,t)},getRoutes:function(){return r.map((function(n){return a[n]}))},addRoutes:function(n){Y(n,r,a,o)}}}function Z(n,e,t){var r=e.match(n);if(!r)return!1;if(!t)return!0;for(var a=1,o=r.length;a<o;++a){var i=n.keys[a-1];i&&(t[i.name||"pathMatch"]="string"==typeof r[a]?c(r[a]):r[a])}return!0}var nn=J&&window.performance&&window.performance.now?window.performance:Date;function en(){return nn.now().toFixed(3)}var tn=en();function rn(){return tn}function an(n){return tn=n}var on=Object.create(null);function sn(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var n=window.location.protocol+"//"+window.location.host,e=window.location.href.replace(n,""),t=a({},window.history.state);return t.key=rn(),window.history.replaceState(t,"",e),window.addEventListener("popstate",pn),function(){window.removeEventListener("popstate",pn)}}function ln(n,e,t,r){if(n.app){var a=n.options.scrollBehavior;a&&n.app.$nextTick((function(){var o=function(){var n=rn();if(n)return on[n]}(),i=a.call(n,e,t,r?o:null);i&&("function"==typeof i.then?i.then((function(n){fn(n,o)})).catch((function(n){0})):fn(i,o))}))}}function cn(){var n=rn();n&&(on[n]={x:window.pageXOffset,y:window.pageYOffset})}function pn(n){cn(),n.state&&n.state.key&&an(n.state.key)}function un(n){return mn(n.x)||mn(n.y)}function dn(n){return{x:mn(n.x)?n.x:window.pageXOffset,y:mn(n.y)?n.y:window.pageYOffset}}function mn(n){return"number"==typeof n}var gn=/^#\d/;function fn(n,e){var t,r="object"==typeof n;if(r&&"string"==typeof n.selector){var a=gn.test(n.selector)?document.getElementById(n.selector.slice(1)):document.querySelector(n.selector);if(a){var o=n.offset&&"object"==typeof n.offset?n.offset:{};e=function(n,e){var t=document.documentElement.getBoundingClientRect(),r=n.getBoundingClientRect();return{x:r.left-t.left-e.x,y:r.top-t.top-e.y}}(a,o={x:mn((t=o).x)?t.x:0,y:mn(t.y)?t.y:0})}else un(n)&&(e=dn(n))}else r&&un(n)&&(e=dn(n));e&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:e.x,top:e.y,behavior:n.behavior}):window.scrollTo(e.x,e.y))}var hn,vn=J&&((-1===(hn=window.navigator.userAgent).indexOf("Android 2.")&&-1===hn.indexOf("Android 4.0")||-1===hn.indexOf("Mobile Safari")||-1!==hn.indexOf("Chrome")||-1!==hn.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function bn(n,e){cn();var t=window.history;try{if(e){var r=a({},t.state);r.key=rn(),t.replaceState(r,"",n)}else t.pushState({key:an(en())},"",n)}catch(t){window.location[e?"replace":"assign"](n)}}function kn(n){bn(n,!0)}var yn={redirected:2,aborted:4,cancelled:8,duplicated:16};function Sn(n,e){return wn(n,e,yn.redirected,'Redirected when going from "'+n.fullPath+'" to "'+function(n){if("string"==typeof n)return n;if("path"in n)return n.path;var e={};return En.forEach((function(t){t in n&&(e[t]=n[t])})),JSON.stringify(e,null,2)}(e)+'" via a navigation guard.')}function xn(n,e){return wn(n,e,yn.cancelled,'Navigation cancelled from "'+n.fullPath+'" to "'+e.fullPath+'" with a new navigation.')}function wn(n,e,t,r){var a=new Error(r);return a._isRouter=!0,a.from=n,a.to=e,a.type=t,a}var En=["params","query","hash"];function Dn(n){return Object.prototype.toString.call(n).indexOf("Error")>-1}function Cn(n,e){return Dn(n)&&n._isRouter&&(null==e||n.type===e)}function In(n,e,t){var r=function(a){a>=n.length?t():n[a]?e(n[a],(function(){r(a+1)})):r(a+1)};r(0)}function Tn(n){return function(e,t,r){var a=!1,o=0,i=null;On(n,(function(n,e,t,s){if("function"==typeof n&&void 0===n.cid){a=!0,o++;var l,c=Rn((function(e){var a;((a=e).__esModule||_n&&"Module"===a[Symbol.toStringTag])&&(e=e.default),n.resolved="function"==typeof e?e:V.extend(e),t.components[s]=e,--o<=0&&r()})),p=Rn((function(n){var e="Failed to resolve async component "+s+": "+n;i||(i=Dn(n)?n:new Error(e),r(i))}));try{l=n(c,p)}catch(n){p(n)}if(l)if("function"==typeof l.then)l.then(c,p);else{var u=l.component;u&&"function"==typeof u.then&&u.then(c,p)}}})),a||r()}}function On(n,e){return An(n.map((function(n){return Object.keys(n.components).map((function(t){return e(n.components[t],n.instances[t],n,t)}))})))}function An(n){return Array.prototype.concat.apply([],n)}var _n="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function Rn(n){var e=!1;return function(){for(var t=[],r=arguments.length;r--;)t[r]=arguments[r];if(!e)return e=!0,n.apply(this,t)}}var Pn=function(n,e){this.router=n,this.base=function(n){if(!n)if(J){var e=document.querySelector("base");n=(n=e&&e.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else n="/";"/"!==n.charAt(0)&&(n="/"+n);return n.replace(/\/$/,"")}(e),this.current=h,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function Fn(n,e,t,r){var a=On(n,(function(n,r,a,o){var i=function(n,e){"function"!=typeof n&&(n=V.extend(n));return n.options[e]}(n,e);if(i)return Array.isArray(i)?i.map((function(n){return t(n,r,a,o)})):t(i,r,a,o)}));return An(r?a.reverse():a)}function Bn(n,e){if(e)return function(){return n.apply(e,arguments)}}Pn.prototype.listen=function(n){this.cb=n},Pn.prototype.onReady=function(n,e){this.ready?n():(this.readyCbs.push(n),e&&this.readyErrorCbs.push(e))},Pn.prototype.onError=function(n){this.errorCbs.push(n)},Pn.prototype.transitionTo=function(n,e,t){var r,a=this;try{r=this.router.match(n,this.current)}catch(n){throw this.errorCbs.forEach((function(e){e(n)})),n}var o=this.current;this.confirmTransition(r,(function(){a.updateRoute(r),e&&e(r),a.ensureURL(),a.router.afterHooks.forEach((function(n){n&&n(r,o)})),a.ready||(a.ready=!0,a.readyCbs.forEach((function(n){n(r)})))}),(function(n){t&&t(n),n&&!a.ready&&(Cn(n,yn.redirected)&&o===h||(a.ready=!0,a.readyErrorCbs.forEach((function(e){e(n)}))))}))},Pn.prototype.confirmTransition=function(n,e,t){var r=this,a=this.current;this.pending=n;var o=function(n){!Cn(n)&&Dn(n)&&(r.errorCbs.length?r.errorCbs.forEach((function(e){e(n)})):console.error(n)),t&&t(n)},i=n.matched.length-1,s=a.matched.length-1;if(k(n,a)&&i===s&&n.matched[i]===a.matched[s])return this.ensureURL(),n.hash&&ln(this.router,a,n,!1),o(function(n,e){var t=wn(n,e,yn.duplicated,'Avoided redundant navigation to current location: "'+n.fullPath+'".');return t.name="NavigationDuplicated",t}(a,n));var l=function(n,e){var t,r=Math.max(n.length,e.length);for(t=0;t<r&&n[t]===e[t];t++);return{updated:e.slice(0,t),activated:e.slice(t),deactivated:n.slice(t)}}(this.current.matched,n.matched),c=l.updated,p=l.deactivated,u=l.activated,d=[].concat(function(n){return Fn(n,"beforeRouteLeave",Bn,!0)}(p),this.router.beforeHooks,function(n){return Fn(n,"beforeRouteUpdate",Bn)}(c),u.map((function(n){return n.beforeEnter})),Tn(u)),m=function(e,t){if(r.pending!==n)return o(xn(a,n));try{e(n,a,(function(e){!1===e?(r.ensureURL(!0),o(function(n,e){return wn(n,e,yn.aborted,'Navigation aborted from "'+n.fullPath+'" to "'+e.fullPath+'" via a navigation guard.')}(a,n))):Dn(e)?(r.ensureURL(!0),o(e)):"string"==typeof e||"object"==typeof e&&("string"==typeof e.path||"string"==typeof e.name)?(o(Sn(a,n)),"object"==typeof e&&e.replace?r.replace(e):r.push(e)):t(e)}))}catch(n){o(n)}};In(d,m,(function(){In(function(n){return Fn(n,"beforeRouteEnter",(function(n,e,t,r){return function(n,e,t){return function(r,a,o){return n(r,a,(function(n){"function"==typeof n&&(e.enteredCbs[t]||(e.enteredCbs[t]=[]),e.enteredCbs[t].push(n)),o(n)}))}}(n,t,r)}))}(u).concat(r.router.resolveHooks),m,(function(){if(r.pending!==n)return o(xn(a,n));r.pending=null,e(n),r.router.app&&r.router.app.$nextTick((function(){S(n)}))}))}))},Pn.prototype.updateRoute=function(n){this.current=n,this.cb&&this.cb(n)},Pn.prototype.setupListeners=function(){},Pn.prototype.teardown=function(){this.listeners.forEach((function(n){n()})),this.listeners=[],this.current=h,this.pending=null};var Mn=function(n){function e(e,t){n.call(this,e,t),this._startLocation=jn(this.base)}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router,t=e.options.scrollBehavior,r=vn&&t;r&&this.listeners.push(sn());var a=function(){var t=n.current,a=jn(n.base);n.current===h&&a===n._startLocation||n.transitionTo(a,(function(n){r&&ln(e,n,t,!0)}))};window.addEventListener("popstate",a),this.listeners.push((function(){window.removeEventListener("popstate",a)}))}},e.prototype.go=function(n){window.history.go(n)},e.prototype.push=function(n,e,t){var r=this,a=this.current;this.transitionTo(n,(function(n){bn(D(r.base+n.fullPath)),ln(r.router,n,a,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,a=this.current;this.transitionTo(n,(function(n){kn(D(r.base+n.fullPath)),ln(r.router,n,a,!1),e&&e(n)}),t)},e.prototype.ensureURL=function(n){if(jn(this.base)!==this.current.fullPath){var e=D(this.base+this.current.fullPath);n?bn(e):kn(e)}},e.prototype.getCurrentLocation=function(){return jn(this.base)},e}(Pn);function jn(n){var e=window.location.pathname,t=e.toLowerCase(),r=n.toLowerCase();return!n||t!==r&&0!==t.indexOf(D(r+"/"))||(e=e.slice(n.length)),(e||"/")+window.location.search+window.location.hash}var Ln=function(n){function e(e,t,r){n.call(this,e,t),r&&function(n){var e=jn(n);if(!/^\/#/.test(e))return window.location.replace(D(n+"/#"+e)),!0}(this.base)||Nn()}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router.options.scrollBehavior,t=vn&&e;t&&this.listeners.push(sn());var r=function(){var e=n.current;Nn()&&n.transitionTo($n(),(function(r){t&&ln(n.router,r,e,!0),vn||Hn(r.fullPath)}))},a=vn?"popstate":"hashchange";window.addEventListener(a,r),this.listeners.push((function(){window.removeEventListener(a,r)}))}},e.prototype.push=function(n,e,t){var r=this,a=this.current;this.transitionTo(n,(function(n){zn(n.fullPath),ln(r.router,n,a,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this,a=this.current;this.transitionTo(n,(function(n){Hn(n.fullPath),ln(r.router,n,a,!1),e&&e(n)}),t)},e.prototype.go=function(n){window.history.go(n)},e.prototype.ensureURL=function(n){var e=this.current.fullPath;$n()!==e&&(n?zn(e):Hn(e))},e.prototype.getCurrentLocation=function(){return $n()},e}(Pn);function Nn(){var n=$n();return"/"===n.charAt(0)||(Hn("/"+n),!1)}function $n(){var n=window.location.href,e=n.indexOf("#");return e<0?"":n=n.slice(e+1)}function Un(n){var e=window.location.href,t=e.indexOf("#");return(t>=0?e.slice(0,t):e)+"#"+n}function zn(n){vn?bn(Un(n)):window.location.hash=n}function Hn(n){vn?kn(Un(n)):window.location.replace(Un(n))}var qn=function(n){function e(e,t){n.call(this,e,t),this.stack=[],this.index=-1}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.push=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index+1).concat(n),r.index++,e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var r=this;this.transitionTo(n,(function(n){r.stack=r.stack.slice(0,r.index).concat(n),e&&e(n)}),t)},e.prototype.go=function(n){var e=this,t=this.index+n;if(!(t<0||t>=this.stack.length)){var r=this.stack[t];this.confirmTransition(r,(function(){var n=e.current;e.index=t,e.updateRoute(r),e.router.afterHooks.forEach((function(e){e&&e(r,n)}))}),(function(n){Cn(n,yn.duplicated)&&(e.index=t)}))}},e.prototype.getCurrentLocation=function(){var n=this.stack[this.stack.length-1];return n?n.fullPath:"/"},e.prototype.ensureURL=function(){},e}(Pn),Vn=function(n){void 0===n&&(n={}),this.app=null,this.apps=[],this.options=n,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=X(n.routes||[],this);var e=n.mode||"hash";switch(this.fallback="history"===e&&!vn&&!1!==n.fallback,this.fallback&&(e="hash"),J||(e="abstract"),this.mode=e,e){case"history":this.history=new Mn(this,n.base);break;case"hash":this.history=new Ln(this,n.base,this.fallback);break;case"abstract":this.history=new qn(this,n.base);break;default:0}},Kn={currentRoute:{configurable:!0}};Vn.prototype.match=function(n,e,t){return this.matcher.match(n,e,t)},Kn.currentRoute.get=function(){return this.history&&this.history.current},Vn.prototype.init=function(n){var e=this;if(this.apps.push(n),n.$once("hook:destroyed",(function(){var t=e.apps.indexOf(n);t>-1&&e.apps.splice(t,1),e.app===n&&(e.app=e.apps[0]||null),e.app||e.history.teardown()})),!this.app){this.app=n;var t=this.history;if(t instanceof Mn||t instanceof Ln){var r=function(n){t.setupListeners(),function(n){var r=t.current,a=e.options.scrollBehavior;vn&&a&&"fullPath"in n&&ln(e,n,r,!1)}(n)};t.transitionTo(t.getCurrentLocation(),r,r)}t.listen((function(n){e.apps.forEach((function(e){e._route=n}))}))}},Vn.prototype.beforeEach=function(n){return Gn(this.beforeHooks,n)},Vn.prototype.beforeResolve=function(n){return Gn(this.resolveHooks,n)},Vn.prototype.afterEach=function(n){return Gn(this.afterHooks,n)},Vn.prototype.onReady=function(n,e){this.history.onReady(n,e)},Vn.prototype.onError=function(n){this.history.onError(n)},Vn.prototype.push=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.push(n,e,t)}));this.history.push(n,e,t)},Vn.prototype.replace=function(n,e,t){var r=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){r.history.replace(n,e,t)}));this.history.replace(n,e,t)},Vn.prototype.go=function(n){this.history.go(n)},Vn.prototype.back=function(){this.go(-1)},Vn.prototype.forward=function(){this.go(1)},Vn.prototype.getMatchedComponents=function(n){var e=n?n.matched?n:this.resolve(n).route:this.currentRoute;return e?[].concat.apply([],e.matched.map((function(n){return Object.keys(n.components).map((function(e){return n.components[e]}))}))):[]},Vn.prototype.resolve=function(n,e,t){var r=q(n,e=e||this.history.current,t,this),a=this.match(r,e),o=a.redirectedFrom||a.fullPath;return{location:r,route:a,href:function(n,e,t){var r="hash"===t?"#"+e:e;return n?D(n+"/"+r):r}(this.history.base,o,this.mode),normalizedTo:r,resolved:a}},Vn.prototype.getRoutes=function(){return this.matcher.getRoutes()},Vn.prototype.addRoute=function(n,e){this.matcher.addRoute(n,e),this.history.current!==h&&this.history.transitionTo(this.history.getCurrentLocation())},Vn.prototype.addRoutes=function(n){this.matcher.addRoutes(n),this.history.current!==h&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(Vn.prototype,Kn);var Wn=Vn;function Gn(n,e){return n.push(e),function(){var t=n.indexOf(e);t>-1&&n.splice(t,1)}}Vn.install=function n(e){if(!n.installed||V!==e){n.installed=!0,V=e;var t=function(n){return void 0!==n},r=function(n,e){var r=n.$options._parentVnode;t(r)&&t(r=r.data)&&t(r=r.registerRouteInstance)&&r(n,e)};e.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),e.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,r(this,this)},destroyed:function(){r(this)}}),Object.defineProperty(e.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(e.prototype,"$route",{get:function(){return this._routerRoot._route}}),e.component("RouterView",x),e.component("RouterLink",W);var a=e.config.optionMergeStrategies;a.beforeRouteEnter=a.beforeRouteLeave=a.beforeRouteUpdate=a.created}},Vn.version="3.6.5",Vn.isNavigationFailure=Cn,Vn.NavigationFailureType=yn,Vn.START_LOCATION=h,J&&window.Vue&&window.Vue.use(Vn);t(67);var Jn=t(0),Yn=t(129),Qn=t.n(Yn),Xn=t(130),Zn=t.n(Xn),ne={created(){if(this.siteMeta=this.$site.headTags.filter(([n])=>"meta"===n).map(([n,e])=>e),this.$ssrContext){const e=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(n=e)?n.map(n=>{let e="<meta";return Object.keys(n).forEach(t=>{e+=` ${t}="${Zn()(n[t])}"`}),e+">"}).join("\n    "):"",this.$ssrContext.canonicalLink=te(this.$canonicalUrl)}var n},mounted(){this.currentMetaTags=[...document.querySelectorAll("meta")],this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta(){document.title=this.$title,document.documentElement.lang=this.$lang;const n=this.getMergedMetaTags();this.currentMetaTags=re(n,this.currentMetaTags)},getMergedMetaTags(){const n=this.$page.frontmatter.meta||[];return Qn()([{name:"description",content:this.$description}],n,this.siteMeta,ae)},updateCanonicalLink(){ee(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",te(this.$canonicalUrl))}},watch:{$page(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy(){re(null,this.currentMetaTags),ee()}};function ee(){const n=document.querySelector("link[rel='canonical']");n&&n.remove()}function te(n=""){return n?`<link href="${n}" rel="canonical" />`:""}function re(n,e){if(e&&[...e].filter(n=>n.parentNode===document.head).forEach(n=>document.head.removeChild(n)),n)return n.map(n=>{const e=document.createElement("meta");return Object.keys(n).forEach(t=>{e.setAttribute(t,n[t])}),document.head.appendChild(e),e})}function ae(n){for(const e of["name","property","itemprop"])if(n.hasOwnProperty(e))return n[e]+e;return JSON.stringify(n)}var oe=t(131),ie={mounted(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(oe)()((function(){this.setActiveHash()}),300),setActiveHash(){const n=[].slice.call(document.querySelectorAll(".sidebar-link")),e=[].slice.call(document.querySelectorAll(".header-anchor")).filter(e=>n.some(n=>n.hash===e.hash)),t=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),r=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),a=window.innerHeight+t;for(let n=0;n<e.length;n++){const o=e[n],i=e[n+1],s=0===n&&0===t||t>=o.parentElement.offsetTop+10&&(!i||t<i.parentElement.offsetTop-10),l=decodeURIComponent(this.$route.hash);if(s&&l!==decodeURIComponent(o.hash)){const t=o;if(a===r)for(let t=n+1;t<e.length;t++)if(l===decodeURIComponent(e[t].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(t.hash),()=>{this.$nextTick(()=>{this.$vuepress.$set("disableScrollBehavior",!1)})})}}}},beforeDestroy(){window.removeEventListener("scroll",this.onScroll)}},se=(t(272),Object.assign||function(n){for(var e=1;e<arguments.length;e++){var t=arguments[e];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&(n[r]=t[r])}return n}),le=function(n){return"IMG"===n.tagName},ce=function(n){return n&&1===n.nodeType},pe=function(n){return".svg"===(n.currentSrc||n.src).substr(-4).toLowerCase()},ue=function(n){try{return Array.isArray(n)?n.filter(le):function(n){return NodeList.prototype.isPrototypeOf(n)}(n)?[].slice.call(n).filter(le):ce(n)?[n].filter(le):"string"==typeof n?[].slice.call(document.querySelectorAll(n)).filter(le):[]}catch(n){throw new TypeError("The provided selector is invalid.\nExpects a CSS selector, a Node element, a NodeList or an array.\nSee: https://github.com/francoischalifour/medium-zoom")}},de=function(n){var e=document.createElement("div");return e.classList.add("medium-zoom-overlay"),e.style.background=n,e},me=function(n){var e=n.getBoundingClientRect(),t=e.top,r=e.left,a=e.width,o=e.height,i=n.cloneNode(),s=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,l=window.pageXOffset||document.documentElement.scrollLeft||document.body.scrollLeft||0;return i.removeAttribute("id"),i.style.position="absolute",i.style.top=t+s+"px",i.style.left=r+l+"px",i.style.width=a+"px",i.style.height=o+"px",i.style.transform="",i},ge=function(n,e){var t=se({bubbles:!1,cancelable:!1,detail:void 0},e);if("function"==typeof window.CustomEvent)return new CustomEvent(n,t);var r=document.createEvent("CustomEvent");return r.initCustomEvent(n,t.bubbles,t.cancelable,t.detail),r};!function(n,e){void 0===e&&(e={});var t=e.insertAt;if(n&&"undefined"!=typeof document){var r=document.head||document.getElementsByTagName("head")[0],a=document.createElement("style");a.type="text/css","top"===t&&r.firstChild?r.insertBefore(a,r.firstChild):r.appendChild(a),a.styleSheet?a.styleSheet.cssText=n:a.appendChild(document.createTextNode(n))}}(".medium-zoom-overlay{position:fixed;top:0;right:0;bottom:0;left:0;opacity:0;transition:opacity .3s;will-change:opacity}.medium-zoom--opened .medium-zoom-overlay{cursor:pointer;cursor:zoom-out;opacity:1}.medium-zoom-image{cursor:pointer;cursor:zoom-in;transition:transform .3s cubic-bezier(.2,0,.2,1)!important}.medium-zoom-image--hidden{visibility:hidden}.medium-zoom-image--opened{position:relative;cursor:pointer;cursor:zoom-out;will-change:transform}");var fe=function n(e){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},r=window.Promise||function(n){function e(){}n(e,e)},a=function(n){var e=n.target;e!==D?-1!==k.indexOf(e)&&f({target:e}):g()},o=function(){if(!S&&E.original){var n=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0;Math.abs(x-n)>w.scrollOffset&&setTimeout(g,150)}},i=function(n){var e=n.key||n.keyCode;"Escape"!==e&&"Esc"!==e&&27!==e||g()},s=function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},e=n;if(n.background&&(D.style.background=n.background),n.container&&n.container instanceof Object&&(e.container=se({},w.container,n.container)),n.template){var t=ce(n.template)?n.template:document.querySelector(n.template);e.template=t}return w=se({},w,e),k.forEach((function(n){n.dispatchEvent(ge("medium-zoom:update",{detail:{zoom:C}}))})),C},l=function(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};return n(se({},w,e))},c=function(){for(var n=arguments.length,e=Array(n),t=0;t<n;t++)e[t]=arguments[t];var r=e.reduce((function(n,e){return[].concat(n,ue(e))}),[]);return r.filter((function(n){return-1===k.indexOf(n)})).forEach((function(n){k.push(n),n.classList.add("medium-zoom-image")})),y.forEach((function(n){var e=n.type,t=n.listener,a=n.options;r.forEach((function(n){n.addEventListener(e,t,a)}))})),C},p=function(){for(var n=arguments.length,e=Array(n),t=0;t<n;t++)e[t]=arguments[t];E.zoomed&&g();var r=e.length>0?e.reduce((function(n,e){return[].concat(n,ue(e))}),[]):k;return r.forEach((function(n){n.classList.remove("medium-zoom-image"),n.dispatchEvent(ge("medium-zoom:detach",{detail:{zoom:C}}))})),k=k.filter((function(n){return-1===r.indexOf(n)})),C},u=function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};return k.forEach((function(r){r.addEventListener("medium-zoom:"+n,e,t)})),y.push({type:"medium-zoom:"+n,listener:e,options:t}),C},d=function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};return k.forEach((function(r){r.removeEventListener("medium-zoom:"+n,e,t)})),y=y.filter((function(t){return!(t.type==="medium-zoom:"+n&&t.listener.toString()===e.toString())})),C},m=function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},e=n.target,t=function(){var n={width:document.documentElement.clientWidth,height:document.documentElement.clientHeight,left:0,top:0,right:0,bottom:0},e=void 0,t=void 0;if(w.container)if(w.container instanceof Object)e=(n=se({},n,w.container)).width-n.left-n.right-2*w.margin,t=n.height-n.top-n.bottom-2*w.margin;else{var r=(ce(w.container)?w.container:document.querySelector(w.container)).getBoundingClientRect(),a=r.width,o=r.height,i=r.left,s=r.top;n=se({},n,{width:a,height:o,left:i,top:s})}e=e||n.width-2*w.margin,t=t||n.height-2*w.margin;var l=E.zoomedHd||E.original,c=pe(l)?e:l.naturalWidth||e,p=pe(l)?t:l.naturalHeight||t,u=l.getBoundingClientRect(),d=u.top,m=u.left,g=u.width,f=u.height,h=Math.min(Math.max(g,c),e)/g,v=Math.min(Math.max(f,p),t)/f,b=Math.min(h,v),k="scale("+b+") translate3d("+((e-g)/2-m+w.margin+n.left)/b+"px, "+((t-f)/2-d+w.margin+n.top)/b+"px, 0)";E.zoomed.style.transform=k,E.zoomedHd&&(E.zoomedHd.style.transform=k)};return new r((function(n){if(e&&-1===k.indexOf(e))n(C);else{if(E.zoomed)n(C);else{if(e)E.original=e;else{if(!(k.length>0))return void n(C);var r=k;E.original=r[0]}if(E.original.dispatchEvent(ge("medium-zoom:open",{detail:{zoom:C}})),x=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,S=!0,E.zoomed=me(E.original),document.body.appendChild(D),w.template){var a=ce(w.template)?w.template:document.querySelector(w.template);E.template=document.createElement("div"),E.template.appendChild(a.content.cloneNode(!0)),document.body.appendChild(E.template)}if(E.original.parentElement&&"PICTURE"===E.original.parentElement.tagName&&E.original.currentSrc&&(E.zoomed.src=E.original.currentSrc),document.body.appendChild(E.zoomed),window.requestAnimationFrame((function(){document.body.classList.add("medium-zoom--opened")})),E.original.classList.add("medium-zoom-image--hidden"),E.zoomed.classList.add("medium-zoom-image--opened"),E.zoomed.addEventListener("click",g),E.zoomed.addEventListener("transitionend",(function e(){S=!1,E.zoomed.removeEventListener("transitionend",e),E.original.dispatchEvent(ge("medium-zoom:opened",{detail:{zoom:C}})),n(C)})),E.original.getAttribute("data-zoom-src")){E.zoomedHd=E.zoomed.cloneNode(),E.zoomedHd.removeAttribute("srcset"),E.zoomedHd.removeAttribute("sizes"),E.zoomedHd.removeAttribute("loading"),E.zoomedHd.src=E.zoomed.getAttribute("data-zoom-src"),E.zoomedHd.onerror=function(){clearInterval(o),console.warn("Unable to reach the zoom image target "+E.zoomedHd.src),E.zoomedHd=null,t()};var o=setInterval((function(){E.zoomedHd.complete&&(clearInterval(o),E.zoomedHd.classList.add("medium-zoom-image--opened"),E.zoomedHd.addEventListener("click",g),document.body.appendChild(E.zoomedHd),t())}),10)}else if(E.original.hasAttribute("srcset")){E.zoomedHd=E.zoomed.cloneNode(),E.zoomedHd.removeAttribute("sizes"),E.zoomedHd.removeAttribute("loading");var i=E.zoomedHd.addEventListener("load",(function(){E.zoomedHd.removeEventListener("load",i),E.zoomedHd.classList.add("medium-zoom-image--opened"),E.zoomedHd.addEventListener("click",g),document.body.appendChild(E.zoomedHd),t()}))}else t()}}}))},g=function(){return new r((function(n){if(!S&&E.original){S=!0,document.body.classList.remove("medium-zoom--opened"),E.zoomed.style.transform="",E.zoomedHd&&(E.zoomedHd.style.transform=""),E.template&&(E.template.style.transition="opacity 150ms",E.template.style.opacity=0),E.original.dispatchEvent(ge("medium-zoom:close",{detail:{zoom:C}})),E.zoomed.addEventListener("transitionend",(function e(){E.original.classList.remove("medium-zoom-image--hidden"),document.body.removeChild(E.zoomed),E.zoomedHd&&document.body.removeChild(E.zoomedHd),document.body.removeChild(D),E.zoomed.classList.remove("medium-zoom-image--opened"),E.template&&document.body.removeChild(E.template),S=!1,E.zoomed.removeEventListener("transitionend",e),E.original.dispatchEvent(ge("medium-zoom:closed",{detail:{zoom:C}})),E.original=null,E.zoomed=null,E.zoomedHd=null,E.template=null,n(C)}))}else n(C)}))},f=function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},e=n.target;return E.original?g():m({target:e})},h=function(){return w},v=function(){return k},b=function(){return E.original},k=[],y=[],S=!1,x=0,w=t,E={original:null,zoomed:null,zoomedHd:null,template:null};"[object Object]"===Object.prototype.toString.call(e)?w=e:(e||"string"==typeof e)&&c(e),w=se({margin:0,background:"#fff",scrollOffset:40,container:null,template:null},w);var D=de(w.background);document.addEventListener("click",a),document.addEventListener("keyup",i),document.addEventListener("scroll",o),window.addEventListener("resize",g);var C={open:m,close:g,toggle:f,update:s,clone:l,attach:c,detach:p,on:u,off:d,getOptions:h,getImages:v,getZoomedImage:b};return C},he={data:()=>({zoom:null}),mounted(){this.updateZoom()},updated(){this.updateZoom()},methods:{updateZoom(){setTimeout(()=>{this.zoom&&this.zoom.detach(),this.zoom=fe(".theme-reco-content :not(a) > img",void 0)},1e3)}}},ve=t(41),be=t.n(ve),ke={mounted(){be.a.configure({showSpinner:!1}),this.$router.beforeEach((n,e,t)=>{n.path===e.path||r.b.component(n.name)||be.a.start(),t()}),this.$router.afterEach(()=>{be.a.done(),this.isSidebarOpen=!1})}},ye=t(132),Se=t.n(ye),xe={mounted(){Se.a.polyfill()}},we=t(133),Ee={noCopy:!1,noSelect:!1,disabled:!1,minLength:100,authorName:"https://gordonchanfz.github.io/"},De={props:{html:String,lang:String},created(){this.authorName="string"==typeof Ee.authorName?Ee.authorName:this.getI18nValue(Ee.authorName),this.text=this.getI18nValue(we),this.location=String(location).replace(/#.+$/,"")},methods:{getI18nValue(n){return this.lang in n?n[this.lang]:n["en-US"]}}},Ce=t(3),Ie=Object(Ce.a)(De,(function(){var n=this,e=n._self._c;return e("div",[e("p",[n._v(n._s(n.text.beforeAuthor)+n._s(n.authorName||n.text.author)+n._s(n.text.afterAuthor)),e("a",{attrs:{href:n.location}},[n._v(n._s(decodeURIComponent(n.location)))])]),n._v("\n\n"),e("div",{domProps:{innerHTML:n._s(n.html)}})])}),[],!1,null,null,null).exports,Te={data:()=>({isElement:!1}),created(){this.onCopy=n=>{const e=getSelection().getRangeAt(0);if(String(e).length<this.minLength)return;if(n.preventDefault(),this.noCopy)return;const t=document.createElement("div");t.appendChild(getSelection().getRangeAt(0).cloneContents());const a=this.$lang,o=new r.b({render:n=>n(Ie,{props:{html:t.innerHTML,lang:a}})}).$mount(),{innerHTML:i,innerText:s}=o.$el;n.clipboardData?(n.clipboardData.setData("text/html",i),n.clipboardData.setData("text/plain",s)):window.clipboardData&&window.clipboardData.setData("text",s)}},watch:{isElement(n){if(!n)return;let{copyright:e=!Ee.disabled}=this.$frontmatter;if(!e)return;"object"!=typeof e&&(e={});const t=e.noSelect||Ee.noSelect;this.minLength=e.minLength||Ee.minLength,this.noCopy=e.noCopy||Ee.noCopy,t?this.$el.style.userSelect="none":this.$el.addEventListener("copy",this.onCopy)}},updated(){this.isElement="#comment"!==this.$el.nodeName},beforeDestory(){this.$el.removeEventListener("copy",this.onCopy)}},Oe={props:{parent:Object,code:String,options:{align:String,color:String,backgroundTransition:Boolean,backgroundColor:String,successText:String,staticIcon:Boolean}},data:()=>({success:!1,originalBackground:null,originalTransition:null}),computed:{alignStyle(){let n={};return n[this.options.align]="7.5px",n},iconClass(){return this.options.staticIcon?"":"hover"}},mounted(){this.originalTransition=this.parent.style.transition,this.originalBackground=this.parent.style.background},beforeDestroy(){this.parent.style.transition=this.originalTransition,this.parent.style.background=this.originalBackground},methods:{hexToRgb(n){let e=/^#?([a-f\d]{2})([a-f\d]{2})([a-f\d]{2})$/i.exec(n);return e?{r:parseInt(e[1],16),g:parseInt(e[2],16),b:parseInt(e[3],16)}:null},copyToClipboard(n){if(navigator.clipboard)navigator.clipboard.writeText(this.code).then(()=>{this.setSuccessTransitions()},()=>{});else{let n=document.createElement("textarea");document.body.appendChild(n),n.value=this.code,n.select(),document.execCommand("Copy"),n.remove(),this.setSuccessTransitions()}},setSuccessTransitions(){if(clearTimeout(this.successTimeout),this.options.backgroundTransition){this.parent.style.transition="background 350ms";let n=this.hexToRgb(this.options.backgroundColor);this.parent.style.background=`rgba(${n.r}, ${n.g}, ${n.b}, 0.1)`}this.success=!0,this.successTimeout=setTimeout(()=>{this.options.backgroundTransition&&(this.parent.style.background=this.originalBackground,this.parent.style.transition=this.originalTransition),this.success=!1},500)}}},Ae=(t(273),Object(Ce.a)(Oe,(function(){var n=this,e=n._self._c;return e("div",{staticClass:"code-copy"},[e("svg",{class:n.iconClass,style:n.alignStyle,attrs:{xmlns:"http://www.w3.org/2000/svg",width:"24",height:"24",viewBox:"0 0 24 24"},on:{click:n.copyToClipboard}},[e("path",{attrs:{fill:"none",d:"M0 0h24v24H0z"}}),n._v(" "),e("path",{attrs:{fill:n.options.color,d:"M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm-1 4l6 6v10c0 1.1-.9 2-2 2H7.99C6.89 23 6 22.1 6 21l.01-14c0-1.1.89-2 1.99-2h7zm-1 7h5.5L14 6.5V12z"}})]),n._v(" "),e("span",{class:n.success?"success":"",style:n.alignStyle},[n._v("\n        "+n._s(n.options.successText)+"\n    ")])])}),[],!1,null,"49140617",null).exports),_e=(t(274),[ne,ie,he,ke,xe,Te,{updated(){this.update()},methods:{update(){setTimeout(()=>{document.querySelectorAll('div[class*="language-"] pre').forEach(n=>{if(n.classList.contains("code-copy-added"))return;let e=new(r.b.extend(Ae));e.options={align:"bottom",color:"#27b1ff",backgroundTransition:!0,backgroundColor:"#0075b8",successText:"Copied!",staticIcon:!1},e.code=n.innerText,e.parent=n,e.$mount(),n.classList.add("code-copy-added"),n.appendChild(e.$el)})},100)}}}]),Re={name:"GlobalLayout",computed:{layout(){const n=this.getLayout();return Object(Jn.i)("layout",n),r.b.component(n)}},methods:{getLayout(){if(this.$page.path){const n=this.$page.frontmatter.layout;return n&&(this.$vuepress.getLayoutAsyncComponent(n)||this.$vuepress.getVueComponent(n))?n:"Layout"}return"NotFound"}}},Pe=Object(Ce.a)(Re,(function(){return(0,this._self._c)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;Object(Jn.g)(Pe,"mixins",_e);const Fe=[{name:"v-aae13ec4",path:"/2019/01/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-aae13ec4").then(t)}},{path:"/2019/01/01/idea/",redirect:"/2019/01/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8/"},{path:"/2019/01/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8/index.html",redirect:"/2019/01/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8/"},{path:"/tool/IDEA/IDEA.html",redirect:"/2019/01/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8/"},{name:"v-af65573c",path:"/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-af65573c").then(t)}},{path:"/index.html",redirect:"/"},{name:"v-0ea853f1",path:"/2022/03/09/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-0ea853f1").then(t)}},{path:"/2022/03/09/emoji/",redirect:"/2022/03/09/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji/"},{path:"/2022/03/09/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji/index.html",redirect:"/2022/03/09/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji/"},{path:"/tool/emoji/emoji.html",redirect:"/2022/03/09/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji/"},{name:"v-0cbdd054",path:"/2022/03/12/%E5%88%A9%E7%94%A8markdown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84emoji/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-0cbdd054").then(t)}},{path:"/2022/03/12/markdownemoji/",redirect:"/2022/03/12/%E5%88%A9%E7%94%A8markdown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84emoji/"},{path:"/2022/03/12/%E5%88%A9%E7%94%A8markdown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84emoji/index.html",redirect:"/2022/03/12/%E5%88%A9%E7%94%A8markdown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84emoji/"},{path:"/tool/markdown/MarkDownEmoji.html",redirect:"/2022/03/12/%E5%88%A9%E7%94%A8markdown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84emoji/"},{name:"v-0bf5ebde",path:"/2022/03/09/git%E5%9C%A8-gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-0bf5ebde").then(t)}},{path:"/2022/03/09/git-gitignore/",redirect:"/2022/03/09/git%E5%9C%A8-gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/"},{path:"/2022/03/09/git%E5%9C%A8-gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/index.html",redirect:"/2022/03/09/git%E5%9C%A8-gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/"},{path:"/tool/git/git.gitignore.html",redirect:"/2022/03/09/git%E5%9C%A8-gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/"},{name:"v-46e67ace",path:"/1970/01/01/aboutme/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-46e67ace").then(t)}},{path:"/1970/01/01/aboutme/index.html",redirect:"/1970/01/01/aboutme/"},{path:"/aboutme.html",redirect:"/1970/01/01/aboutme/"},{name:"v-6048fa40",path:"/2022/03/26/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-6048fa40").then(t)}},{path:"/2022/03/26//",redirect:"/2022/03/26/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99/"},{path:"/2022/03/26/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99/index.html",redirect:"/2022/03/26/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99/"},{path:"/tool/resource/.html",redirect:"/2022/03/26/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99/"},{name:"v-5ed1fdaa",path:"/2022/03/09/github%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-5ed1fdaa").then(t)}},{path:"/2022/03/09/github/",redirect:"/2022/03/09/github%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/"},{path:"/2022/03/09/github%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/index.html",redirect:"/2022/03/09/github%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/"},{path:"/tool/git/GitHub.html",redirect:"/2022/03/09/github%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/"},{name:"v-7d8ca27f",path:"/2022/07/08/kafka/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-7d8ca27f").then(t)}},{path:"/2022/07/08/kafka/index.html",redirect:"/2022/07/08/kafka/"},{path:"//kafka.html",redirect:"/2022/07/08/kafka/"},{name:"v-dffff514",path:"/2022/03/09/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-dffff514").then(t)}},{path:"/2022/03/09//",redirect:"/2022/03/09/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/"},{path:"/2022/03/09/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/index.html",redirect:"/2022/03/09/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/"},{path:"//.html",redirect:"/2022/03/09/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/"},{name:"v-33a310e8",path:"/2023/06/10/docker/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-33a310e8").then(t)}},{path:"/2023/06/10/docker/index.html",redirect:"/2023/06/10/docker/"},{path:"//Docker.html",redirect:"/2023/06/10/docker/"},{name:"v-700a68a1",path:"/2022/10/08/rpc/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-700a68a1").then(t)}},{path:"/2022/10/08/rpc/index.html",redirect:"/2022/10/08/rpc/"},{path:"//RPC.html",redirect:"/2022/10/08/rpc/"},{name:"v-2c8e9208",path:"/1970/01/01/k8s/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-2c8e9208").then(t)}},{path:"/1970/01/01/k8s/index.html",redirect:"/1970/01/01/k8s/"},{path:"//k8s.html",redirect:"/1970/01/01/k8s/"},{name:"v-01787dc2",path:"/2019/09/08/geohash%E7%AE%97%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-01787dc2").then(t)}},{path:"/2019/09/08/geohash/",redirect:"/2019/09/08/geohash%E7%AE%97%E6%B3%95/"},{path:"/2019/09/08/geohash%E7%AE%97%E6%B3%95/index.html",redirect:"/2019/09/08/geohash%E7%AE%97%E6%B3%95/"},{path:"//geohash.html",redirect:"/2019/09/08/geohash%E7%AE%97%E6%B3%95/"},{name:"v-83d8af48",path:"/2023/06/10/fink-on-k8s/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-83d8af48").then(t)}},{path:"/2023/06/10/fink-on-k8s/index.html",redirect:"/2023/06/10/fink-on-k8s/"},{path:"//fink-on-k8s.html",redirect:"/2023/06/10/fink-on-k8s/"},{name:"v-c38fec2a",path:"/2023/06/10/helm/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-c38fec2a").then(t)}},{path:"/2023/06/10/helm/index.html",redirect:"/2023/06/10/helm/"},{path:"//helm.html",redirect:"/2023/06/10/helm/"},{name:"v-7e69d236",path:"/2022/05/10/sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-7e69d236").then(t)}},{path:"/2022/05/10/sqoop/",redirect:"/2022/05/10/sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{path:"/2022/05/10/sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/index.html",redirect:"/2022/05/10/sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{path:"//Sqoop.html",redirect:"/2022/05/10/sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{name:"v-cdeeea2e",path:"/2023/06/10/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-cdeeea2e").then(t)}},{path:"/2023/06/10/linux/",redirect:"/2023/06/10/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/"},{path:"/2023/06/10/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/index.html",redirect:"/2023/06/10/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/"},{path:"//linux.html",redirect:"/2023/06/10/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/"},{name:"v-10e8b782",path:"/2022/10/08/gitlab%E9%80%9A%E8%BF%87cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-10e8b782").then(t)}},{path:"/2022/10/08/gitlabcicd/",redirect:"/2022/10/08/gitlab%E9%80%9A%E8%BF%87cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2/"},{path:"/2022/10/08/gitlab%E9%80%9A%E8%BF%87cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2/index.html",redirect:"/2022/10/08/gitlab%E9%80%9A%E8%BF%87cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2/"},{path:"//gitlabCICD.html",redirect:"/2022/10/08/gitlab%E9%80%9A%E8%BF%87cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2/"},{name:"v-851571e2",path:"/2023/06/10/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-851571e2").then(t)}},{path:"/2023/06/10/hash/",redirect:"/2023/06/10/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/"},{path:"/2023/06/10/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/index.html",redirect:"/2023/06/10/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/"},{path:"//hash.html",redirect:"/2023/06/10/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/"},{name:"v-7350f07e",path:"/2023/02/08/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-7350f07e").then(t)}},{path:"/2023/02/08//",redirect:"/2023/02/08/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8/"},{path:"/2023/02/08/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8/index.html",redirect:"/2023/02/08/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8/"},{path:"//.html",redirect:"/2023/02/08/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8/"},{name:"v-22a43ff6",path:"/2023/06/10/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-22a43ff6").then(t)}},{path:"/2023/06/10//",redirect:"/2023/06/10/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/"},{path:"/2023/06/10/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/index.html",redirect:"/2023/06/10/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/"},{path:"//.html",redirect:"/2023/06/10/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/"},{name:"v-0ddfb2e2",path:"/2022/10/08/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-0ddfb2e2").then(t)}},{path:"/2022/10/08//",redirect:"/2022/10/08/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95/"},{path:"/2022/10/08/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95/index.html",redirect:"/2022/10/08/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95/"},{path:"//.html",redirect:"/2022/10/08/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95/"},{name:"v-41ccc7e2",path:"/2023/06/10/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-41ccc7e2").then(t)}},{path:"/2023/06/10//",redirect:"/2023/06/10/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F/"},{path:"/2023/06/10/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F/index.html",redirect:"/2023/06/10/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F/"},{path:"//.html",redirect:"/2023/06/10/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F/"},{name:"v-29f87cb0",path:"/2023/06/10/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-29f87cb0").then(t)}},{path:"/2023/06/10//",redirect:"/2023/06/10/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98/"},{path:"/2023/06/10/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98/index.html",redirect:"/2023/06/10/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98/"},{path:"//.html",redirect:"/2023/06/10/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98/"},{name:"v-2e09059c",path:"/2022/01/08/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-2e09059c").then(t)}},{path:"/2022/01/08//",redirect:"/2022/01/08/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/"},{path:"/2022/01/08/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/index.html",redirect:"/2022/01/08/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/"},{path:"//.html",redirect:"/2022/01/08/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/"},{name:"v-7ea9e72a",path:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-7ea9e72a").then(t)}},{path:"/1970/01/01//",redirect:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84/"},{path:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84/index.html",redirect:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84/"},{path:"//.html",redirect:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84/"},{name:"v-63f06f0b",path:"/1970/01/01/spring/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-63f06f0b").then(t)}},{path:"/1970/01/01/spring/index.html",redirect:"/1970/01/01/spring/"},{path:"//spring.html",redirect:"/1970/01/01/spring/"},{name:"v-61c5b94b",path:"/2022/10/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-61c5b94b").then(t)}},{path:"/2022/10/08//",redirect:"/2022/10/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86/"},{path:"/2022/10/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86/index.html",redirect:"/2022/10/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86/"},{path:"//.html",redirect:"/2022/10/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86/"},{name:"v-5692179e",path:"/2023/05/08/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-5692179e").then(t)}},{path:"/2023/05/08//",redirect:"/2023/05/08/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{path:"/2023/05/08/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0/index.html",redirect:"/2023/05/08/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{path:"//.html",redirect:"/2023/05/08/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{name:"v-227baaf0",path:"/2023/06/10/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-227baaf0").then(t)}},{path:"/2023/06/10/io/",redirect:"/2023/06/10/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/"},{path:"/2023/06/10/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/index.html",redirect:"/2023/06/10/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/"},{path:"//IO.html",redirect:"/2023/06/10/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/"},{name:"v-e7d27b94",path:"/1970/01/01/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-e7d27b94").then(t)}},{path:"/1970/01/01/git/",redirect:"/1970/01/01/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{path:"/1970/01/01/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/index.html",redirect:"/1970/01/01/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{path:"//git.html",redirect:"/1970/01/01/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{name:"v-7f30b557",path:"/1970/01/01/redis/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-7f30b557").then(t)}},{path:"/1970/01/01/redis/index.html",redirect:"/1970/01/01/redis/"},{path:"//redis.html",redirect:"/1970/01/01/redis/"},{name:"v-4f7f9be4",path:"/2023/06/10/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-4f7f9be4").then(t)}},{path:"/2023/06/10/k8s/",redirect:"/2023/06/10/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{path:"/2023/06/10/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/index.html",redirect:"/2023/06/10/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{path:"//k8s.html",redirect:"/2023/06/10/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{name:"v-24ffb3db",path:"/2022/08/08/hbase/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-24ffb3db").then(t)}},{path:"/2022/08/08/hbase/index.html",redirect:"/2022/08/08/hbase/"},{path:"//hbase.html",redirect:"/2022/08/08/hbase/"},{name:"v-e463dc58",path:"/2023/06/10/protobuf3%E8%AF%AD%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-e463dc58").then(t)}},{path:"/2023/06/10/protobuf3/",redirect:"/2023/06/10/protobuf3%E8%AF%AD%E6%B3%95/"},{path:"/2023/06/10/protobuf3%E8%AF%AD%E6%B3%95/index.html",redirect:"/2023/06/10/protobuf3%E8%AF%AD%E6%B3%95/"},{path:"//protobuf3.html",redirect:"/2023/06/10/protobuf3%E8%AF%AD%E6%B3%95/"},{name:"v-007b24d3",path:"/2019/10/08/elasticsearch/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-007b24d3").then(t)}},{path:"/2019/10/08/elasticsearch/index.html",redirect:"/2019/10/08/elasticsearch/"},{path:"//ElasticSearch.html",redirect:"/2019/10/08/elasticsearch/"},{name:"v-305ac8e0",path:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-305ac8e0").then(t)}},{path:"/1970/01/01//",redirect:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{path:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/index.html",redirect:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{path:"//.html",redirect:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{name:"v-4fd15c58",path:"/1970/01/01/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-4fd15c58").then(t)}},{path:"/1970/01/01/redis/",redirect:"/1970/01/01/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86/"},{path:"/1970/01/01/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86/index.html",redirect:"/1970/01/01/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86/"},{path:"//redis.html",redirect:"/1970/01/01/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86/"},{name:"v-36184f82",path:"/2023/04/08/apache-beam/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-36184f82").then(t)}},{path:"/2023/04/08/apache-beam/index.html",redirect:"/2023/04/08/apache-beam/"},{path:"//Apache-beam.html",redirect:"/2023/04/08/apache-beam/"},{name:"v-49f14a1b",path:"/2019/08/08/flink/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-49f14a1b").then(t)}},{path:"/2019/08/08/flink/index.html",redirect:"/2019/08/08/flink/"},{path:"//flink.html",redirect:"/2019/08/08/flink/"},{name:"v-9f20b3be",path:"/2019/09/08/spark/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Layout","v-9f20b3be").then(t)}},{path:"/2019/09/08/spark/index.html",redirect:"/2019/09/08/spark/"},{path:"//spark.html",redirect:"/2019/09/08/spark/"},{name:"v-b1564aac",path:"/tag/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tags","v-b1564aac").then(t)},meta:{pid:"tags",id:"tags"}},{path:"/tag/index.html",redirect:"/tag/"},{name:"v-ef9325c4",path:"/categories/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("FrontmatterKey","v-ef9325c4").then(t)},meta:{pid:"categories",id:"categories"}},{path:"/categories/index.html",redirect:"/categories/"},{name:"v-6319eb4e",path:"/timeline/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("TimeLines","v-6319eb4e").then(t)},meta:{pid:"timeline",id:"timeline"}},{path:"/timeline/index.html",redirect:"/timeline/"},{name:"v-3ae5b494",path:"/tag/markdown/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-3ae5b494").then(t)},meta:{pid:"tags",id:"markdown"}},{path:"/tag/markdown/index.html",redirect:"/tag/markdown/"},{name:"v-584666fc",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-584666fc").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-23a8b635",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-23a8b635").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-c481210e",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-c481210e").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-598aa1ae",path:"/tag/-/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-598aa1ae").then(t)},meta:{pid:"tags",id:"-"}},{path:"/tag/-/index.html",redirect:"/tag/-/"},{name:"v-6b29cdd0",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-6b29cdd0").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-7ed06156",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-7ed06156").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-1f026d94",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-1f026d94").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-036115ab",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-036115ab").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-6dd70f48",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-6dd70f48").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-5f2b2f45",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-5f2b2f45").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-32360c9a",path:"/tag/k8s/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-32360c9a").then(t)},meta:{pid:"tags",id:"k8s"}},{path:"/tag/k8s/index.html",redirect:"/tag/k8s/"},{name:"v-f57983ce",path:"/tag/Flink/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-f57983ce").then(t)},meta:{pid:"tags",id:"Flink"}},{path:"/tag/Flink/index.html",redirect:"/tag/Flink/"},{name:"v-1560bc14",path:"/tag/beam/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-1560bc14").then(t)},meta:{pid:"tags",id:"beam"}},{path:"/tag/beam/index.html",redirect:"/tag/beam/"},{name:"v-53d8f7ba",path:"/tag/k8s/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-53d8f7ba").then(t)},meta:{pid:"tags",id:"k8s"}},{path:"/tag/k8s/index.html",redirect:"/tag/k8s/"},{name:"v-b6d24872",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-b6d24872").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-70e3a6de",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-70e3a6de").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-70980f1d",path:"/tag/linux/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-70980f1d").then(t)},meta:{pid:"tags",id:"linux"}},{path:"/tag/linux/index.html",redirect:"/tag/linux/"},{name:"v-18e4cd44",path:"/tag/CICD/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-18e4cd44").then(t)},meta:{pid:"tags",id:"CICD"}},{path:"/tag/CICD/index.html",redirect:"/tag/CICD/"},{name:"v-9f88a2c0",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-9f88a2c0").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-6ac82e63",path:"/tag/pipeline/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-6ac82e63").then(t)},meta:{pid:"tags",id:"pipeline"}},{path:"/tag/pipeline/index.html",redirect:"/tag/pipeline/"},{name:"v-60190584",path:"/tag/redis/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-60190584").then(t)},meta:{pid:"tags",id:"redis"}},{path:"/tag/redis/index.html",redirect:"/tag/redis/"},{name:"v-7e42d028",path:"/tag/hash/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-7e42d028").then(t)},meta:{pid:"tags",id:"hash"}},{path:"/tag/hash/index.html",redirect:"/tag/hash/"},{name:"v-257df835",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-257df835").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-38e037b7",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-38e037b7").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-2ad4af63",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-2ad4af63").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-08174efe",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-08174efe").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-35aa74ba",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-35aa74ba").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-1aaea625",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-1aaea625").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-2370020b",path:"/tag/bisdiff/bispatch/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-2370020b").then(t)},meta:{pid:"tags",id:"bisdiff/bispatch"}},{path:"/tag/bisdiff/bispatch/index.html",redirect:"/tag/bisdiff/bispatch/"},{name:"v-84b6fb3e",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-84b6fb3e").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-d5dea5f8",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-d5dea5f8").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-812754e8",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-812754e8").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-2a06d83f",path:"/tag/id/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-2a06d83f").then(t)},meta:{pid:"tags",id:"id"}},{path:"/tag/id/index.html",redirect:"/tag/id/"},{name:"v-5b26dd53",path:"/tag/kafka/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-5b26dd53").then(t)},meta:{pid:"tags",id:"kafka"}},{path:"/tag/kafka/index.html",redirect:"/tag/kafka/"},{name:"v-0dd50f9b",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-0dd50f9b").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-19522da4",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-19522da4").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-76ecf1d8",path:"/tag/spring/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-76ecf1d8").then(t)},meta:{pid:"tags",id:"spring"}},{path:"/tag/spring/index.html",redirect:"/tag/spring/"},{name:"v-7ed250eb",path:"/tag/IO/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-7ed250eb").then(t)},meta:{pid:"tags",id:"IO"}},{path:"/tag/IO/index.html",redirect:"/tag/IO/"},{name:"v-6367c596",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-6367c596").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-771b1f98",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-771b1f98").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-1fdf3cdf",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-1fdf3cdf").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-402a1f0e",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-402a1f0e").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-134dedee",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-134dedee").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-099ad802",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-099ad802").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-dd643e04",path:"/tag//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-dd643e04").then(t)},meta:{pid:"tags",id:""}},{path:"/tag//index.html",redirect:"/tag//"},{name:"v-093485d8",path:"/tag/sparksql/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Tag","v-093485d8").then(t)},meta:{pid:"tags",id:"sparksql"}},{path:"/tag/sparksql/index.html",redirect:"/tag/sparksql/"},{name:"v-638ddf39",path:"/categories/tool/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-638ddf39").then(t)},meta:{pid:"categories",id:"tool"}},{path:"/categories/tool/index.html",redirect:"/categories/tool/"},{name:"v-b571f312",path:"/categories//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-b571f312").then(t)},meta:{pid:"categories",id:""}},{path:"/categories//index.html",redirect:"/categories//"},{name:"v-2bf76980",path:"/categories//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-2bf76980").then(t)},meta:{pid:"categories",id:""}},{path:"/categories//index.html",redirect:"/categories//"},{name:"v-ab31fcde",path:"/categories//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-ab31fcde").then(t)},meta:{pid:"categories",id:""}},{path:"/categories//index.html",redirect:"/categories//"},{name:"v-c77fd4f6",path:"/categories//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-c77fd4f6").then(t)},meta:{pid:"categories",id:""}},{path:"/categories//index.html",redirect:"/categories//"},{name:"v-6f5d94e8",path:"/categories//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-6f5d94e8").then(t)},meta:{pid:"categories",id:""}},{path:"/categories//index.html",redirect:"/categories//"},{name:"v-7fc479ec",path:"/categories//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-7fc479ec").then(t)},meta:{pid:"categories",id:""}},{path:"/categories//index.html",redirect:"/categories//"},{name:"v-f37f30be",path:"/categories//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-f37f30be").then(t)},meta:{pid:"categories",id:""}},{path:"/categories//index.html",redirect:"/categories//"},{name:"v-5db6d2ed",path:"/categories//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-5db6d2ed").then(t)},meta:{pid:"categories",id:""}},{path:"/categories//index.html",redirect:"/categories//"},{name:"v-60c96f6a",path:"/categories/CICD/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-60c96f6a").then(t)},meta:{pid:"categories",id:"CICD"}},{path:"/categories/CICD/index.html",redirect:"/categories/CICD/"},{name:"v-2dd62492",path:"/categories//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-2dd62492").then(t)},meta:{pid:"categories",id:""}},{path:"/categories//index.html",redirect:"/categories//"},{name:"v-03f2e000",path:"/categories/nosql/",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-03f2e000").then(t)},meta:{pid:"categories",id:"nosql"}},{path:"/categories/nosql/index.html",redirect:"/categories/nosql/"},{name:"v-c1e9d86a",path:"/categories//",component:Pe,beforeEnter:(n,e,t)=>{Object(Jn.a)("Category","v-c1e9d86a").then(t)},meta:{pid:"categories",id:""}},{path:"/categories//index.html",redirect:"/categories//"},{path:"*",component:Pe}],Be={title:"Gordon",description:"",base:"./",headTags:[["link",{rel:"icon",href:"./favicon.ico"}],["meta",{name:"viewport",content:"width=device-width,initial-scale=1,user-scalable=no"}],["meta",{name:"robots",content:"all"}],["meta",{name:"apple-mobile-web-app-capable",content:"yes"}],["script",{},'\n            var _hmt = _hmt || [];\n            (function() {\n              var hm = document.createElement("script");\n              hm.src = "https://hm.baidu.com/hm.js?55943ae09e5901d7a9f5705133737eec";\n              var s = document.getElementsByTagName("script")[0];\n              s.parentNode.insertBefore(hm, s);\n            })();\n           '],["script",{src:"https://hm.baidu.com/hm.js?xxxxxxxxxxx"}]],pages:[{title:"IDEA",frontmatter:{title:"IDEA",date:"2019-01-01T00:00:00.000Z",publish:!1},regularPath:"/tool/IDEA/IDEA%E7%9A%84%E4%BD%BF%E7%94%A8.html",relativePath:"tool/IDEA/IDEA.md",key:"v-aae13ec4",path:"/2019/01/01/idea%E7%9A%84%E4%BD%BF%E7%94%A8/",headers:[{level:2,title:"Java",slug:"java"},{level:2,title:"Swagger",slug:"swagger"}],lastUpdated:"2023-6-23 6:36:19 F10: PM",lastUpdatedTimestamp:1687516579e3,content:"  \n Java \n \n /**\n * description:\n $params$\n * @return $return$\n *\n * @author $USER$\n * Date: $DATE$ $TIME$\n */ \n \n 1 2 3 4 5 6 7 8 \n params \n groovyScript ( \"\n def  result = '' ;  \n def  params = \\\" $ { _1 } \\\" . replaceAll ( '[\\\\\\\\[|\\\\\\\\]|\\\\\\\\s]' ,   '' ) . split ( ',' ) . toList ( ) ;  \n for ( i  =   0 ;  i  <  params . size ( ) ;  i ++ )   { \n    result += '* @param '   +  params [ i ]   +   ( ( i  <  params . size ( )   -   1 )   ?   '\\\\n '   :   '' ) \n } ; \n return  result\" ,   methodParameters ( ) ) methodParameters ( ) ) \n \n 1 2 3 4 5 6 7 javajavaJavaDoc \n Swagger \n \n @ApiOperation ( value  =   \"\" ,  notes  =   \"\" ) \n$swaggerParam$\n \n 1 2 \n swaggerParam: \n groovyScript ( \"\ndef result = '' ; \ndef params = \\\"$ { _1 } \\\" . replaceAll ( ' [ \\\\\\\\ [ | \\\\\\\\ ] | \\\\\\\\s ] ', ' ' ) . split ( ',' ) . toList ( ) ;  \ndef paramStr  = '' ; \n for ( i  =   0 ;  i  <  params . size ( ) ;  i ++ ) { \n\tparamStr += ' @ApiImplicitParam ( name  =  \\\"'  +  params [ i ]   +  '\\\"\\ , value  =  \\\"'  +  params [ i ]   +   '\\\"\\)'   +   ( ( i  <  params . size ( )   -   1 )   ?  '\\ , \\\\n         ' : ' \\\\n' ) \n } ; \nresult  += ''  + ( ( params . size ( ) > 1 )    ?  ' @ApiImplicitParams ( { \\\\n        ' + paramStr + '})' :  '' + paramStr + ' ')+' ' ; \n return  result ; \n\" , methodParameters ( ) ) ; \n \n 1 2 3 4 5 6 7 8 9 10 swfswagger \n"},{title:"Home",frontmatter:{home:!0,heroText:null,tagline:"",bgImageStyle:{height:"450px"},isShowTitleInHome:!1,permalink:"/"},regularPath:"/",relativePath:"README.md",key:"v-af65573c",path:"/",lastUpdated:"2023-6-23 6:36:19 F10: PM",lastUpdatedTimestamp:1687516579e3,content:" \n"},{title:"Emoji",frontmatter:{title:"Emoji",date:"2022-03-09T00:00:00.000Z",publish:!1},regularPath:"/tool/emoji/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji.html",relativePath:"tool/emoji/emoji.md",key:"v-0ea853f1",path:"/2022/03/09/%E6%88%91%E7%9A%84%E5%B8%B8%E7%94%A8emoji/",headers:[{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:2,title:"",slug:""}],lastUpdated:"2023-6-23 6:36:19 F10: PM",lastUpdatedTimestamp:1687516579e3,content:"  \n \n Emoji \n https://gist.github.com/rxaviers/7360908 \n \n Emoji \n  \n \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n  \n \n       \n       \n \n"},{title:"MarkDownEmoji",frontmatter:{title:"MarkDownEmoji",date:"2022-03-12T00:00:00.000Z",categories:["tool"],tags:["markdown"],subSidebar:!1},regularPath:"/tool/markdown/%E5%88%A9%E7%94%A8MarkDown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84Emoji.html",relativePath:"tool/markdown/MarkDownEmoji.md",key:"v-0cbdd054",path:"/2022/03/12/%E5%88%A9%E7%94%A8markdown%E6%94%B6%E9%9B%86%E5%B8%B8%E7%94%A8%E7%9A%84emoji/",headers:[{level:2,title:"",slug:""},{level:2,title:"emoji",slug:"emoji"},{level:2,title:"emoji",slug:"emoji"},{level:2,title:"emoji",slug:"emoji"},{level:3,title:"",slug:"--"},{level:3,title:"",slug:"--"},{level:3,title:"flex",slug:"-flex-"},{level:2,title:"",slug:""}],excerpt:'<div class="custom-block tip"><p class="title"></p><ol>\n<li>emoji</li>\n<li> markdown Emoji</li>\n</ol>\n</div>',lastUpdated:"2023-6-23 6:36:19 F10: PM",lastUpdatedTimestamp:1687516579e3,content:' \n emoji \n  markdown Emoji \n  \n \n Emoji  emoji 13.01814emoji \n \n  MarkDown emojigithubemoji \n \n emoji  Unicode   \n  MarkDown \n   MarkDown  emoji  \n  \n \n \n  MarkDown  emoji \n \n \n  emoji / \n \n \n   Emoji   emoji  \n \n \n \n \n \n  MarkDown  emoji \n emoji \n \n Emoji \n https://gist.github.com/rxaviers/7360908 \n emoji \n Emoji \n emoji \n   Emoji  emoji \n  \n  MarkDown  \n |        |        |        | \n |   ----   |   ----   |   ----   | \n |        |        |        | \n |        |        |        | \n |        |        |        | \n \n 1 2 3 4 5      \n  HTML  MarkDown  \n < table   border = " 1px "   align = " center "   bordercolor = " black "   width = " 80% "   height = " 100px " > \n     < tr   align = " center " > \n         < td >  </ td > \n         < td >  </ td > \n         < td >  </ td > \n     </ tr > \n     < tr   align = " center " > \n         < td >  </ td > \n         < td >  < br > </ td > \n         < td >  </ td > \n     </ tr > \n </ table > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 CSS \n   <td></td>   <tr></tr>    \n  \n  \n < div   style = " display : grid ; grid-template-columns :   repeat ( auto-fill ,  12.5% ) ; font-size : 30px ; justify-items : center ; align-items : center ; line-height : normal ; text-align : center " > \n     < span >  < br >  </ span > \n     < span >  < br >  </ span > \n     < span >  < br >  </ span > \n     < span >  < br >  </ span > \n     < span >  < br >  </ span > \n     < span >  < br >  </ span > \n     < span >  < br >  </ span > \n     < span >  < br >  </ span > \n     < span >  < br >  </ span > \n     < span >  < br >  </ span > \n     < span >  < br >  </ span > \n </ div > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 HTML \n grid-template-columns :   repeat ( auto-fill ,  10% ) ; \n \n 1 10%  10 \n 5 20%  \n  \n \n 10 \n flex \n < div   style = " display :  flex ; flex-direction :  row ; flex-wrap :  wrap ; justify-content :  flex-start ; text-align :  center ; font-size :  30px ; line-height : normal ; " > \n     < span   style = " flex-basis :  10% ; " >  < br >  </ span > \n     < span   style = " flex-basis :  10% ; " >  < br >  </ span > \n     < span   style = " flex-basis :  10% ; " >  < br >  </ span > \n     < span   style = " flex-basis :  10% ; " >  < br >  </ span > \n     < span   style = " flex-basis :  10% ; " >  < br >  </ span > \n     < span   style = " flex-basis :  10% ; " >  < br >  </ span > \n     < span   style = " flex-basis :  10% ; " >  < br >  </ span > \n     < span   style = " flex-basis :  10% ; " >  < br >  </ span > \n     < span   style = " flex-basis :  10% ; " >  < br >  </ span > \n     < span   style = " flex-basis :  10% ; " >  < br >  </ span > \n     < span   style = " flex-basis :  10% ; " >  < br >  </ span > \n </ div > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 #   \n \n !!! Markdown \n \n'},{title:"git",frontmatter:{title:"git",date:"2022-03-09T00:00:00.000Z",publish:!1},regularPath:"/tool/git/git%E5%9C%A8.gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8.html",relativePath:"tool/git/git.gitignore.md",key:"v-0bf5ebde",path:"/2022/03/09/git%E5%9C%A8-gitignore%E6%B7%BB%E5%8A%A0%E5%BF%BD%E7%95%A5%E6%96%87%E4%BB%B6%E4%B8%8D%E8%B5%B7%E4%BD%9C%E7%94%A8/",headers:[{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:'  \n git.gitignorecommitIDEA \n  \n .gitignoregitgitpushtrack \n  \n  \n  \n .gitignore \n  \n  \n  \n  \n  \n \n Git Bash \n . \n \n git rm  - r  -- cached  . \n \n 1 \n . \n \n git add  . \n \n 1 \n  .gitignore  \n \n git commit  - m  ".gitignore is now working" \n \n 1  \n  \n 2 \n git rm  - r  -- cached  * / target / \n \n 1 '},{frontmatter:{},regularPath:"/aboutme.html",relativePath:"aboutme.md",key:"v-46e67ace",path:"/1970/01/01/aboutme/",headers:[{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:2,title:"",slug:""}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:"  Gordon  \n  \n  \n  \n   \n  \n  \n \n  \n  \n  \n :123456789@qq.com \n"},{title:"",frontmatter:{title:"",date:"2022-03-26T00:00:00.000Z",categories:["",""],tags:["",""]},regularPath:"/tool/resource/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99.html",relativePath:"tool/resource/.md",key:"v-6048fa40",path:"/2022/03/26/%E5%AE%89%E5%88%A9%E4%B8%80%E4%BA%9B%E7%94%B5%E5%AD%90%E5%9B%BE%E4%B9%A6%E4%B8%8B%E8%BD%BD%E7%BD%91%E7%AB%99/",headers:[{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"",slug:""},{level:3,title:"Z-Library",slug:"-z-library"},{level:3,title:"",slug:"-"},{level:3,title:"",slug:""},{level:2,title:"",slug:""}],excerpt:'<div class="custom-block tip"><p class="title"></p><p><strong></strong>  <strong></strong></p>\n<p></p>\n<p><a href="https://zh.3lib.net/" target="_blank" rel="noopener noreferrer">https://zh.3lib.net/<OutboundLink/></a></p>\n<p><a href="https://m.ibookben.com/" target="_blank" rel="noopener noreferrer">https://m.ibookben.com/<OutboundLink/></a></p>\n</div>',lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:"       \n  \n  https://zh.3lib.net/ \n  https://m.ibookben.com/ \n  \n  \n IT \n  MarkDown  \n  \n  \n \n  \n   \n  \n   2   readreading \n \n \n    \n \n   \n  \n : \n \n  \n  \n  \n  \n \n  \n  \n  Z-Library \n  \n \n   \n \n  https://zh.3lib.net/ \n  http://libgen.st/ \n \n   \n \n   9,945,695    84,837,000 \n   \n  510 \n \n  \n \n  \n       \n  \n   \n  \n  \n  \n  \n  \n \n \n  \n \n  \n   \n \n  \n  \n  \n  \n \n   \n \n \n \n  \n 15 \n \n \n    \n \n  \n \n     \n   \n   \n  https://m.ibookben.com/ \n  \n \n  UI TXT  \n  TXT \n  \n  \n"},{title:"githup",frontmatter:{title:"githup",date:"2022-03-09T00:00:00.000Z",publish:!1},regularPath:"/tool/git/GitHub%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5.html",relativePath:"tool/git/GitHub.md",key:"v-5ed1fdaa",path:"/2022/03/09/github%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5/",headers:[{level:2,title:"",slug:""},{level:2,title:"GitHub",slug:"github"},{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"",slug:""}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:"  \n CSDN leetcode   GitHubGitHub \n README.mdGitHub \n \n CSDN- \n \n GitHub \n \n GitHub \n GitHub \n    README.md  \n \nREADME \n  \n GitHub  \n < ! -- \n * * shiwei - Ren / shiwei - Ren * *  is a  _special_  repository because its ` README . md`  ( this  file )  appears on your  GitHub   profile . \n\nHere  are some ideas  to   get  you started : \n\n -    I m currently working on  . . . \n -    I m currently learning  . . . \n -    I m looking  to   collaborate  on  . . . \n -    I m looking  for  help  with   . . . \n -    Ask  me about  . . . \n -    How   to   reach  me :   . . . \n -    Pronouns :   . . . \n -    Fun  fact :   . . . \n -- > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #   \n https://github.com/duktig666/duktig666 \n  \n \n https://github.com/kautukkundan/Awesome-Profile-README-templates \n https://github.com/rahuldkjain/github-profile-readme-generator \n  \n  \n GitHub README.md  \n README.md  Simple Icons   Shields IO   \n \n \n Build :  \n \n \n Code Coverage :  \n \n \n Analysis :  \n \n \n Chat :  \n \n \n Dependencies :  \n \n \n Size :  \n \n \n Downloads :  \n \n \n Funding :  \n \n \n Issue Tracking :  \n \n \n License :  \n \n \n Rating :  \n \n \n Social :  \n \n \n Version :  \n \n \n Platform & Version Support :  \n \n \n Monitoring :  \n \n \n Activity :  \n \n \n Other :  \n  \n GitHub GitHub Readme Stats  \n  \n \n GitHub \n \n GitHub \n  \n  GitHub ActionGitHub   GitHub Actions   \n ........ \n  \n ! [  ] ( https : / / komarev . com / ghpvc / ? username = your - github - username & color = green ) \n \n 1 \n  \n  Github \n  \n  https://github.com/abhisheknaiidu/awesome-github-profile-readme \n  \n \n GitHub \n https://sanii.cn/article/302 \n https://ld246.com/article/1602996971277 \n GitHub  GitHub Action  \n https://juejin.cn/post/6844904035103801352 \n https://juejin.cn/post/6844904114711691272 \n \n"},{title:"kafka",frontmatter:{title:"kafka",date:"2022-07-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["",""],tags:["","-"]},regularPath:"/%E4%B8%AD%E9%97%B4%E4%BB%B6/kafka.html",relativePath:"/kafka.md",key:"v-7d8ca27f",path:"/2022/07/08/kafka/",headers:[{level:2,title:"kafka",slug:"kafka"},{level:2,title:"kafka",slug:"kafka"},{level:2,title:"kafka",slug:"kafka"},{level:3,title:"kafkashell",slug:"kafkashell"},{level:3,title:"kafka",slug:"kafka"},{level:3,title:"kafka java API",slug:"kafka-java-api"},{level:2,title:"kafka",slug:"kafka"},{level:3,title:"Kafka",slug:"kafka"},{level:3,title:"Kafka",slug:"kafka"},{level:3,title:"Broker",slug:"broker"},{level:3,title:"Kafka",slug:"kafka"},{level:2,title:"Kafka",slug:"kafka"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"",slug:""},{level:2,title:"Rebalance",slug:"rebalance"},{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:3,title:"kafka",slug:"kafka"},{level:2,title:"kafka",slug:"kafka"},{level:2,title:"kafka",slug:"kafka"},{level:2,title:"KafkaQuotas",slug:"kafka-quotas"},{level:3,title:"producer",slug:"producer"},{level:3,title:"consumer",slug:"consumer"},{level:3,title:"KafkaQuota",slug:"kafkaquota"},{level:2,title:"Kafka",slug:"kafka"},{level:3,title:"",slug:""},{level:2,title:"Kafka topic",slug:"kafka-topic"},{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"_, HBase",slug:"--hbase"},{level:3,title:"_Phoenix",slug:"-phoenix"},{level:3,title:"_hive",slug:"-hive"}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:'  \n  \n 1 \n2 \n \n \n12345 \n \n \n123:4:56 \n3 \n3.1  \n3.2  \n \n \n \n  \n \n \n \n activeMQ:  apache, , ,  \n \n \n \n \n RabbitMQ:  java \n \n \n \n \n RocketMQ:  , , , ,  java, c, \n \n \n \n \n kafka :     , kafka \n \n \n \n  \n \tjava, : JMS(java massage server) \n \n : \n \n  ,  \n :  .... \n \n \n : \n \n  ,  \n :    \n kafka \n \t\t: http://www.kafka.apache.org \n \t    kafka, Scala , kafkaJMS, ,  kafka, , kakfazookeeper \n kafka, \n \n kafka: \n \n \n \n :   ,  \n \n \n \n \n : kafka \n \n \n \n \n :  ,  \n \n \n \n \n :   10w  , , ,  \n kafka \n : \n \n \n \n  server.properties  \n \n \n \n \n  \n \n \n \n \n , server.properties id   \n \n \n \n  \n 1)   zookeeper: zookeeper \n\n 2)   kafka:   \n    cd   /export/server/kafka_2.12-2.4.1/bin \n   \n     : \n    \t\t./kafka-server-start.sh   ../config/server.properties \n     : \n    \t\tnohup   ./kafka-server-start.sh ../config/server.properties 2>&1 & \n   \t\t\n     :   , , ,  \n   \n 3)    kafka:   \n\n  :    JPS   \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n  :   zookeeper \n \n 1 \n \n kafka: \n \n  :   Linux: \n    mkdir   -p /export/onekey \n    cd   /export/onekey \n\n  :    kafka,  \n    rz    \n  :    \n    chmod    755 * \n\n  :    \n\n\n  :   , zookeeper \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 #  kafka \n kafkashell \n \n \n \n  topic \n \n \n \n cd  /export/server/kafka_2.12-2.4.1/bin\n: \n./kafka-topics.sh   --create   --zookeeper  node1:2181,node2:2181,node3:2181   --topic  test01  --partitions   3  --replication-factor  2 \n\n:\n --partitions   :  topic\n--replication-factor:    (  ) \n \n 1 2 3 4 5 6 7 \n \n \n  shell  \n \n \n \n ./kafka-console-consumer.sh  --bootstrap-server node1:9092,node2:9092,node3:9092  --topic   test01\n \n 1 \n \n \n shell \n \n \n \n ./kafka-console-producer.sh --broker-list node1:9092,node2:9092,node3:9092  --topic  test01\n \n 1 \n \n \n topic: \n \n \n \n ./kafka-topics.sh   --list   --zookeeper  node1:2181,node2:2181,node3:2181\n \n 1 \n \n \n topic \n \n \n \n ./kafka-topics.sh   --describe   --zookeeper  node1:2181,node2:2181,node3:2181  --topic  test01\n \n 1 \n \n \n \n topic  \n \n \n \n ./kafka-topics.sh   --alter   --zookeeper  node1:2181,node2:2181,node3:2181  --topic  test01   --partitions   4 \n\n:\n    \n \n 1 2 3 4 \n \n \n topic \n \n \n \n ./kafka-topics.sh   --delete   --zookeeper  node1:2181,node2:2181,node3:2181  --topic  test01\n\n:\n  kafka topic  (  ) , : topic, kafka\n  \n  , :  \n  \t\t: /export/server/kafka_2.12-2.4.1/data\n  \n   :\n     server.properties:  \n          delete.topic.enable = true\n \n 1 2 3 4 5 6 7 8 9 10 11 #  kafka \n \n \n \n topic \n \n \n \n ./kafka-topics.sh   --create   --zookeeper  node1:2181,node2:2181,node3:2181   --topic  test02  --partitions   3  --replication-factor  1 \n \n 1 \n \n \n   \n \n \n \n ./kafka-producer-perf-test.sh  --topic  test02 --num-records  5000000   --throughput   -1  --record-size  1000  --producer-props  bootstrap.servers = node1.itcast.cn:9092,node2.itcast.cn:9092,node3.itcast.cn:9092  acks = 1 \n \n 1 \n \n : \n      37M    3.8w\n\n:\n   CPU: i7  7700k\n   :  6.1GB\n   : \n \n 1 2 3 4 5 6 7 \n \n \n : \n \n \n \n ./kafka-consumer-perf-test.sh  --topic  test02  --broker-list node1:9092,node2:9092,node3:9092 --fetch-size  1048576    --messages   5000000 \n \n 1 \n \n : \n 1) :     ack 0\n2) :  broker,   , \n \n 1 2 #  kafka java API \n : \n \n \n \n  pom \n \n \n \n < repositories > \x3c!----\x3e \n         < repository > \n             < id > aliyun </ id > \n             < url > http://maven.aliyun.com/nexus/content/groups/public/ </ url > \n             < releases > < enabled > true </ enabled > </ releases > \n             < snapshots > \n                 < enabled > false </ enabled > \n                 < updatePolicy > never </ updatePolicy > \n             </ snapshots > \n         </ repository > \n     </ repositories > \n\n     < dependencies > \n\n         < dependency > \n             < groupId > org.apache.kafka </ groupId > \n             < artifactId > kafka-clients </ artifactId > \n             < version > 2.4.1 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.commons </ groupId > \n             < artifactId > commons-io </ artifactId > \n             < version > 1.3.2 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-log4j12 </ artifactId > \n             < version > 1.7.6 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > log4j </ groupId > \n             < artifactId > log4j </ artifactId > \n             < version > 1.2.16 </ version > \n         </ dependency > \n\n     </ dependencies > \n\n     < build > \n         < plugins > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.1 </ version > \n                 < configuration > \n                     < target > 1.8 </ target > \n                     < source > 1.8 </ source > \n                 </ configuration > \n             </ plugin > \n         </ plugins > \n     </ build > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 \n \n \n : com.itheima.kafka \n \n \n \n 1.1 java API  kafka \n \n \n : \n http://kafka.apache.org/24/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html \n \n \n : \n \n \n // kafka \n public   class   KafkaProducerTest   { \n\n     public   static   void   main ( String [ ]  args )   { \n\n         //1. kafka:   KafkaProducer \n\n         //1.1:  \n         Properties  props  =   new   Properties ( ) ; \n        \n        props . put ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        props . put ( "acks" ,   "all" ) ;   // ack,  \n         //  key  vlaue  \n        props . put ( "key.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n        props . put ( "value.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n\n         Producer < String ,   String >  producer  =   new   KafkaProducer < > ( props ) ; \n\n         //2.  \n         for   ( int  i  =   0 ;  i  <   100 ;  i ++ )   { \n             //2.1:   \n             ProducerRecord < String ,   String >  producerRecord  =   new   ProducerRecord < > ( "test01" ,   Integer . toString ( i ) ) ; \n\n             //2.2:  \n            producer . send ( producerRecord ) ; \n         } \n         //3.  \n        producer . close ( ) ; \n\n     } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 1.2 javaAPI kafka \n \n \n : \n http://kafka.apache.org/24/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html \n \n \n  \n \n \n // kafka \n public   class   KafkaConsumerTest   { \n\n     public   static   void   main ( String [ ]  args )   { \n         //1. kafka:  KafkaConsumer \n\n         //1.1:  \n         Properties  props  =   new   Properties ( ) ; \n        \n        props . setProperty ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        props . setProperty ( "group.id" ,   "test" ) ;   //  \n        props . setProperty ( "enable.auto.commit" ,   "true" ) ;   //   \n        props . setProperty ( "auto.commit.interval.ms" ,   "1000" ) ;   //  \n         //  \n        props . setProperty ( "key.deserializer" ,   "org.apache.kafka.common.serialization.StringDeserializer" ) ; \n        props . setProperty ( "value.deserializer" ,   "org.apache.kafka.common.serialization.StringDeserializer" ) ; \n\n         KafkaConsumer < String ,   String >  consumer  =   new   KafkaConsumer < > ( props ) ; \n\n         //2. topic \n        consumer . subscribe ( Arrays . asList ( "test01" ) ) ; \n\n         //3.  \n         while   ( true )   { \n             //3.1: ,  \n             ConsumerRecords < String ,   String >  records  =  consumer . poll ( Duration . ofMillis ( 100 ) ) ; \n\n             //3.2:  \n             for   ( ConsumerRecord < String ,   String >  record  :  records ) { \n                 System . out . println ( ":" + record . value ( )   +   ":" + record . offset ( ) ) ; \n             } \n\n         } \n\n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #  kafka \n \n kafka   cluster: kafka \n broker :    kakfa \n producer :    \n consumer :    \n topic   :   () ,  \n shard :    \n replicas :    \n zookeeper :     \n \n 1 2 3 4 5 6 7 8 #  Kafka \n Topic---PartitionSegmentlog+index \n Topic  partition . \n  partition  log  log  producer  Producer log  offset  offset \n \n Kafka   partition  segment segment .index.log \n topic +testtopictest-0, test-1, test-2 \n  .index.log  message  Timemindexsegment(TTL) \n Kafka \n  \n producer  push  broker  append patition ****kafka \n Partition \n topic  topicPartition Logs() \n \n \n Partition  Partition log offset  \n 1 \n 1PartitiontopicPartition \n 2 Partition  \n 2 \n 1patition \n 2patitionkeykeyhashtopicpatitionpartition() \n 3patitionkeypatitiontopicpartitionpartitionroundrobin() \n Replication \n partitionreplication server.properties  default.replication.factor=Nreplicationbroker  patition producerpatitionreplicationpartitionreplication replicationleaderproducerconsumerleader replicationfollowerleader  \n  \n \n kafkatoolstate \n \n 1producerzookeeper "/brokers/.../state"partitionleader \n 2producerleader \n 3leaderlog \n 4followersleader pulllogleaderACK \n 5leader ISRreplicationACK  HWhigh watermark commit offsetproducerACK \n  \n ACK \n  producer  topictopic  partition  \n producer  producer  ackacknowledgement  \n producer  ack \n     ack follwersack ? \n (1)  followerleaderleaderackleaderfollowerleader. \n (2)  followersfollowers \n \n Kafka  \n 1. n  2n+1 (nn+1) n+1 Kafka  \n 2.** Kafka  \n 2ISR \n  leader  follower  \n  follower leader  leader  \n  ack \n Leader  in-sync replica set (ISR) leader  follower  \n   ISR  follower leader  follower  ack  follower \n    leader        follower     ISR        replica.lag.time.max.ms(10s)  Leader  ISR  leader \n 3ack  \n  \n  ISR  follower  \n  Kafka   \n  \n acks  \n acks 0producer  broker  ackbroker  broker  \n 1producer  broker  ackpartition  leader  ack  follower leader   \n \n -1allproducer  broker  ackpartition  leader  follower  ack  follower broker  ack  leader  \n  \n  \n LEO offset (offset) \n HW offsetISR  LEO \n HW \n HWleaderoffset19leader16-19follower16-19offsetHWHW=12Offset \n \n 1follower  \n follower  ISR follower follower  HW log  HW  HW  leader  follower  LEO  Partition  HW  follower  leader  ISR  \n 2leader  \n leader  ISR  leader follower  log  HW  leader \n  \n  \n Exactly-Once \n ACK-1ProducerServerAt LeasetOnceACK0AtMostOnce0.11kafkaProducerServerServerAtLeastOnce \n AtLeastOnce+=ExactlyOnce \n Producerennable.idompotencetrueKafkaProducerPIDPartitionSequenceNumberBroker<PID, Partition,SequenceNumber>Broker \n Broker \n  \n topicpatition server.properties num.partitions=3patitionpatition \n  \n kafka \n 1log.retention.hours=168 \n 2log.retention.bytes=1073741824 \n KafkaO(1) Kafka  \n Zk \n \n producerzkzk \n Kafka 0.9 consumer  offset  Zookeeper  0.9 consumer  offset  Kafka  topic  topic __consumer_offsets \n Kafka \n  \n \n l consumerRangeAssignor \n l  consumeroffsetZKoffset \n l   leader \n l  offset \n API \n kafkaconsumer APIConsumer APIConsumer API \n API \n 1API \n API  \n offsetzookeeper \n . \n zookeeperoffset 1zookeeperoffset \n  grouptopic  groupoffsettopicoffset \n 2API \n offset \n zk \n API \n 1 API  \n offset \n  \n zookeeperoffsetzkoffset \n 2API \n offsetleader  \n  \n \n consumer grouptopicgroupgrouppartition group  \n group \n  \n consumerpullbroker \n pushbrokerconsumerpullconsumer \n Kafkapullbrokerconsumerconsumer \n pullkafka \n  \n  consumer group  consumer topic  partition \n  partition  partition  consumer  \n Kafka  RoundRobin Range \n RoundRobin \n \n Range \n \n  \n  \n Kafka \n Kafka   \n   \n  \n   \n ( IO ) \nselector ->  ->  \n Reactor1 \n \n (1)  ServerSocketChannel  Selector  OP_ACCEPT ServerSocketChannel \n2 Selector  OP_ACCEPT  Acceptor  OP_ACCEPT .\n3 Acceptor  socket  SocketChannel SocketChannel  Selector  I/O OP_WRITER,OP_READ  socket \n4 socket  Selector  OP_READ read handler Selector  OP_WRITER writer handler\n\nreactor\n \n 1 2 3 4 5 6 Reactor2 \n \n Accept  ExecutorService  Accept ExecutorService Read handler  OP_READ  socket Read handler  MessageQueue Handler Poll  MessageQueue Handler Poll  handler Pool  OP_WRITER \nMessageQueue 500 write ThreadPool    I/O Selector  Selector \n \n 1 2 Reactor3 \n \n Accepetor Processor SocketChannel  SocketChannel  OP_READ Processor  Request  RequestChannel  RequestQueue  8  ReponseQueue  Processor  ReponseQueue  OP_WRITER \n \n 1 Kafka \n   \n  \n \n OS Cache \n  \n \n  \n \n ConcurrentSkipListMap,keybaseOffSetvaluelogSegment \n  \n \n \n \n Kafka \n kafka \n   \n \n \n \n  \nKafka  kafka 4k .logindex \n log.segment.bytes Kafka  log  log  1G log.index.interval.bytes  4kbkafka  4kb .log index   \n \n Kafka \n  \n1.kafka \n2.kafkaos cache \n3.os cache \n4.os cache  kafka \n5.kafka  socket cache \n6.socket cache \n \n KafkaZero-CopyJava java.nio.channels.FileChannel  transferTo()  \n  \n \n  \n \n kafka \n 104 * 10 = 40 1 + 10 = 11 10 10kafka \n  \n \n \n : \n \n kafka :   \n   1)  (2.4) --\x3e \n   2)  hash \n   3)    \n   4)  \n       \n \n 1 2 3 4 5 6 \n : \n \n 1 )   \n     :   DefaultPartitioner \n\n 2 )   ,  \n     // , DefaultPartitioner, ,  \n     public   ProducerRecord ( String  topic ,   V  value )   { \n          this ( topic ,   null ,   null ,   null ,  value ,   null ) ; \n     } \n     // ,  DefaultPartitionerhash \n     public   ProducerRecord ( String  topic ,   K  key ,   V  value )   { \n          this ( topic ,   null ,   null ,  key ,  value ,   null ) ; \n      } \n     // , . ,  DefaultPartitioner \n      public   ProducerRecord ( String  topic ,   Integer  partition ,   K  key ,   V  value )   { \n           this ( topic ,  partition ,   null ,  key ,  value ,   null ) ; \n       } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \n : \n \n  :      DefaultPartitioner \n      1)      Partitioner \n      2)   :  partition( .....) \n              1 :   topic \n              2 :   key \n              3 :   key \n              4 :    value \n              5 :   valye \n              6 :    (topic) \n     3)   partition, () \n                    0 \n     4)   : properties \n               partitioner.class :    \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 topicKafka \n \n \n --2.4 \n \n \n  \n \n \n key \n \n \n  \n  \n \n l     \n l  keynull \n  \n  \n key \n \n keykeykey \n  \n Kafkakey \n  \n  \n \n  \n \n public   class   KeyWithRandomPartitioner   implements   Partitioner   { \n\n     private   Random  r ; \n\n     @Override \n     public   void   configure ( Map < String ,   ? >  configs )   { \n        r  =   new   Random ( ) ; \n     } \n\n     @Override \n     public   int   partition ( String  topic ,   Object  key ,   byte [ ]  keyBytes ,   Object  value ,   byte [ ]  valueBytes ,   Cluster  cluster )   { \n         // cluster.partitionCountForTopic topic \n         return  r . nextInt ( 1000 )   %  cluster . partitionCountForTopic ( topic ) ; \n     } \n\n     @Override \n     public   void   close ( )   { \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \n Kafka \n \n props . put ( ProducerConfig . PARTITIONER_CLASS_CONFIG , KeyWithRandomPartitioner . class . getName ( ) ) ; \n \n 1 #  Rebalance \n Rebalance \n KafkaRebalanceKafkaConsumer groupconsumertopic \n Rebalance \n \n \n consumerconsumerconsumer \n \n \n topic \n \n \n  \n \n topic \n Rebalance \n l Rebalanceconsumer groupconsumer Kafka \n l Rebalanceconsumer groupRebalanceRebalance \n  \n Range \n **RangeKafka** \n RangleTopic \n  \n partition.assignment.strategyorg.apache.kafka.clients.consumer.RangeAssignor \n  \n n =  /  \n m =  %  \n mn+1 \n n \n RoundRobin \n RoundRobinAssignortopicpartitiontopichashcode \n  \n partition.assignment.strategyorg.apache.kafka.clients.consumer.RoundRobinAssignor \n Stricky \n Kafka 0.11.x \n \n \n  \n \n \n rebalance \n \n \n rebalanceStrikyRoundRobin \n \n consumer2rebalanceRange \n  consumer0consumer1 \n \n Strikyrebalanceconsumer2consumer0consumer1consumer0consumer1rebalanceconsumer0consumer1 \n  \n BrokerBroker \n producerACKs \n produceracks,acks \n     0: ack0, , kafka, \n    \n    1: ack1, (leader)ack, \n    \n  -1(all): ack-1, (follwer),  \n \n  \n Properties  props  =   new   Properties ( ) ; \nprops . put ( "bootstrap.servers" ,   "node1.itcast.cn:9092" ) ; \nprops . put ( "acks" ,   "all" ) ; \nprops . put ( "key.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \nprops . put ( "value.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n\n \n 1 2 3 4 5 6 l  Kafka****leaderfollower \n l  leaderfollowerleader \n l  follower****consumerleader \n leaderKafkaleaderleaderKafkafollowerARISROSR \n l  ARAssigned Replicas \n l leader leader  ISRIn-Sync Replicas \n l follower leader  OSROut-of-Sync Replias \n l AR = ISR + OSR \n l followerleaderAR = ISROSR \n ISR \n \n Kafka EagleTopicpartitionISR \n \n \n \n id0brokerbrokertopicISR \n kafka \n  \n \n \n ack: \n \n  :    ack \n        \n         0 :   ack0, , kafka,  \n        \n         1 :   ack1, (leader)ack,  \n        \n       -1(all) :   ack-1, (follwer),   \n\n\n  :   \n         -1   > 1 > 0 \n  : \n         0    >  1 > -1 \n\n ,   ?  \n        ack\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n ack? \n \n  :  javaAPIproperties\n    props . put ( "acks" ,   "all" ) ; \n \n 1 2 \n  \n \n 1)   , brokerACK, brokerack, ? \n  : \n      ,   (),, ack? \n      ,   , , ,  ,  \n         , ,  \n\n 2)   , brokerack, , , , , ?  \n  : \n      ,   , , , broker, broker() \n       : \n          ,   , ,  \n          ,   ,  \n\n 3)   , broker brokerack, , ? \n  : \n     ,   , () \n      :   , ,  \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 : \n 1) :    \n2) :  120000  (2)\n\t\tdelivery.timeout.ms: 120000\n3) :  2147483647  3\n      retries: 2147483647\n4) : 33554432 (32M)\n\t  buffer.memory: 33554432\n5) : (0)  (16384(16kb))\n\tbatch.size: 16384\n\tlinger.ms : 0\n\n?  javaAPIproperties\n \n 1 2 3 4 5 6 7 8 9 10 11 12 \n  \n \n // kafka --  \n public   class   KafkaProducerTest_sync   { \n\n     public   static   void   main ( String [ ]  args )   { \n\n         //1. kafka:   KafkaProducer \n\n         //1.1:  \n         Properties  props  =   new   Properties ( ) ; \n        props . put ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        props . put ( "acks" ,   "all" ) ;   // ack,  \n\n         //  key  vlaue  \n        props . put ( "key.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n        props . put ( "value.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n\n         Producer < String ,   String >  producer  =   new   KafkaProducer < > ( props ) ; \n\n         //2.  \n         for   ( int  i  =   0 ;  i  <   100 ;  i ++ )   { \n             //2.1:   \n             ProducerRecord < String ,   String >  producerRecord  =   new   ProducerRecord < > ( "test01" ,   Integer . toString ( i ) ) ; \n\n             //2.2:  \n             try   { \n                producer . send ( producerRecord ) . get ( ) ; \n             }   catch   ( Exception  e )   { \n                 // , , ,  \n                 //  .... \n             } \n         } \n         //3.  \n        producer . close ( ) ; \n     } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 \n  \n \n // kafka  --  \n public   class   KafkaProducerTest_asyn   { \n\n     public   static   void   main ( String [ ]  args )   { \n\n         //1. kafka:   KafkaProducer \n\n         //1.1:  \n         Properties  props  =   new   Properties ( ) ; \n        props . put ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        props . put ( "acks" ,   "all" ) ;   // ack,  \n\n         //  key  vlaue  \n        props . put ( "key.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n        props . put ( "value.serializer" ,   "org.apache.kafka.common.serialization.StringSerializer" ) ; \n\n         Producer < String ,   String >  producer  =   new   KafkaProducer < > ( props ) ; \n\n         //2.  \n         for   ( int  i  =   0 ;  i  <   100 ;  i ++ )   { \n             //2.1:   \n             ProducerRecord < String ,   String >  producerRecord  =   new   ProducerRecord < > ( "test01" ,   Integer . toString ( i ) ) ; \n\n             //2.2:  \n\n             //producer.send(producerRecord); // , ,  \n\n            producer . send ( producerRecord ,   new   Callback ( )   { \n                 // , , , exceptionnull \n                 @Override \n                 public   void   onCompletion ( RecordMetadata  metadata ,   Exception  exception )   { \n\n                     if ( exception != null ) { \n                         // null  ,  \n\n\n\n                     } \n                 } \n             } ) ; \n         } \n         //3.  \n        producer . close ( ) ; \n     } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 broker \n  \n \n \n \n ,  ,  \n \n \n \n \n ack -1 \n \n \n \n  \n \n \n  \n \n  : \n       1)   , broker, topic \n       2)   broker, topic \n       3)   ,,  (),  0 , ,  \n        4)   , , , broker \n\n\n        ,   , , , ,  \n \n 1 2 3 4 5 6 7 8 \n : \n \n  : \n     (0.8.x)    broker,  __consumer_offsets topics \n\n     0.8.x,   zookeeper \n    \n    \n __consumer_offset   : \n     50   1 \n \n 1 2 3 4 5 6 7 8 \n  \n \n // kafka--  \n public   class   KafkaConsumerTest_Manual   { \n\n     public   static   void   main ( String [ ]  args )   { \n         //1. kafka:  KafkaConsumer \n\n         //1.1:  \n         Properties  props  =   new   Properties ( ) ; \n        props . setProperty ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        props . setProperty ( "group.id" ,   "test" ) ;   //  \n        props . setProperty ( "enable.auto.commit" ,   "false" ) ;   //   \n         //props.setProperty("auto.commit.interval.ms", "1000"); //  \n         //  \n        props . setProperty ( "key.deserializer" ,   "org.apache.kafka.common.serialization.StringDeserializer" ) ; \n        props . setProperty ( "value.deserializer" ,   "org.apache.kafka.common.serialization.StringDeserializer" ) ; \n\n         KafkaConsumer < String ,   String >  consumer  =   new   KafkaConsumer < > ( props ) ; \n\n         //2. topic \n        consumer . subscribe ( Arrays . asList ( "test01" ) ) ; \n\n         //3.  \n         while   ( true )   { \n             //3.1: ,  \n             ConsumerRecords < String ,   String >  records  =  consumer . poll ( Duration . ofMillis ( 100 ) ) ; \n\n             //3.2:  \n             for   ( ConsumerRecord < String ,   String >  record  :  records ) { \n                 System . out . println ( ":" + record . value ( )   +   ":" + record . offset ( ) ) ; \n                \n                 //,  \n                consumer . commitSync ( ) ; //  \n                 //consumer.commitAsync();// \n             } \n\n         } \n\n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #  kafka \n 4.1 kafka \n \n \n segment: server.properties \n \n \n 4.2 kafka \n \n  : \n 1)   segment \n 2)   index, 368776log \n 3)   log,index,  \n \n 1 2 3 4   \n \n \n 1 #  kafka \n \n : \n 1)   : ? \n       :   topic \n       :   topic \n\n 2)   : ? \n      \n \n 1 2 3 4 5 6 #  KafkaQuotas \n /brokerIO Quotas KafkaProducerConsumerproduce&fetch \n producer \n client id producerTPS1MB/s 1048576/s \n  bin/kafka-configs.sh  --zookeeper  node1.itcast.cn:2181   --alter  --add-config  \'producer_byte_rate=1048576\'  --entity-type  clients --entity-default  \n \n 1  \n bin/kafka-producer-perf-test.sh  --topic   test  --num-records   500000   --throughput   -1  --record-size  1000  --producer-props  bootstrap.servers = node1.itcast.cn:9092,node2.itcast.cn:9092,node3.itcast.cn:9092   acks = 1  \n \n 1  \n 50000 records sent, 1108.156028 records/sec (1.06 MB/sec) \n consumer \n consumerproducer \n topicconsumertopic1MB/s1048576/s \n bin/kafka-configs.sh  --zookeeper   node1.itcast.cn:2181  --alter  --add-config  \'consumer_byte_rate=1048576\'  --entity-type clients --entity-default  \n \n 1  \n bin/kafka-consumer-perf-test.sh --broker-list  node1.itcast.cn:9092,node2.itcast.cn:9092,node3.itcast.cn:9092  --topic   test   --fetch-size  1048576   --messages   500000     \n \n 1  \n MB.sec1.0743 \n KafkaQuota \n KafkaQuota \n bin/kafka-configs.sh  --zookeeper   node1.itcast.cn:2181  --alter  --delete-config  \'producer_byte_rate\'  --entity-type  clients --entity-default  \n\n bin/kafka-configs.sh  --zookeeper   node1.itcast.cn:2181  --alter  --delete-config  \'consumer_byte_rate\'  --entity-type  clients --entity-default  \n \n 1 2 3 #  Kafka \n KafkaKafkaKafkaKafka \n l  Log Deletion   \n l  Log Compactionkeykeyvalue \n Kafkabrokertopic \n \n \n \n  \n  \n  \n \n \n \n \n log.cleaner.enable \n true \n  \n \n \n log.cleanup.policy \n delete \n  \n \n \n log.cleanup.policy \n compaction \n  \n \n \n log.cleanup.policy \n delete,compact \n  \n  \n segment \n  \n Kafka**brokerlog.retention.check.interval.ms300,0005**3 \n \\1.  \n \\2.  \n \\3.  \n  \n Kafka \n l  log.retention.hours \n l  log.retention.minutes \n l  log.retention.ms \n  log.retention.ms > log.retention.minutes > log.retention.hoursbroker \n log.retention.hours=168 \n 1687 \n : \n \\1.  \n \\2. .deleted \n 3.   Kafka.deletedfile.delete.delay.ms600001 \n  \n  broker  log.retention.bytes  -1 \n : \n log.retention.bytes  \n  \n segment logStartOffset \n Kafka topic \n  \n \n2. \n kafka \ntopic \n1topic \n kafka-topics.sh  --create   \\ \n --zookeeper  Linux001:2181    \\ \n --partitions   1     \\ \n--replication-factor  2   \\ \n --topic   test  \n \n 1 2 3 4 5 2topicproducerTp \n kafka-producer-pref-test.sh  \\ \n--producer-props  bootstrap.servers = ${bootstrap.servers}   \\ \n --producer.config   ${KAFKA_HOME} /config/producer.properties  \\ \n --topic   test   \\ \n--record-size  900000   \\    (  ) \n--num-records  10000000   \\ \n --throughput   -1  \n\t--producer-prop "acks=1" \n\t --throughput  -1\nTp = 80MB/s  \n \n 1 2 3 4 5 6 7 8 9 10 3topicconsumerTc \n kafka-consumer-pref-test.sh  \\ \n--broker-list  ${bootstrap.servers}   \\ \n --consumer.config = ${KAFKA_HOME} /config/consumer.properties  \\ \n --topic   test   \\ \n --messages   10000000   \\ \n--reporting-interval  1000   \\ \n--show-detailed-stats\n Tc = 90MB/s\n \n 1 2 3 4 5 6 7 8 4topic \nTnum=T/min(Tp,Tc) \nnum \n 100MB/snum=100/min(80,90)=2 \n Flink3flinkflinkKafkatopic3 \ntopic6912...flinkn:1 \ntopic \n kafka-topics.sh  --alter   \\ \n  --zookeeper   ${zookeeper.url}    \\ \n  --partitions   6 \t \\ \n  --topic  t001 \t\n \n 1 2 3 4 kafka \ntopic \nbatch.size\\longer.ms\\acks,--producer-props \n \n kafka \ntopic \n--fetch-size--threads \n \n kafka \n n=2*(m/s*/100m/s)+1\n\n \n 1 2 #   \n  \n \n :      \n : \n 1)   ,    \n\n 2)    \n 3)        \n 4)       \n \n 1 2 3 4 5 #   \n 2.1  \n  : \n \n 2.2  \n \n \n \n Linux, jar \n \n \n \n mkdir   -p  /export/data/momo_init\n cd  /export/data/momo_init\n \n 1 2 \n \n \n      /export/data/momo_init \n \n \n \n rz \n \n 1 \n \n \n \n  \n \n \n \n mkdir   -p  /export/data/momo_data\n \n 1 \n \n \n jar,  \n \n \n \n :\n     java   -jar   jar      \n \n 1 2 \n  /export/data/momo_data \n \n  MOMO_DATA.dat, ,   \n jar:   MOMO_DATA.dat \n  \n  \n 4.1: apache flume \n \t  apache flume ,cloudera, java,  apache, apache \n \t: http://flume.apache.org \n     :  0.9x,  flume OG   1.0  flume ng, flume NG \n     flume: \n \n \n \n source  :    flume,  \n \n \n \n \n channel :   , sourcechannel sink, flume (  , ) \n \n \n \n \n sink  : ()   flume,  \n \n \n \n : ,  \n flume:  , flume \n flume: flume  agent , , flumeevent \n \n 4.2 apache flume \n \t,  \n 4.3 apache flume \n 4.3.1  \n \n :\n   \n   flume,\n   , \n \n 1 2 3 4 4.3.2  \n \n \n \n  \n \n \n \n cd   /export/server/apache-flume-1.9.0-bin/conf \n vim    01_netcatSource_loggerSink.conf \n\n  : \n1)  \n a1.sources   =   r1 \n a1.channels   =   c1 \n a1.sinks   =   k1 \n2)  \n2.1)  source \n a1.sources.r1.type   =   netcat \n a1.sources.r1.bind   =   node1 \n a1.sources.r1.port   =   44444 \n2.2)  channel \n a1.channels.c1.type   =   memory \n a1.channels.c1.capacity   =   1000 \n a1.channels.c1.transactionCapacity   =   100 \n\n #2.3)  sink \n a1.sinks.k1.type   =   logger \n\n #3)  \n a1.sources.r1.channels   =   c1 \n a1.sinks.k1.channel   =   c1 \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \n \n \n flume  \n \n \n \n  : \n     cd   /export/server/apache-flume-1.9.0-bin \n    \n     bin/flume-ng   agent  -c conf -f conf/01_netcatSource_loggerSink.conf -n a1 -Dflume.root.logger=INFO,console \n    \n  : \n    -c   :    flume \n    -f   :     \n    -n   :     agent \n    -Dflume.root.logger = INFO,console :   \n    \n \n 1 2 3 4 5 6 7 8 9 10 11 \n \n \n \n 44444, , flume \n \n \n \n  : \n    telnet     \n   \n  : \n    telnet    node1 44444 \n\n ,   : \n   -bash :   telnet:  \n\n ,    netcat,  \n yum   -y install telnet \n \n 1 2 3 4 5 6 7 8 9 10 11 :  \n \n flume: \n \n 4.4 flume \n : \n         \n : \n        , , ,  Kafka,  \n : \n 1)    \n      source :      \n          :   Taildir Source \n          : \n             a1.sources.r1.type   =   TAILDIR \npositionFile   \n             a1.sources.r1.positionFile   =   /export/data/flume/taildir_position.json \n             a1.sources.r1.filegroups   =   f1 f2 \n             a1.sources.r1.filegroups.f1   =   /export/data/momo_data/MOMO_DATA.dat \n             a1.sources.r1.filegroups.f2   =   /export/data/momo_data/.*log.* \n      \n       channel :     \n           :   Memory channel \n           : \n              a1.channels.c1.type   =   memory \n \t\t\t a1.channels.c1.capacity   =   1000 \n \t\t\t a1.channels.c1.transactionCapacity   =   100 \n      sink :    kafkasink \n          :   kafka sink \n          : \n             a1.sinks.k1.type   =   org.apache.flume.sink.kafka.KafkaSink \n             a1.sinks.k1.kafka.topic   =   MOMO_MSG \n             a1.sinks.k1.kafka.bootstrap.servers   =   node1:9092,node2:9092,node3:9092 \n             a1.sinks.k1.kafka.flumeBatchSize   =   10 \n             a1.sinks.k1.kafka.producer.acks   =   1 \n             a1.sinks.k1.kafka.producer.linger.ms   =   1 \n             a1.sinks.k1.kafka.producer.compression.type   =   snappy \n\n     \n 2)    \n #2.1: : \n a1.sources   =   r1 \n a1.channels   =   c1 \n a1.sinks   =   k1 \nsource  \n a1.sources.r1.type   =   TAILDIR \n a1.sources.r1.positionFile   =   /export/data/flume/taildir_position.json \n a1.sources.r1.filegroups   =   f1 f2 \n a1.sources.r1.filegroups.f1   =   /export/data/momo_data/MOMO_DATA.dat \n a1.sources.r1.filegroups.f2   =   /export/data/momo_data/.*log.* \nchannel \n a1.channels.c1.type   =   memory \n a1.channels.c1.capacity   =   1000 \n a1.channels.c1.transactionCapacity   =   100 \nsink \n a1.sinks.k1.type   =   org.apache.flume.sink.kafka.KafkaSink \n a1.sinks.k1.kafka.topic   =   MOMO_MSG \n a1.sinks.k1.kafka.bootstrap.servers   =   node1:9092,node2:9092,node3:9092 \n a1.sinks.k1.kafka.flumeBatchSize   =   10 \n a1.sinks.k1.kafka.producer.acks   =   1 \n a1.sinks.k1.kafka.producer.linger.ms   =   1 \n a1.sinks.k1.kafka.producer.compression.type   =   snappy \n\n\n #2.3.  \n a1.sources.r1.channels   =   c1 \n a1.sinks.k1.channel   =   c1 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 : \n \n \n \n flume \n \n \n \n cd    /export/server/apache-flume-1.9.0-bin/conf \n vim   MOMO_tailDirSource_kafkaSink.conf \n\n  : \n #1)  \n a1.sources   =   r1 \n a1.channels   =   c1 \n a1.sinks   =   k1 \n\n #2) source \n a1.sources.r1.type   =   TAILDIR \n a1.sources.r1.positionFile   =   /export/data/flume/taildir_position.json \n a1.sources.r1.filegroups   =   f1 f2 \n a1.sources.r1.filegroups.f1   =   /export/data/momo_data/MOMO_DATA.dat \n a1.sources.r1.filegroups.f2   =   /export/data/momo_data/.*log.* \n\n #3) channel \n a1.channels.c1.type   =   memory \n a1.channels.c1.capacity   =   1000 \n a1.channels.c1.transactionCapacity   =   100 \n\n #4) sink \n a1.sinks.k1.type   =   org.apache.flume.sink.kafka.KafkaSink \n a1.sinks.k1.kafka.topic   =   MOMO_MSG \n a1.sinks.k1.kafka.bootstrap.servers   =   node1:9092,node2:9092,node3:9092 \n a1.sinks.k1.kafka.flumeBatchSize   =   10 \n a1.sinks.k1.kafka.producer.acks   =   1 \n a1.sinks.k1.kafka.producer.linger.ms   =   1 \n a1.sinks.k1.kafka.producer.compression.type   =   snappy \n\n #5)  \n a1.sources.r1.channels   =   c1 \n a1.sinks.k1.channel   =   c1 \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \n \n \n flume:  kafka \n \n \n zookeeper \n kafka \n \n \n \n \n kafka MOMO_MSG topic :   3 2 \n \n \n \n cd   /export/server/kafka_2.12-2.4.1/bin/ \n\n ./kafka-topics.sh    --create --zookeeper node1:2181,node2:2181,node3:2181 --topic MOMO_MSG --partitions 3 --replication-factor 2 \n \n 1 2 3 \n \n \n \n , ,  MOMO_MSG  \n \n \n \n [ root@node3 bin ] # ./kafka-console-consumer.sh  --bootstrap-server node1:9092,node2:9092,node3:9092 --topic MOMO_MSG \n \n 1 \n \n \n \n flume \n \n \n \n   cd  /export/server/apache-flume-1.9.0-bin\n    \nbin/flume-ng agent   -c  conf  -f  conf/MOMO_tailDirSource_kafkaSink.conf  -n  a1  -Dflume.root.logger = INFO,console\n \n 1 2 3 \n \n \n \n , ,   \n \n \n \n java   -jar  MoMo_DataGen.jar  MoMo_Data.xlsx  /export/data/momo_data/  1000 \n \n 1 \n _, HBase \n 5.1 HBASE_ \n \n \n \n HBase \n \n \n \n \n : MOMO_CHAT \n \n \n \n create_namespace   \'MOMO_CHAT\'  \n \n 1 \n \n \n : MOMO_MSG \n \n \n \n  :   \n        1(C1)  : 6  :GZ   \n\n  : \n    create   \'MOMO_CHAT:MOMO_MSG\',{NAME=> \'C1\',COMPRESSION=>\'GZ\'} , {NUMREGIONS=>6,SPLITALGO =>\'HexStringSplit\'} \n \n 1 2 3 4 5 \n 5.2 rowkey \n \n : rowkey \n \n 1) /rowkey\n2)  rowkey  \n3) LONGstring, rowkey\n4) rowkey\n \n 1 2 3 4 \n rowkey \n \n  :   \n             \n\n ROWKEY   :   \n     hash(_)___\n     \n rowkey : \n 1)   /rowkey \n 2)    rowkey   \n 3)   rowkey \n 4)   ,  \n\n  :   \n    ,    regionregion \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 5.3 HBASE \n \n \n \n   \n \n \n \n      < repositories > \x3c!----\x3e \n         < repository > \n             < id > aliyun </ id > \n             < url > http://maven.aliyun.com/nexus/content/groups/public/ </ url > \n             < releases > < enabled > true </ enabled > </ releases > \n             < snapshots > \n                 < enabled > false </ enabled > \n                 < updatePolicy > never </ updatePolicy > \n             </ snapshots > \n         </ repository > \n     </ repositories > \n\n\n     < dependencies > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-java </ artifactId > \n             < version > 1.10.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-java_2.11 </ artifactId > \n             < version > 1.10.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-runtime-web_2.11 </ artifactId > \n             < version > 1.10.0 </ version > \n         </ dependency > \n         \x3c!-- flinkhdfs--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-shaded-hadoop-2-uber </ artifactId > \n             < version > 2.7.5-10.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-connector-kafka_2.11 </ artifactId > \n             < version > 1.10.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.bahir </ groupId > \n             < artifactId > flink-connector-redis_2.11 </ artifactId > \n             < version > 1.0 </ version > \n         </ dependency > \n         \x3c!--Hbase --\x3e \n         < dependency > \n             < groupId > org.apache.hbase </ groupId > \n             < artifactId > hbase-client </ artifactId > \n             < version > 2.1.0 </ version > \n         </ dependency > \n         \x3c!--kafka --\x3e \n         < dependency > \n             < groupId > org.apache.kafka </ groupId > \n             < artifactId > kafka-clients </ artifactId > \n             < version > 2.4.1 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.httpcomponents </ groupId > \n             < artifactId > httpclient </ artifactId > \n             < version > 4.5.4 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > com.alibaba </ groupId > \n             < artifactId > fastjson </ artifactId > \n             < version > 1.2.62 </ version > \n         </ dependency > \n\n\n     </ dependencies > \n\n\n\n\n     < build > \n         < plugins > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.1 </ version > \n                 < configuration > \n                     < target > 1.8 </ target > \n                     < source > 1.8 </ source > \n                 </ configuration > \n             </ plugin > \n         </ plugins > \n     </ build > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 \n \n \n : com.itheima.momo.consumertoHBase; \n \n \n \n \n : \n \n \n \n package   com . itheima . momo . consumertoHBase ; \n\n import   org . apache . hadoop . conf . Configuration ; \n import   org . apache . hadoop . hbase . HBaseConfiguration ; \n import   org . apache . hadoop . hbase . TableName ; \n import   org . apache . hadoop . hbase . client . Connection ; \n import   org . apache . hadoop . hbase . client . ConnectionFactory ; \n import   org . apache . hadoop . hbase . client . Put ; \n import   org . apache . hadoop . hbase . client . Table ; \n import   org . apache . hadoop . hbase . util . MD5Hash ; \n import   org . apache . kafka . clients . consumer . ConsumerRecord ; \n import   org . apache . kafka . clients . consumer . ConsumerRecords ; \n import   org . apache . kafka . clients . consumer . KafkaConsumer ; \n\n import   java . io . IOException ; \n import   java . text . SimpleDateFormat ; \n import   java . time . Duration ; \n import   java . util . Arrays ; \n import   java . util . Properties ; \n\n public   class   MOMOConsumerToHBase   { \n\n     private     static   Connection  hBConn  ; \n     private    static   Table  table ; \n\n     static   {   //   \n         try   { \n             //4.1  \n             Configuration  conf  =   HBaseConfiguration . create ( ) ; \n            conf . set ( "hbase.zookeeper.quorum" , "node1:2181,node2:2181,node3:2181" ) ; \n\n            hBConn  =   ConnectionFactory . createConnection ( conf ) ; \n\n             //4.2  \n            table  =  hBConn . getTable ( TableName . valueOf ( "MOMO_CHAT:MOMO_MSG" ) ) ; \n\n\n         }   catch   ( IOException  e )   { \n            e . printStackTrace ( ) ; \n         } \n\n     } \n\n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         //1.  kafka \n\n         //1.1:  \n         Properties  props  =   new   Properties ( ) ; \n\n        props . setProperty ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        props . setProperty ( "group.id" ,   "MOMO_G1" ) ;   //  \n        props . setProperty ( "enable.auto.commit" ,   "true" ) ;   //   \n        props . setProperty ( "auto.commit.interval.ms" ,   "1000" ) ;   //  \n         //  \n        props . setProperty ( "key.deserializer" ,   "org.apache.kafka.common.serialization.StringDeserializer" ) ; \n        props . setProperty ( "value.deserializer" ,   "org.apache.kafka.common.serialization.StringDeserializer" ) ; \n\n\n         KafkaConsumer < String , String >  kafkaConsumer  =   new   KafkaConsumer < String ,   String > ( props ) ; \n\n\n         //2. topic \n        kafkaConsumer . subscribe ( Arrays . asList ( "MOMO_MSG" ) ) ; \n\n\n         //3.  \n         while ( true ) { \n             //3.1: kafka: \n             ConsumerRecords < String ,   String >  records  =  kafkaConsumer . poll ( Duration . ofSeconds ( 1 ) ) ; \n\n             for   ( ConsumerRecord < String ,   String >  record  :  records )   { \n                 //3.2:  \n                 String  message  =  record . value ( ) ; \n\n                 //----------------, HBASE-------------------- \n\n                 if ( message != null   &&   ! "" . equals ( message . trim ( ) ) ) { \n\n                     String [ ]  fields  =  message . split ( "\\001" ) ; \n                     if ( fields . length  ==   20 ) { \n\n                         //4. HBASE \n                         //4.3: :  \n                         String  rowKey  =   getRowKey ( message ) ; \n                         Put  put  =   new   Put ( rowKey . getBytes ( ) ) ; \n\n                        put . addColumn ( "C1" . getBytes ( ) , "msg_time" . getBytes ( ) , fields [ 0 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_nickyname" . getBytes ( ) , fields [ 1 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_account" . getBytes ( ) , fields [ 2 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_sex" . getBytes ( ) , fields [ 3 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_ip" . getBytes ( ) , fields [ 4 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_os" . getBytes ( ) , fields [ 5 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_phone_type" . getBytes ( ) , fields [ 6 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_network" . getBytes ( ) , fields [ 7 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "sender_gps" . getBytes ( ) , fields [ 8 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_nickyname" . getBytes ( ) , fields [ 9 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_ip" . getBytes ( ) , fields [ 10 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_account" . getBytes ( ) , fields [ 11 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_os" . getBytes ( ) , fields [ 12 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_phone_type" . getBytes ( ) , fields [ 13 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_network" . getBytes ( ) , fields [ 14 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_gps" . getBytes ( ) , fields [ 15 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "receiver_sex" . getBytes ( ) , fields [ 16 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "msg_type" . getBytes ( ) , fields [ 17 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "distance" . getBytes ( ) , fields [ 18 ] . getBytes ( ) ) ; \n                        put . addColumn ( "C1" . getBytes ( ) , "message" . getBytes ( ) , fields [ 19 ] . getBytes ( ) ) ; \n\n\n                        table . put ( put ) ; \n                     } \n\n                 } \n\n             } \n         } \n\n\n     } \n\n     // rowkey   hash(_)___ \n     public   static   String   getRowKey ( String  message )   throws   Exception { \n\n         //  \n\n         String [ ]  fields  =  message . split ( "\\001" ) ; \n\n         //   2021-03-17 09:22:27 \n         String  dateStr  =  fields [ 0 ] ; \n\n         SimpleDateFormat  format1  =   new   SimpleDateFormat ( "yyyy-MM-dd HH:mm:ss" ) ; \n         long  timestamp  =  format1 . parse ( dateStr ) . getTime ( ) ; \n\n\n         //  \n         String  sender_account  =  fields [ 2 ] ; \n\n         //  \n         String  receiver_account  =  fields [ 11 ] ; \n\n         // rowkey \n         StringBuffer  stringBuffer  =   new   StringBuffer ( ) ; \n        stringBuffer . append ( sender_account + "_" ) ; \n        stringBuffer . append ( receiver_account + "_" ) ; \n\n         String  md5Hash  =   MD5Hash . getMD5AsHex ( stringBuffer . toString ( ) . getBytes ( ) ) . substring ( 0 , 8 ) ; \n\n         String  rowkey  =  md5Hash  + "_" + stringBuffer . toString ( ) + timestamp ; \n\n         return   rowkey ; \n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 5.4  \n \n \n \n  \n \n \n \n zookeeper  hadoop  hbase  kafka \n \n 1 \n \n \n  \n \n \n \n \n \n \n \n flumekafka \n \n \n \n  bin/flume-ng   agent  -c conf -f conf/MOMO_tailDirSource_kafkaSink.conf -n a1 -Dflume.root.logger=INFO,console \n \n 1 \n \n \n   jar \n \n \n \n \n  hbase MOMO_CHAT:MOMO_MSG \n _Phoenix \n \n \n \n Phoenix \n \n \n \n cd /export/server/apache-phoenix-5.0.0-HBase-2.0-bin/\n./sqlline.py node1:2181\n \n 1 2 \n \n \n \n hbase \n \n \n \n create   view  MOMO_CHAT . MOMO_MSG ( \n    id  varchar   primary   key , \n    C1 . "msg_time"   varchar , \n    C1 . "sender_nickyname"   varchar , \n    C1 . "sender_account"   varchar , \n    C1 . "sender_sex"   varchar , \n    C1 . "sender_ip"   varchar , \n    C1 . "sender_os"   varchar , \n    C1 . "sender_phone_type"   varchar , \n    C1 . "sender_network"   varchar , \n    C1 . "sender_gps"   varchar , \n    C1 . "receiver_nickyname"   varchar , \n    C1 . "receiver_ip"   varchar , \n    C1 . "receiver_account"   varchar , \n    C1 . "receiver_os"   varchar , \n    C1 . "receiver_phone_type"   varchar , \n    C1 . "receiver_network"   varchar , \n    C1 . "receiver_gps"   varchar , \n    C1 . "receiver_sex"   varchar , \n    C1 . "msg_type"   varchar , \n    C1 . "distance"   varchar , \n    C1 . "message"   varchar \n ) ; \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \n \n \n  \n _hive \n \n \n \n hive, hive \n \n \n \n cd /export/server/hive-2.1.0/bin\nnohup ./hive --service metastore &\nnohup ./hive --service hiveserver2 &\n\n\n \n 1 2 3 4 5 \n \n \n \n \n hbase \n \n \n \n create   database  MOMO_CHAT ; \n USE  MOMO_CHAT ; \n create   external  table   MOMO_MSG  ( \n    id string , \n    msg_time string , \n    sender_nickyname string , \n    sender_account string , \n    sender_sex string , \n    sender_ip string , \n    sender_os string , \n    sender_phone_type string , \n    sender_network string , \n    sender_gps string , \n    receiver_nickyname string , \n    receiver_ip string , \n    receiver_account string , \n    receiver_os string , \n    receiver_phone_type string , \n    receiver_network string , \n    receiver_gps string , \n    receiver_sex string , \n    msg_type string , \n    distance string , \n    message string\n )  stored  by   \'org.apache.hadoop.hive.hbase.HBaseStorageHandler\'   with  serdeproperties ( \'hbase.columns.mapping\' = \':key,C1:msg_time,\nC1:sender_nickyname,\nC1:sender_account,\nC1:sender_sex,\nC1:sender_ip,\nC1:sender_os,\nC1:sender_phone_type,\nC1:sender_network,\nC1:sender_gps,\nC1:receiver_nickyname,\nC1:receiver_ip,\nC1:receiver_account,\nC1:receiver_os,\nC1:receiver_phone_type,\nC1:receiver_network,\nC1:receiver_gps,\nC1:receiver_sex,\nC1:msg_type,\nC1:distance,\nC1:message\' )  tblproperties ( \'hbase.table.name\' = \'MOMO_CHAT:MOMO_MSG\' ) ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 \n \n \n  \n \n \n \n select * from momo_msg limit 1;\nselect count(1) from momo_msg;\n \n 1 2 \n \n'},{title:"",frontmatter:{title:"",date:"2022-03-09T00:00:00.000Z",publish:!1},regularPath:"/%E5%85%B3%E4%BA%8E%E6%9C%AC%E7%AB%99/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98.html",relativePath:"/.md",key:"v-dffff514",path:"/2022/03/09/%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98/",headers:[{level:2,title:"1.build",slug:"_1-build"},{level:2,title:"2.",slug:"_2-"}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:' 1.build \n dist/index.html /  \n \n  / /docs/.vuepress/config.jsbase \n \n base  base"./" \n \n base cannot get \n \n \n  \nnode_modules \n node_modules/@vuepress/core/lib/client/app.jsmode: \'history\', \n \n \n  \n 2. \n Vuepresspublictypora \n  \n \n npm   install  --save-dev markdown-it-disable-url-encode\n \n 1  .vuepress/config.js \n module . exports  =   { \n\t ...  \n     //  \n     markdown :   { \n         extendMarkdown :   md   =>   { \n            md . set ( { breaks :   true } ) \n            md . use ( require ( "markdown-it-disable-url-encode" ) ,   "./" ) \n         } \n     } \n     ... \n } ; \n \n 1 2 3 4 5 6 7 8 9 10 11 3.js500kb \n  \n npm   install  increase-memory-limit  -D \n \n 1  \nwindows  \n set   NODE_OPTIONS = --max_old_space_size = 10240 \n #linux \n export   NODE_OPTIONS = --max_old_space_size = 10240 \n \n 1 2 3 4 4., >  \n \n png \n'},{title:"Docker",frontmatter:{title:"Docker",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["",""]},regularPath:"/%E4%BA%91%E5%8E%9F%E7%94%9F/Docker.html",relativePath:"/Docker.md",key:"v-33a310e8",path:"/2023/06/10/docker/",headers:[{level:2,title:"1. Docker ",slug:"_1-docker-"},{level:3,title:"",slug:""},{level:3,title:" Linux  Docker",slug:"-linux--docker"},{level:2,title:"2.",slug:"_2-"},{level:3,title:"docker",slug:"docker"},{level:3,title:"Image vs Container  vs ",slug:"image-vs-container--vs-"},{level:3,title:"",slug:""},{level:3,title:"docker container ",slug:"docker-container-"},{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:3,title:"Container Mode ",slug:"container-mode-"},{level:3,title:" shell",slug:"-shell"},{level:3,title:" Container vs VM",slug:"-container-vs-vm"},{level:3,title:"docker container run ",slug:"docker-container-run-"},{level:2,title:"3. ",slug:"_3-"},{level:3,title:"registrypull",slug:"registrypull"},{level:3,title:"load",slug:"load"},{level:3,title:"dockerfile build",slug:"dockerfile-build"},{level:2,title:"4.Dockerfile",slug:"_4-dockerfile"},{level:3,title:" (FROM)",slug:"-from"},{level:3,title:"imageRUN ",slug:"imagerun-"},{level:3,title:" (ADD,COPY)",slug:"-add-copy"},{level:3,title:" (ARG vs ENV)",slug:"-arg-vs-env"},{level:3,title:" CMD",slug:"-cmd"},{level:3,title:" ENTRYPOINT",slug:"-entrypoint"},{level:3,title:"Shell  Exec ",slug:"shell--exec-"},{level:3,title:" Python Flask ",slug:"--python-flask-"},{level:3,title:"Dockerfile ",slug:"dockerfile--"},{level:3,title:"Dockerfile root",slug:"dockerfile--root"},{level:2,title:"5.Docker",slug:"_5-docker"},{level:3,title:"Data Volume",slug:"data-volume"},{level:3,title:"Data Volume  MySQL",slug:"data-volume--mysql"},{level:3,title:"",slug:""},{level:2,title:"6.Docker",slug:"_6-docker"},{level:3,title:"",slug:""},{level:3,title:"",slug:"-"},{level:3,title:"hostbridge",slug:"hostbridge"},{level:3,title:"",slug:""},{level:2,title:"7.Docker Compose",slug:"_7-docker-compose"},{level:3,title:"docker compose ",slug:"docker-compose-"},{level:3,title:"docker compose ",slug:"docker-compose-"},{level:3,title:"docker-compose",slug:"docker-compose"},{level:3,title:"docker-compose ",slug:"docker-compose-"},{level:3,title:"docker-compose ",slug:"docker-compose-"},{level:3,title:" scale",slug:"-scale"},{level:3,title:"docker compose ",slug:"docker-compose-"},{level:3,title:"docker compose ",slug:"docker-compose-"},{level:3,title:"hadoop",slug:"hadoop"},{level:2,title:"8.Docker Swarm",slug:"_8-docker-swarm"},{level:3,title:"docker swarm ",slug:"docker-swarm-"},{level:3,title:"Swarm ",slug:"swarm-"},{level:3,title:"Docker Stack",slug:"docker-stack"},{level:2,title:"9.Dockergitlab CICD",slug:"_9-dockergitlab-cicd"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"10.Docker Sonar,",slug:"_10-docker-sonar-"}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:' 1. Docker  \n  \n containerDocker \n  \n \n 90PC \n 00 \n 10cloud \n 11container \n \n container( \n  \n Package Software into Standardized Units for Development, Shipment and Deployment \n \n  \n  \n  \n \n  \n  \n \n  \n \n  vs  \n \n Linux Container2008Docker2013ITLinux ContainerLXCLinux ContainerNamespace Cgroups   \n \n Namespace2002 \n Cgroups(Control Groups)CPU/MEMGoogle2008Linux Kernel \n \n  \n docker != container\n \n 1 2015GoogleDockerOCIOpen Container Initiative \n  runtime spec \n  \n image spec \n  \n  \n \n  \n  \n  \n  \n  \n  \n \n  \n 202050%container  Gartner \n  Linux  Docker \n  \n #get-dcoker.sh,version \n curl   -fsSL  get.docker.com  -o  get-docker.sh  \n sh  get-docker.sh  --version   < VERSION > \n # \n sudo  systemctl start  docker \n sudo   docker  version\n \n 1 2 3 4 5 6 \n 2. \n docker \n docker +  +  \n  \n \n docker image pull nginx  nginxdocker image \n docker container stop web  webdocker container \n \n docker  version docker info  # \n docker   help   #docker \n docker  container  --help \n docker  container  ps   # \n docker  container  ps   -a   # \n docker  container  ls   #psps \n docker  image  ls \n docker  image  rm  \n \n 1 2 3 4 5 6 7 8 #  Image vs Container  vs  \n image \n \n Docker image  read-only   \n application \n  \n docker image \n \n container \n \n docker image \n imageimage  read-write     container layer  , \n imagecontainer \n \n \n docker image \n \n  \n registrydocker hub \n \n maven \n \n \n dockerpull \n \n daemon.json \n \n /etc/docker/daemon.json \n \n  \n \n { \n   "registry-mirrors" :   [ \n     "https://registry.docker-cn.com" ,\n     "http://hub-mirror.c.163.com" ,\n     "https://docker.mirrors.ustc.edu.cn" \n   ] \n } \n #docker    "https://registry.docker-cn.com", \n #docker  "http://hub-mirror.c.163.com", \n #ustc "https://docker.mirrors.ustc.edu.cn" \n \n 1 2 3 4 5 6 7 8 9 10 \n dockerdocker info  \n \n sudo  systemctl daemon-reload\n sudo  systemctl restart  docker \n sudo   docker  info\n \n 1 2 3 docker: \n  \n docker container  \n  \n \n \n $  docker  container  ps \nCONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS     NAMES\ncd3a825fedeb   nginx      "/docker-entrypoint."     7  seconds ago    Up  6  seconds     80 /tcp    mystifying_leakey\n269494fe89fa   nginx      "/docker-entrypoint."     9  seconds ago    Up  8  seconds     80 /tcp    funny_gauss\n34b68af9deef   nginx      "/docker-entrypoint."     12  seconds ago   Up  10  seconds    80 /tcp    interesting_mahavira\n7513949674fc   nginx      "/docker-entrypoint."     13  seconds ago   Up  12  seconds    80 /tcp    kind_nobel\n \n 1 2 3 4 5 6 1 \n $  docker  container stop cd3  269  34b  751 \n \n 1 2 \n $  docker  container stop  $( docker  container  ps   -aq ) \ncd3a825fedeb\n269494fe89fa\n34b68af9deef\n7513949674fc\n$  docker  container  ps   -a \nCONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS                     PORTS     NAMES\ncd3a825fedeb   nginx      "/docker-entrypoint."     30  seconds ago   Exited  ( 0 )   2  seconds ago             mystifying_leakey\n269494fe89fa   nginx      "/docker-entrypoint."     32  seconds ago   Exited  ( 0 )   2  seconds ago             funny_gauss\n34b68af9deef   nginx      "/docker-entrypoint."     35  seconds ago   Exited  ( 0 )   2  seconds ago             interesting_mahavira\n7513949674fc   nginx      "/docker-entrypoint."     36  seconds ago   Exited  ( 0 )   2  seconds ago             kind_nobel\n$\n \n 1 2 3 4 5 6 7 8 9 10 11 12 \n  \n \n \n   docker container rm $(docker container ps -aq) \n \n docker system prune -a -f  image \n Container Mode  \n attach  \n : docker container run -p 80:80 nginx \n \n  \n  logctrl + c  \n  \n detach () \n : \n docker  container run  -d   -p   80 :80 nginx\n \n 1 \n \n  \n \n \n log  \n \n \n # ctl+cstop \n docker  container attach  id \n #docker logs \n docker  container logs  id \n \n 1 2 3 4 \n  shell \n  \n docker  container run  -it \n \n 1 ~  docker  container run  -it  busybox  sh \n/  # \n/  # \n/  # ls \nbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/  # ps \nPID    USER      TIME  COMMAND\n     1  root       0 :00  sh \n     8  root       0 :00  ps \n/  # exit \n \n 1 2 3 4 5 6 7 8 9 10 command \n docker  container  exec   -it \n \n 1   ~ docker container run -d nginx\n33d2ee50cfc46b5ee0b290f6ad75d724551be50217f691e68d15722328f11ef6\n  ~\n  ~ docker container exec -it 33d sh\nls\nbin  boot  dev  docker-entrypoint.d  docker-entrypoint.sh  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\nexit\n  ~\n \n 1 2 3 4 5 6 7 8 9 10 11 #   Container vs VM \n \n \n \n Mini \n \n \n Containers are just processes \n \n \n CPU \n \n \n  \n docker container run  \n $ docker container run -d --publish 80:80 --name webhost nginx\n \n 1 \n \n \n nginximage \n \n \n \n \n image registrynginxregistryDocker Hub) \n \n \n \n \n nginx nginx:latest ) \n \n \n \n \n nginx \n \n \n \n \n docker engineIP \n \n \n \n \n 8080 \n \n \n \n \n shellnginx \n 3.  \n \n pull from  registry  (online) registry\n \n public \n private \n \n \n build from  Dockerfile  (online) Dockerfile \n load from  file  (offline)   \n \n \n  \n $  docker  image\n\nUsage:   docker  image COMMAND\n\nManage images\n\nCommands:\nbuild       Build an image from a Dockerfile\n history      Show the  history  of an image\n import       Import the contents from a tarball to create a filesystem image\ninspect     Display detailed information on one or  more  images\nload        Load an image from a  tar  archive or STDIN\n ls           List images\nprune       Remove unused images\npull        Pull an image or a repository from a registry\npush        Push an image or a repository to a registry\n rm           Remove one or  more  images\nsave        Save one or  more  images to a  tar  archive  ( streamed to STDOUT by default ) \ntag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE\n\nRun  \'docker image COMMAND --help\'   for   more  information on a command.\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #  registrypull \n docker  pull nginx  #Docker Hub \n docker  pull nginx:1.20.0  #  \n docker  pull quay.io/bitnami/nginx  #Quay \n docker  image  ls   # \n docker  image  rm   id   # \n \n 1 2 3 4 5 #  load \n  \n docker  image save nginx:1.20.0  -o  nginx.myself\n \n 1 \n docker  image load  -i  nginx.myself\n \n 1 \n dockerfile build \n Dockerfile  \n Docker can build images automatically by reading the instructions from a  Dockerfile . A Dockerfile is a  text  document that contains all the commands a user could call on the command line to assemble an image. Using docker build users can create an automated build that executes several command-line instructions in succession. \n https://docs.docker.com/engine/reference/builder/ \n \n Dockerfiledocker \n Dockerfile \n Dockerfile \n Dockerfile \n centos "hello docker !" \n hello.sh \n echo   "hello docker !" \n \n 1 Dockerfile \n FROM  centos:7 \n ADD  hello.sh / \n CMD  [ "sh" ,  "/hello.sh" ] \n \n 1 2 3 \n Hello Docker java \n HelloWorld, \n public   class   HelloDocker   { \n     public   static   void   main ( String [ ]  args )   { \n         System . out . println ( "Hello Docker !" ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 Dockerfile \n FROM  openjdk:8-jdk-alpine \n ADD  *.jar app.jar \n ENTRYPOINT  [ "java" , "-Djava.security.egd=file:/dev/./urandom" , "-jar" , "/app.jar" ] \n \n 1 2 3 linux \n \n  \n docker  image build  -t  docker-demo-java  . \n \n 1 \n  \n docker  image tag docker-demo-java docker-demo-java:2.0\n # \n docker  tag  [ ImageId ]  registry.cn-shenzhen.aliyuncs.com/gordonchan/quickstart: [  ] \n \n 1 2 3 \n  \n docker  login  --username = XXXXXXX registry.cn-shenzhen.aliyuncs.com\n docker  push registry.cn-shenzhen.aliyuncs.com/gordonchan/quickstart: [  ] \n \n 1 2 \n \n  \n \n \n Python \n  \n centos:7hello.pyPython \n hello.py \n print("hello docker")\n \n 1  Python \n yum install -y python3.9 python3-pip python3.9-dev\n \n 1  hello.py \n $ python3 hello.py\nhello docker\n \n 1 2 \n Dockerfile \n Dockerfile \n ARG  image=centos:7 \n FROM   ${image} \n RUN   yum install -y python3.9 python3-pip python3.9-dev \n ADD  hello.py / \n CMD  [ "python3" ,  "/hello.py" ] \n \n 1 2 3 4 5 #  4.Dockerfile \n  (FROM) \n  \n \n Dockerfile \n taglatest \n  \n \n $  docker  image  ls \nREPOSITORY      TAG             IMAGE ID       CREATED          SIZE\nbitnami/nginx    1.18 .0          dfe237636dde    28  minutes ago    89 .3MB\nnginx            1.21 .0-alpine   a6eb2a334a9f    2  days ago        22 .6MB\nnginx            1.21 .0          d1a364dc548d    2  days ago       133MB\n \n 1 2 3 4 5 BuildNginx \n   index.html   \n < h1 > Hello Docker </ h1 > \n \n 1 Dockerfile \n FROM  nginx:1.21.0-alpine \n\n ADD  ..//index.html /usr/share/nginx/html/index.html \n \n 1 2 3 #  imageRUN  \n RUN  build Image \n $  apt-get  update\n$  apt-get   install   wget \n$  wget  https://github.com/ipinfo/cli/releases/download/ipinfo-2.0.1/ipinfo_2.0.1_linux_amd64.tar.gz\n$  tar  zxf ipinfo_2.0.1_linux_amd64.tar.gz\n$  mv  ipinfo_2.0.1_linux_amd64 /usr/bin/ipinfo\n$  rm   -rf  ipinfo_2.0.1_linux_amd64.tar.gz\n \n 1 2 3 4 5 6 Dockerfile \n FROM  ubuntu:20.04 \n RUN  apt-get update \n RUN  apt-get install -y wget \n RUN  wget https://github.com/ipinfo/cli/releases/download/ipinfo-2.0.1/ipinfo_2.0.1_linux_amd64.tar.gz \n RUN  tar zxf ipinfo_2.0.1_linux_amd64.tar.gz \n RUN  mv ipinfo_2.0.1_linux_amd64 /usr/bin/ipinfo \n RUN  rm -rf ipinfo_2.0.1_linux_amd64.tar.gz \n \n 1 2 3 4 5 6 7  \n $  docker  image  ls \nREPOSITORY   TAG       IMAGE ID       CREATED         SIZE\nipinfo       latest    97bb429363fb    4  minutes ago   138MB\nubuntu        21.04      478aa0080b60    4  days ago       74 .1MB\n$  docker  image  history  97b\nIMAGE          CREATED         CREATED BY                                      SIZE      COMMENT\n97bb429363fb    4  minutes ago   RUN /bin/sh  -c   rm   -rf  ipinfo_2.0.1_linux_amd   0B        buildkit.dockerfile.v0\n < missing >        4  minutes ago   RUN /bin/sh  -c   mv  ipinfo_2.0.1_linux_amd64 /    9 .36MB    buildkit.dockerfile.v0\n < missing >        4  minutes ago   RUN /bin/sh  -c   tar  zxf ipinfo_2.0.1_linux_am    9 .36MB    buildkit.dockerfile.v0\n < missing >        4  minutes ago   RUN /bin/sh  -c   wget  https://github.com/ipinf    4 .85MB    buildkit.dockerfile.v0\n < missing >        4  minutes ago   RUN /bin/sh  -c   apt-get   install   -y   wget   # bui   7.58MB    buildkit.dockerfile.v0 \n < missing >        4  minutes ago   RUN /bin/sh  -c   apt-get  update  # buildkit        33MB      buildkit.dockerfile.v0 \n < missing >        4  days ago      /bin/sh  -c   #(nop)  CMD ["/bin/bash"]            0B \n < missing >        4  days ago      /bin/sh  -c   mkdir   -p  /run/systemd  &&   echo   \'do   7B\n<missing>      4 days ago      /bin/sh -c [ -z "$(apt-get indextargets)" ]     0B\n<missing>      4 days ago      /bin/sh -c set -xe   && echo \' #!/bin/sh\' > /   811B \n < missing >        4  days ago      /bin/sh  -c   #(nop) ADD file:d6b6ba642344138dc   74.1MB \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 RUNimage layer,  \n Dockerfile \n FROM  ubuntu:20.04 \n RUN  apt-get update &&  \\ \n    apt-get install -y wget &&  \\ \n    wget https://github.com/ipinfo/cli/releases/download/ipinfo-2.0.1/ipinfo_2.0.1_linux_amd64.tar.gz &&  \\ \n    tar zxf ipinfo_2.0.1_linux_amd64.tar.gz &&  \\ \n    mv ipinfo_2.0.1_linux_amd64 /usr/bin/ipinfo &&  \\ \n    rm -rf ipinfo_2.0.1_linux_amd64.tar.gz \n \n 1 2 3 4 5 6 7 $  docker  image  ls \nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\nipinfo-new   latest    fe551bc26b92    5  seconds ago    124MB\nipinfo       latest    97bb429363fb    16  minutes ago   138MB\nubuntu        21.04      478aa0080b60    4  days ago        74 .1MB\n$  docker  image  history  fe5\nIMAGE          CREATED          CREATED BY                                      SIZE      COMMENT\nfe551bc26b92    16  seconds ago   RUN /bin/sh  -c   apt-get  update  &&      apt-get    49 .9MB    buildkit.dockerfile.v0\n < missing >        4  days ago       /bin/sh  -c   #(nop)  CMD ["/bin/bash"]            0B \n < missing >        4  days ago       /bin/sh  -c   mkdir   -p  /run/systemd  &&   echo   \'do   7B\n<missing>      4 days ago       /bin/sh -c [ -z "$(apt-get indextargets)" ]     0B\n<missing>      4 days ago       /bin/sh -c set -xe   && echo \' #!/bin/sh\' > /   811B \n < missing >        4  days ago       /bin/sh  -c   #(nop) ADD file:d6b6ba642344138dc   74.1MB \n$\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #   (ADD,COPY) \n  COPY    ADD  ,  \n  \n COPY    ADD   local  \n FROM  python:3.9.5-alpine3.13 \n COPY  hello.py /app/hello.py \n \n 1 2  hello.py  /app  /appfolder \n  \n ADD   COPYgzipADD \n FROM  python:3.9.5-alpine3.13 \n ADD  hello.tar.gz /app/ \n \n 1 2  \n  COPY  ADD  COPY  ADD \n  (ARG vs ENV) \n ARG    ENV  Dockerfile  \n FROM  ubuntu:20.04 \n RUN  apt-get update &&  \\ \n    apt-get install -y wget &&  \\ \n    wget https://github.com/ipinfo/cli/releases/download/ipinfo-2.0.1/ipinfo_2.0.1_linux_amd64.tar.gz &&  \\ \n    tar zxf ipinfo_2.0.1_linux_amd64.tar.gz &&  \\ \n    mv ipinfo_2.0.1_linux_amd64 /usr/bin/ipinfo &&  \\ \n    rm -rf ipinfo_2.0.1_linux_amd64.tar.gz \n \n 1 2 3 4 5 6 7 ENV \n FROM  ubuntu:20.04 \n ENV  VERSION=2.0.1 \n RUN  apt-get update &&  \\ \n    apt-get install -y wget &&  \\ \n    wget https://github.com/ipinfo/cli/releases/download/ipinfo- ${VERSION} /ipinfo_ ${VERSION} _linux_amd64.tar.gz &&  \\ \n    tar zxf ipinfo_ ${VERSION} _linux_amd64.tar.gz &&  \\ \n    mv ipinfo_ ${VERSION} _linux_amd64 /usr/bin/ipinfo &&  \\ \n    rm -rf ipinfo_ ${VERSION} _linux_amd64.tar.gz \n \n 1 2 3 4 5 6 7 8 ARG \n FROM  ubuntu:20.04 \n ARG  VERSION=2.0.1 \n RUN  apt-get update &&  \\ \n    apt-get install -y wget &&  \\ \n    wget https://github.com/ipinfo/cli/releases/download/ipinfo- ${VERSION} /ipinfo_ ${VERSION} _linux_amd64.tar.gz &&  \\ \n    tar zxf ipinfo_ ${VERSION} _linux_amd64.tar.gz &&  \\ \n    mv ipinfo_ ${VERSION} _linux_amd64 /usr/bin/ipinfo &&  \\ \n    rm -rf ipinfo_ ${VERSION} _linux_amd64.tar.gz \n \n 1 2 3 4 5 6 7 8  \n \n ARG buildvalue,   --build-arg \n $ docker image build -f .\\Dockerfile-arg -t ipinfo-arg-2.0.0 --build-arg VERSION=2.0.0 .\n$ docker image ls\nREPOSITORY         TAG       IMAGE ID       CREATED          SIZE\nipinfo-arg-2.0.0   latest    0d9c964947e2   6 seconds ago    124MB\n$ docker container run -it ipinfo-arg-2.0.0\nroot@b64285579756:/#\nroot@b64285579756:/# ipinfo version\n2.0.0\nroot@b64285579756:/#\n \n 1 2 3 4 5 6 7 8 9 ENV Image \n  CMD \n CMD \n \n  \n docker container runCMD \n CMD \n \n FROM ubuntu:20.04\nENV VERSION=2.0.1\nRUN apt-get update && \\\n    apt-get install -y wget && \\\n    wget https://github.com/ipinfo/cli/releases/download/ipinfo-${VERSION}/ipinfo_${VERSION}_linux_amd64.tar.gz && \\\n    tar zxf ipinfo_${VERSION}_linux_amd64.tar.gz && \\\n    mv ipinfo_${VERSION}_linux_amd64 /usr/bin/ipinfo && \\\n    rm -rf ipinfo_${VERSION}_linux_amd64.tar.gz\n \n 1 2 3 4 5 6 7 8 $ docker image build -t ipinfo .\n$ docker container run -it ipinfo\nroot@8cea7e5e8da8:/#\nroot@8cea7e5e8da8:/#\nroot@8cea7e5e8da8:/#\nroot@8cea7e5e8da8:/# pwd\n/\nroot@8cea7e5e8da8:/#\n \n 1 2 3 4 5 6 7 8 shellubuntuCMD \n $docker image history ipinfo\nIMAGE          CREATED        CREATED BY                                      SIZE      COMMENT\ndb75bff5e3ad   24 hours ago   RUN /bin/sh -c apt-get update &&     apt-get   50MB      buildkit.dockerfile.v0\n<missing>      24 hours ago   ENV VERSION=2.0.1                               0B        buildkit.dockerfile.v0\n<missing>      7 days ago     /bin/sh -c #(nop)  CMD ["/bin/bash"]            0B\n<missing>      7 days ago     /bin/sh -c mkdir -p /run/systemd && echo \'do   7B\n<missing>      7 days ago     /bin/sh -c [ -z "$(apt-get indextargets)" ]     0B\n<missing>      7 days ago     /bin/sh -c set -xe   && echo \'#!/bin/sh\' > /   811B\n<missing>      7 days ago     /bin/sh -c #(nop) ADD file:d6b6ba642344138dc   74.1MB\n \n 1 2 3 4 5 6 7 8 9 #   ENTRYPOINT \n ENTRYPOINT CMD \n \n CMD  docker container run   CMD    ENTRYPOINT   \n ENTRYPOINT    CMD   ENTRYPOINT  CMD \n \n FROM ubuntu:20.04\nCMD ["echo", "hello docker"]\n \n 1 2 Dockerfile build  demo-cmd   \n $ docker image ls\nREPOSITORY        TAG       IMAGE ID       CREATED      SIZE\ndemo-cmd          latest    5bb63bb9b365   8 days ago   74.1MB\n \n 1 2 3 FROM ubuntu:20.04\nENTRYPOINT ["echo", "hello docker"]\n \n 1 2 build  demo-entrypoint   \n $ docker image ls\nREPOSITORY        TAG       IMAGE ID       CREATED      SIZE\ndemo-entrypoint   latest    b1693a62d67a   8 days ago   74.1MB\n \n 1 2 3 CMDCMDhello docker \n $ docker container run -it --rm demo-cmd\nhello docker\n \n 1 2 docker container runCMD \n $ docker container run -it --rm demo-cmd echo "hello world"\nhello world\n \n 1 2 ENTRYPOINTENTRYPOINT \n $ docker container run -it --rm demo-entrypoint\nhello docker\n$ docker container run -it --rm demo-entrypoint echo "hello world"\nhello docker echo hello world\n$\n \n 1 2 3 4 5 #  Shell  Exec  \n  \n CMDENTRYPOINTshellExec \n Shell \n CMD echo "hello docker"\n \n 1 ENTRYPOINT echo "hello docker"\n \n 1 Exec \n  \n ENTRYPOINT ["echo", "hello docker"]\n \n 1 CMD ["echo", "hello docker"]\n \n 1 shell \n FROM ubuntu:20.04\nENV NAME=docker\nCMD echo "hello $NAME"\n \n 1 2 3 CMDExec,  \n FROM ubuntu:20.04\nENV NAME=docker\nCMD ["echo", "hello $NAME"]\n \n 1 2 3   hello $NAME  ,   hello docker  , shell \n FROM ubuntu:20.04\nENV NAME=docker\nCMD ["sh", "-c", "echo hello $NAME"]\n \n 1 2 3 #   Python Flask  \n Python  \n from  flask  import  Flask\n\napp  =  Flask ( __name__ ) \n\n\n @app . route ( \'/\' ) \n def   hello_world ( ) : \n     return   \'Hello, World!\' \n \n 1 2 3 4 5 6 7 8 Dockerfile \n FROM  python:3.9.5-slim \n\n COPY  app.py /src/app.py \n\n RUN  pip install flask \n\n WORKDIR  /src \n ENV  FLASK_APP=app.py \n\n EXPOSE  5000 \n\n CMD  [ "flask" ,  "run" ,  "-h" ,  "0.0.0.0" ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 docker  image build  -t  flask  . \n docker  run  -d   -p   5000 :5000 flask\n \n 1 2 \n Dockerfile  \n Dockerfileapp.pyinstall flaskcache \n FROM  python:3.9.5-slim \n RUN  pip install flask \n WORKDIR  /src \n ENV  FLASK_APP=app.py \n EXPOSE  5000 \n COPY  app.py /src/app.py \n CMD  [ "flask" ,  "run" ,  "-h" ,  "0.0.0.0" ] \n \n 1 2 3 4 5 6 7 \n Dockerfile  .dockerignore \n Docker build context \n Dockerclient-serverClientServer \n dockerCLIclientServerbuild context \n  \n Dockerfile \n FROM  python:3.9.5-slim \n RUN  pip install flask \n WORKDIR  /src \n ENV  FLASK_APP=app.py \n EXPOSE  5000 \n COPY  ./ /src/ \n CMD  [ "flask" ,  "run" ,  "-h" ,  "0.0.0.0" ] \n \n 1 2 3 4 5 6 7 \n \n vim .dockerignore \n ./hello.image\n \n 1 .dockerignorebuild, build context \n Dockerfile  \n  \n \n C \n \n CDocker \n #include <stdio.h>\n\nvoid main(int argc, char *argv[])\n{\n    printf("hello %s\\n", argv[argc - 1]);\n}\n \n 1 2 3 4 5 6 C \n $ gcc --static -o hello hello.c\n$ ls\nhello  hello.c\n$ ./hello docker\nhello docker\n$ ./hello world\nhello world\n$ ./hello friends\nhello friends\n$\n \n 1 2 3 4 5 6 7 8 9 10 DockerCgccimage \n FROM gcc:9.4\n\nCOPY hello.c /src/hello.c\n\nWORKDIR /src\n\nRUN gcc --static -o hello hello.c\n\nENTRYPOINT [ "/src/hello" ]\n\nCMD []\n \n 1 2 3 4 5 6 7 8 9 10 11 build \n $ docker build -t hello .\nSending build context to Docker daemon   5.12kB\nStep 1/6 : FROM gcc:9.4\n---\x3e be1d0d9ce039\nStep 2/6 : COPY hello.c /src/hello.c\n---\x3e Using cache\n---\x3e 70a624e3749b\nStep 3/6 : WORKDIR /src\n---\x3e Using cache\n---\x3e 24e248c6b27c\nStep 4/6 : RUN gcc --static -o hello hello.c\n---\x3e Using cache\n---\x3e db8ae7b42aff\nStep 5/6 : ENTRYPOINT [ "/src/hello" ]\n---\x3e Using cache\n---\x3e 7f307354ee45\nStep 6/6 : CMD []\n---\x3e Using cache\n---\x3e 7cfa0cbe4e2a\nSuccessfully built 7cfa0cbe4e2a\nSuccessfully tagged hello:latest\n$ docker image ls\nREPOSITORY     TAG          IMAGE ID       CREATED       SIZE\nhello          latest       7cfa0cbe4e2a   2 hours ago   1.14GB\ngcc            9.4          be1d0d9ce039   9 days ago    1.14GB\n$ docker run --rm -it hello docker\nhello docker\n$ docker run --rm -it hello world\nhello world\n$ docker run --rm -it hello friends\nhello friends\n$\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 1.14GB \n hello.cGCCalpine \n  \n FROM gcc:9.4 AS builder\n\nCOPY hello.c /src/hello.c\n\nWORKDIR /src\n\nRUN gcc --static -o hello hello.c\n\n\n\nFROM alpine:3.13.5\n\nCOPY --from=builder /src/hello /src/hello\n\nENTRYPOINT [ "/src/hello" ]\n\nCMD []\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  \n $ docker build -t hello-apline -f Dockerfile-new .\nSending build context to Docker daemon   5.12kB\nStep 1/8 : FROM gcc:9.4 AS builder\n---\x3e be1d0d9ce039\nStep 2/8 : COPY hello.c /src/hello.c\n---\x3e Using cache\n---\x3e 70a624e3749b\nStep 3/8 : WORKDIR /src\n---\x3e Using cache\n---\x3e 24e248c6b27c\nStep 4/8 : RUN gcc --static -o hello hello.c\n---\x3e Using cache\n---\x3e db8ae7b42aff\nStep 5/8 : FROM alpine:3.13.5\n---\x3e 6dbb9cc54074\nStep 6/8 : COPY --from=builder /src/hello /src/hello\n---\x3e Using cache\n---\x3e 18c2bce629fb\nStep 7/8 : ENTRYPOINT [ "/src/hello" ]\n---\x3e Using cache\n---\x3e 8dfb9d9d6010\nStep 8/8 : CMD []\n---\x3e Using cache\n---\x3e 446baf852214\nSuccessfully built 446baf852214\nSuccessfully tagged hello-apline:latest\n$ docker image ls\nREPOSITORY     TAG          IMAGE ID       CREATED       SIZE\nhello-alpine   latest       446baf852214   2 hours ago   6.55MB\nhello          latest       7cfa0cbe4e2a   2 hours ago   1.14GB\ndemo           latest       079bae887a47   2 hours ago   125MB\ngcc            9.4          be1d0d9ce039   9 days ago    1.14GB\n$ docker run --rm -it hello-alpine docker\nhello docker\n$ docker run --rm -it hello-alpine world\nhello world\n$ docker run --rm -it hello-alpine friends\nhello friends\n$\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 6.55MB \n \n Java  \n \n openjdk:8-jdk-alpine \n Dockerfile root \n Root \n dockerrootdockerroot \n demosudo/root \n [demo@docker-host ~]$ sudo ls /root\n[sudo] password for demo:\ndemo is not in the sudoers file.  This incident will be reported.\n[demo@docker-host ~]$\n \n 1 2 3 4 dockerdockergroup \n [demo@docker-host ~]$ groups\ndemo docker\n[demo@docker-host ~]$ docker image ls\nREPOSITORY   TAG       IMAGE ID       CREATED      SIZE\nbusybox      latest    a9d583973f65   2 days ago   1.23MB\n[demo@docker-host ~]$\n \n 1 2 3 4 5 6 Docker/rootdocker container \n [demo@docker-host vagrant]$ docker run -it -v /root/:/root/tmp busybox sh\n/ # cd /root/tmp\n~/tmp # ls\nanaconda-ks.cfg  original-ks.cfg\n~/tmp # ls -l\ntotal 16\n-rw-------    1 root     root          5570 Apr 30  2020 anaconda-ks.cfg\n-rw-------    1 root     root          5300 Apr 30  2020 original-ks.cfg\n~/tmp #\n \n 1 2 3 4 5 6 7 8 9 sudosudo \n [demo@docker-host ~]$ sudo vim /etc/sudoers\n[sudo] password for demo:\ndemo is not in the sudoers file.  This incident will be reported.\n[demo@docker-host ~]$\n \n 1 2 3 4  \n [demo@docker-host ~]$ docker run -it -v /etc/sudoers:/root/sudoers busybox sh\n/ # echo "demo    ALL=(ALL)       ALL" >> /root/sudoers\n/ # more /root/sudoers | grep demo\ndemo    ALL=(ALL)       ALL\n \n 1 2 3 4 containerbingosudo \n [demo@docker-host ~]$ sudo more /etc/sudoers | grep demo\ndemo    ALL=(ALL)       ALL\n[demo@docker-host ~]$\n \n 1 2 3 root \n DockerfileDockerfile \n FROM python:3.9.5-slim\n\nRUN pip install flask\n\nCOPY app.py /src/app.py\n\nWORKDIR /src\nENV FLASK_APP=app.py\n\nEXPOSE 5000\n\nCMD ["flask", "run", "-h", "0.0.0.0"]\n \n 1 2 3 4 5 6 7 8 9 10 11 12   flask-demo \n Dockerfileroot  flask-no-root  Dockerfile \n \n groupadduseraddflask \n USERflask \n \n FROM python:3.9.5-slim\n\nRUN pip install flask && \\\n    groupadd -r flask && useradd -r -g flask flask && \\\n    mkdir /src && \\\n    chown -R flask:flask /src\n\nUSER flask\n\nCOPY app.py /src/app.py\n\nWORKDIR /src\nENV FLASK_APP=app.py\n\nEXPOSE 5000\n\nCMD ["flask", "run", "-h", "0.0.0.0"]\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ docker image ls\nREPOSITORY      TAG          IMAGE ID       CREATED          SIZE\nflask-no-root   latest       80996843356e   41 minutes ago   126MB\nflask-demo      latest       2696c68b51ce   49 minutes ago   125MB\npython          3.9.5-slim   609da079b03a   2 weeks ago      115MB\n \n 1 2 3 4 5  \n $ docker run -d --name flask-root flask-demo\nb31588bae216951e7981ce14290d74d377eef477f71e1506b17ee505d7994774\n$ docker run -d --name flask-no-root flask-no-root\n83aaa4a116608ec98afff2a142392119b7efe53617db213e8c7276ab0ae0aaa0\n$ docker container ps\nCONTAINER ID   IMAGE           COMMAND                  CREATED          STATUS          PORTS      NAMES\n83aaa4a11660   flask-no-root   "flask run -h 0.0.0.0"   4 seconds ago    Up 3 seconds    5000/tcp   flask-no-root\nb31588bae216   flask-demo      "flask run -h 0.0.0.0"   16 seconds ago   Up 15 seconds   5000/tcp   f\n \n 1 2 3 4 5 6 7 8 #  5.Docker \n Data Volume \n  \n  \n Dockerfile  my-cron \n $ ls\nDockerfile  my-cron\n$ more Dockerfile\nFROM alpine:latest\nRUN apk update\nRUN apk --no-cache add curl\nENV SUPERCRONIC_URL=https://github.com/aptible/supercronic/releases/download/v0.1.12/supercronic-linux-amd64 \\\n    SUPERCRONIC=supercronic-linux-amd64 \\\n    SUPERCRONIC_SHA1SUM=048b95b48b708983effb2e5c935a1ef8483d9e3e\nRUN curl -fsSLO "$SUPERCRONIC_URL" \\\n    && echo "${SUPERCRONIC_SHA1SUM}  ${SUPERCRONIC}" | sha1sum -c - \\\n    && chmod +x "$SUPERCRONIC" \\\n    && mv "$SUPERCRONIC" "/usr/local/bin/${SUPERCRONIC}" \\\n    && ln -s "/usr/local/bin/${SUPERCRONIC}" /usr/local/bin/supercronic\nCOPY my-cron /app/my-cron\nWORKDIR /app\n\nVOLUME ["/app"]\nRUN cron job\nCMD ["/usr/local/bin/supercronic", "/app/my-cron"]\n$\n$ more my-cron\n*/1 * * * * date >> /app/test.txt\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 supercronichttps://github.com/aptible/supercronic/  \n my-cronmy-croncrontab,  \n  \n $  docker  image build  -t  my-cron  . \n$  docker  image  ls \nREPOSITORY   TAG       IMAGE ID       CREATED         SIZE\nmy-cron      latest    e9fbd9a562c9    4  seconds ago    24 .7MB\n \n 1 2 3 4 (-v)VolumemountpointDockervolumeDockerfilevolume  VOLUME ["/app"] \n $  docker  run  -d  my-cron\n9a8fa93f03c42427a498b21ac520660752122e20bcdbf939661646f71d277f8f\n$  docker  volume  ls \nDRIVER    VOLUME NAME\n local      043a196c21202c484c69f2098b6b9ec22b9a9e4e4bb8d4f55a4c3dce13c15264\n$  docker  volume inspect 043a196c21202c484c69f2098b6b9ec22b9a9e4e4bb8d4f55a4c3dce13c15264\n [ \n     { \n         "CreatedAt" :   "2021-06-22T23:06:13+02:00" ,\n         "Driver" :   "local" ,\n         "Labels" :  null,\n         "Mountpoint" :   "/var/lib/docker/volumes/043a196c21202c484c69f2098b6b9ec22b9a9e4e4bb8d4f55a4c3dce13c15264/_data" ,\n         "Name" :   "043a196c21202c484c69f2098b6b9ec22b9a9e4e4bb8d4f55a4c3dce13c15264" ,\n         "Options" :  null,\n         "Scope" :   "local" \n     } \n ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 (-v) \n   -v  VolumeDockerfileVOLUME \n DockerfileVOLUME \n FROM alpine:latest\nRUN apk update\nRUN apk --no-cache add curl\nENV SUPERCRONIC_URL=https://github.com/aptible/supercronic/releases/download/v0.1.12/supercronic-linux-amd64 \\\n    SUPERCRONIC=supercronic-linux-amd64 \\\n    SUPERCRONIC_SHA1SUM=048b95b48b708983effb2e5c935a1ef8483d9e3e\nRUN curl -fsSLO "$SUPERCRONIC_URL" \\\n    && echo "${SUPERCRONIC_SHA1SUM}  ${SUPERCRONIC}" | sha1sum -c - \\\n    && chmod +x "$SUPERCRONIC" \\\n    && mv "$SUPERCRONIC" "/usr/local/bin/${SUPERCRONIC}" \\\n    && ln -s "/usr/local/bin/${SUPERCRONIC}" /usr/local/bin/supercronic\nCOPY my-cron /app/my-cron\nWORKDIR /app\nRUN cron job\nCMD ["/usr/local/bin/supercronic", "/app/my-cron"]\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 build-v : \n docker  image build  -t  my-cron  . \n docker  container run  -d   -v  cron-data:/app my-cron\n \n 1 2 43c6d0357b0893861092a752c61ab01bdfa62ea766d01d2fcb8b3ecb6c88b3de\n$ docker volume ls\nDRIVER    VOLUME NAME\nlocal     cron-data\n$ docker volume inspect cron-data\n[\n    {\n        "CreatedAt": "2021-06-22T23:25:02+02:00",\n        "Driver": "local",\n        "Labels": null,\n        "Mountpoint": "/var/lib/docker/volumes/cron-data/_data",\n        "Name": "cron-data",\n        "Options": null,\n        "Scope": "local"\n    }\n]\n$ ls /var/lib/docker/volumes/cron-data/_data\nmy-cron\n$ ls /var/lib/docker/volumes/cron-data/_data\nmy-cron  test.txt\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  \n volume \n $  docker   rm   -f   $( docker  container  ps   -aq )   # \n$  docker  system prune  -f   #contianer cache \n$  docker  volume prune  -f   #Remove all unused local volumes \n \n 1 2 3 #  Data Volume  MySQL \n MySQLtag5.7 \n Dockerfile https://github.com/docker-library/mysql/tree/master/5.7 \n  \n docker  pull mysql:5.7\n \n 1  \n MySQLdockerhub https://hub.docker.com/_/mysql?tab=description&page=1&ordering=last_updated \n Dockerfile Volume https://github.com/docker-library/mysql/tree/master/5.7 \n #--name docker-mysql  -e  -vdockermysql-data \n docker  container run  --name  docker-mysql  -e   MYSQL_ROOT_PASSWORD = 123456   -d   -v  mysql-data:/var/lib/mysql mysql:5.7\n \n 1 2 \n$ docker volume ls\nDRIVER    VOLUME NAME\nlocal     mysql-data\n$ docker volume inspect mysql-data\n[\n    {\n        "CreatedAt": "2021-06-21T23:55:23+02:00",\n        "Driver": "local",\n        "Labels": null,\n        "Mountpoint": "/var/lib/docker/volumes/mysql-data/_data",\n        "Name": "mysql-data",\n        "Options": null,\n        "Scope": "local"\n    }\n]\n$\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  \n MySQLshell \n docker  container  exec   -it  022  sh \nmysql  -u  root  -p \n \n 1 2 Enter password:\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 2\nServer version: 5.7.34 MySQL Community Server (GPL)\n\nCopyright (c) 2000, 2021, Oracle and/or its affiliates.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType \'help;\' or \'\\h\' for help. Type \'\\c\' to clear the current input statement.\n\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n4 rows in set (0.00 sec)\n\nmysql> create database demo;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| demo               |\n| mysql              |\n| performance_schema |\n| sys                |\n+--------------------+\n5 rows in set (0.00 sec)\n\nmysql> exit\nBye\nexit\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  demo \n data volume \n $  docker  volume inspect mysql-data\n [ \n     { \n         "CreatedAt" :   "2021-06-22T00:01:34+02:00" ,\n         "Driver" :   "local" ,\n         "Labels" :  null,\n         "Mountpoint" :   "/var/lib/docker/volumes/mysql-data/_data" ,\n         "Name" :   "mysql-data" ,\n         "Options" :  null,\n         "Scope" :   "local" \n     } \n ] \n$  ls   /var/lib/docker/volumes/mysql-data/_data\nauto.cnf    client-cert.pem  ib_buffer_pool  ibdata1  performance_schema  server-cert.pem\nca-key.pem  client-key.pem   ib_logfile0     ibtmp1   private_key.pem     server-key.pem\nca.pem      demo             ib_logfile1     mysql    public_key.pem      sys\n$\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #   \n \n  https://docs.docker.com/storage/volumes/#share-data-among-machines \n Dockervolumedrivervolume driverlocal \n $ docker volume inspect vscode\n[\n    {\n        "CreatedAt": "2021-06-23T21:33:57Z",\n        "Driver": "local",\n        "Labels": null,\n        "Mountpoint": "/var/lib/docker/volumes/vscode/_data",\n        "Name": "vscode",\n        "Options": null,\n        "Scope": "local"\n    }\n]\n \n 1 2 3 4 5 6 7 8 9 10 11 12 sshfsdriverdockervolume \n  \n LinuxSSH \n \n \n \n hostname \n ip \n ssh username \n ssh password \n \n \n \n \n docker-host1 \n 192.168.200.10 \n vagrant \n vagrant \n \n \n docker-host2 \n 192.168.200.11 \n vagrant \n vagrant \n \n \n docker-host3 \n 192.168.200.12 \n vagrant \n vagrant \n \n \n \n plugin \n plugin  vieux/sshfs \n [vagrant@docker-host1 ~]$ docker plugin install --grant-all-permissions vieux/sshfs\nlatest: Pulling from vieux/sshfs\nDigest: sha256:1d3c3e42c12138da5ef7873b97f7f32cf99fb6edde75fa4f0bcf9ed277855811\n52d435ada6a4: Complete\nInstalled plugin vieux/sshfs\n \n 1 2 3 4 5 [vagrant@docker-host2 ~]$ docker plugin install --grant-all-permissions vieux/sshfs\nlatest: Pulling from vieux/sshfs\nDigest: sha256:1d3c3e42c12138da5ef7873b97f7f32cf99fb6edde75fa4f0bcf9ed277855811\n52d435ada6a4: Complete\nInstalled plugin vieux/sshfs\n \n 1 2 3 4 5 volume \n [vagrant@docker-host1 ~]$ docker volume create --driver vieux/sshfs \\\n                          -o sshcmd=vagrant@192.168.200.12:/home/vagrant \\\n                          -o password=vagrant \\\n                          sshvolume\n \n 1 2 3 4  \n [vagrant@docker-host1 ~]$ docker volume ls\nDRIVER               VOLUME NAME\nvieux/sshfs:latest   sshvolume\n[vagrant@docker-host1 ~]$ docker volume inspect sshvolume\n[\n    {\n        "CreatedAt": "0001-01-01T00:00:00Z",\n        "Driver": "vieux/sshfs:latest",\n        "Labels": {},\n        "Mountpoint": "/mnt/volumes/f59e848643f73d73a21b881486d55b33",\n        "Name": "sshvolume",\n        "Options": {\n            "password": "vagrant",\n            "sshcmd": "vagrant@192.168.200.12:/home/vagrant"\n        },\n        "Scope": "local"\n    }\n]\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Volume \n sshvolume/appshell/apptest.txt \n [vagrant@docker-host1 ~]$ docker run -it -v sshvolume:/app busybox sh\nUnable to find image \'busybox:latest\' locally\nlatest: Pulling from library/busybox\nb71f96345d44: Pull complete\nDigest: sha256:930490f97e5b921535c153e0e7110d251134cc4b72bbb8133c6a5065cc68580d\nStatus: Downloaded newer image for busybox:latest\n/ #\n/ # ls\napp   bin   dev   etc   home  proc  root  sys   tmp   usr   var\n/ # cd /app\n/app # ls\n/app # echo "this is ssh volume"> test.txt\n/app # ls\ntest.txt\n/app # more test.txt\nthis is ssh volume\n/app #\n/app #\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 docker-host3 \n [vagrant@docker-host3 ~]$ pwd\n/home/vagrant\n[vagrant@docker-host3 ~]$ ls\ntest.txt\n[vagrant@docker-host3 ~]$ more test.txt\nthis is ssh volume\n \n 1 2 3 4 5 6  \n  \n docker \n  \n Dockerjsonjson \n json \n /var/lib/docker/containers/<ID>/config.v2.json \n /var/lib/docker/containers/<ID>/hostconfig.json \n  \n \n \n   docker ps -a container12ID docker inspect id 64ID(ID) \n \n \n container  service docker stop docker Docker  \n \n \n  /var/lib/docker/containers/<64ID>/   config.v2.json   hostconfig.json root \n 4. ~/ () \n cd ~/ \n touch config.v2.json hostconfig.json \n \n \n 5. \n sudo -i   \n cd /var/lib/docker/containers/<64ID>/ \n GG \n cp config.v2.json config.v2.json.back \n cp hostconfig.json hostconfig.back \n  \n ** ~/ ** \n 1. config.v2.json  \n \n  MountPoints  /root/root/  \n 2. hostconfig.json  \n  hostconfig.json  Binds  /root/root/dataset  \n \n  \n  /  config.v2.json  hostconfig.json  \n /var/lib/docker/containers/<ID>/config.v2.json \n /var/lib/docker/containers/<ID>/hostconfig.json \n docker  \n service docker start\n \n 1 #  6.Docker \n Bridge  \n   \n  docker0 Linux bridge \n  \n docker  container run  -d    --name  box1 busybox /bin/sh  -c   "while true; do sleep 3600; done" \n\n docker  container run  -d    --name  box2 busybox /bin/sh  -c   "while true; do sleep 3600; done" \n \n 1 2 3 docker  network  ls \n docker  network inspect bridge\n # \nbrctl show\n \n 1 2 3 4 NETWORK ID     NAME      DRIVER    SCOPE\n1847e179a316   bridge    bridge     local \na647a4ad0b4f    host        host        local \nfbd81b56c009   none      null       local \n$  docker  network inspect bridge\n [ \n     { \n         "Name" :   "bridge" ,\n         "Id" :   "1847e179a316ee5219c951c2c21cf2c787d431d1ffb3ef621b8f0d1edd197b24" ,\n         "Created" :   "2021-07-01T15:28:09.265408946Z" ,\n         "Scope" :   "local" ,\n         "Driver" :   "bridge" ,\n         "EnableIPv6" :  false,\n         "IPAM" :   { \n             "Driver" :   "default" ,\n             "Options" :  null,\n             "Config" :   [ \n                 { \n                     "Subnet" :   "172.17.0.0/16" ,\n                     "Gateway" :   "172.17.0.1" \n                 } \n             ] \n         } ,\n         "Internal" :  false,\n         "Attachable" :  false,\n         "Ingress" :  false,\n         "ConfigFrom" :   { \n             "Network" :   "" \n         } ,\n         "ConfigOnly" :  false,\n         "Containers" :   { \n             "03494b034694982fa085cc4052b6c7b8b9c046f9d5f85f30e3a9e716fad20741" :   { \n                 "Name" :   "box1" ,\n                 "EndpointID" :   "072160448becebb7c9c333dce9bbdf7601a92b1d3e7a5820b8b35976cf4fd6ff" ,\n                 "MacAddress" :   "02:42:ac:11:00:02" ,\n                 "IPv4Address" :   "172.17.0.2/16" ,\n                 "IPv6Address" :   "" \n             } ,\n             "4f3303c84e5391ea37db664fd08683b01decdadae636aaa1bfd7bb9669cbd8de" :   { \n                 "Name" :   "box2" ,\n                 "EndpointID" :   "4cf0f635d4273066acd3075ec775e6fa405034f94b88c1bcacdaae847612f2c5" ,\n                 "MacAddress" :   "02:42:ac:11:00:03" ,\n                 "IPv4Address" :   "172.17.0.3/16" ,\n                 "IPv6Address" :   "" \n             } \n         } ,\n         "Options" :   { \n             "com.docker.network.bridge.default_bridge" :   "true" ,\n             "com.docker.network.bridge.enable_icc" :   "true" ,\n             "com.docker.network.bridge.enable_ip_masquerade" :   "true" ,\n             "com.docker.network.bridge.host_binding_ipv4" :   "0.0.0.0" ,\n             "com.docker.network.bridge.name" :   "docker0" ,\n             "com.docker.network.driver.mtu" :   "1500" \n         } ,\n         "Labels" :   { } \n     } \n ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 brctl` , CentOS,  `sudo yum install -y bridge-utils` . Ubuntu,  `sudo apt-get install -y bridge-utils\n$ brctl show\nbridge name     bridge id               STP enabled     interfaces\ndocker0         8000.0242759468cf       no              veth8c9bb82\n                                                        vethd8f9afb\n \n 1 2 3 4 5  \n $ ip route\ndefault via 10.0.2.2 dev eth0 proto dhcp metric 100\n10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100\n172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1\n192.168.200.0/24 dev eth1 proto kernel scope link src 192.168.200.10 metric 101\n \n 1 2 3 4 5 #    \n host \n  \n docker  container run  -d   --rm   --name  web  -p   8080 :80 nginx:1.20.0\n # \nfirewall-cmd  --zone = public --add-port = 80 /tcp  --permanent \n # \nsystemctl restart firewalld\n # \nfirewall-cmd  --zone = public --list-ports\n \n 1 2 3 4 5 6 7 \n ,box1webindex \n docker  container  exec   -it  box1  wget  http://172.17.0.5\n \n 1 \n web8080 \n \n \n host  \n docker  container run  -d   --rm   --name  web2  -p   8080 :80 nginx:1.20.0\n \n 1 \n networkhost80 \n docker  container run  -d   --rm   --name  web1  --network   host  nginx:1.20.0\n \n 1 \n hostbridge \n  bridge \n web \n docker  network disconnect bridge web\n \n 1 \n mybridge \n docker  network create mybridge\n \n 1 \n webmybridgebox1webbox1mybridge,box2pingweb \n docker  network connect mybridge web\n docker  container  exec   -it  box1  ping  web\n docker  network connect mybridge box1\n \n 1 2 3 \n \n ip \n docker  network create  -d  bridge  --gateway   172.200 .0.1  --subnet   172.200 .0.0/16 detailbridge\n \n 1 \n  \n ----------------------------------------- \n LinuxNamespaceNamespace user namespace, process namespace, network namespace \n   Linux  Docker  \n 1.iptables \n 2. \n 3. \n  3  \n \n  A  IP  172.17.42.1  B  \n \n Linux  veth  \n Docker    \n \n \n  \n  \n vim  add-ns-to-br.sh\n \n 1 #!/bin/bash \n\n bridge = $1 \n namespace = $2 \n addr = $3 \n\n # \n #docker0 \n vethA = veth- $namespace \n # \n vethB = eth00- $namespace \n # \n sudo   ip  netns  add   $namespace \n #veth pair \n sudo   ip   link   add   $vethA   type  veth peer name  $vethB \n # \n sudo   ip   link   set   $vethB  netns  $namespace \n #ip \n sudo   ip  netns  exec   $namespace   ip  addr  add   $addr  dev  $vethB \n # namespace vethB  \n sudo   ip  netns  exec   $namespace   ip   link   set   $vethB  up\n\n sudo   ip   link   set   $vethA  up\n\n sudo  brctl addif  $bridge   $vethA \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 brctl --help \n\nUsage: brctl  [ commands ] \ncommands:\n   addbr            < bridge >                  add  bridge\n   delbr            < bridge >                 delete bridge\n   addif            < bridge >   < device >         add  interface to bridge\n   delif            < bridge >   < device >        delete interface from bridge\n   hairpin          < bridge >   < port >   { on | off }         turn hairpin on/off\n   setageing        < bridge >   < time >           set  ageing  time \n   setbridgeprio    < bridge >   < prio >           set  bridge priority\n   setfd            < bridge >   < time >           set  bridge forward delay\n   sethello         < bridge >   < time >           set  hello  time \n   setmaxage        < bridge >   < time >           set  max message age\n   setpathcost      < bridge >   < port >   < cost >    set  path cost\n   setportprio      < bridge >   < port >   < prio >    set  port priority\n   show             [   < bridge >   ]             show a list of bridges\n   showmacs         < bridge >                 show a list of mac addrs\n   showstp          < bridge >                 show bridge stp info\n   stp              < bridge >   { on | off }        turn stp on/off\n\n [ root@container1 docker ] # ip netns help \nUsage:  ip  netns list\n   ip  netns  add  NAME\n   ip  netns  set  NAME NETNSID\n   ip   [ -all ]  netns delete  [ NAME ] \n   ip  netns identify  [ PID ] \n   ip  netns pids NAME\n   ip   [ -all ]  netns  exec   [ NAME ]  cmd  .. .\n   ip  netns monitor\n   ip  netns list-id\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  \n docker  container run  -d   --name  box3  --network  mybridge  busybox /bin/sh  -c   "while true; do sleep 3600; done" \n docker  container run  -d   --name  box4  --network  mybridge  busybox /bin/sh  -c   "while true; do sleep 3600; done" \n\n \n 1 2 3 \n \n bridge name \n brctl show\n \n 1 \n  \n sh  add-ns-to-br.sh br-1379584d9670 ns1  172.19 .0.2/16\n sh  add-ns-to-br.sh br-1379584d9670 ns2  172.19 .0.3/16\n #list \n sudo   ip  netns  ls \n #ip \n sudo   ip  netns  exec  ns2 \n ip  a\n ping   172.19 .0.2\n \n 1 2 3 4 5 6 7 8 \n  \n ping \n sudo   ip   link   set  veth-ns1 down\n \n 1 \n # \n service  network restart\n # \n ifconfig \n # \n cat  /var/log/messages  |   grep  network\n #docker \nsystemctl status docker.service\n # \n ifconfig  docker0 down\n # \nbrctl delbr docker0\n \n 1 2 3 4 5 6 7 8 9 10 11 12 #  7.Docker Compose \n docker compose  \n shelldocker composeyml \n docker compose  \n WindowsMacdocker desktopdocker-compose \n PS C:\\Users\\Peng Xiao\\docker.tips> docker-compose --version\ndocker-compose version 1.29.2, build 5becea4c\n \n 1 2 Linux \n  https://github.com/docker/compose/releases \n $ sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose\n$ sudo chmod +x /usr/local/bin/docker-compose\n$ docker-compose --version\ndocker-compose version 1.29.2, build 5becea4c\n \n 1 2 3 4 pythonpipdocker-Compose \n #pip \n wget  https://bootstrap.pypa.io/pip/3.6/get-pip.py\npython3 get-pip.py\n \n 1 2 3 pip install docker - compose\n \n 1 #  docker-compose \n compose  \n docker compose https://docs.docker.com/compose/compose-file/ \n  \n version: "3.8"\n\nservices: # \n  servicename: #  bridge DNS name\n    image: # \n    command: #  CMD\n    environment: #  docker run --env\n    volumes: # docker run -v\n    networks: #  docker run --network\n    ports: #  docker run -p\n  servicename2:\n\nvolumes: #  docker volume create\n\nnetworks: #  docker network create\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  Python Flask + Redisdocker-compose \n flask-demo \n from  flask  import  Flask\n from  redis  import  Redis\n import  os\n import  socket\n\napp  =  Flask ( __name__ ) \nredis  =  Redis ( host = os . environ . get ( \'REDIS_HOST\' ,   \'127.0.0.1\' ) ,  port = 6379 ) \n\n\n @app . route ( \'/\' ) \n def   hello ( ) : \n    redis . incr ( \'hits\' ) \n     return   f"Hello Container World! I have been seen  { redis . get ( \'hits\' ) . decode ( \'utf-8\' ) }  times and my hostname is  { socket . gethostname ( ) } .\\n" \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 Dockerfile \n FROM  python:3.9.5-slim \n\n RUN  python -m pip install --upgrade pip &&  \\ \n    pip install flask redis &&  \\ \n    groupadd -r flask && useradd -r -g flask flask &&  \\ \n    mkdir /src &&  \\ \n    chown -R flask:flask /src \n\n USER  flask \n\n WORKDIR  /src \n\n ENV  FLASK_APP=app.py REDIS_HOST=redis \n\n COPY  app.py /src/app.py \n EXPOSE  5000 \n\n CMD  [ "flask" ,  "run" ,  "-h" ,  "0.0.0.0" ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 docker-compose \n docker image pull redis\ndocker image build -t flask-demo .\ncreate network\ndocker network create -d bridge demo-network\ncreate container\ndocker container run -d --name redis-server --network demo-network redis\ndocker container run -d --network demo-network --name flask-demo --env REDIS_HOST=redis-server -p 5000:5000 flask-demo\n \n 1 2 3 4 5 6 7 8 9 flask-demo \n docker  image build  -t  flask-demo  . \n \n 1 docker-compose.yml  \n version :   "3.8" \n\n services : \n   flask-demo : \n     image :  flask - demo : latest\n     environment : \n       -  REDIS_HOST=redis - server\n     networks : \n       -  demo - network\n     ports : \n       -  8080 : 5000 \n\n   redis-server : \n     image :  redis : latest\n     networks : \n       -  demo - network\n\n networks : \n   demo-network : \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  \n Usage:\n docker-compose   [ -f  < arg > .. . ]   [ --profile  < name > .. . ]   [ options ]   [ -- ]   [ COMMAND ]   [ ARGS .. . ] \n docker-compose  -h | --help\n\nCommands:\nbuild              Build or rebuild services\nconfig             Validate and view the Compose  file \ncreate             Create services\ndown               Stop and remove resources\nevents             Receive real  time  events from containers\n exec                Execute a  command   in  a running container\n help                Get  help  on a  command \nimages             List images\n kill                Kill containers\nlogs               View output from containers\npause              Pause services\nport               Print the public port  for  a port binding\n ps                  List containers\npull               Pull  service  images\npush               Push  service  images\nrestart            Restart services\n rm                  Remove stopped containers\nrun                Run a one-off  command \nscale              Set number of containers  for  a  service \nstart              Start services\nstop               Stop services\n top                 Display the running processes\nunpause            Unpause services\nup                 Create and start containers\nversion            Show version information and quit\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # \n docker-compose  config\nbuildDockerfile \n docker-compose  build \n #build \n docker-compose  up  -d \n \n 1 2 3 4 5 6 \n \n #composecompose \n docker-compose  images\n # \n docker-compose   ps \n \n 1 2 3 4 \n #docker \n docker   ps \n docker   exec   -it  0b  sh \n #redis \n set  k1 v1\nget k1\n \n 1 2 3 4 5 6 \n # \n docker-compose  stop\n #container \n docker-compose   rm \n \n 1 2 3 4 \n docker-compose  \n buildyml \n version :   "3.8" \n\n services : \n   flask-demo : \n     image :  flask - demo : latest\n     build : \n       context :  .\n       dockerfile :  Dockerfile\n     environment : \n       -  REDIS_HOST=redis - server\n     networks : \n       -  demo - network\n     ports : \n       -  8080 : 5000 \n\n   redis-server : \n     image :  redis : latest\n     networks : \n       -  demo - network\n\n networks : \n   demo-network : \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \n #Dockerfile, \n docker-compose  up  --build   -d \n # \n docker-compose  up  -d   --build  flask\n \n 1 2 3 4 \n Dockerfile docker-compose build --no-cache  docker-compose up -d  \nDockerfile \n docker-compose  \n docker-compose_default_default \n version :   "3.8" \n\n services : \n   box1 : \n     image :  busybox\n \n 1 2 3 4 5 \n  \n version :   "3.8" \n\n services : \n   box1 : \n     image :  busybox\n     command :  /bin/sh  - c "while true; do sleep 3600; done"\n     networks : \n       -  demo - network1\n\n   box2 : \n     image :  busybox\n     command :  /bin/sh  - c "while true; do sleep 3600; done"\n     networks : \n       -  demo - network2\n   box3 : \n     image :  busybox\n     command :  /bin/sh  - c "while true; do sleep 3600; done"\n     networks : \n       -  demo - network1\n       -  demo - network2\n\n networks : \n   demo-network1 : \n     ipam : \n       driver :  defaule\n       config : \n         -   subnet :  172.28.0.0/16\n   demo-network2 : \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \n box1box2 pingbox3 \n  scale \n  \n version:  "3.8" \n\nservices:\n  flask:\n    build:\n      context:  .. /flask\n      dockerfile: Dockerfile\n    image: flask-demo:latest\n    environment:\n      -  REDIS_HOST = redis-server\n    networks:\n      - demo-network\n\n  redis-server:\n    image: redis:latest\n    networks:\n      - demo-network\n  client:\n    image: busybox\n    command: /bin/sh  -c   "while true; do sleep 3600; done" \n    networks:\n      - demo-network\n\nnetworks:\n  demo-network:\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 docker-compose  up  -d   --build   --scale   flask = 3  \n \n 1 \n  \n ping  flask\n #busyboxcurlwget \n wget  -O- flask:5000\n #image \nxiaopeng163/net-box:latest\n \n 1 2 3 4 5 \n wget -O- \'-\'file \n \n \n \n flask \n nginx \n nginx.conf  \n server {\n  listen  80 default_server;\n  location / {\n    proxy_pass http://flask:5000;\n  }\n}\n \n 1 2 3 4 5 6 yml \n version :   "3.8" \n\n services : \n   flask : \n     build : \n       context :  ../flask\n       dockerfile :  Dockerfile\n     image :  flask - demo : latest\n     environment : \n       -  REDIS_HOST=redis - server\n     depends_on : \n       -  redis - server\n     networks : \n       -  frontend\n       -  backend\n\n   redis-server : \n     image :  redis : latest\n     networks : \n       -  backend\n   nginx : \n     image :  nginx : stable - alpine\n     ports : \n       -  8000 : 80 \n     depends_on : \n       -  flask\n     volumes : \n       -  ../nginx/nginx.conf : /etc/nginx/conf.d/default.conf : ro\n       -  ../log/nginx : /var/log/nginx\n     networks : \n       -  frontend\n\n networks : \n     backend : \n     frontend : \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \n nginxflaskflaskredisdepend_on \n  ports  expose  expose  \n \n .dockerignore \n .env\n.dockerignore\nDockerfile\ndocker-compose.yaml\nnginx.conf\nvar\n \n 1 2 3 4 5 6 docker-compose  config\n docker-compose  up  -d   --build   --scale   flask = 3 \n \n 1 2 \n docker compose  \n redis.env.env.dockerignore \n .env \n REDIS_PASSWORD = ABC123\n \n 1 yml \n version :   "3.8" \n\n services : \n   flask : \n     build : \n       context :  ../flask\n       dockerfile :  Dockerfile\n     image :  flask - demo : latest\n     environment : \n       -  REDIS_HOST=redis - server\n       -  REDIS_PASS=$ { REDIS_PASSWORD } \n     networks : \n       -  frontend\n       -  backend\n\n   redis-server : \n     image :  redis : latest\n     command :  redis - server  - - requirepass $ { REDIS_PASSWORD } \n     networks : \n       -  backend\n   nginx : \n     image :  nginx : stable - alpine\n     ports : \n       -  8000 : 80 \n     depends_on : \n       -  flask\n     volumes : \n       -  ../nginx/nginx.conf : /etc/nginx/conf.d/default.conf : ro\n       -  ../log/nginx : /var/log/nginx\n     networks : \n       -  frontend\n\n networks : \n     backend : \n     frontend : \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 app.py \n from  flask  import  Flask\n from  redis  import  StrictRedis\n import  os\n import  socket\n\napp  =  Flask ( __name__ ) \nredis  =  StrictRedis ( host = os . environ . get ( \'REDIS_HOST\' ,   \'127.0.0.1\' ) , \n                    port = 6379 ,  password = os . environ . get ( \'REDIS_PASS\' ) ) \n\n\n @app . route ( \'/\' ) \n def   hello ( ) : \n    redis . incr ( \'hits\' ) \n     return   f"Hello Container World! I have been seen  { redis . get ( \'hits\' ) . decode ( \'utf-8\' ) }  times and my hostname is  { socket . gethostname ( ) } .\\n" \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n docker compose  \n Dockerfile healthcheck https://docs.docker.com/engine/reference/builder/#healthcheck \n docker compose https://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck \n  up \n  \n Dockerfiledocker container run  \n --health-cmd string              Command to run to check health\n--health-interval duration       Time between running the check\n                                (ms|s|m|h) (default 0s)\n--health-retries int             Consecutive failures needed to\n                                report unhealthy\n--health-start-period duration   Start period for the container to\n                                initialize before starting\n                                health-retries countdown\n                                (ms|s|m|h) (default 0s)\n--health-timeout duration        Maximum time to allow one check to\n \n 1 2 3 4 5 6 7 8 9 10 Dockerfile healthcheck \n flaskDockerfile \n FROM  python:3.9.5-slim \n\n RUN  pip install flask redis &&  \\ \n    apt-get update &&  \\ \n    apt-get install -y curl &&  \\ \n    groupadd -r flask && useradd -r -g flask flask &&  \\ \n    mkdir /src &&  \\ \n    chown -R flask:flask /src \n\n USER  flask \n\n COPY  app.py /src/app.py \n\n WORKDIR  /src \n\n ENV  FLASK_APP=app.py REDIS_HOST=redis \n\n EXPOSE  5000 \n\n HEALTHCHECK   --interval = 30s   --timeout = 3s   \\ \n     CMD  curl -f http://localhost:5000/ || exit 1 \n\n CMD  [ "flask" ,  "run" ,  "-h" ,  "0.0.0.0" ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 DockerfiliHEALTHCHECK  301 \n  \n $ docker container ls\nCONTAINER ID   IMAGE        COMMAND                  CREATED       STATUS                            PORTS      NAMES\n059c12486019   flask-demo   "flask run -h 0.0.0.0"   4 hours ago   Up 8 seconds (health: starting)   5000/tcp   dazzling_tereshkova\n \n 1 2 3 docker container inspect 059  health \n "Health": {\n"Status": "starting",\n"FailingStreak": 1,\n"Log": [\n    {\n        "Start": "2021-07-14T19:04:46.4054004Z",\n        "End": "2021-07-14T19:04:49.4055393Z",\n        "ExitCode": -1,\n        "Output": "Health check exceeded timeout (3s)"\n    }\n]\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 redis3healthstarting unhealthy \n \n redisflask healthy \n \n docker compose health \n Dockerfiledocker-compose.yml \n version :   "3.8" \n\n services : \n   flask : \n     build : \n       context :  ../flask\n       dockerfile :  Dockerfile\n     image :  flask - demo : latest\n     environment : \n       -  REDIS_HOST=redis - server\n     healthcheck : \n       test :   [ "CMD" ,   "curl" ,   "-f" ,   "http://localhost:5000" ] \n       interval :  30s\n       timeout :  3s\n       retries :   3 \n       start_period :  40s\n     depends_on : \n       redis-server : \n         condition :  service_healthy\n     networks : \n       -  frontend\n       -  backend\n\n   redis-server : \n     image :  redis : latest\n     healthcheck : \n       test :   [ "CMD" ,   "redis-cli" ,   "ping" ] \n       interval :  1s\n       timeout :  3s\n       retries :   30 \n     networks : \n       -  backend\n   nginx : \n     image :  nginx : stable - alpine\n     ports : \n       -  8000 : 80 \n     depends_on : \n       flask : \n         condition :  service_healthy\n     volumes : \n       -  ../nginx/nginx.conf : /etc/nginx/conf.d/default.conf : ro\n       -  ../log/nginx : /var/log/nginx\n     networks : \n       -  frontend\n\n networks : \n     backend : \n     frontend : \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 \n hadoop \n  docker-compose  Hadoop  -  -  (cnblogs.com) \n zookeeper 3.8.0mysql5.7hadoop3.3.5hive3.1.3spark 3.3.2Flink1.17.0 \n 1 \n git clone https://gitee.com/hadoop-bigdata/docker-compose-hadoop.git\n \n 1 #  2 \nhadoop_networkhs2\ndocker network create hadoop-network\n\ndocker network ls\n \n 1 2 3 4 5 #  3 mysql5.7 \n cd docker-compose-hadoop/mysql\n\ndocker-compose -f mysql-compose.yaml up -d\n\ndocker-compose -f mysql-compose.yaml ps\n\n#root 123456\ndocker exec -it mysql mysql -uroot -p123456\n \n 1 2 3 4 5 6 7 8 #  4 Hadoop Hive \n cd docker-compose-hadoop/hadoop_hive\n\ndocker-compose -f docker-compose.yaml up -d\n\ndocker-compose -f docker-compose.yaml ps\n\ndocker ps --format "table {{.ID}}\\t{{.Names}}\\t{{.Ports}}"\nhive\ndocker exec -it hive-hiveserver2 hive -e "show databases";\nhiveserver2\ndocker exec -it hive-hiveserver2 beeline -u jdbc:hive2://hive-hiveserver2:10000  -n hadoop -e "show databases;"\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 5.hdfs UI  yarn UI http: //ip:30070 http: //ip:30889 \n 6.yarn uihadoop-yarn-nmdocker-compose.yaml \n < property > \n                 < name > yarn.web-proxy.address </ name > \n                 < value > container1:9111 </ value > \n         </ property > \n\n \n 1 2 3 4 5 \n 8.Docker Swarm \n docker swarm  \n docker-compose \n \n  \n scale \n  \n  \n Key \n  \n \n  swarm \n \n Swarm \n Swarm  \n  \n docker engineswarm  \n docker  info\n \n 1 \n swarm \n \n swarmmanager \n swarm \n \n Swarm Commands:\nconfig      Manage Swarm configs\n node         Manage Swarm nodes\nsecret      Manage Swarm secrets\n service      Manage Swarm services\nstack       Manage Swarm stacks\nswarm       Manage Swarm\n\n\n\nUsage:   docker  swarm COMMAND\n\nManage Swarm\n\nCommands:\nca          Display and rotate the root CA\ninit        Initialize a swarm\n join         Join a swarm as a  node  and/or manager\njoin-token  Manage  join  tokens\nleave       Leave the swarm\nunlock      Unlock swarm\nunlock-key  Manage the unlock key\nupdate      Update the swarm\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n # \n docker  swarm init\n # \n docker   node   ls \n #node \n docker   node  inspect  < node id > \n \n 1 2 3 4 5 6 \n docker swarm init  \n PKI \n \n swarm \n manager \n tokens \n \n Raft \n RAFT \n \n http://thesecretlivesofdata.com/raft/ \n https://raft.github.io/ \n https://docs.docker.com/engine/swarm/raft/ \n \n  Raft  \n https://mp.weixin.qq.com/s/p8qBcIhM04REuQ-uG4gnbw \n swarm join-token join token\n docker  swarm join-token workerwoker\n docker  swarm join-token managermanager\n docker  swarm join-token  --rotate  workerwokerToken\n docker  swarm join-token  -q  workerToken\n \n 1 2 3 4 5 #worker  \n docker  swarm  join   --token  SWMTKN-1-3hocsrayh278lv0lz17g0ty5bxedekofilq3pbiaj9xskqcjmh-331n58myelyc6bej36wosd69v  192.168 .8.10:2377\n \n 1 2 \n #managernode \n docker   node   ls \n \n 1 2 \n # \n docker   service  create  --name  nginx  -d   --replicas   3  nginx:stable-alpine\n docker   service   ps  nginx\n \n 1 2 3 servicedocker swarmservice--replicas docker scale \n #scale  \n docker   service  scale  nginx = 2 \n \n 1 2 \n # \n docker   service  update  --image  nginx:latest nginx\n # \n docker   service  update   --rollback  nginx\n \n 1 2 3 4 \n Docker Stack \n docker docker-compose \n docker swarmdocker stackstack \n docker stackdocker-composestackcompose \n stack.envgithup \n docker stack deploy  in 1.13 doesn\'t load  .env  file as  docker-compose up  does  Issue #29133  moby/moby (github.com) \n env   $( cat  .env  |   grep  ^ [ A-Z ]   |   xargs )   docker  stack deploy  -c  stack.yaml STACK\n \n 1 k8s \n 9.Dockergitlab CICD \n  \n Dockerfile \n FROM  openjdk:8-jre-alpine \n VOLUME  /tmp \n ###app-springboot \n COPY  ./target/cicd-0.0.1-SNAPSHOT.jar app.jar \n\n ENTRYPOINT  [ "java" , "-Djava.security.egd=file:/dev/./urandom" , "-jar" , "/app.jar" ] \n \n 1 2 3 4 5 6 .gitlab-ci.yml \n  \n #1 \n #2 DOCKER_HOST \n #3maven  \n #4docker:dind docker  docker  \n   # docker  jenkins  docker  \n   # docker  docker  \n   # docker  docker-engine \n   #docker:dindAdockerBBhostAdocker daemon \n\n #5stages Gitlab CI Stages  Jobs  \n #6  Jobs  jar  maven  package script  \n #7 Jobs  Docker  deploy  build  runonly  master  \n\n image :  docker : latest   #1 \n variables :    #2 \n   DOCKER_DRIVER :  overlay2\n   DOCKER_HOST :  tcp : //192.168.8.10 : 2375    # docker host \n   DOCKER_TLS_CERTDIR :   \'\' \n   TAG :  root/hello - spring : v0.1   #  \n cache :    #3 \n   key :  m2 - repo\n   paths : \n     -  .m2/repository\n services :    #4 \n   -  docker : dind\n stages :    #5 \n   -  package\n   -  deploy\n maven-package :    #6 \n   image :  maven : 3.5.0 - jdk - 8 \n   tags : \n     -  maven\n   stage :  package\n   script : \n     -  mvn clean package  - Dmaven.test.skip=true\n   artifacts : \n     paths : \n       -  target/ *.jar \n build-master :    #7 \n   tags : \n     -  docker\n   stage :  deploy\n   script : \n     -  docker build  - t $TAG .\n     -  docker rm  - f test  | |  true\n     -  docker run  - d  - - name test  - p 8888 : 8888 $TAG\n   only : \n     -  master\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  \n #1 \n #2 DOCKER_HOST \n #3maven  \n #4docker:dind docker  docker  \n   # docker  jenkins  docker  \n   # docker  docker  \n   # docker  docker-engine \n   #docker:dindAdockerBBhostAdocker daemon \n\n #5stages Gitlab CI Stages  Jobs  \n #6  Jobs  jar  maven  package script  \n #7 Jobs  Docker  deploy  build  runonly  master  \n\nimage: docker:latest   #1 \nvariables:   #2 \n  DOCKER_DRIVER: overlay2\n  DOCKER_HOST: tcp://192.168.8.10:2375   # docker host \n  DOCKER_TLS_CERTDIR:  \'\' \n  USERSERVICE_TAG:  \':1.0\'    #  \n  USERSERVICE_NAME: cicd  # \n  USERSERVICE_RPORT:  8888   #  \n  USERSERVICE_DIRECTORY: cicd  # \n  PROFILE_ACTIVE: prd\ncache:   #3 \n  key: m2-repo\n  paths:\n    - .m2/repository\nservices:   #4 \n  - docker:dind\nstages:   #5 \n  - package\n  - deploy-dev\n  - deploy-prd\nmaven-package:   #6 \n  image: maven:3.5.0-jdk-8\n  tags:\n    - maven\n  stage: package\n  script:\n    - mvn clean package  -P   $PROFILE_ACTIVE   -Dmaven.test.skip = true\n  artifacts:\n    paths:\n      - target/*.jar\nbuild-dev:\n  tags:\n    -  docker \n  stage: deploy-dev\n  script:\n    -  docker  login  $CI_REGISTRY   -u   $CI_REGISTRY_USER   -p   $CI_REGISTRY_PASSWORD \n    -  docker   rm   -f   $USERSERVICE_NAME   ||   true \n    -  docker  rmi  -f   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG   ||   true \n    -  docker  build  -f  Dockerfile  -t   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG   . \n    -  docker  push  $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG \n    -  docker  run  -d   --name   $USERSERVICE_NAME   -v   $USERSERVICE_NAME :/apps  --privileged = true  -p   $USERSERVICE_RPORT : $USERSERVICE_RPORT   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG \n    -  docker  image prune  -f \n  only:  #(branches and tags) \n    refs:  # \n      - dev\n     #changes: #  \n - .gitlab-ci.yml \nbuild-prd:   #7 \n  tags:\n    -  docker \n  stage: deploy-prd\n  before_script:\n    -  docker  login  $CI_REGISTRY   -u   $CI_REGISTRY_USER   -p   $CI_REGISTRY_PASSWORD \n    -  docker  rmi  -f   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG   ||   true \n    -  docker  build  -f  Dockerfile  -t   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG   . \n    -  docker  push  $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG \n    -  docker  image prune  -f \n    -  \'which  ssh-agent || ( yum update -y  && yum install openssh-client git -y )\' \n    -  eval   $( ssh-agent  -s ) \n    -  echo   " $SSH_PRIVATE_KEY "   |   tr   -d   \'\\r\'   |  ssh-add -\n    -  mkdir   -p  ~/.ssh\n    -  chmod   700  ~/.ssh\n    - ssh-keyscan  192.168 .8.103  >>  ~/.ssh/known_hosts  #ssh-keyscan  SSH ssh-keyscan  SSH  ssh_known_hosts  \n    -  chmod   644  ~/.ssh/known_hosts\n  script:\n-tq  \n    -  ssh   -tq   192.168 .8.103  <<  EOF\n    -  docker  login  $CI_REGISTRY   -u   $CI_REGISTRY_USER   -p   $CI_REGISTRY_PASSWORD \n    -  docker   rm   -f   $USERSERVICE_NAME   ||   true \n    -  docker  rmi  -f   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG   ||   true \n    -  docker  run  -d   --name   $USERSERVICE_NAME   -v   $USERSERVICE_NAME :/apps  --privileged = true  -p   $USERSERVICE_RPORT : $USERSERVICE_RPORT   $CI_REGISTRY_IMAGE / $USERSERVICE_NAME - $PROFILE_ACTIVE $USERSERVICE_TAG \n    -  docker  image prune  -f \n    -  exit \n    - EOF\n  only:\n    variables:  [   $PROFILE_ACTIVE   ==   "prd"   ] \n    refs:\n      - master\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 #   \n .gitlab-ci.yml \n #stages: \n - triggers \n - triggers1 \n #trigger_a: \n stage: triggers \n trigger: \n   include: szbxl-service/szbxl-user-service/.gitlab-ci.yml \n rules: \n   - changes: \n       - szbxl-service/szbxl-user-service/* \n #trigger_b: \n stage: triggers1 \n trigger: \n   include: szbxl-getway/.gitlab-ci.yml \n rules: \n   - changes: \n       - szbxl-getway/* \n\n variables :   #2 \n   DOCKER_DRIVER :  overlay2\n   DOCKER_HOST :  tcp : //192.168.8.10 : 2375    # docker host \n   DOCKER_TLS_CERTDIR :   \'\' \n   PROFILE_ACTIVE :  prd  #,dev/prod,Gitlab \n   GIT_STRATEGY :  clone  #Initialized empty Git repository as it did in the first run of the first job, it now says Reinitialized existing Git repository. \n   #https://stackoverflow.com/questions/57779750/does-gitlab-runner-or-docker-cache-the-builds-directory-by-default \n.gitlab-ci.yml \n stages : \n   -  service_first - package\n   -  service_first - dev - deploy\n   -  service_first - prd - deploy\n   -  service_second - package\n   -  service_second - dev - deploy\n   -  service_second - prd - deploy\n include : \n   -   local :  service_first/.gitlab - ci.yml\n   -   local :  service_second/.gitlab - ci.yml\n\n #include \n   #gitlab-ci.ymlgitlab-ci.yml \n\n   #include:local \n   #include:local.yml .ymal \n   #include:project \n       #include:project gitlab gitlab-ci.yml \n       # \n\n       #projectgitlab \n       #file \n       #refmaster(main) \n   #include:remote \n     #include:remote URLgitlab-ci.yml \n     #HTTP/GTTPS GET \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  Dockerfile \n \nopenjdk:8-jre-alpine \n1easy-captcha \n2 DockerERROR: failed to launch: exec.d: failed to execute exec.d file at path \'/layers/paketo-buildpacks_bellsoft-liberica/helper/exec.d/memory-calculator\': exit status 1 \n FROM  openjdk:8-jre-alpine \n \n LABEL  author= "gordon" \n \n RUN  echo  "Asia/Shanghai"  > /etc/timezone \n \n ENV  MYPATH /apps/service_first/ \n \n RUN  mkdir -p  ${MYPATH} \n \n WORKDIR   ${MYPATH} \n, \n(/var/lib/docker/volumes/xxxxxxxxxxxxxxxx) \ndocker inspect szbxl-user-service \ndocker volume ls \n #VOLUME ${MYPATH} \nTomcat /tmp/var/lib/docker/tmp \n #VOLUME /tmp \n #VOLUME /log \njar,WORKDIR \n COPY  service_first/target/*.jar app.jar \n \n #EXPOSE 802 \n \n #ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","app.jar"] \n #ENTRYPOINT ["sh", "-c", "java --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/java.lang.reflect=ALL-UNNAMED -Djava.security.egd=file:/dev/./urandom -jar app.jar"] \n ENTRYPOINT  [ "java" , "-Djava.security.egd=file:/dev/./urandom" , "-jar" , "app.jar" ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 .gitlab-ci.yml \n #1 \n #2 DOCKER_HOST \n #3maven  \n #4docker:dind docker  docker  \n   # docker  jenkins  docker  \n   # docker  docker  \n   # docker  docker-engine \n   #docker:dindAdockerBBhostAdocker daemon \n\n #5stages Gitlab CI Stages  Jobs  \n #6  Jobs  jar  maven  package script  \n #7 Jobs  Docker  deploy  build  runonly  master  \n\n image :  docker : latest   #1 \n variables :    #2 \n   # \n   #DOCKER_DRIVER: overlay2 \n   #DOCKER_HOST: tcp://192.168.8.10:2375  # docker host \n   #DOCKER_TLS_CERTDIR: \'\' \n   #PROFILE_ACTIVE: prd \n   FIRST_TAG :   \':1.0\'    #  \n   FIRST_NAME :  service_first  # \n   FIRST_RPORT :   8888   #  \n   FIRST_DIRECTORY :  service_first  # \n\n cache :    #3 \n   key :  m2 - repo\n   paths : \n     -  .m2/repository\n services :    #4 \n   -  docker : dind\n stages :    #5 \n   -  service_first - package\n   -  service_first - dev - deploy\n   -  service_first - prd - deploy\n maven-package:service_first :    #6 \n   image :  maven : 3.5.0 - jdk - 8 \n   tags : \n     -  maven\n   stage :  service_first - package\n   script : \n     #mvn clean package -pl service_first -am -Dmaven.test.skip=true -pl --projects <arg> -am --also-make  \n     -  mvn clean package  - P $PROFILE_ACTIVE  - Dmaven.test.skip=true  - pl $FIRST_DIRECTORY  - am\n   artifacts : \n     paths : \n       -  $FIRST_DIRECTORY/target/ *.jar \n build-dev:service_first : \n   tags : \n     -  docker\n   stage :  service_first - dev - deploy\n   script : \n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rm  - f $FIRST_NAME  | |  true\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG  | |  true\n     -  docker build  - f $FIRST_DIRECTORY/Dockerfile  - t $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG .\n     -  docker push $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG\n     -  docker run  - d  - - name $FIRST_NAME  - v /apps/$FIRST_DIRECTORY : /apps/$FIRST_DIRECTORY/log  - - privileged=true  - p $FIRST_RPORT : $FIRST_RPORT $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG\n     -  docker image prune  - f\n   only :   #(branches and tags) \n     refs :   # \n       -  dev\n     #changes: #  \n - .gitlab-ci.yml \n build-prd:service_first :   #7 \n   tags : \n     -  docker\n   stage :  service_first - prd - deploy\n   before_script : \n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG  | |  true\n     -  docker build  - f $FIRST_DIRECTORY/Dockerfile  - t $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG .\n     -  docker push $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG\n     -  docker image prune  - f\n     -   \'which  ssh-agent || ( yum update -y  && yum install openssh-client git -y )\' \n     -  eval $(ssh - agent  - s)\n     -  echo "$SSH_PRIVATE_KEY"  |  tr  - d \'\\r\'  |  ssh - add  - \n     -  mkdir  - p ~/.ssh\n     -  chmod 700 ~/.ssh\n     -  ssh - keyscan 192.168.8.103  > >  ~/.ssh/known_hosts  #ssh-keyscan  SSH ssh-keyscan  SSH  ssh_known_hosts  \n     -  chmod 644 ~/.ssh/known_hosts\n   script : \n-tq  \n     -  ssh  - tq 192.168.8.103 << EOF\n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rm  - f $FIRST_NAME  | |  true\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG  | |  true\n     -  docker run  - d  - - name $FIRST_NAME  - v /apps/$FIRST_DIRECTORY : /apps/$FIRST_DIRECTORY/log  - - privileged=true  - p $FIRST_RPORT : $FIRST_RPORT $CI_REGISTRY_IMAGE/$FIRST_NAME - $PROFILE_ACTIVE$FIRST_TAG\n     -  docker image prune  - f\n     -  exit\n     -  EOF\n   only : \n     variables :   [  $PROFILE_ACTIVE == "prd"  ] \n     refs : \n       -  main\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94  Dockerfile \n \nopenjdk:8-jre-alpine \n1easy-captcha \n2 DockerERROR: failed to launch: exec.d: failed to execute exec.d file at path \'/layers/paketo-buildpacks_bellsoft-liberica/helper/exec.d/memory-calculator\': exit status 1 \n FROM  openjdk:8-jre-alpine \n \n LABEL  author= "gordon" \n \n RUN  echo  "Asia/Shanghai"  > /etc/timezone \n \n ENV  MYPATH /apps/service_second/ \n \n RUN  mkdir -p  ${MYPATH} \n \n WORKDIR   ${MYPATH} \n, \n(/var/lib/docker/volumes/xxxxxxxxxxxxxxxx) \ndocker inspect szbxl-user-service \ndocker volume ls \n #VOLUME ${MYPATH} \nTomcat /tmp/var/lib/docker/tmp \n #VOLUME /tmp \n #VOLUME /log \njar \n COPY  service_second/target/*.jar app.jar \n \n #EXPOSE 802 \n \n #ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","app.jar"] \n #ENTRYPOINT ["sh", "-c", "java --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/java.lang.reflect=ALL-UNNAMED -Djava.security.egd=file:/dev/./urandom -jar app.jar"] \n ENTRYPOINT  [ "java" , "-Djava.security.egd=file:/dev/./urandom" , "-jar" , "app.jar" ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 .gitlab-ci.yml \n #1 \n #2 DOCKER_HOST \n #3maven  \n #4docker:dind docker  docker  \n   # docker  jenkins  docker  \n   # docker  docker  \n   # docker  docker-engine \n   #docker:dindAdockerBBhostAdocker daemon \n\n #5stages Gitlab CI Stages  Jobs  \n #6  Jobs  jar  maven  package script  \n #7 Jobs  Docker  deploy  build  runonly  master  \n\n image :  docker : latest   #1 \n variables :    #2 \n   # \n   #DOCKER_DRIVER: overlay2 \n   #DOCKER_HOST: tcp://192.168.8.10:2375  # docker host \n   #DOCKER_TLS_CERTDIR: \'\' \n   #PROFILE_ACTIVE: prd \n   SECOND_TAG :   \':1.0\'    #  \n   SECOND_NAME :  service_second  # \n   SECOND_RPORT :   9999   #  \n   SECOND_DIRECTORY :  service_second  # \n\n cache :    #3 \n   key :  m2 - repo\n   paths : \n     -  .m2/repository\n services :    #4 \n   -  docker : dind\n stages :    #5 \n   -  service_second - package\n   -  service_second - dev - deploy\n   -  service_second - prd - deploy\n maven-package:service_second :    #6 \n   image :  maven : 3.5.0 - jdk - 8 \n   tags : \n     -  maven\n   stage :  service_second - package\n   script : \n     #mvn clean package -pl service_first -am -Dmaven.test.skip=true -pl --projects <arg> -am --also-make  \n     -  mvn clean package  - P $PROFILE_ACTIVE  - Dmaven.test.skip=true  - pl $SECOND_DIRECTORY  - am\n   artifacts : \n     paths : \n       -  $SECOND_DIRECTORY/target/ *.jar \n build-dev:service_second : \n   tags : \n     -  docker\n   stage :  service_second - dev - deploy\n   script : \n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rm  - f $SECOND_NAME  | |  true\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG  | |  true\n     -  docker build  - f $SECOND_DIRECTORY/Dockerfile  - t $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG .\n     -  docker push $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG\n     -  docker run  - d  - - name $SECOND_NAME  - v /apps/$SECOND_DIRECTORY : /apps/$SECOND_DIRECTORY/log  - - privileged=true  - p $SECOND_RPORT : $SECOND_RPORT $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG\n     -  docker image prune  - f\n   only :   #(branches and tags) \n     refs :   # \n       -  dev\n     #changes: #  \n - .gitlab-ci.yml \n build-prd:service_second :   #7 \n   tags : \n     -  docker\n   stage :  service_second - prd - deploy\n   before_script : \n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG  | |  true\n     -  docker build  - f $SECOND_DIRECTORY/Dockerfile  - t $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG .\n     -  docker push $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG\n     -  docker image prune  - f\n     -   \'which  ssh-agent || ( yum update -y  && yum install openssh-client git -y )\' \n     -  eval $(ssh - agent  - s)\n     -  echo "$SSH_PRIVATE_KEY"  |  tr  - d \'\\r\'  |  ssh - add  - \n     -  mkdir  - p ~/.ssh\n     -  chmod 700 ~/.ssh\n     -  ssh - keyscan 192.168.8.103  > >  ~/.ssh/known_hosts  #ssh-keyscan  SSH ssh-keyscan  SSH  ssh_known_hosts  \n     -  chmod 644 ~/.ssh/known_hosts\n   script : \n-tq  \n     -  ssh  - tq 192.168.8.103 << EOF\n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rm  - f $SECOND_NAME  | |  true\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG  | |  true\n     -  docker run  - d  - - name $SECOND_NAME  - v /apps/$SECOND_DIRECTORY : /apps/$SECOND_DIRECTORY/log  - - privileged=true  - p $SECOND_RPORT : $SECOND_RPORT $CI_REGISTRY_IMAGE/$SECOND_NAME - $PROFILE_ACTIVE$SECOND_TAG\n     -  docker image prune  - f\n     -  exit\n     -  EOF\n   only : \n     variables :   [  $PROFILE_ACTIVE == "prd"  ] \n     refs : \n       -  main\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94  \n  \n \n #daemon \nsystemctl daemon-reload\n \n 1 2  \n The image pull policy: never, if-not-present or always (default). \n neverif-not-present always  \n \n vi  /etc/gitlab-runner/config.toml     \npull_policy  =   "if-not-present"  \n \n 1 2 3 \n \n maven \n mavenrunner \n \n [ [ runners ] ] \n  name  =   "Docker Maven Runner" \n  url  =   "http://192.168.8.10:2280" \n  token  =   "mPciztaPrpZsYSsWuRAX" \n  executor  =   "docker" \n   [ runners.custom_build_dir ] \n   [ runners.cache ] \n     [ runners.cache.s3 ] \n     [ runners.cache.gcs ] \n     [ runners.cache.azure ] \n   [ runners.docker ] \n    tls_verify  =   false \n    image  =   "maven:3.5.0-jdk-8" \n    privileged  =   true \n    pull_policy  =   "if-not-present" \n    disable_entrypoint_overwrite  =   false \n    oom_kill_disable  =   false \n    disable_cache  =   false \n    volumes  =   [ "/cache" ,  "/export/server/bigdata-stack/maven/apache-maven-3.9.2:/root/.m2" ] \n    shm_size  =   0 \n\n [ [ runners ] ] \n  name  =   "Docker Build Runner" \n  url  =   "http://192.168.8.10:2280" \n  token  =   "pRuA3NL6tsJsBgq-Ps65" \n  executor  =   "docker" \n   [ runners.custom_build_dir ] \n   [ runners.cache ] \n     [ runners.cache.s3 ] \n     [ runners.cache.gcs ] \n     [ runners.cache.azure ] \n   [ runners.docker ] \n    tls_verify  =   false \n    image  =   "docker:latest" \n    privileged  =   true \n    pull_policy  =   "if-not-present" \n    disable_entrypoint_overwrite  =   false \n    oom_kill_disable  =   false \n    disable_cache  =   false \n    volumes  =   [ "/cache" ,  "/export/server/bigdata-stack/maven/apache-maven-3.9.2:/root/.m2" ] \n    shm_size  =   0 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 \n 10.Docker Sonar, \n .yml \n #1 \n #2 DOCKER_HOST \n #3maven  \n #4docker:dind docker  docker  \n # docker  jenkins  docker  \n # docker  docker  \n # docker  docker-engine \n #docker:dindAdockerBBhostAdocker daemon \n\n #5stages Gitlab CI Stages  Jobs  \n #6  Jobs  jar  maven  package script  \n #7 Jobs  Docker  deploy  build  runonly  master  \n\n image :  docker : latest   #1 \n variables :    #2 \n   DOCKER_DRIVER :  overlay2\n   DOCKER_HOST :  tcp : //192.168.8.10 : 2375    # docker host \n   DOCKER_TLS_CERTDIR :   \'\' \n   USERSERVICE_TAG :   \':1.0\'    #  \n   USERSERVICE_NAME :  cicd  # \n   USERSERVICE_RPORT :   6666   #  \n   USERSERVICE_DIRECTORY :  cicd  # \n   PROFILE_ACTIVE :  prd\nsonner scanner  \n   SCANNER_HOME :   "/export/server/sonar-scanner" \n \n   SCAN_DIR :   "src" \n cache :    #3 \n   key :  m2 - repo\n   paths : \n     -  .m2/repository\n services :    #4 \n   -  docker : dind\n stages :    #5 \n   -  package\n   -  sonarqube - check\n   -  deploy - dev\n   -  deploy - prd\n maven-package :    #6 \n   image :  maven : 3.5.0 - jdk - 8 \n   tags : \n     -  maven\n   stage :  package\n   script : \n     -  mvn clean org.jacoco : jacoco - maven - plugin : prepare - agent install dependency : copy - dependencies\n       - Dmaven.test.failure.ignore=true package  - P $PROFILE_ACTIVE\nMavenJar \n   artifacts : \n     expire_in :  1 days\n     paths : \n       -  target/\n sonarqube-check : \n   image : \n     name :  sonarsource/sonar - scanner - cli : latest\n     entrypoint :   [ "" ] \n   tags : \n     -  sonar\n   stage :  sonarqube - check\n   needs :   [ maven - package ] \n   dependencies : \n     -  maven - package\n   variables : \n     SONAR_USER_HOME :   "${CI_PROJECT_DIR}/.sonar"    # Defines the location of the analysis task cache \n     GIT_DEPTH :   "0"    # Tells git to fetch all the branches of the project, required by the analysis task \n   cache : \n     key :   "${CI_JOB_NAME}" \n     paths : \n       -  .sonar/cache\n   script : \n     -  sonar - scanner\n       - Dsonar.projectKey=sonarqube - check\n       - Dsonar.sources=src/main/\n       - Dsonar.tests=src/test/\n       - Dsonar.coverage.exclusions=src/main/java/com/gordon/cicd/CicdApplication.java\n       - Dsonar.scm.disabled=true\n       - Dsonar.language=java\n       - Dsonar.sourceEncoding=UTF - 8 \n       - Dsonar.host.url=http : //192.168.8.10 : 9000 \n       - Dsonar.login=b95cb41cb57230c28a79a177a9f0fc3bddf02a79\n       - Dsonar.java.binaries=target/classes\n       - Dsonar.java.test.binaries=target/test - classes\n       - Dsonar.java.libraries=target/dependency\n       - Dsonar.java.test.libraries=target/dependency\n       - Dsonar.java.surefire.report=target/surefire - reports\n       - Dsonar.coverage.jacoco.xmlReportPaths=target/site/jacoco/jacoco.xml\n     # \n     -  awk  - F" , " \' {  instructions += $4 + $5; covered += $5  }  END  {  print 100  *covered/instructions ,   "% covered"   } \' target/site/jacoco/jacoco.csv\n\n   allow_failure :   false \n   only : \n     -  merge_requests\n     -  master  # or the name of your main branch \n     -  develop\n   artifacts : \n     paths : \n       -  target/\n\n build-dev : \n   tags : \n     -  docker\n   stage :  deploy - dev\n   needs :   [ sonarqube - check ] \n   script : \n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rm  - f $USERSERVICE_NAME  | |  true\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG  | |  true\n     -  docker build  - f Dockerfile  - t $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG .\n     -  docker push $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG\n     -  docker run  - d  - - name $USERSERVICE_NAME  - v $USERSERVICE_NAME : /apps  - - privileged=true  - p $USERSERVICE_RPORT : $USERSERVICE_RPORT $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG\n     -  docker image prune  - f\n   only :   #(branches and tags) \n     refs :   # \n       -  dev\n     #changes: #  \n - .gitlab-ci.yml \n build-prd :    #7 \n   tags : \n     -  docker\n   stage :  deploy - prd\n   needs :   [ sonarqube - check ] \n   before_script : \n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG  | |  true\n     -  docker build  - f Dockerfile  - t $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG .\n     -  docker push $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG\n     -  docker image prune  - f\n     -   \'which  ssh-agent || ( yum update -y  && yum install openssh-client git -y )\' \n     -  eval $(ssh - agent  - s)\n     -  echo "$SSH_PRIVATE_KEY"  |  tr  - d \'\\r\'  |  ssh - add  - \n     -  mkdir  - p ~/.ssh\n     -  chmod 700 ~/.ssh\n     -  ssh - keyscan 192.168.8.103  > >  ~/.ssh/known_hosts  #ssh-keyscan  SSH ssh-keyscan  SSH  ssh_known_hosts  \n     -  chmod 644 ~/.ssh/known_hosts\n   script : \n-tq  \n     -  ssh  - tq 192.168.8.103 << EOF\n     -  docker login $CI_REGISTRY  - u $CI_REGISTRY_USER  - p $CI_REGISTRY_PASSWORD\n     -  docker rm  - f $USERSERVICE_NAME  | |  true\n     -  docker rmi  - f $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG  | |  true\n     -  docker run  - d  - - name $USERSERVICE_NAME  - v $USERSERVICE_NAME : /apps  - - privileged=true  - p $USERSERVICE_RPORT : $USERSERVICE_RPORT $CI_REGISTRY_IMAGE/$USERSERVICE_NAME - $PROFILE_ACTIVE$USERSERVICE_TAG\n     -  docker image prune  - f\n     -  exit\n     -  EOF\n   only : \n     variables :   [  $PROFILE_ACTIVE == "prd"  ] \n     refs : \n       -  master\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 \n \n readmepomjacocoexclude \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n     < parent > \n         < groupId > org.springframework.boot </ groupId > \n         < artifactId > spring-boot-starter-parent </ artifactId > \n         < version > 2.0.1.RELEASE </ version > \n         < relativePath />   \x3c!-- lookup parent from repository --\x3e \n     </ parent > \n\n     < groupId > com.gordon </ groupId > \n     < artifactId > cicd </ artifactId > \n     < version > 0.0.1-SNAPSHOT </ version > \n     < packaging > jar </ packaging > \n     < name > cicd </ name > \n     < description > Demo project for Spring Boot </ description > \n\n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n     </ properties > \n\n     < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-web </ artifactId > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n\n     </ dependencies > \n\n\n     < build > \n         < pluginManagement > \n             < plugins > \n                 < plugin > \n                     < groupId > org.sonarsource.scanner.maven </ groupId > \n                     < artifactId > sonar-maven-plugin </ artifactId > \n                     < version > 3.7.0.1746 </ version > \n                 </ plugin > \n             </ plugins > \n         </ pluginManagement > \n         < plugins > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.1 </ version > \n                 < configuration > \n                     < source > ${java.version} </ source > \n                     < target > ${java.version} </ target > \n                 </ configuration > \n             </ plugin > \n                 \x3c!----\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-surefire-plugin </ artifactId > \n                 < version > 2.21.0 </ version > \n                 < configuration > \n                     < testFailureIgnore > true </ testFailureIgnore > \n                     < argLine > @{argLine} -Dfile.encoding=utf-8 </ argLine > \n                 </ configuration > \n             </ plugin > \n             < plugin > \n                 < groupId > org.jacoco </ groupId > \n                 < artifactId > jacoco-maven-plugin </ artifactId > \n                 < version > 0.8.7 </ version > \n                 < configuration > \n                     < excludes > \n                         < exclude > **/CicdApplication.class </ exclude > \n                     </ excludes > \n                 </ configuration > \n                 < executions > \n                     < execution > \n                         < id > prepare-agent </ id > \n                         < goals > \n                             < goal > prepare-agent </ goal > \n                         </ goals > \n                     </ execution > \n                     < execution > \n                         < id > report </ id > \n                         < goals > \n                             < goal > report </ goal > \n                         </ goals > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n\n\n         </ plugins > \n     </ build > \n\n </ project > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100  \n \\ d+. \\ d+  \\ % covered\n \n 1 \n \n  \n  \n |    .gitlab-ci.yml\n |    pom.xml\n |    README.md\n+---report-aggregate\n |        .gitlab-ci.yml\n |        pom.xml\n | \n+---service_first\n |     |    .gitlab-ci.yml\n |     |    Dockerfile\n |     |    pom.xml\n |     | \n |     \\ ---src\n |        +---main\n |         |    +---java\n |         |     |     \\ ---com\n |         |     |         \\ ---gordon\n |         |     |             |    FirstApplication.java\n |         |     |             | \n |         |     |             \\ ---controller\n |         |     |                    TestController.java\n |         |     | \n |         |     \\ ---resources\n |         |            application.properties\n |         | \n |         \\ ---test\n |             \\ ---java\n |                 \\ ---com\n |                     \\ ---gordon\n |                         \\ ---controller\n |                                TestControllerTest.java\n | \n \\ ---service_second\n     |    .gitlab-ci.yml\n     |    Dockerfile\n     |    pom.xml\n     | \n     \\ ---src\n        +---main\n         |    +---java\n         |     |     \\ ---com\n         |     |         \\ ---gordon\n         |     |             |    SecondApplication.java\n         |     |             | \n         |     |             \\ ---controller\n         |     |                    TestController.java\n         |     | \n         |     \\ ---resources\n         |            application.properties\n         | \n         \\ ---test\n             \\ ---java\n                 \\ ---com\n                     \\ ---gordon\n                         \\ ---controller\n                                TestControllerTest.java\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  \n pom \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 " \n          xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < parent > \n         < artifactId > cicd_muil </ artifactId > \n         < groupId > com.gordon </ groupId > \n         < version > 1.0-SNAPSHOT </ version > \n     </ parent > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     < artifactId > report-aggregate </ artifactId > \n     < version > 1.0-SNAPSHOT </ version > \n\n     < dependencies > \n         < dependency > \n             < groupId > com.gordon </ groupId > \n             < artifactId > service_first </ artifactId > \n             < version > 1.0-SNAPSHOT </ version > \n         </ dependency > \n         < dependency > \n             < groupId > com.gordon </ groupId > \n             < artifactId > service_second </ artifactId > \n             < version > 1.0-SNAPSHOT </ version > \n         </ dependency > \n     </ dependencies > \n\n     < build > \n         < plugins > \n             < plugin > \n                 < groupId > org.jacoco </ groupId > \n                 < artifactId > jacoco-maven-plugin </ artifactId > \n                 < version > 0.8.7 </ version > \n                 < configuration > \n                     < excludes > \n                         < exclude > **/FirstApplication.class </ exclude > \n                         < exclude > **/SecondApplication.class </ exclude > \n                     </ excludes > \n                 </ configuration > \n                 < executions > \n                     < execution > \n                         < id > jacoco-report-aggregate </ id > \n                         \x3c!-- maven  --\x3e \n                         < phase > install </ phase > \n                         < goals > \n                             < goal > report-aggregate </ goal > \n                         </ goals > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n         </ plugins > \n     </ build > \n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 .gitlab-ci.yml \n image :  docker : latest   #1 \n variables :    #2 \n   # \n   #DOCKER_DRIVER: overlay2 \n   #DOCKER_HOST: tcp://192.168.8.10:2375  # docker host \n   #DOCKER_TLS_CERTDIR: \'\' \n   #PROFILE_ACTIVE: prd \n   REPORT_NAME :  report - aggregate  # \n   REPORT_DIRECTORY :  report - aggregate  # \n\n cache :    #3 \n   key :  m2 - repo\n   paths : \n     -  .m2/repository\n services :    #4 \n   -  docker : dind\n stages :    #5 \n   -  report - aggregate\n\n maven-package:report-aggregate :    #6 \n   image :  maven : 3.5.0 - jdk - 8 \n   tags : \n     -  maven\n   stage :  report - aggregate\n   script : \n     #mvn clean package -pl service_first -am -Dmaven.test.skip=true -pl --projects <arg> -am --also-make  \n     -  mvn clean install dependency : copy - dependencies  - Dmaven.test.failure.ignore=true  - pl $REPORT_DIRECTORY   - am\n     -  awk  - F" , " \' {  instructions += $4 + $5; covered += $5  }  END  {  print 100  *covered/instructions ,   "% covered"   } \' $REPORT_DIRECTORY/target/site/jacoco - aggregate/jacoco.csv\n   artifacts : \n     paths : \n       -  $REPORT_DIRECTORY/target/\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 pipeline \n \n test \n \n'},{title:"RPC",frontmatter:{title:"RPC",date:"2022-10-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:[""]},regularPath:"/%E5%85%B6%E4%BB%96/RPC.html",relativePath:"/RPC.md",key:"v-700a68a1",path:"/2022/10/08/rpc/",headers:[{level:2,title:"1.",slug:"_1--"},{level:2,title:"2.",slug:"_2-"},{level:2,title:"3. RPC ",slug:"_3--rpc-"},{level:3,title:"3.1 Dubbo",slug:"_3-1-dubbo"},{level:3,title:"https://github.com/YClimb/redis-demo",slug:"https-github-com-yclimb-redis-demo"},{level:3,title:"3.2 BRPC",slug:"_3-2-brpc"}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:' 1. \n  \n (ORM) \n   \n (MVC) \n   \n (RPC) \n  \n (SOA) \n (SOA) \n  \n    (ORM)     Web(MVC)     (RPC)     (SOA)  \n \n RPC()()()RPC \n 2. \n RPC \n (1). \n  . \n  \n public   interface   HelloService   { \n     String   hello ( String  name ) ; \n     String   hi ( String  msg ) ; \n } \n \n 1 2 3 4  \n public   class   HelloServiceImpl   implements   HelloService { \n     @Override \n     public   String   hello ( String  name )   { \n         return   "Hello "   +  name ; \n     } \n \n     @Override \n     public   String   hi ( String  msg )   { \n         return   "Hi, "   +  msg ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 RPC \n public   class   RpcProvider   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         HelloService  service  =   new   HelloServiceImpl ( ) ; \n         // RPC \n         RpcFramework . export ( service ,   1234 ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 (2). \n \n  \n public   interface   HelloService   { \n   String   hello ( String  name ) ; \n   String   hi ( String  msg ) ; \n } \n \n 1 2 3 4 RPCRPC \n  \n public   class   RpcConsumer   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         // RpcFrameworkHelloService \n         HelloService  service  =   RpcFramework . refer ( HelloService . class ,   "127.0.0.1" ,   1234 ) ; \n         String  hello  =  service . hello ( "World" ) ; \n         System . out . println ( "  "   +  hello ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 (3).RPC \n RPC \n public   class   RpcFramework   { \n     /**\n     * \n     *\n     * @param service \n     * @param port    \n     * @throws Exception\n     */ \n     public   static   void   export ( final   Object  service ,   int  port )   throws   Exception   { \n         if   ( service  ==   null )   { \n             throw   new   IllegalArgumentException ( "service instance == null" ) ; \n         } \n         if   ( port  <=   0   ||  port  >   65535 )   { \n             throw   new   IllegalArgumentException ( "Invalid port "   +  port ) ; \n         } \n         System . out . println ( "Export service "   +  service . getClass ( ) . getName ( )   +   " on port "   +  port ) ; \n         // Socket \n         ServerSocket  server  =   new   ServerSocket ( port ) ; \n         for   ( ;   ;   )   { \n             try   { \n                 // Socket \n                 final   Socket  socket  =  server . accept ( ) ; \n                 new   Thread ( new   Runnable ( )   { \n                     @Override \n                     public   void   run ( )   { \n                         try   { \n                             try   { \n                                 /* Server*/ \n                                 //  \n                                 ObjectInputStream  input  =   new   ObjectInputStream ( \n                                    socket . getInputStream ( ) ) ; \n                                 try   { \n \n                                     System . out . println ( "\\nServer  " ) ; \n                                     String  methodName  =  input . readUTF ( ) ; \n                                     System . out . println ( "methodName : "   +  methodName ) ; \n                                     //  \n                                     Class < ? > [ ]  parameterTypes  =   ( Class < ? > [ ] ) input . readObject ( ) ; \n                                     System . out . println ( \n                                         "parameterTypes : "   +   Arrays . toString ( parameterTypes ) ) ; \n                                     Object [ ]  arguments  =   ( Object [ ] ) input . readObject ( ) ; \n                                     System . out . println ( "arguments : "   +   Arrays . toString ( arguments ) ) ; \n \n \n                                     /* Server */ \n                                     ObjectOutputStream  output  =   new   ObjectOutputStream ( \n                                        socket . getOutputStream ( ) ) ; \n                                     try   { \n                                         // serviceObject() \n                                         //  \n                                         Method  method  =  service . getClass ( ) . getMethod ( methodName , \n                                            parameterTypes ) ; \n                                         Object  result  =  method . invoke ( service ,  arguments ) ; \n                                         System . out . println ( "\\nServer  " ) ; \n                                         System . out . println ( "result : "   +  result ) ; \n                                        output . writeObject ( result ) ; \n                                     }   catch   ( Throwable  t )   { \n                                        output . writeObject ( t ) ; \n                                     }   finally   { \n                                        output . close ( ) ; \n                                     } \n                                 }   finally   { \n                                    input . close ( ) ; \n                                 } \n                             }   finally   { \n                                socket . close ( ) ; \n                             } \n                         }   catch   ( Exception  e )   { \n                            e . printStackTrace ( ) ; \n                         } \n                     } \n                 } ) . start ( ) ; \n             }   catch   ( Exception  e )   { \n                e . printStackTrace ( ) ; \n             } \n         } \n     } \n \n \n     /**\n     * \n     *\n     * @param <T>            \n     * @param interfaceClass \n     * @param host           \n     * @param port           \n     * @return \n     * @throws Exception\n     */ \n     @SuppressWarnings ( "unchecked" ) \n     public   static   < T >   T   refer ( final   Class < T >  interfaceClass ,   final   String  host ,   final   int  port ) \n         throws   Exception   { \n         if   ( interfaceClass  ==   null )   { \n             throw   new   IllegalArgumentException ( "Interface class == null" ) ; \n         } \n         // JDK  \n         if   ( ! interfaceClass . isInterface ( ) )   { \n             throw   new   IllegalArgumentException ( \n                 "The "   +  interfaceClass . getName ( )   +   " must be interface class!" ) ; \n         } \n         if   ( host  ==   null   ||  host . length ( )   ==   0 )   { \n             throw   new   IllegalArgumentException ( "Host == null!" ) ; \n         } \n         if   ( port  <=   0   ||  port  >   65535 )   { \n             throw   new   IllegalArgumentException ( "Invalid port "   +  port ) ; \n         } \n         System . out . println ( \n             "Get remote service "   +  interfaceClass . getName ( )   +   " from server "   +  host  +   ":"   +  port ) ; \n \n         // JDK  \n         T  proxy  =   ( T ) Proxy . newProxyInstance ( interfaceClass . getClassLoader ( ) , \n             new   Class < ? > [ ]   { interfaceClass } ,   new   InvocationHandler ( )   { \n                 // invokeRPC \n                 @Override \n                 public   Object   invoke ( Object  proxy ,   Method  method ,   Object [ ]  arguments ) \n                     throws   Throwable   { \n                     // Socket \n                     Socket  socket  =   new   Socket ( host ,  port ) ; \n                     try   { \n                         /* */ \n                         // Socket \n                         ObjectOutputStream  output  =   new   ObjectOutputStream ( \n                            socket . getOutputStream ( ) ) ; \n                         try   { \n                             //  \n                             System . out . println ( "\\nClient  " ) ; \n                            output . writeUTF ( method . getName ( ) ) ; \n                             System . out . println ( "methodName : "   +  method . getName ( ) ) ; \n                            output . writeObject ( method . getParameterTypes ( ) ) ; \n                             System . out . println ( "parameterTypes : "   +   Arrays . toString ( method\n                                 . getParameterTypes ( ) ) ) ; \n                            output . writeObject ( arguments ) ; \n                             System . out . println ( "arguments : "   +   Arrays . toString ( arguments ) ) ; \n \n \n                             /* */ \n                             ObjectInputStream  input  =   new   ObjectInputStream ( \n                                socket . getInputStream ( ) ) ; \n                             try   { \n                                 Object  result  =  input . readObject ( ) ; \n                                 if   ( result  instanceof   Throwable )   { \n                                     throw   ( Throwable ) result ; \n                                 } \n                                 System . out . println ( "\\nClient  " ) ; \n                                 System . out . println ( "result : "   +  result ) ; \n                                 return  result ; \n                             }   finally   { \n                                input . close ( ) ; \n                             } \n                         }   finally   { \n                            output . close ( ) ; \n                         } \n                     }   finally   { \n                        socket . close ( ) ; \n                     } \n                 } \n             } ) ; \n         return  proxy ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 RPCServerSocketJava/PRC \nRPCSocketJava/PRCPRC RPC \n () \n  3. RPC  \n  RPC  \n  RPC  \n Dubbo RPC  2011  Java  \nMotan RPC  2016  Java  \nTars RPC  2017  C++  \nSpring Cloud Pivotal  2014  RPC  Java  API  \n RPC  \n gRPCGoogle  2015  RPC  IDL  HTTP/2IDL  ProtoBuf  \nThrift Facebook  RPC 2007  Apache  Apache  IDL  \n  3.1 Dubbo \n  Apache Dubbo \n 3.1.1 Dubbo \n  \n dubbo \n  \n /** \n* xml \n*/  \n public   interface   ProviderService   {  \n     String   SayHello ( String  word ) ;  \n } \n \n 1 2 3 4 5 6  SayHello  \n   \n /** \n* xml \n*/ \n     public   class   ProviderServiceImpl   implements   ProviderService {    \n         public   String   SayHello ( String  word )   {    \n             return  word ;     \n         }  \n     } \n \n 1 2 3 4 5 6 7 8  \n  maven  \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 " \n          xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     < groupId > com.ouyangsihai </ groupId > \n     < artifactId > dubbo-provider </ artifactId > \n     < version > 1.0-SNAPSHOT </ version > \n\n     < dependencies > \n         < dependency > \n             < groupId > junit </ groupId > \n             < artifactId > junit </ artifactId > \n             < version > 3.8.1 </ version > \n             < scope > test </ scope > \n         </ dependency > \n         \x3c!-- https://mvnrepository.com/artifact/com.alibaba/dubbo --\x3e \n         < dependency > \n             < groupId > com.alibaba </ groupId > \n             < artifactId > dubbo </ artifactId > \n             < version > 2.6.6 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.zookeeper </ groupId > \n             < artifactId > zookeeper </ artifactId > \n             < version > 3.4.10 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > com.101tec </ groupId > \n             < artifactId > zkclient </ artifactId > \n             < version > 0.5 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > io.netty </ groupId > \n             < artifactId > netty-all </ artifactId > \n             < version > 4.1.32.Final </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.curator </ groupId > \n             < artifactId > curator-framework </ artifactId > \n             < version > 2.8.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.curator </ groupId > \n             < artifactId > curator-recipes </ artifactId > \n             < version > 2.8.0 </ version > \n         </ dependency > \n\n     </ dependencies > \n </ project > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 dubbo2.6.6dubbonettycurator \n  zookeeper \n dubbo  \n **** xml    \nresource META-INF.spring  provider.xml  \n ![](RPC.assets/Tue, 30 May 2023 120318.png) \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xmlns: dubbo = " http://code.alibabatech.com/schema/dubbo " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://code.alibabatech.com/schema/dubbo        http://code.alibabatech.com/schema/dubbo/dubbo.xsd " > \n\n     \x3c!----\x3e \n     < dubbo: application   name = " provider "   owner = " sihai " > \n         < dubbo: parameter   key = " qos.enable "   value = " true " /> \n         < dubbo: parameter   key = " qos.accept.foreign.ip "   value = " false " /> \n         < dubbo: parameter   key = " qos.port "   value = " 55555 " /> \n     </ dubbo: application > \n\n     < dubbo: monitor   protocol = " registry " /> \n\n     \x3c!--dubbo--\x3e \n     \x3c!--<dubbo:registry address="N/A"/>--\x3e \n     < dubbo: registry   address = " N/A "   /> \n\n     \x3c!--webserviceThriftHessainhttp--\x3e \n     < dubbo: protocol   name = " dubbo "   port = " 20880 " /> \n\n     \x3c!----\x3e \n     < dubbo: service \n             interface = " com.sihai.dubbo.provider.service.ProviderService " \n             ref = " providerService " /> \n\n     \x3c!--Bean bean--\x3e \n     < bean   id = " providerService "   class = " com.sihai.dubbo.provider.service.ProviderServiceImpl " /> \n\n </ beans > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  \n 1 springdubbo  spring \n2dubbo:application \nnameowner \n \n3dubbo:monitor \n  \n4dubbo:registry \n zookeeper address  N/A  dubbo \n5dubbo:protocol \n dubbo dubbowebservicehttp \n6dubbo:service \ninterfaceref  bean \n7spring bean \n  main  \n  \n package   com . sihai . dubbo . provider ; \n\n import   com . alibaba . dubbo . config . ApplicationConfig ; \n import   com . alibaba . dubbo . config . ProtocolConfig ; \n import   com . alibaba . dubbo . config . RegistryConfig ; \n import   com . alibaba . dubbo . config . ServiceConfig ; \n import   com . alibaba . dubbo . container . Main ; \n import   com . sihai . dubbo . provider . service . ProviderService ; \n import   com . sihai . dubbo . provider . service . ProviderServiceImpl ; \n import   org . springframework . context . support . ClassPathXmlApplicationContext ; \n\n import   java . io . IOException ; \n\n /**\n * xml\n *\n */ \n public   class   App  \n { \n     public   static   void   main (   String [ ]  args  )   throws   IOException   { \n         //xml \n         ClassPathXmlApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "META-INF/spring/provider.xml" ) ; \n        context . start ( ) ; \n         System . in . read ( ) ;   //  \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 dubbo spring  ClassPathXmlApplicationContext xml context.start() \n  \n ![](RPC.assets/Tue, 30 May 2023 120339.png) \n  dubbo  url \n Dubbo  URL \n dubbo://192.168.234.1:20880/com.sihai.dubbo.provider.service.ProviderService?anyhost=true&application=provider&bean.name=com.sihai.dubbo.provider.service.ProviderService&bind.ip=192.168.234.1&bind.port=20880&dubbo=2.0.2&generic=false&interface=com.sihai.dubbo.provider.service.ProviderService&methods=SayHello&owner=sihai&pid=8412&qos.accept.foreign.ip=false&qos.enable=true&qos.port=55555&side=provider&timestamp=1562077289380\n \n 1  \n 1 dubbo http dubbo \n2dubbo://192.168.234.1:20880/com.sihai.dubbo.provider.service.ProviderService \n ? ://ip:/  \n3 \n anyhost=true&application=provider&bean.name=com.sihai.dubbo.provider.service.ProviderService&bind.ip=192.168.234.1&bind.port=20880&dubbo=2.0.2&generic=false&interface=com.sihai.dubbo.provider.service.ProviderService&methods=SayHello&owner=sihai&pid=8412&qos.accept.foreign.ip=false&qos.enable=true&qos.port=55555&side=provider&timestamp=1562077289380\n \n 1 ?provider.xml&  http  \n dubbo  url \n  \n  \n  \n resource consumer.xml \n ![](RPC.assets/Tue, 30 May 2023 120310.png) \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xmlns: dubbo = " http://code.alibabatech.com/schema/dubbo " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://code.alibabatech.com/schema/dubbo        http://code.alibabatech.com/schema/dubbo/dubbo.xsd " > \n\n     \x3c!----\x3e \n     < dubbo: application   name = " consumer "   owner = " sihai " /> \n\n     \x3c!--dubbo--\x3e \n     \x3c!----\x3e \n     < dubbo: registry   address = " N/A "   /> \n     \x3c!--<dubbo:registry address="zookeeper://localhost:2181" check="false"/>--\x3e \n\n     \x3c!----\x3e \n     \x3c!----\x3e \n     < dubbo: reference   id = " providerService " \n interface = " com.sihai.dubbo.provider.service.ProviderService " \n url = " dubbo://192.168.234.1:20880/com.sihai.dubbo.provider.service.ProviderService " /> \n\n     \x3c!--<dubbo:reference id="providerService"  \n                    interface="com.sihai.dubbo.provider.service.ProviderService"/>--\x3e \n </ beans > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \n 1 dubbo:application  dubbo:registry  \n2dubbo:reference  url \n maven  \n \n  \n package   com . sihai . dubbo . consumer ; \n\n import   com . alibaba . dubbo . config . ApplicationConfig ; \n import   com . alibaba . dubbo . config . ReferenceConfig ; \n import   com . alibaba . dubbo . config . RegistryConfig ; \n import   com . sihai . dubbo . provider . service . ProviderService ; \n import   org . springframework . context . support . ClassPathXmlApplicationContext ; \n\n import   java . io . IOException ; \n\n /**\n * xml\n *\n */ \n public   class   App  \n { \n     public   static   void   main (   String [ ]  args  )   throws   IOException   { \n\n         ClassPathXmlApplicationContext  context = new   ClassPathXmlApplicationContext ( "consumer.xml" ) ; \n        context . start ( ) ; \n         ProviderService  providerService  =   ( ProviderService )  context . getBean ( "providerService" ) ; \n         String  str  =   providerService . SayHello ( "hello" ) ; \n         System . out . println ( str ) ; \n         System . in . read ( ) ; \n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  \n ![](RPC.assets/Tue, 30 May 2023 120305.png) \n  \n zookeeper \n dubbo + zookeeper zookeeper  zookeeper \n zk \n  \n  \n provider.xml  \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xmlns: dubbo = " http://code.alibabatech.com/schema/dubbo " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://code.alibabatech.com/schema/dubbo        http://code.alibabatech.com/schema/dubbo/dubbo.xsd " > \n\n     \x3c!----\x3e \n     < dubbo: application   name = " provider "   owner = " sihai " > \n         < dubbo: parameter   key = " qos.enable "   value = " true " /> \n         < dubbo: parameter   key = " qos.accept.foreign.ip "   value = " false " /> \n         < dubbo: parameter   key = " qos.port "   value = " 55555 " /> \n     </ dubbo: application > \n\n     < dubbo: monitor   protocol = " registry " /> \n\n     \x3c!--dubbo--\x3e \n     \x3c!--<dubbo:registry address="N/A"/>--\x3e \n     < dubbo: registry   address = " zookeeper://localhost:2181 "   check = " false " /> \n\n     \x3c!--webserviceThriftHessainhttp--\x3e \n     < dubbo: protocol   name = " dubbo "   port = " 20880 " /> \n\n     \x3c!----\x3e \n     < dubbo: service \n             interface = " com.sihai.dubbo.provider.service.ProviderService " \n             ref = " providerService " /> \n\n     \x3c!--Bean bean--\x3e \n     < bean   id = " providerService "   class = " com.sihai.dubbo.provider.service.ProviderServiceImpl " /> \n\n </ beans > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  \n < dubbo: registry   address = " zookeeper://localhost:2181 "   /> \n \n 1  address  zookeeper  \n zookeeper \n < dubbo: registry    protocol = " zookeeper "   address = " 192.168.11.129:2181,192.168.11.137:2181,192.168.11.138:2181 " /> \n \n 1    \n  \n  consumer.xml  \n 1 \n < dubbo: registry   address = " zookeeper://localhost:2181 " /> \n \n 1 2dubbo:reference \n zookeeper  dubbo  url  \n < dubbo: reference   id = " providerService "                       interface = " com.sihai.dubbo.provider.service.ProviderService " /> \n \n 1  \n  \n     dubbo    urlzookeeper****  zookeeper zookeeper    \n  \n   \n  xml  dubbo  \n API \n xml \n  \n  \n ![](RPC.assets/Tue, 30 May 2023 120401.png) \n  apiprovider.xml AppApi  mainapi \n package   com . sihai . dubbo . provider ; \n\n import   com . alibaba . dubbo . config . ApplicationConfig ; \n import   com . alibaba . dubbo . config . ProtocolConfig ; \n import   com . alibaba . dubbo . config . RegistryConfig ; \n import   com . alibaba . dubbo . config . ServiceConfig ; \n import   com . sihai . dubbo . provider . service . ProviderService ; \n import   com . sihai . dubbo . provider . service . ProviderServiceImpl ; \n\n import   java . io . IOException ; \n\n /**\n * Api\n * api\n * \n * Apixml\n */ \n public   class   AppApi \n { \n     public   static   void   main (   String [ ]  args  )   throws   IOException   { \n\n         //  \n         ProviderService  providerService  =   new   ProviderServiceImpl ( ) ; \n\n         //  \n         ApplicationConfig  application  =   new   ApplicationConfig ( ) ; \n        application . setName ( "provider" ) ; \n        application . setOwner ( "sihai" ) ; \n\n         //  \n         RegistryConfig  registry  =   new   RegistryConfig ( ) ; \n        registry . setAddress ( "zookeeper://localhost:2181" ) ; \n //        registry.setUsername("aaa"); \n //        registry.setPassword("bbb"); \n\n         //  \n         ProtocolConfig  protocol  =   new   ProtocolConfig ( ) ; \n        protocol . setName ( "dubbo" ) ; \n        protocol . setPort ( 20880 ) ; \n         //protocol.setThreads(200); \n\n         // ServiceConfig \n         // \n\n         //  \n         //  \n         // \n         ServiceConfig < ProviderService >  service  =   new   ServiceConfig < ProviderService > ( ) ;  \n        service . setApplication ( application ) ; \n         // setRegistries() \n        service . setRegistry ( registry ) ;  \n         // setProtocols() \n        service . setProtocol ( protocol ) ;  \n        service . setInterface ( ProviderService . class ) ; \n        service . setRef ( providerService ) ; \n        service . setVersion ( "1.0.0" ) ; \n\n         //  \n        service . export ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62  \n  xml \n registry  xml \n < dubbo: registry   protocol = " zookeeper "   address = " localhost:2181 " /> \n \n 1 API  \n RegistryConfig  registry  =   new   RegistryConfig ( ) ;  registry . setAddress ( "zookeeper://localhost:2181" ) ; \n \n 1 dubbo:registryRegistryConfig xml API set APIxml  \n  API \n org . apache . dubbo . config . ServiceConfig   org . apache . dubbo . config . ReferenceConfig   org . apache . dubbo . config . ProtocolConfig   org . apache . dubbo . config . RegistryConfig   org . apache . dubbo . config . MonitorConfig   org . apache . dubbo . config . ApplicationConfig   org . apache . dubbo . config . ModuleConfig   org . apache . dubbo . config . ProviderConfig   org . apache . dubbo . config . ConsumerConfig   org . apache . dubbo . config . MethodConfig   org . apache . dubbo . config . ArgumentConfig \n \n 1  \n http:// dubbo .apache.org/zh-cn \n Api \n  \n consumer.xml  main \n ![](RPC.assets/Tue, 30 May 2023 120157.png) \n package   com . sihai . dubbo . consumer ; \n\n import   com . alibaba . dubbo . config . ApplicationConfig ; \n import   com . alibaba . dubbo . config . ReferenceConfig ; \n import   com . alibaba . dubbo . config . RegistryConfig ; \n import   com . sihai . dubbo . provider . service . ProviderService ; \n\n /**\n * api\n * api\n * \n * Apixml\n */ \n public   class   AppApi   { \n\n     public   static   void   main ( String [ ]  args )   { \n         //  \n         ApplicationConfig  application  =   new   ApplicationConfig ( ) ; \n        application . setName ( "consumer" ) ; \n        application . setOwner ( "sihai" ) ; \n\n         //  \n         RegistryConfig  registry  =   new   RegistryConfig ( ) ; \n        registry . setAddress ( "zookeeper://localhost:2181" ) ; \n\n         // ReferenceConfig \n         // \n\n         //  \n         ReferenceConfig < ProviderService >  reference  =   new   ReferenceConfig < ProviderService > ( ) ;   //  \n        reference . setApplication ( application ) ; \n        reference . setRegistry ( registry ) ;   // setRegistries() \n        reference . setInterface ( ProviderService . class ) ; \n\n         // beanxxxService \n         ProviderService  providerService  =  reference . get ( ) ;   //  \n         providerService . SayHello ( "hello dubbo! I am sihai!" ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  API xml  \n  \n   \n dubbo  \n  \n ![](RPC.assets/Tue, 30 May 2023 120207.png) \n    annotation    \n package   com . sihai . dubbo . provider . service . annotation ; \n\n /**\n * \n */ \n public   interface   ProviderServiceAnnotation   { \n     String   SayHelloAnnotation ( String  word ) ; \n } \n\n \n 1 2 3 4 5 6 7 8 9 package   com . sihai . dubbo . provider . service . annotation ; \n\n import   com . alibaba . dubbo . config . annotation . Service ; \n\n /**\n * \n */ \n @Service ( timeout  =   5000 ) \n public   class   ProviderServiceImplAnnotation   implements   ProviderServiceAnnotation { \n\n     public   String   SayHelloAnnotation ( String  word )   { \n         return  word ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Service \n @Service  Dubbo  \n   Spring  Java Config @Configuration annotation @EnableDubboDubbo \n package   com . sihai . dubbo . provider . configuration ; \n\n import   com . alibaba . dubbo . config . ApplicationConfig ; \n import   com . alibaba . dubbo . config . ProtocolConfig ; \n import   com . alibaba . dubbo . config . ProviderConfig ; \n import   com . alibaba . dubbo . config . RegistryConfig ; \n import   com . alibaba . dubbo . config . spring . context . annotation . EnableDubbo ; \n import   org . springframework . context . annotation . Bean ; \n import   org . springframework . context . annotation . Configuration ; \n\n /**\n * \n */ \n @Configuration \n @EnableDubbo ( scanBasePackages  =   "com.sihai.dubbo.provider.service.annotation" ) \n public   class   DubboConfiguration   { \n\n     @Bean   // #1  \n     public   ProviderConfig   providerConfig ( )   { \n         ProviderConfig  providerConfig  =   new   ProviderConfig ( ) ; \n        providerConfig . setTimeout ( 1000 ) ; \n         return  providerConfig ; \n     } \n\n     @Bean   // #2  \n     public   ApplicationConfig   applicationConfig ( )   { \n         ApplicationConfig  applicationConfig  =   new   ApplicationConfig ( ) ; \n        applicationConfig . setName ( "dubbo-annotation-provider" ) ; \n         return  applicationConfig ; \n     } \n\n     @Bean   // #3  \n     public   RegistryConfig   registryConfig ( )   { \n         RegistryConfig  registryConfig  =   new   RegistryConfig ( ) ; \n        registryConfig . setProtocol ( "zookeeper" ) ; \n        registryConfig . setAddress ( "localhost" ) ; \n        registryConfig . setPort ( 2181 ) ; \n         return  registryConfig ; \n     } \n\n     @Bean   // #4  dubbo \n     public   ProtocolConfig   protocolConfig ( )   { \n         ProtocolConfig  protocolConfig  =   new   ProtocolConfig ( ) ; \n        protocolConfig . setName ( "dubbo" ) ; \n        protocolConfig . setPort ( 20880 ) ; \n         return  protocolConfig ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  \n1 @EnableDubbocom.sihai.dubbo.provider.service.annotation @Service  \n 2 @Configuration DubboConfiguration @BeanJava Config Dubbo@Service  \n  \n ProviderConfig \n\nApplicationConfig\n\n RegistryConfig \n\nProtocolConfig\n \n 1 2 3 4 5 6 7  \n package   com . sihai . dubbo . provider ; \n\n import   com . alibaba . dubbo . config . spring . context . annotation . DubboComponentScan ; \n import   com . sihai . dubbo . provider . configuration . DubboConfiguration ; \n import   org . springframework . context . annotation . AnnotationConfigApplicationContext ; \n import   sun . applet . Main ; \n\n import   java . io . IOException ; \n\n /**\n * \n */ \n public   class   AppAnnotation   { \n\n     public   static   void   main ( String [ ]  args )   throws   IOException   { \n         AnnotationConfigApplicationContext  context  =   new   AnnotationConfigApplicationContext ( DubboConfiguration . class ) ;  \n        context . start ( ) ; \n         System . in . read ( ) ;  \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  success \n ![](RPC.assets/Tue, 30 May 2023 120958.png) \n  \n  \n ![](RPC.assets/Tue, 30 May 2023 121020.png) \n  \n package   com . sihai . dubbo . consumer . Annotation ; \n\n import   com . alibaba . dubbo . config . annotation . Reference ; \n import   com . sihai . dubbo . provider . service . annotation . ProviderServiceAnnotation ; \n import   org . springframework . stereotype . Component ; \n\n /**\n * service\n */ \n @Component ( "annotatedConsumer" ) \n public   class   ConsumerAnnotationService   { \n\n     @Reference \n     private   ProviderServiceAnnotation  providerServiceAnnotation ; \n\n     public   String   doSayHello ( String  name )   { \n         return   providerServiceAnnotation . SayHelloAnnotation ( name ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  ConsumerAnnotationService  @Reference  \n  ProviderServiceAnnotation  Maven intallpom  \n < dependency > \n           < groupId > com.ouyangsihai </ groupId > \n             < artifactId > dubbo-provider </ artifactId > \n             < version > 1.0-SNAPSHOT </ version > \n </ dependency > \n\n \n 1 2 3 4 5 6 jarrpc \n  \n  \n package   com . sihai . dubbo . consumer . configuration ; \n\n import   com . alibaba . dubbo . config . ApplicationConfig ; \n import   com . alibaba . dubbo . config . ConsumerConfig ; \n import   com . alibaba . dubbo . config . RegistryConfig ; \n import   com . alibaba . dubbo . config . spring . context . annotation . EnableDubbo ; \n import   org . springframework . context . annotation . Bean ; \n import   org . springframework . context . annotation . ComponentScan ; \n import   org . springframework . context . annotation . Configuration ; \n\n import   java . util . HashMap ; \n import   java . util . Map ; \n\n /**\n * \n */ \n @Configuration \n @EnableDubbo ( scanBasePackages  =   "com.sihai.dubbo.consumer.Annotation" ) \n @ComponentScan ( value  =   { "com.sihai.dubbo.consumer.Annotation" } ) \n public   class   ConsumerConfiguration   { \n     @Bean   //  \n     public   ApplicationConfig   applicationConfig ( )   { \n         ApplicationConfig  applicationConfig  =   new   ApplicationConfig ( ) ; \n        applicationConfig . setName ( "dubbo-annotation-consumer" ) ; \n         Map < String ,   String >  stringStringMap  =   new   HashMap < String ,   String > ( ) ; \n        stringStringMap . put ( "qos.enable" , "true" ) ; \n        stringStringMap . put ( "qos.accept.foreign.ip" , "false" ) ; \n        stringStringMap . put ( "qos.port" , "33333" ) ; \n        applicationConfig . setParameters ( stringStringMap ) ; \n         return  applicationConfig ; \n     } \n\n     @Bean   //  \n     public   ConsumerConfig   consumerConfig ( )   { \n         ConsumerConfig  consumerConfig  =   new   ConsumerConfig ( ) ; \n        consumerConfig . setTimeout ( 3000 ) ; \n         return  consumerConfig ; \n     } \n\n     @Bean   //  \n     public   RegistryConfig   registryConfig ( )   { \n         RegistryConfig  registryConfig  =   new   RegistryConfig ( ) ; \n        registryConfig . setProtocol ( "zookeeper" ) ; \n        registryConfig . setAddress ( "localhost" ) ; \n        registryConfig . setPort ( 2181 ) ; \n         return  registryConfig ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  \n mainSpring ContextDubbo   \n package   com . sihai . dubbo . consumer ; \n\n import   com . sihai . dubbo . consumer . Annotation . ConsumerAnnotationService ; \n import   com . sihai . dubbo . consumer . configuration . ConsumerConfiguration ; \n import   com . sihai . dubbo . provider . service . ProviderService ; \n import   org . springframework . context . annotation . AnnotationConfigApplicationContext ; \n import   org . springframework . context . support . ClassPathXmlApplicationContext ; \n\n import   java . io . IOException ; \n\n /**\n * \n *\n */ \n public   class   AppAnnotation \n { \n     public   static   void   main (   String [ ]  args  )   throws   IOException   { \n\n         AnnotationConfigApplicationContext  context  =   new   AnnotationConfigApplicationContext ( ConsumerConfiguration . class ) ;  \n        context . start ( ) ;   //  \n         ConsumerAnnotationService  consumerAnnotationService  =  context . getBean ( ConsumerAnnotationService . class ) ;  \n         String  hello  =  consumerAnnotationService . doSayHello ( "annotation" ) ;   //  \n         System . out . println ( "result: "   +  hello ) ;   //  \n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  \n ![](RPC.assets/Tue, 30 May 2023 121238.png) \n 3.1.2  \n   \n Dubbo  Springcheck="true" \n  \n  \n  \n < dubbo: registry   protocol = " zookeeper "   address = " localhost:2181,localhost:2182,localhost:2183 "   check = " false " /> \n \n 1  \n < dubbo: reference   check = " false "   id = " providerService "                       interface = " com.sihai.dubbo.provider.service.ProviderService " /> \n \n 1  \n  \n dubbo  failover \n  \n \n \n \n  \n  \n  \n \n \n \n \n Failover Cluster \n retries="2" () \n cluster="xxx"xxx cluster="failover" \n \n \n Failfast Cluster \n  \n \n \n \n Failsafe Cluster \n  \n \n \n \n Failback Cluster \n  \n \n \n \n Forking Cluster \n forks="2"  \n \n \n \n Broadcast Cluster \n  \n \n \n \n \n  \n \n \x3c!----\x3e \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n             interface = " com.sihai.dubbo.provider.service.ProviderService " \n             ref = " providerService " /> \n\n \n 1 2 3 4 5 \n < dubbo: reference   cluster = " failover "   retries = " 2 "   check = " false "   id = " providerService " \n                      interface = " com.sihai.dubbo.provider.service.ProviderService " /> \n\n \n 1 2 3 #    \n dubbo dubbo \n \n \n \n  \n  \n  \n \n \n \n \n Random LoadBalance \n   \n <dubbo:service loadbalance="xxx"/>xxx \n \n \n RoundRobin LoadBalance \n   \n \n \n \n LeastActive LoadBalance \n   \n \n \n \n ConsistentHash LoadBalance \n  Hash  \n   \n  \n  \n < dubbo: reference   id = " providerService "                       interface = " com.sihai.dubbo.provider.service.ProviderService "                       url = " dubbo://192.168.234.1:20880/com.sihai.dubbo.provider.service.ProviderService " /> \n \n 1 dubbo:reference url \n  \n  \n  \n  \n() \n < dubbo: registry   register = " false "   protocol = " zookeeper "   address = " localhost:2181,localhost:2182,localhost:2183 "   check = " false " /> \n \n 1 1 \n register="false"  \n < dubbo: reference   cluster = " failover "   retries = " 2 "   check = " false "   id = " providerService "                       interface = " com.sihai.dubbo.provider.service.ProviderService " /> \n \n 1  \n ![](RPC.assets/Tue, 30 May 2023 121910.png) \n  zookeeper netty \n 2 \n < dubbo: registry   protocol = " zookeeper "   address = " localhost:2181,localhost:2182,localhost:2183 "   check = " false " /> \n \n 1  \n ![](RPC.assets/Tue, 30 May 2023 121932.png) \n  zookeeper  \n  \n  \n  \n  \n  \n < dubbo: registry   subscribe = " false "   address = " localhost:2181 " > </ dubbo: registry > \n \n 1  dubbo:registry  subscribe="false"  \n  \n ![](RPC.assets/Tue, 30 May 2023 121956.png) \n   \n  dubbo  dubbormihessian \n  \n 1 \n  \n   \x3c!--webseroviceThriftHessainhttp--\x3e \n     < dubbo: protocol   name = " dubbo "   port = " 20880 " /> \n     < dubbo: protocol   name = " rmi "   port = " 1099 "   /> \n\n \n 1 2 3 4  \n \x3c!----\x3e \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n             interface = " com.sihai.dubbo.provider.service.ProviderService " \n             ref = " providerService " /> \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n                    interface = " com.sihai.dubbo.provider.service.ProviderService " \n                    ref = " providerService "   protocol = " rmi " /> \n\n \n 1 2 3 4 5 6 7 8 rmi \n rmi://192.168.234.1:1099/com.sihai.dubbo.provider.service.ProviderService?anyhost=true&application=provider&bean.name=com.sihai.dubbo.provider.service.ProviderService&cluster=failover&dubbo=2.0.2&generic=false&interface=com.sihai.dubbo.provider.service.ProviderService&methods=SayHello&owner=sihai&pid=796&retries=2&side=provider&timestamp=1564281053185, dubbo version: 2.6.6, current host: 192.168.234.1\n \n 1 2 \n \n < dubbo: service   cluster = " failover "   retries = " 2 "                     interface = " com.sihai.dubbo.provider.service.ProviderService "                     ref = " providerService "   protocol = " rmi,dubbo " /> \n \n 1 protocol, \n  \n Dubbo \n  \n  \n  \n \x3c!----\x3e \n     < dubbo: registry   protocol = " zookeeper "   id = " reg1 "   timeout = " 10000 "   address = " localhost:2181 " /> \n     < dubbo: registry   protocol = " zookeeper "   id = " reg2 "   timeout = " 10000 "   address = " localhost:2182 " /> \n     < dubbo: registry   protocol = " zookeeper "   id = " reg3 "   timeout = " 10000 "   address = " localhost:2183 " /> \n\n \n 1 2 3 4 5  \n \x3c!----\x3e \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n             interface = " com.sihai.dubbo.provider.service.ProviderService " \n             ref = " providerService "   registry = " reg1 " /> \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n                    interface = " com.sihai.dubbo.provider.service.ProviderService " \n                    ref = " providerService "   protocol = " rmi "   registry = " reg2 " /> \n\n \n 1 2 3 4 5 6 7 8 registry="reg2"registry="reg1,,reg2" \n  \n ; \n \x3c!----\x3e \n     < dubbo: registry   protocol = " zookeeper "   id = " reg1 "   timeout = " 10000 "   address = " localhost:2181 " /> \n     < dubbo: registry   protocol = " zookeeper "   id = " reg2 "   timeout = " 10000 "   address = " localhost:2182 " /> \n     < dubbo: registry   protocol = " zookeeper "   id = " reg3 "   timeout = " 10000 "   address = " localhost:2183 " /> \n\n \n 1 2 3 4 5  \n \x3c!----\x3e \n     < dubbo: reference   cluster = " failover "   retries = " 2 "   check = " false "   id = " providerService " \n                      interface = " com.sihai.dubbo.provider.service.ProviderService "   registry = " reg1 " /> \n     < dubbo: reference   cluster = " failover "   retries = " 2 "   check = " false "   id = " providerService2 " \n                      interface = " com.sihai.dubbo.provider.service.ProviderService "   registry = " reg2 " /> \n\n \n 1 2 3 4 5 6 12 \n  \n  \n \x3c!----\x3e \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n             interface = " com.sihai.dubbo.provider.service.ProviderService " \n             ref = " providerService "   registry = " reg1 "   version = " 1.0.0 " /> \n     < dubbo: service   cluster = " failover "   retries = " 2 " \n                    interface = " com.sihai.dubbo.provider.service.ProviderService " \n                    ref = " providerService "   protocol = " rmi "   registry = " reg2 "   version = " 1.0.0 " /> \n\n \n 1 2 3 4 5 6 7 8  \n  \n dubbo \n 1accessloglog4j \n < dubbo: protocol   accesslog = " true "   name = " dubbo "   port = " 20880 " /> \n < dubbo: protocol   accesslog = " true "   name = " rmi "   port = " 1099 "   /> \n\n \n 1 2 3 2 \n < dubbo: protocol   accesslog = " http://localhost/log.txt "   name = " dubbo "   port = " 20880 " /> \n < dubbo: protocol   accesslog = " http://localhost/log2.txt "   name = " rmi "   port = " 1099 "   /> \n \n 1 2 #  Dubbo \n Dubbo  \n \n Dubbo10 \n 1.  \n  \n 2.  \n Spring \n 3. \n  \n 4. \n  \n 5. \n Dubbo \n 6. \n RPC \n 7. \n RPCAB \n 8.  \n  \n 9. \n  \n 10. \n  \n Dubbo \n Dubbo 11 \n \n 1. \n 2. \n 3. \n 4.Cluster \n 5.Cluster \n 6. \n 7.Exchangerequest \n 9. \n 10.Exchange \n 11. \n dubboredis \n (180) springBoot+dubboRedis - _-CSDN \n https://github.com/YClimb/redis-demo \n https://github.com/fhhly/springboot-dubbo.git \n 3.2 BRPC \n'},{frontmatter:{},regularPath:"/%E4%BA%91%E5%8E%9F%E7%94%9F/k8s.html",relativePath:"/k8s.md",key:"v-2c8e9208",path:"/1970/01/01/k8s/",lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:""},{title:"geohash",frontmatter:{title:"geohash",date:"2019-09-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["","",""]},regularPath:"/%E5%85%B6%E4%BB%96/geohash%E7%AE%97%E6%B3%95.html",relativePath:"/geohash.md",key:"v-01787dc2",path:"/2019/09/08/geohash%E7%AE%97%E6%B3%95/",headers:[{level:3,title:"",slug:""},{level:3,title:"Geohash",slug:"geohash"},{level:3,title:"",slug:"-"},{level:3,title:"",slug:"-"},{level:3,title:"Base32",slug:"-base32"},{level:3,title:"",slug:""},{level:3,title:"geohash",slug:"geohash"},{level:3,title:"(8)",slug:"-8-"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"geohashZ)",slug:"geohash-z"}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:"  \n  \n app100 \n  \n \n \n 0.000011 \n \n \n \n  \n  \n \n \n \n \n 0.0000110.0001100.0011000.0110000.110000 \n 0.000011.10.0001110.0011110.0111130.111132 \n Geohash \n GeoHash    \n GeoHashpoi \n (-90, 90)(-90, 0)(0, 90)01(-180, 180)   Base32  \n  \n ,  104.05968430.559545 \n  \n \n \n \n  \n  \n 0 \n 1 \n 30.559545 \n \n \n \n \n 1 \n (-90, 90) \n (-90, 0.0) \n (0.0, 90) \n 1 \n \n \n 2 \n (0.0, 90) \n (0.0, 45.0) \n (45.0, 90) \n 0 \n \n \n 3 \n (0.0, 45.0) \n (0.0, 22.5) \n (22.5, 45.0) \n 1 \n \n \n 4 \n (22.5, 45.0) \n (22.5, 33.75) \n (33.75, 45.0) \n 0 \n \n \n 5 \n (22.5, 33.75) \n (22.5, 28.125) \n (28.125, 33.75) \n 1 \n \n \n 6 \n (28.125, 33.75) \n (28.125, 30.9375) \n (30.9375, 33.75) \n 0 \n \n \n 7 \n (28.125, 30.9375) \n (28.125, 29.53125) \n (29.53125, 30.9375) \n 1 \n \n \n 8 \n (29.53125, 30.9375) \n (29.53125, 30.234375) \n (30.234375, 30.9375) \n 1 \n \n \n 9 \n (30.234375, 30.9375) \n (30.234375, 30.5859375) \n (30.5859375, 30.9375) \n 0 \n \n \n 10 \n (30.234375, 30.5859375) \n (30.234375, 30.41015625) \n (30.41015625, 30.5859375) \n 1 \n \n \n 11 \n (30.41015625, 30.5859375) \n (30.41015625, 30.498046875) \n (30.498046875, 30.5859375) \n 1 \n \n \n 12 \n (30.498046875, 30.5859375) \n (30.498046875, 30.541992188) \n (30.541992188, 30.5859375) \n 1 \n \n \n 13 \n (30.541992188, 30.5859375) \n (30.541992188, 30.563964844) \n (30.563964844, 30.5859375) \n 0 \n \n \n 14 \n (30.541992188, 30.563964844) \n (30.541992188, 30.552978516) \n (30.552978516, 30.563964844) \n 1 \n \n \n 15 \n (30.552978516, 30.563964844) \n (30.552978516, 30.55847168) \n (30.55847168, 30.563964844) \n 1 \n \n \n \n 101010110111011, (104.059684)110010011111111 \n   \n 0 \n \n \n \n  \n 0 \n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n 10 \n 11 \n 12 \n 13 \n 14 \n 15 \n 16 \n 17 \n 18 \n 19 \n 20 \n 21 \n 22 \n 23 \n 24 \n 25 \n 26 \n 27 \n 28 \n 29 \n \n \n \n \n  \n 1 \n 1 \n 1 \n 0 \n 0 \n 1 \n 0 \n 0 \n 1 \n 1 \n 0 \n 0 \n 0 \n 1 \n 1 \n 1 \n 1 \n 0 \n 1 \n 1 \n 1 \n 1 \n 1 \n 1 \n 1 \n 0 \n 1 \n 1 \n 1 \n 1 \n Base32 \n 56Base32 \n Base320-9b-za, i, l, o32. \n \n 11100   10011   00011   11011   11111   01111 \n 28 ( w )   19 ( m )   3 ( 3 )    27 ( v )   31 ( z )   15 ( g ) \n \n 1 2 wm3vzg \n   geohash   Geohash  \n \n GeoHash,  \n 10GeoHash GeoHashwm3vzgwm3vzwm3vzg  \n  \n GeohashgeohashGeohash (Geohash: \n \n \n \n  \n  \n  \n \n \n \n \n 1 \n 5000Km \n 5000Km \n \n \n 2 \n 1250Km \n 625Km \n \n \n 3 \n 156Km \n 156Km \n \n \n 4 \n 39.1Km \n 19.5Km \n \n \n 5 \n 4.89Km \n 4.89Km \n \n \n 6 \n 1.22Km \n 0.61Km \n \n \n 7 \n 153m \n 153m \n \n \n 8 \n 38.2m \n 19.1m \n \n \n 9 \n 4.77m \n 4.77m \n \n \n 10 \n 1.19m \n 0.596m \n \n \n \n Geohash \n 61.20.6 \n geohash \n  (8) \n  \nGeohash**1111**Geohash \n  \n Geohash61.2Km*0.6Km. \n 0.0011110.6Km0.005405405 \n wgs84( 104.0550330.562251),  101010110111011110010011111111, Geohashwm3vzg \n 0.005405405 30.562251 +  0.005405405 = 30.567656405 () 101010110111100 \n 101010110111011 1101010110111011 + 000000000000001  =  101010110111100 \n  \n wgs84( 104.0550330.562251),  101010110111011110010011111111, Geohashwm3vzg \n N1101010110111100Geohash wm3vzu \n http://geohash.co/ (104.0570068359375,30.56671142578125 \n https://www.box3.cn/tools/lbs.html 2531m( \n \n 8 \n \n \n \n ** *Geohash* wm3vzs101010110111100110010011111110(Lat_bin + 1, Lon_bin - 1) \n ** *Geohash* wm3vzu101010110111100110100101101010(Lat_bin + 1, Lon_bin) \n ** *Geohash* wm6jbh101010110111100110010100000000(Lat_bin + 1, Lon_bin + 1) \n \n \n \n \n **Geohash**wm3vze101010110111011110010011111110(Lat_bin, Lon_bin - 1) \n **Geohash**wm3vzg101010110111011110010011111111(Lat_bin, Lon_bin) \n **Geohash**wm6jb5101010110111011110010100000000(Lat_bin, Lon_bin + 1) \n \n \n **Geohash**wm3vzd101010110111010110010011111110(Lat_bin - 1, Lon_bin - 1) \n **Geohash**wm3vzf101010110111010110010011111111(Lat_bin - 1, Lon_bin) \n **Geohash**wm6jb4101010110111010110010100000000(Lat_bin - 1, Lon_bin + 1) \n \n \n \n 3 \n geohashZ) \n GeoHash   \n 00011011ZZ Peano  \n  Peano   01111000  \n \n  Peano   Hilbert   Peano Hilbert    GeoHash  Hilbert  PeanoPeano   \n"},{title:"Flink on K8s",frontmatter:{title:"Flink on K8s",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["","k8s","Flink","beam"],sticky:1},regularPath:"/%E4%BA%91%E5%8E%9F%E7%94%9F/fink-on-k8s.html",relativePath:"/fink-on-k8s.md",key:"v-83d8af48",path:"/2023/06/10/fink-on-k8s/",headers:[{level:2,title:"",slug:""},{level:3,title:"Flink",slug:"flink"},{level:2,title:"Flink Kubernetes Operator",slug:"flink-kubernetes-operator"},{level:2,title:"Flink Kubernetes Operator",slug:"flink-kubernetes-operator"},{level:3,title:"1Flink Kubernetes Operator",slug:"_1flink-kubernetes-operator"},{level:3,title:"2Flink Kubernetes Operator()",slug:"_2flink-kubernetes-operator-"},{level:3,title:"3Flink Webhook",slug:"_3flink-webhook"},{level:3,title:"4cert-manager",slug:"_4cert-manager"},{level:3,title:"5.Flink Kubernetes Operator",slug:"_5-flink-kubernetes-operator"},{level:2,title:"",slug:""},{level:3,title:"1Flink",slug:"_1flink"},{level:3,title:"2Flink",slug:"_2flink"},{level:3,title:"3Flink",slug:"_3flink"},{level:2,title:"",slug:""},{level:3,title:"1.Apllication",slug:"_1-apllication"},{level:3,title:"2.Session",slug:"_2-session"},{level:3,title:"3.ApplicationSession",slug:"_3-applicationsession"},{level:2,title:"HPA",slug:"-hpa"},{level:3,title:"Flink Reactivek8s HPA",slug:"flink-reactivek8s-hpa"},{level:3,title:"flink 1.17",slug:"flink-1-17"},{level:2,title:"beamtop on flink run k8s",slug:"beamtop-on-flink-run-k8s"},{level:3,title:"",slug:""},{level:3,title:"--",slug:"-"}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:'  \n Apache FlinkBIApache FlinkResource ProvidersKubernetesFlinkResource ProivderKubernetesFlink Kubernetes OperatorFlink \n Flink \n Apache Flink1.144Resouce ProvidersStandalone KubernetesYARNMesos1.143Standalone KubernetesYARNMesosFlink Resouce ProvidersApache Flink flink resource providers  \n StandaloneYARNYARNKubernetes \n 1Standalone \n StandaloneFlinkJobManagerTaskManagerJobManagerTaskManagerStandaloneFlinkJobManagerTaskManagerCPUTaskManagerSlotworkerTaskManagerTaskManager \n  StandaloneFlink Slot Standalone \n 2YARN \n FlinkStandalone  YARNKubernetesHadoopYARNYARNFlinkFlinkResourceManagerResourceManagerNodeManagerContainerJobManagerTaskMangerFlinkFlinkslotTaskkMangerJobManagerTaskMangerYARN \n FlinkYARNApllicationSessionPer-JobPer-JobYARN1.15 \n 3Kubernetes \n KubernetesFlinkK8sPodFlinkJobManagerTaskManagerPodYARNContainer \n Kubernetes FlinkNative KubernetesFlink Kubernetes OperatorNative KubernetesFlink Kubernetes Operator Flink Kubernetes OperatorApache FlinkFlinkK8sKubernetes Operator Kubernetes Operator  \n Kubernetes CPU   Kubernetes  KubernetesBigData On K8s \n Flink Kubernetes Operator \n Flink Kubernetes OperatorFlink flink-kubernetes-operator  \n Flink Kubernetes OperatorKubernetes APIFlink:\n1Flink ApplicationSessionFlinkDeploymentFlinkDeploymentFlinkK8s\n2FlinkDeployment\n3\n4Flink Kubernetes\n \n 1 2 3 4 5 \n Flink Kubernetes OperatorKubernetesControl planeFlinkFlinkNativek8sFlinkCRDOperatorFlink On K8s \n Flink Kubernetes Operator \n 1Flink Kubernetes Operator \n Flink Kubernetes Operator \n \n , Flink Kubernetes OperatorFlinkFlink Kubernetes OperatorK8s NameSpaceNameSpaceflinkFlink \n Flink Kubernetes Operator2, FlinkDeploymentFlinkSessionJob, 2, API Server, Flink Kubernetes Operator \n Flink Kubernetes Operator, 2Container2ContainerPodflink-operatorflink-webhookPodReplicaSetDeploymentJobManagerTaskManager, ServiceConfigMap, Serviceflink-webhook, ConfigMapFlinkoperator. \n FlinkK8s, Flink, FlinkDeployment YamlkubectlK8sFlink Kubernetes OperatorYamlFlink, YamlJobManager22JobManagerPodDeploymentServiceConfigMapIngress \n 2Flink Kubernetes Operator() \n Flink Kubernetes OperatorFlinkDeploymentFlinkSessionJobFlink Kubernetes OperatorFlinkreconcile \n 3Flink Webhook \n WebhookHTTPHTTP POSTWebhook \n KubernetesWebhookAPI Server \n Flink WebhookWebhookValidating Admission Webhook/validate WebhookMutating Admission Webhook/mutate \n Flink WebhookFlinkDeploymentETCDAPI Server/mutateinit Containersidecar ContainerFlinkDeploymentETCDAPI Server/validateyamlyaml \n Flink WebhookTLSHTTPSFlink Kubernetes Operatorcert-manager \n  4cert-manager \n Flink WebhookTLSHTTPSFlink Kubernetes Operatorcert-manager \n cert-manager.yamlcert-managercert-manager.yamlcert-manager cert-manager cert-manager.yamlcert-manager.yamlcert-manager.yaml3image: \n registry.cn-hangzhou.aliyuncs.com/cm_ns01/cert-manager-webhook:v1.10.0\nregistry.cn-hangzhou.aliyuncs.com/cm_ns01/cert-manager-cainjector:v1.10.0\nregistry.cn-hangzhou.aliyuncs.com/cm_ns01/cert-manager-controller:v1.10.0\n \n 1 2 3 cert-manager.yamlkubectl \n kubectl apply -f cert-manager.yaml\n \n 1 cert-manager3Pod2Service  \n  kubectl get all  -n  cert-manager\n \n 1 \n 5.Flink Kubernetes Operator \n Helmcert-managerFlink Kubernetes Operator \n 5.1 helm \n Flink Kubernetes Operatorhelm \n helm repo  add  flink-operator-repo https://downloads.apache.org/flink/flink-kubernetes-operator-1.5.0/\nhelm  install  flink-kubernetes-operator flink-operator-repo/flink-kubernetes-operator   --namespace  flink --create-namespace\n \n 1 2 Flink Kubernetes OperatorK8sflink Namespaceflink Namespace \n  \n kubectl get all -n flink -owide\nhelm list -n flink\n \n 1 2 5.2  \n Flink Kubernetes OperatorHAhelmFlink Kubernetes OperatorHelm flink-kubernetes-operator-1.5.0-helm.tgz  values.yaml \n #12 \nreplicas:  2 \n #HA mode \nkubernetes.operator.leader-election.enabled:  true \nkubernetes.operator.leader-election.lease-name: flink-operator-lease\n \n 1 2 3 4 5  \n helm  install   -f   myvalues.yaml flink-kubernetes-operator  .   --namespace  flink --create-namespace\n #helm uninstall flink-kubernetes-operator -n flink \n \n 1 2 image \n #The Helm chart by default points to the ghcr.io/apache/flink-kubernetes-operator image repository. If you have connectivity issues or prefer to use Dockerhub instead you can use  \n --set   image.repository = apache/flink-kubernetes-operator \n \n 1 2 HAFlink Kubernetes Operator22flink kubernetes operator pod1Flink WebhookService \n #2 \nreplicas:  2 \n #HA mode \nkubernetes.operator.leader-election.enabled:  true \nkubernetes.operator.leader-election.lease-name: flink-operator-lease\n \n 1 2 3 4 5 \n  \n  1Flink \n Flink Kubernetes OperatorFlink Kubernetes Operator  https://github.com/apache/flink-kubernetes-operator/tags  Flink Kubernetes Operator \n basic.yamlFlink Kubernetes Operatorexamplesbasic.yaml \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment   # FlinkK8s \n metadata : \n   name :  basic - example   #  \n   namespace :  flink         # flink \n spec : \n   image :  apache/flink : 1.14.6 - scala_2.12       # Flink1.14.6  \n   flinkVersion :  v1_14     # Flink \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "2" \n   serviceAccount :  flink - service - account\n   jobManager : \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   taskManager : \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   job : \n     jarURI :  local : ///opt/flink/examples/streaming/StateMachineExample.jar   # FlinkJar \n     parallelism :   2 \n     upgradeMode :  stateless\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #   2Flink \n basic.yamlFlinkK8s \n kubectl apply  -f  basic.yaml\n \n 1  \n kubectl get all  -n  flink\n #kubectl get FlinkDeployment -n flink \n \n 1 2 basic.yamlK8sK8sflink2PodJobManagerPodTaskManagerPodbasic-exampleFlinkDeployment \n \n web ui ingress-flinkk8singress-nginx-controller \n apiVersion :  networking.k8s.io/v1\n kind :  Ingress\n metadata : \n   name :  flink - ingress\n   namespace :  flink\n spec : \n   rules : \n     -   host :  ingress.flink.com\n       http : \n         paths : \n           -   path :   "/" \n             pathType :  Prefix\n             backend : \n               service : \n                 name :  basic - example - rest\n                 port : \n                   number :   8081 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply  -f  ingress-flink.yaml\n \n 1 ingress \n kubectl get ingress  -n  flink\n \n 1 web ui \n 3Flink \n Flink \n kubectl delete  -f  basic.yaml\n #kubectl delete FlinkDeployment basic-example -n flink \n \n 1 2 #   \n KubernetesApllicationSession \n 1.Apllication \n 1.1Application \n ApplicationKubernetes FlinkFlink FlinkJobManager PodTaskManager PodTaskManager PodFlinkslotTaskManagerslotFlink10slotTaskManager4slotKubernetes3TaskManager PodFlinkFlinkKubernetesPod \n ApplicationFlink \n Application Kubernetes 2Flink Flink JarFlink FlinkFlinkDeployment yamlFlinkFlinkJar Flink Flink   Jar NFSKubernetesPVC+PVPod Base Flink  \n 1.2 \n WordCountFlinksocket \n maven pom.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 " \n          xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     < groupId > com.gordon </ groupId > \n     < artifactId > flink-on-k8s </ artifactId > \n     < version > 1.0-SNAPSHOT </ version > \n     < properties > \n         < maven.compiler.source > 8 </ maven.compiler.source > \n         < maven.compiler.target > 8 </ maven.compiler.target > \n         < flink.version > 1.14.6 </ flink.version > \n         < java.version > 1.8 </ java.version > \n         < scala.binary.version > 2.12 </ scala.binary.version > \n         < slf4j.version > 1.7.30 </ slf4j.version > \n     </ properties > \n\n     < dependencies > \n         \x3c!-- Flink--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-java </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-java_${scala.binary.version} </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-clients_${scala.binary.version} </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-scala_${scala.binary.version} </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         \x3c!--kafka--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-connector-kafka_${scala.binary.version} </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.commons </ groupId > \n             < artifactId > commons-lang3 </ artifactId > \n             < version > 3.9 </ version > \n         </ dependency > \n\n         \x3c!-- --\x3e \n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-api </ artifactId > \n             < version > ${slf4j.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-log4j12 </ artifactId > \n             < version > ${slf4j.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.logging.log4j </ groupId > \n             < artifactId > log4j-to-slf4j </ artifactId > \n             < version > 2.14.0 </ version > \n         </ dependency > \n\n     </ dependencies > \n\n\n     < build > \n         < plugins > \n             \x3c!--docker--\x3e \n             < plugin > \n                 < groupId > com.spotify </ groupId > \n                 < artifactId > docker-maven-plugin </ artifactId > \n                 < version > 1.0.0 </ version > \n                 < configuration > \n                     \x3c!--<dockerDirectory>src/main/docker</dockerDirectory>--\x3e \n                     < dockerDirectory > ${project.basedir} </ dockerDirectory > \n                     < resources > \n                         < resource > \n                             < targetPath > / </ targetPath > \n                             < directory > ${project.build.directory} </ directory > \n                             < includes > ${project.build.finalName}.jar </ includes > \n                         </ resource > \n                     </ resources > \n                 </ configuration > \n             </ plugin > \n             \x3c!--,--\x3e \n             < plugin > \n                 < artifactId > maven-antrun-plugin </ artifactId > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < configuration > \n                             < tasks > \n                                 < copy   todir = " ${project.basedir} "   file = " target/${project.artifactId}-${project.version}.${project.packaging} " > </ copy > \n                             </ tasks > \n                         </ configuration > \n                         < goals > \n                             < goal > run </ goal > \n                         </ goals > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n             \x3c!-- --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-shade-plugin </ artifactId > \n                 < version > 2.3 </ version > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < goals > \n                             < goal > shade </ goal > \n                         </ goals > \n                         < configuration > \n                             < filters > \n                                 < filter > \n                                     < artifact > *:* </ artifact > \n                                     < excludes > \n                                         \x3c!--\n                                        zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --\x3e \n                                         < exclude > META-INF/*.SF </ exclude > \n                                         < exclude > META-INF/*.DSA </ exclude > \n                                         < exclude > META-INF/*.RSA </ exclude > \n                                     </ excludes > \n                                 </ filter > \n                             </ filters > \n                             < transformers > \n                                 < transformer   implementation = " org.apache.maven.plugins.shade.resource.ManifestResourceTransformer " > \n                                     \x3c!-- jar() --\x3e \n                                     < mainClass > </ mainClass > \n                                 </ transformer > \n                             </ transformers > \n                         </ configuration > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n             \x3c!--<plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <version>3.0.0</version>\n                <configuration>\n                    <descriptorRefs>\n                        <descriptorRef>jar-with-dependencies</descriptorRef>\n                    </descriptorRefs>\n                </configuration>\n                <executions>\n                    <execution>\n                        <id>make-assembly</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>--\x3e \n\n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.8.0 </ version > \n                 < configuration > \n                     < source > ${java.version} </ source > \n                     < target > ${java.version} </ target > \n                     < encoding > UTF-8 </ encoding > \n                 </ configuration > \n             </ plugin > \n\n         </ plugins > \n     </ build > \n\n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 StreamWordCount \n package   com . gordon . basic ; \n\n import   org . apache . flink . api . common . typeinfo . Types ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n import   org . apache . log4j . Logger ; \n import   java . util . Arrays ; \n\n\n public   class   StreamWordCount   { \n     private   static   Logger  logger  =   Logger . getLogger ( StreamWordCount . class ) ; \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         // 1.  \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         // 2.   k8s01 nc -lk 7777 \n         DataStreamSource < String >  lineDSS  =  env . socketTextStream ( "192.168.8.103" ,   7777 ) ; \n         // 3.  \n         SingleOutputStreamOperator < Tuple2 < String ,   Long > >  wordAndOne  =  lineDSS\n                 . flatMap ( ( String  line ,   Collector < String >  words )   ->   { \n                     Arrays . stream ( line . split ( " " ) ) . forEach ( words :: collect ) ; \n                 } ) \n                 . returns ( Types . STRING ) \n                 . map ( word  ->   Tuple2 . of ( word ,   1L ) ) \n                 . returns ( Types . TUPLE ( Types . STRING ,   Types . LONG ) ) ; \n         // 4.  \n         KeyedStream < Tuple2 < String ,   Long > ,   String >  wordAndOneKS  =  wordAndOne\n                 . keyBy ( t  ->  t . f0 ) ; \n         // 5.  \n         SingleOutputStreamOperator < Tuple2 < String ,   Long > >  result  =  wordAndOneKS\n                 . sum ( 1 ) ; \n         // 6.  \n        result . print ( ) ; \n         // 7.  \n        env . execute ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 FlinkJarKubernetes \n  1.3 \n 1 JarFlink \n DockerfileFlink Dockerfile \n # \n #FROM apache/flink:1.14.6-scala_2.12 \n FROM  personalharbor.com/bigdata/flink:1.14.6-scala_2.12 \n WORKDIR  /opt/flink \n #jarjar \n COPY  flink-on-k8s-1.0-SNAPSHOT.jar /opt/flink/flink-on-k8s-1.0-SNAPSHOT.jar \n ENTRYPOINT  [ "/docker-entrypoint.sh" ] \n EXPOSE  6123 8081 \n CMD  [ "help" ] \n \n 1 2 3 4 5 6 7 8 9 jarDockerfile \n flink-wc1.13.6 \n docker build -t flink-wc-add-jar:1.14.6 .\n \n 1 flink-wcKubernetes NodeFlinkyaml \n docker tag image_id registry_address/flink-wc-add-jar:1.14.6\ndocker push registry_address/flink-wc-add-jar:1.14.6\n \n 1 2 idedockerbuild \n pombuildplugin \n \x3c!--docker--\x3e \n             < plugin > \n                 < groupId > com.spotify </ groupId > \n                 < artifactId > docker-maven-plugin </ artifactId > \n                 < version > 1.0.0 </ version > \n                 < configuration > \n                     \x3c!--<dockerDirectory>src/main/docker</dockerDirectory>--\x3e \n                     < dockerDirectory > ${project.basedir} </ dockerDirectory > \n                     < resources > \n                         < resource > \n                             < targetPath > / </ targetPath > \n                             < directory > ${project.build.directory} </ directory > \n                             < includes > ${project.build.finalName}.jar </ includes > \n                         </ resource > \n                     </ resources > \n                 </ configuration > \n             </ plugin > \n             \x3c!--,--\x3e \n             < plugin > \n                 < artifactId > maven-antrun-plugin </ artifactId > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < configuration > \n                             < tasks > \n                                 < copy   todir = " ${project.basedir} "   file = " target/${project.artifactId}-${project.version}.${project.packaging} " > </ copy > \n                             </ tasks > \n                         </ configuration > \n                         < goals > \n                             < goal > run </ goal > \n                         </ goals > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 maven installjar \n \n dockerfilebuildrun \n # \n #FROM apache/flink:1.14.6-scala_2.12 \n FROM  personalharbor.com/bigdata/flink:1.14.6-scala_2.12 \n WORKDIR  /opt/flink \n #jarjar \n COPY  *.jar /opt/flink/flink-on-k8s-1.0-SNAPSHOT.jar \n ENTRYPOINT  [ "/docker-entrypoint.sh" ] \n EXPOSE  6123 8081 \n CMD  [ "help" ] \n \n 1 2 3 4 5 6 7 8 9 \n \n  \n docker  push personalharbor.com/bigdata/flink-wc-add-jar:1.14.6\n \n 1 \n Flinkyaml yaml \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   namespace :  flink\n   name :  application - deployment\n spec : \n   image :  personalharbor.com/bigdata/flink - wc - add - jar : 1.14.6   #  \n   flinkVersion :  v1_14\n   imagePullPolicy :  IfNotPresent    #  \n   ingress :     # ingressflink web \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "2" \n   serviceAccount :  flink\n   jobManager : \n     replicas :   1 \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   taskManager : \n     replicas :   1 \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   job : \n     jarURI :  local : ///opt/flink/flink - on - k8s - 1.0 - SNAPSHOT.jar\n     entryClass :  com.gordon.basic.StreamWordCount\n     args : \n     parallelism :   1 \n     upgradeMode :  stateless\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 yamlFlinkDeploymentFlink Kubernetes OperatorFlinkCRDimagesFlinkjob.jarURIFlinkJarjob.entryClassFlink \n Flink FlinkncFlinkncnc -lk 7777Flink \n FlinkyamlKubernetesPod \n kubectl apply  -f  flink-wc.yaml\nkubectl get all  -n  flink\n \n 1 2 \n nckubectl logs taskmanager-pod-name -n flinkFlink \n \n ingressyamlwebuihttp://flink.k8s.io/flink/application-deployment/ \n  2 JarPV \n Kubernetes PVC PVFlinkJarPVPVCPVNFSKubernetesNFSStorageClass \n flink-jar-pvc.yaml \n Flink jar pvc \n apiVersion :  v1\n kind :  PersistentVolumeClaim\n metadata : \n   name :  flink - jar - pvc   # jar pvc \n   namespace :  flink\n spec : \n   storageClassName :  nfs - client    #sc,kubectl get sc \n   accessModes : \n     -  ReadOnlyMany    #ReadOnlyMany \n   resources : \n     requests : \n       storage :  1Gi     # \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 \n NFSflink-jar-pvc \n #kubectl get pvc -n flinkVolumes \n #kubectl describe pv pvc-944394b1-2fe2-4bfe-80a8-5eff0ffd19ca  -n flink  \n #jarnfs \n scp  flink-on-k8s-1.0-SNAPSHOT.jar  192.168 .8.210:/opt/nfsdata/flink-flink-jar-pvc-pvc-944394b1-2fe2-4bfe-80a8-5eff0ffd19ca\n \n 1 2 3 4 \n Flinkyaml application-deployment-with-pv.yamlyaml \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   namespace :  flink\n   name :  application - deployment\n spec : \n   image :  personalharbor.com/bigdata/flink : 1.14.6 - scala_2.12   #  \n   flinkVersion :  v1_14\n   imagePullPolicy :  IfNotPresent    #  \n   ingress :     # ingressflink web \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "2" \n   serviceAccount :  flink\n   jobManager : \n     replicas :   1 \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   taskManager : \n     replicas :   1 \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   podTemplate : \n     spec : \n       containers : \n         -   name :  flink - main - container\n           volumeMounts : \n             -   name :  flink - jar   # nfsjar \n              #flink \n              #/opt/flink, \n               mountPath :  /opt/flink/jar\n       volumes : \n         -   name :  flink - jar\n           persistentVolumeClaim : \n             claimName :  flink - jar - pvc\n   job : \n     jarURI :  local : ///opt/flink/jar/flink - on - k8s - 1.0 - SNAPSHOT.jar\n     entryClass :  com.gordon.basic.StreamWordCount\n     args : \n     parallelism :   1 \n     upgradeMode :  stateless\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 FlinkyamlyamlimagesFlinkFlinkpodTemplatePVFlink/opt/flink/jar(  )job.jarURIFlinkJarjob.entryClassFlink \n , \n kubectl apply  -f  flink-wc-with-pv.yaml\nkubectl get all  -n  flink\n \n 1 2 \n cpu \n 0 /3 nodes are available:  1  node ( s )  had taint  { node.kubernetes.io/disk-pressure:  } , that the pod didn\'t tolerate,  2  Insufficient cpu.\n \n 1  \n kubectl get  node   -o  yaml  |   grep  taint  -A   5 \n \n 1 #masterpod \n kubectl taint nodes  --all  node-role.kubernetes.io/master-\n \n 1  \n 1.4 \n Application2,  \n Application2JarFlink  \n JarFlink1Flink600M \n JarPVFlinkJarFlink \n 2.Session \n 2.1Session \n Session KubernetesFlink JobManagerFlinkHAJobManagerTaskManagerKubernetesTaskManagerTaskManager PodFlinkslotTaskManagerslot JobManager TaskManagerslotFlinkTaskManagerPodJobManagerFlink \n SessionFlinkJobManager Flink   Flink  \n SessionKubernetes2Flink \n Flink Web UIRestfulFlinkJar FlinkJobManagerFlink Web UIJarFlink RestfulJarFlink \n HTTPFlinkJar FlinkJarFlinkJarHTTPtomcatnginx, FllinkFlinkSessionJob yaml, KuberneteskubectlKubernetes APIFlinkSessionJob \nFlinkDeploymentFlinkSessionJobFlink Kubernetes OperatorFlinkK8sCRDFlinkSessionFlinkDeploymentFlinkFlink Web UIIngressFlinkJobManagerTaskManagerFlinkSessionJobFlinkJarURI \n  2.2 \n StreamWordCount \n  2.3 \n  1 RestfulFlink Web UIJar \n FlinkyamlFlink yaml \nFlink Session \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   namespace :  flink\n   name :  session - deployment - only\n spec : \n   image :  personalharbor.com/bigdata/flink : 1.14.6 - scala_2.12\n   flinkVersion :  v1_14\n   imagePullPolicy :  IfNotPresent    #  \n   ingress :     # ingressflink web \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "2" \n   serviceAccount :  flink\n   jobManager : \n     replicas :   1 \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   taskManager : \n     replicas :   1 \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 yamlFlinkDeploymentApplicationimagesFlinkApplicationFlinkIngressRestfulJobManagerIngressFlinkFlink \nApplicationyamlSessionyamljob \nFlinkyamlKubernetesPod \n kubectl apply  -f  session-deployment-only.yaml\nkubectl get all  -n  flink\n \n 1 2 \n Filnk Web UIjar IngressFlink Web UIIngressflink.k8s.ioWindowsC:\\Windows\\System32\\drivers\\etc\\hostsIPIPIngress PodKubernetes NodeIPUbuntuMac/etc/hosts \nIngress \n kubectl get ingress  -n  flink \nkubectl get pod  -n  ingress-nginx  -owide \n \n 1 2 \n controller/etc/hosts \n web ui yaml / \n flink.k8s.io/ { { namespace } } / { { name } } ( / | $ ) ( .* ) \n == > http://flink.k8s.io/flink/session-deployment-only/\n \n 1 2 \n \n \n web uicancell jobweb.cancel.enablefalse \n #cm \nkubectl get cm  -n  flink \n #session-deployment-only.yaml \nweb.cancel.enable:  "true" \n # \nkubectl apply  -f  session-deployment-only.yaml\n \n 1 2 3 4 5 6 \n 2 HTTPFlinkJar \n HTTPjarmaven \n https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.14.6/flink-examples-streaming_2.12-1.14.6-TopSpeedWindowing.jar \n HTTP TomcatTomcat \nwget https://dlcdn.apache.org/tomcat/tomcat-10/v10.1.7/bin/apache-tomcat-10.1.7.tar.gz \n  \n 1.8080 \n netstat  -anp | grep   8080 \n \n 1 server.xml \n \n 2.TomcatTomcatconf/web.xmllistingstrue \n \n FlinkJarTomcatwebappsFlinkJarwebapps/jars/flinkFlinkHTTPJarTaskManager PodTomcatTomcatKubernetesNode \n TomcatTomcatFlinkJar \n \n FlinkyamlFlink Restful \n Flinkjaryaml  \nFlink \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkSessionJob\n metadata : \n   namespace :  flink\n   name :  session - job - only\n spec : \n   deploymentName :  session - deployment - only   #  \n   job : \n     #jarURI: https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.14.6/flink-examples-streaming_2.12-1.14.6-TopSpeedWindowing.jar # httpjar \n     jarURI :  http : //192.168.8.104 : 9090/jars/flink/flink - on - k8s - 1.0 - SNAPSHOT.jar  # tomcat httpjar \n     entryClass :  com.gordon.basic.StreamWordCount\n     args : \n     parallelism :   1    #  \n     upgradeMode :  stateless\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 mavenjar \n \n tomcat \n 2.4 \n Session2FlinkJarSession2 \n 3.ApplicationSession \n ApplicationSession \nApplicationSession \n ApplicationFlink RestfulSessionApplicationApplicationFlink On K8s \n HPA \n Flink Reactivek8s HPA \n 1. \n KafkaStringGeneratorJob \n import   org . apache . commons . lang3 . RandomStringUtils ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . api . java . utils . ParameterTool ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . connector . kafka . sink . KafkaRecordSerializationSchema ; \n import   org . apache . flink . connector . kafka . sink . KafkaSink ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichParallelSourceFunction ; \n\n import   java . util . Random ; \n\n public   class   KafkaStringGeneratorJob   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         final   ParameterTool  params  =   ParameterTool . fromArgs ( args ) ; \n         String  kafkaTopic  =  params . get ( "kafka-topic" ,   "flink-kafka" ) ; \n         String  brokers  =  params . get ( "brokers" ,   "192.168.8.102:9092,192.168.8.103:9092,192.168.8.104:9092" ) ; \n\n         final   StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . addSource ( new   RichParallelSourceFunction < String > ( )   { \n             private   Random  random  ; \n             private   boolean  flag  =   true ; \n\n             @Override \n             public   void   open ( Configuration  parameters )   throws   Exception   { \n                 super . open ( parameters ) ; \n                random  =   new   Random ( ) ; \n             } \n\n             @Override \n             public   void   run ( SourceContext < String >  sourceContext )   throws   Exception   { \n                 while   ( flag ) { \n                     String  value  =   RandomStringUtils . randomAlphanumeric ( random . nextInt ( 5 ) ) ; \n                    sourceContext . collect ( value ) ; \n                     //Thread.sleep(50); \n                 } \n             } \n             @Override \n             public   void   cancel ( )   { \n                flag  =   false ; \n             } \n         } ) \n                 . sinkTo ( \n                         KafkaSink . < String > builder ( ) \n                                 . setBootstrapServers ( brokers ) \n                                 . setRecordSerializer ( \n                                         KafkaRecordSerializationSchema . builder ( ) \n                                                 . setValueSerializationSchema ( new   SimpleStringSchema ( ) ) \n                                                 . setTopic ( kafkaTopic ) \n                                                 . build ( ) ) \n                                 . build ( ) ) \n                 . setParallelism ( 6 ) ; \n\n        env . execute ( "HpaWordCount Kafka data generator job" ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 HpaWordCountForCpu auto scale \n import   org . apache . flink . api . common . eventtime . WatermarkStrategy ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . api . java . utils . ParameterTool ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . connector . kafka . source . KafkaSource ; \n import   org . apache . flink . connector . kafka . source . enumerator . initializer . OffsetsInitializer ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . windowing . ProcessAllWindowFunction ; \n import   org . apache . flink . streaming . api . windowing . assigners . TumblingProcessingTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n import   org . apache . flink . streaming . api . windowing . windows . TimeWindow ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . HashMap ; \n\n public   class   HpaWordCountForCpu   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         final   ParameterTool  params  =   ParameterTool . fromArgs ( args ) ; \n         String  kafkaTopic  =  params . get ( "kafka-topic" ,   "hpa-test" ) ; \n         System . out . println ( "kafkaTopic = "   +  kafkaTopic ) ; \n         String  brokers  =  params . get ( "brokers" ,   "192.168.8.102:9092,192.168.8.103:9092,192.168.8.104:9092" ) ; \n         int  kafkaParallelism  =  params . getInt ( "kafka-parallelism" ,   1 ) ; \n         System . out . println ( "kafkaParallelism = "   +  kafkaParallelism ) ; \n         // create the environment to create streams and configure execution \n         final   StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . enableCheckpointing ( 5   *   60   *   1000L ) ; \n\n         KafkaSource < String >  kafkaSource  =   KafkaSource . < String > builder ( ) \n                 . setBootstrapServers ( brokers ) \n                 . setGroupId ( "stateMachineExample" ) \n                 . setTopics ( kafkaTopic ) \n                 . setValueOnlyDeserializer ( new   SimpleStringSchema ( ) ) \n                 . setStartingOffsets ( OffsetsInitializer . latest ( ) ) \n                 . build ( ) ; \n         DataStream < String >  source  =  env . fromSource ( \n                kafkaSource ,   WatermarkStrategy . noWatermarks ( ) ,   "HpaWordCount" ) . setParallelism ( kafkaParallelism ) . rebalance ( ) ; \n         //source.print(); \n         SingleOutputStreamOperator < String >  reduce  =  source\n                 . map ( new   RichMapFunction < String ,   Tuple2 < Integer ,   Integer > > ( )   { \n                     @Override \n                     public   Tuple2 < Integer ,   Integer >   map ( String  s )   throws   Exception   { \n                         return   Tuple2 . of ( s . length ( ) ,   1 ) ; \n                     } \n                 } ) \n                 . keyBy ( t  ->  t . f0 ) \n                 . window ( TumblingProcessingTimeWindows . of ( Time . minutes ( 2 ) ) ) \n                 . reduce ( ( x ,  y )   ->   ( Tuple2 . of ( x . f0 ,  x . f1  +  y . f1 ) ) ) \n                 . map ( x  ->   String . format ( "key is %S,value is %s" ,  x . f0 ,  x . f1 ) ) ; \n\n        reduce . print ( ) ; \n\n         // trigger program execution \n        env . execute ( "HpaWordCount" ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 #  2.yaml \n  \n flink-service-account \n apiVersion :  v1\n kind :  ServiceAccount\n metadata : \n   name :  flink - service - account\n --- \n apiVersion :  rbac.authorization.k8s.io/v1\n kind :  Role\n metadata : \n   name :  flink - service\n rules : \n   -   apiGroups :   [ "" ] \n     resources :   [ "pods" ,   "services" , "configmaps" ] \n     verbs :   [ "create" ,   "get" ,   "list" ,   "watch" ,   "delete" ] \n   -   apiGroups :   [ "" ] \n     resources :   [ "pods/log" ] \n     verbs :   [ "get" ] \n   -   apiGroups :   [ "batch" ] \n     resources :   [ "jobs" ] \n     verbs :   [ "create" ,   "get" ,   "list" ,   "watch" ,   "delete" ] \n   -   apiGroups :   [ "extensions" ] \n     resources :   [ "ingresses" ] \n     verbs :   [ "create" ,   "get" ,   "list" ,   "watch" ,   "delete" ] \n --- \n apiVersion :  rbac.authorization.k8s.io/v1\n kind :  RoleBinding\n metadata : \n   name :  flink - service - role - binding\n roleRef : \n   apiGroup :  rbac.authorization.k8s.io\n   kind :  Role\n   name :  flink - service\n subjects : \n   -   kind :  ServiceAccount\n     name :  flink - service - account\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 helm \n 1.chart \n helm create flink-hpa\n \n 1 2.values \n mv  values.yaml values.yaml.bak\n \n 1 3.template \n flink-hpa\n charts\n Chart.yaml\n templates\n    flink-configmap.yaml\n    jobmanager-application.yaml\n    jobmanager-rest-service.yaml\n    jobmanager-service.yaml\n    taskmanager-autoscaler.yaml\n    taskmanager-job-deployment.yaml\n    taskmanager-query-state-service.yaml\n values.yaml.bak\n \n 1 2 3 4 5 6 7 8 9 10 11 12 jobmanager-application.yaml \n apiVersion :  batch / v1\nkind :   Job \nmetadata : \n  name :  flink - jobmanager\nspec : \n  parallelism :   1  #  Set  the value  to   greater  than  1   to   start  standby  JobManagers \n  template : \n    metadata : \n      #annotations : \n prometheus . io / port :   \'9249\' \n prometheus . io / scrape :   \'true\' \n      labels : \n        app :  flink\n        component :  jobmanager\n    spec : \n      restartPolicy :   OnFailure \n\n      hostAliases : \n         -  ip :   192.168 .8 .102 \n          hostnames : \n             -   "node3" \n         -  ip :   192.168 .8 .103 \n          hostnames : \n             -   "node2" \n         -  ip :   192.168 .8 .104 \n          hostnames : \n             -   "node1" \n      containers : \n         -  name :  jobmanager\n          image :  personalharbor . com / bigdata / flink : 1.14 .6 - scala_2 . 12 \n          imagePullPolicy :   IfNotPresent \n          env : \n          args :   [ "standalone-job" ,   "--job-classname" ,   "com.gordon.hpa.HpaWordCountForCpu" , "--kafka-topic" , "hpa-test" , "--kafkaParallelism" , "1"   ] \n          ports : \n             -  containerPort :   6123 \n              name :  rpc\n             -  containerPort :   6124 \n              name :  blob - server\n             -  containerPort :   8081 \n              name :  webui\n          livenessProbe : \n            tcpSocket : \n              port :   6123 \n            initialDelaySeconds :   30 \n            periodSeconds :   60 \n          volumeMounts : \n             -  name :  flink - config - volume\n              mountPath :   / opt / flink / conf\n             -  name :  flink - data - volume\n              mountPath :   / flink - data\n             -  name :  flink - jar\n              mountPath :   / opt / flink / usrlib\n          securityContext : \n            runAsUser :   9999   # refers  to   user  _flink_ from official flink image ,  change  if  necessary\n      serviceAccountName :  flink - service - account #  Service  account which has the permissions  to   create ,  edit ,  delete  ConfigMaps \n      volumes : \n         -  name :  flink - config - volume\n          configMap : \n            name :  flink - config\n            items : \n               -  key :  flink - conf . yaml\n                path :  flink - conf . yaml\n               -  key :  log4j - console . properties\n                path :  log4j - console . properties\n         -  name :  flink - data - volume\n Flink  is designed  to   run  as a specific user  with   restricted  privileges . \n The  owner of the host path should be set  to   "flink"   with   the  user  ID   ( UID )  of  9999. \n          hostPath : \n            # The  base directory of the  JobResultStore  isn\'t  accessible .  No  dirty  JobResults  can be restored\n            path :   / tmp / flink   # chown  9999 : 9999   - R   / tmp / flink\n            type :   Directory \n         -  name :  flink - jar\n          persistentVolumeClaim : \n            claimName :  flink - jar - pvc\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 taskmanager-job-deployment.yaml \n apiVersion :  apps/v1\n kind :  Deployment\n metadata : \n   name :  flink - taskmanager\n spec : \n   replicas :   1   # here, we configure the scale \n   selector : \n     matchLabels : \n       app :  flink\n       component :  taskmanager\n   template : \n     metadata : \n       #annotations: \n prometheus.io/port: \'9249\' \n prometheus.io/scrape: \'true\' \n       labels : \n         app :  flink\n         component :  taskmanager\n     spec : \n \n       hostAliases : \n         -   ip :  192.168.8.102\n           hostnames : \n             -   "node3" \n         -   ip :  192.168.8.103\n           hostnames : \n             -   "node2" \n         -   ip :  192.168.8.104\n           hostnames : \n             -   "node1" \n       containers : \n       -   name :  taskmanager\n         image :  personalharbor.com/bigdata/flink : 1.14.6 - scala_2.12\n         imagePullPolicy :  IfNotPresent\n         resources : \n           requests : \n             cpu :  500m\n           limits : \n             cpu :  1000m\n         env : \n         args :   [ "taskmanager" ] \n         ports : \n         -   containerPort :   6122 \n           name :  rpc\n         -   containerPort :   6125 \n           name :  query - state\n         livenessProbe : \n           tcpSocket : \n             port :   6122 \n           initialDelaySeconds :   30 \n           periodSeconds :   60 \n         volumeMounts : \n         -   name :  flink - config - volume\n           mountPath :  /opt/flink/conf/\n         -   name :  flink - data - volume\n           mountPath :  /flink - data\n         -   name :  flink - jar\n           mountPath :  /opt/flink/usrlib\n         securityContext : \n           runAsUser :   9999    # refers to user _flink_ from official flink image, change if necessary \n       serviceAccountName :  flink - service - account  # Service account which has the permissions to create, edit, delete ConfigMaps \n       volumes : \n       -   name :  flink - config - volume\n         configMap : \n           name :  flink - config\n           items : \n           -   key :  flink - conf.yaml\n             path :  flink - conf.yaml\n           -   key :  log4j - console.properties\n             path :  log4j - console.properties\n       -   name :  flink - data - volume\nFlink is designed to run as a specific user with restricted privileges. \nThe owner of the host path should be set to "flink" with the user ID (UID) of 9999. \n         hostPath : \n             #The base directory of the JobResultStore isn\'t accessible. No dirty JobResults can be restored \n           path :  /tmp/flink    # chown 9999:9999 -R /tmp/flink \n           type :  Directory\n       -   name :  flink - jar\n         persistentVolumeClaim : \n           claimName :  flink - jar - pvc\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 flink-configmap.yaml conf \n apiVersion :  v1\n kind :  ConfigMap\n metadata : \n   name :  flink - config\n   labels : \n     app :  flink\n data : \n   flink-conf.yaml :   | +\n     jobmanager.rpc.address :  flink - jobmanager\n     taskmanager.numberOfTaskSlots :   2 \n     blob.server.port :   6124 \n     jobmanager.rpc.port :   6123 \n     taskmanager.rpc.port :   6122 \n     jobmanager.memory.process.size :  4096m\n     taskmanager.memory.process.size :  4096m\n     taskmanager.memory.managed.fraction :   0.7 \n     state.backend.rocksdb.metrics.block-cache-capacity :   true \n     state.backend.rocksdb.metrics.block-cache-pinned-usage :   true \n     state.backend.rocksdb.metrics.block-cache-usage :   true \n     restart-strategy :  fixeddelay\n     restart-strategy.fixed-delay.attempts :   100000 \n     scheduler-mode :  reactive\n     heartbeat.timeout :   10000 \n     heartbeat.interval :   5000 \n     rest.flamegraph.enabled :   true \n     # \n     state.savepoints.dir :  file : ///flink - data/savepoints\n     state.checkpoints.dir :  file : ///flink - data/checkpoints\n     kubernetes.cluster-id :  cluster1337\n     high-availability :  org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory\n     high-availability.storageDir :  file : ///flink - data/ha\n     execution.checkpointing.interval :   120000 \n     web.cancel.enable :   true \n     metrics.reporter.prom.class :  org.apache.flink.metrics.prometheus.PrometheusReporter\n     metrics.reporter.prom.port :   9249 \n     metrics.latency.interval :   1000 \n     metrics.latency.granularity :  operator\n     kubernetes.namespace :  flink\n     kubernetes.service-account :  flink - service - account\n   log4j-console.properties :   | +\nThis affects logging for both user code and Flink \n    rootLogger.level = INFO\n    rootLogger.appenderRef.console.ref = ConsoleAppender\n    rootLogger.appenderRef.rolling.ref = RollingFileAppender\nUncomment this if you want to _only_ change Flink\'s logging \n     #logger.flink.name = org.apache.flink \n     #logger.flink.level = INFO \nThe following lines keep the log level of common libraries/connectors on \nlog level INFO. The root logger does not override this. You have to manually \nchange the log levels here. \n    logger.akka.name = akka\n    logger.akka.level = INFO\n    logger.kafka.name= org.apache.kafka\n    logger.kafka.level = INFO\n    logger.hadoop.name = org.apache.hadoop\n    logger.hadoop.level = INFO\n    logger.zookeeper.name = org.apache.zookeeper\n    logger.zookeeper.level = INFO\nLog all infos to the console \n    appender.console.name = ConsoleAppender\n    appender.console.type = CONSOLE\n    appender.console.layout.type = PatternLayout\n    appender.console.layout.pattern = %d { yyyy - MM - dd HH : mm : ss , SSS }  % - 5p % - 60c %x  -  %m%n\nLog all infos in the given rolling file \n    appender.rolling.name = RollingFileAppender\n    appender.rolling.type = RollingFile\n    appender.rolling.append = false\n    appender.rolling.fileName = $ { sys : log.file } \n    appender.rolling.filePattern = $ { sys : log.file } .%i\n    appender.rolling.layout.type = PatternLayout\n    appender.rolling.layout.pattern = %d { yyyy - MM - dd HH : mm : ss , SSS }  % - 5p % - 60c %x  -  %m%n\n    appender.rolling.policies.type = Policies\n    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy\n    appender.rolling.policies.size.size=100MB\n    appender.rolling.strategy.type = DefaultRolloverStrategy\n    appender.rolling.strategy.max = 10\nSuppress the irrelevant (wrong) warnings from the Netty channel handler \n    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline\n    logger.netty.level = OFF    \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 jobmanager-service.yaml \n apiVersion :  v1\n kind :  Service\n metadata : \n   name :  flink - jobmanager\n spec : \n   type :  ClusterIP\n   ports : \n   -   name :  rpc\n     port :   6123 \n   -   name :  blob - server\n     port :   6124 \n   -   name :  webui\n     port :   8081 \n   selector : \n     app :  flink\n     component :  jobmanager\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 jobmanager-rest-service.yaml \n apiVersion :  v1\n kind :  Service\n metadata : \n   name :  flink - jobmanager - rest\n spec : \n   type :  NodePort\n   selector : \n     app :  flink\n     component :  jobmanager\n   ports : \n   -   name :  rest\n     port :   8081 \n     targetPort :   8081 \n     nodePort :   30091 \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 taskmanager-query-state-service.yaml \n apiVersion :  v1\n kind :  Service\n metadata : \n   name :  flink - taskmanager - query - state\n spec : \n   type :  NodePort\n   ports : \n     -   name :  query - state\n       port :   6125 \n       targetPort :   6125 \n       nodePort :   30025 \n   selector : \n     app :  flink\n     component :  taskmanager\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 taskmanager-autoscaler.yaml \n apiVersion :  autoscaling/v2\n kind :  HorizontalPodAutoscaler\n metadata : \n   name :  flink - taskmanager\n spec : \n   scaleTargetRef : \n     apiVersion :  apps/v1\n     kind :  Deployment\n     name :  flink - taskmanager\n   minReplicas :   1 \n   maxReplicas :   6 \n   metrics : \n     -   type :  Resource\n       resource : \n         name :  cpu\n         target : \n           type :  Utilization\n           averageUtilization :   60 \n     #- type: Pods \n pods: \n   metric: \n     name: state_usage \n   target: \n     type: AverageValue \n     averageValue: 0.6 \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #  3.helm \n helmyaml \n helm  install  flink-hpa flink-hpa/  -n  flink \n \n 1 4.target tm  \n \n \n kafkabehavior0 \n  \n \n  \n \n 1pvcjar/opt/flink/jar \n Caused by: java.lang.ClassNotFoundException: com.gordon.hpa.HpaWordCountForCpu\n \n 1 1flinkjarliblib/opt/flink/jar/opt/flink/usrlib \n  \n 1.usrlib \n \n 2.usrlib \n \n usrliblib \n #jar \nOne way is putting them  in  the  ` lib/ `  directory,  which  will result  in  the user code jar being loaded by the system classloader.\n #jar \nThe other way is creating a  ` usrlib/ `  directory  in  the parent directory of  ` lib/ `  and putting the user code jar  in  the  ` usrlib/ `  directory.\n \n 1 2 3 4 2 \n Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: GET at: https://10.10.0.1/api/v1/namespaces/default/pods?labelSelector=app%3Dkaibo-test%2Ccomponent%3Dtaskmanager%2Ctype%3Dflink-native-kubernetes. Message: Forbidden!Configured service account doesn\'t have access. Service account may have been revoked. pods is forbidden: User "system:serviceaccount:default:default" cannot list resource "pods" in API group "" in the namespace "default".\n \n 1 2flink-service-account.yaml \n kubectl apply  -f  flink-service-account.yaml  -n  flink\n # \nkubectl create serviceaccount flink-service-account\nkubectl create clusterrolebinding flink-role-binding-flink  --clusterrole = edit  --serviceaccount = flink:flink-service-account\n \n 1 2 3 4 3 \n #\nThe base directory of the JobResultStore isn\'t accessible. No dirty JobResults can be restored\n#\nMkdirs failed to create file\n \n 1 2 3 4 3 \n chown   9999 :9999  -R  /tmp/flink\n \n 1 #  flink 1.17 \n flink 1.171.15/1.16  Job vertex parallelism overrides  (must have) \n  \n pom \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 " \n          xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     < groupId > com.gordon </ groupId > \n     < artifactId > flink-on-k8s </ artifactId > \n     < version > 1.0-SNAPSHOT </ version > \n\n     < repositories > \n         < repository > \n             < id > central </ id > \n             < url > https://repo1.maven.org/maven2/ </ url > \n         </ repository > \n     </ repositories > \n\n     < properties > \n         < maven.compiler.source > 8 </ maven.compiler.source > \n         < maven.compiler.target > 8 </ maven.compiler.target > \n        \x3c!-- <flink.version>1.14.6</flink.version>--\x3e \n         < flink.version > 1.17.1 </ flink.version > \n         < java.version > 1.8 </ java.version > \n         < scala.binary.version > 2.12 </ scala.binary.version > \n         < slf4j.version > 1.7.30 </ slf4j.version > \n     </ properties > \n\n\n     < dependencies > \n         \x3c!-- Flink--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-java </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-java </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-clients </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-scala_${scala.binary.version} </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-connector-kafka </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n\n         \x3c!--1.14.6--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-clients_${scala.binary.version}</artifactId>\n            <version>${flink.version}</version>\n            <scope>provided</scope>\n        </dependency>--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-streaming-java_${scala.binary.version}</artifactId>\n            <version>${flink.version}</version>\n            <scope>provided</scope>\n        </dependency>--\x3e \n         \x3c!--kafka--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-connector-kafka_${scala.binary.version}</artifactId>\n            <version>${flink.version}</version>\n        </dependency>--\x3e \n\n         < dependency > \n             < groupId > org.apache.commons </ groupId > \n             < artifactId > commons-lang3 </ artifactId > \n             < version > 3.9 </ version > \n         </ dependency > \n\n         \x3c!-- --\x3e \n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-api </ artifactId > \n             < version > ${slf4j.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-log4j12 </ artifactId > \n             < version > ${slf4j.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.logging.log4j </ groupId > \n             < artifactId > log4j-to-slf4j </ artifactId > \n             < version > 2.14.0 </ version > \n         </ dependency > \n\n     </ dependencies > \n\n\n     < build > \n         < plugins > \n             \x3c!--docker--\x3e \n             \x3c!--<plugin>\n                <groupId>com.spotify</groupId>\n                <artifactId>docker-maven-plugin</artifactId>\n                <version>1.0.0</version>\n                <configuration>\n                    &lt;!&ndash;<dockerDirectory>src/main/docker</dockerDirectory>&ndash;&gt;\n                    <dockerDirectory>${project.basedir}</dockerDirectory>\n                    <resources>\n                        <resource>\n                            <targetPath>/</targetPath>\n                            <directory>${project.build.directory}</directory>\n                            <includes>${project.build.finalName}.jar</includes>\n                        </resource>\n                    </resources>\n                </configuration>\n            </plugin>\n            &lt;!&ndash;,&ndash;&gt;\n            <plugin>\n                <artifactId>maven-antrun-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <configuration>\n                            <tasks>\n                                <copy todir="${project.basedir}" file="target/${project.artifactId}-${project.version}.${project.packaging}"></copy>\n                            </tasks>\n                        </configuration>\n                        <goals>\n                            <goal>run</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>--\x3e \n             \x3c!-- --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-shade-plugin </ artifactId > \n                 < version > 2.3 </ version > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < goals > \n                             < goal > shade </ goal > \n                         </ goals > \n                         < configuration > \n                             < filters > \n                                 < filter > \n                                     < artifact > *:* </ artifact > \n                                     < excludes > \n                                         \x3c!--\n                                        zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --\x3e \n                                         < exclude > META-INF/*.SF </ exclude > \n                                         < exclude > META-INF/*.DSA </ exclude > \n                                         < exclude > META-INF/*.RSA </ exclude > \n                                     </ excludes > \n                                 </ filter > \n                             </ filters > \n                             < transformers > \n                                 < transformer   implementation = " org.apache.maven.plugins.shade.resource.ManifestResourceTransformer " > \n                                     \x3c!-- jar() --\x3e \n                                     < mainClass > com.gordon.basic.StreamWordCount </ mainClass > \n                                 </ transformer > \n                             </ transformers > \n                         </ configuration > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n             \x3c!--<plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <version>3.0.0</version>\n                <configuration>\n                    <descriptorRefs>\n                        <descriptorRef>jar-with-dependencies</descriptorRef>\n                    </descriptorRefs>\n                </configuration>\n                <executions>\n                    <execution>\n                        <id>make-assembly</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>--\x3e \n\n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.8.0 </ version > \n                 < configuration > \n                     < source > ${java.version} </ source > \n                     < target > ${java.version} </ target > \n                     < encoding > UTF-8 </ encoding > \n                 </ configuration > \n             </ plugin > \n\n         </ plugins > \n     </ build > \n\n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 AutoscalingExample \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . sink . SinkFunction ; \n import   org . slf4j . Logger ; \n import   org . slf4j . LoggerFactory ; \n\n /** Autoscaling Example For Flink 1.17.1. */ \n public   class   AutoscalingExample   { \n     private   static   final   Logger   LOG   =   LoggerFactory . getLogger ( AutoscalingExample . class ) ; \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         long  numIterations  =   Long . parseLong ( args [ 0 ] ) ; \n         DataStream < Long >  stream  = \n                env . fromSequence ( Long . MIN_VALUE ,   Long . MAX_VALUE ) . filter ( i  ->   System . nanoTime ( )   >   1 ) ; \n        stream  = \n                stream . shuffle ( ) \n                         . map ( \n                                 new   RichMapFunction < Long ,   Long > ( )   { \n\n                                     @Override \n                                     public   Long   map ( Long  i )   throws   Exception   { \n                                         //LOG.info("LOG: current is {}",i); \n                                         //System.out.println(String.format("current is %s",i)); \n                                         long  end  =   0 ; \n                                         for   ( int  j  =   0 ;  j  <  numIterations ;  j ++ )   { \n                                            end  =   System . nanoTime ( ) ; \n                                         } \n                                         return  end ; \n                                     } \n                                 } ) ; \n        stream . addSink ( \n                 new   SinkFunction < Long > ( )   { \n                     @Override \n                     public   void   invoke ( Long  value ,   Context  context )   throws   Exception   { \n                         // Do nothing \n                     } \n                 } ) ; \n        env . execute ( "Autoscaling Example" ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   name :  autoscaling - example\n   namespace :  flink\n spec : \n   image :  personalharbor.com/bigdata/flink : 1.17.1 - scala_2.12\n   flinkVersion :  v1_17\n   imagePullPolicy :  IfNotPresent    #  \n   flinkConfiguration : \n     kubernetes.operator.job.autoscaler.enabled :   "true" \n     pipeline.max-parallelism :   "12" \n     kubernetes.operator.job.autoscaler.metrics.window :   "3m" \n     kubernetes.operator.job.autoscaler.stabilization.interval :   "1m" \n     kubernetes.operator.job.autoscaler.target.utilization :   "0.6" \n     kubernetes.operator.job.autoscaler.target.utilization.boundary :   "0.2" \n     kubernetes.operator.job.autoscaler.restart.time :   "2m" \n     kubernetes.operator.job.autoscaler.catch-up.duration :   "5m" \n     taskmanager.numberOfTaskSlots :   "4" \n     state.savepoints.dir :  file : ///flink - data/savepoints\n     state.checkpoints.dir :  file : ///flink - data/checkpoints\n     high-availability :  org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory\n     high-availability.storageDir :  file : ///flink - data/ha\n     execution.checkpointing.interval :   "1m" \n     web.cancel.enable :   "true" \n   serviceAccount :  flink\n   ingress :     # ingressflink web \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   jobManager : \n     resource : \n       memory :   "1024m" \n       cpu :   1 \n   taskManager : \n     resource : \n       memory :   "1024m" \n       cpu :   0.5 \n   podTemplate : \n     spec : \n \n       hostAliases : \n         -   ip :  192.168.8.102\n           hostnames : \n             -   "node3" \n         -   ip :  192.168.8.103\n           hostnames : \n             -   "node2" \n         -   ip :  192.168.8.104\n           hostnames : \n             -   "node1" \n       containers : \n         -   name :  flink - main - container\n           volumeMounts : \n             -   mountPath :  /flink - data\n               name :  flink - volume\n             -   mountPath :  /opt/flink/jar\n               name :  flink - jar\n           securityContext : \n             runAsUser :   9999    # refers to user _flink_ from official flink image, change if necessary \n       volumes : \n         -   name :  flink - jar\n           persistentVolumeClaim : \n             claimName :  flink - jar - pvc\n         -   name :  flink - volume\nFlink is designed to run as a specific user with restricted privileges. \nThe owner of the host path should be set to "flink" with the user ID (UID) of 9999. \n           hostPath : \n             #The base directory of the JobResultStore isn\'t accessible. No dirty JobResults can be restored \n             path :  /tmp/flink    # chown 9999:9999 -R /tmp/flink \n             type :  Directory\n   job : \n     jarURI :  local : ///opt/flink/jar/flink - on - k8s - 1.0 - SNAPSHOT.jar\n     entryClass :  com.gordon.hpa.AutoscalingExample\n     args :   [ "10" ] \n     parallelism :   1 \n     upgradeMode :  last - state\n     state :  running\n     savepointTriggerNonce :   0 \n   logConfiguration : \n     "log4j-console.properties" :   | \n      rootLogger.level = DEBUG\n      rootLogger.appenderRef.file.ref = LogFile\n      rootLogger.appenderRef.console.ref = LogConsole\n      appender.file.name = LogFile\n      appender.file.type = File\n      appender.file.append = false\n      appender.file.fileName = ${sys:log.file}\n      appender.file.layout.type = PatternLayout\n      appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n      appender.console.name = LogConsole\n      appender.console.type = CONSOLE\n      appender.console.layout.type = PatternLayout\n      appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n      logger.akka.name = akka\n      logger.akka.level = INFO\n      logger.kafka.name= org.apache.kafka\n      logger.kafka.level = INFO\n      logger.hadoop.name = org.apache.hadoop\n      logger.hadoop.level = INFO\n      logger.zookeeper.name = org.apache.zookeeper\n      logger.zookeeper.level = INFO\n      logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline\n      logger.netty.level = OFF \n\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107  \n JobVertexScaler \n  \n Component  responsible  for  computing vertex parallelism based on the scaling metrics . \n \n 1  \n      public   int   computeScaleTargetParallelism ( \n             AbstractFlinkResource < ? ,   ? >  resource , \n             Configuration  conf , \n             JobVertexID  vertex , \n             Map < ScalingMetric ,   EvaluatedScalingMetric >  evaluatedMetrics , \n             SortedMap < Instant ,   ScalingSummary >  history )   { \n\t\t // \n         var  currentParallelism  =   ( int )  evaluatedMetrics . get ( PARALLELISM ) . getCurrent ( ) ; \n         //record \n         double  averageTrueProcessingRate  =  evaluatedMetrics . get ( TRUE_PROCESSING_RATE ) . getAverage ( ) ; \n        \n\t\t //targetCapacity =Math.round(lagCatchupTargetRate + restartCatchupRate + inputTargetAtUtilization) \n         //++ \n         double  targetCapacity  = \n                 AutoScalerUtils . getTargetProcessingCapacity ( \n                        evaluatedMetrics ,  conf ,  conf . get ( TARGET_UTILIZATION ) ,   true ) ; \n         //minScaleFactor<=scaleFactor<=maxScaleFactor \n         double  scaleFactor  =  targetCapacity  /  averageTrueProcessingRate ; \n         double  minScaleFactor  =   1   -  conf . get ( MAX_SCALE_DOWN_FACTOR ) ; \n         double  maxScaleFactor  =   1   +  conf . get ( MAX_SCALE_UP_FACTOR ) ; \n         if   ( scaleFactor  <  minScaleFactor )   { \n             LOG . debug ( \n                     "Computed scale factor of {} for {} is capped by maximum scale down factor to {}" , \n                    scaleFactor , \n                    vertex , \n                    minScaleFactor ) ; \n            scaleFactor  =  minScaleFactor ; \n         }   else   if   ( scaleFactor  >  maxScaleFactor )   { \n             LOG . debug ( \n                     "Computed scale factor of {} for {} is capped by maximum scale up factor to {}" , \n                    scaleFactor , \n                    vertex , \n                    maxScaleFactor ) ; \n            scaleFactor  =  maxScaleFactor ; \n         } \n\n         // TargetCapacity \n         double  cappedTargetCapacity  =  averageTrueProcessingRate  *  scaleFactor ; \n         LOG . debug ( "Capped target processing capacity for {} is {}" ,  vertex ,  cappedTargetCapacity ) ; \n        \n\t\t // \n         //int newParallelism =(int) Math.min(Math.ceil(scaleFactor * parallelism), Integer.MAX_VALUE) \n         //newParallelism \n        \n         int  newParallelism  = \n                 scale ( \n                        currentParallelism , \n                         ( int )  evaluatedMetrics . get ( MAX_PARALLELISM ) . getCurrent ( ) , \n                        scaleFactor , \n                        conf . getInteger ( VERTEX_MIN_PARALLELISM ) , \n                        conf . getInteger ( VERTEX_MAX_PARALLELISM ) ) ; \n  \n         if   ( newParallelism  ==  currentParallelism\n                 ||   blockScalingBasedOnPastActions ( \n                        resource , \n                        vertex , \n                        conf , \n                        evaluatedMetrics , \n                        history , \n                        currentParallelism , \n                        newParallelism ) )   { \n             return  currentParallelism ; \n         } \n\n         // We record our expectations for this scaling operation \n        evaluatedMetrics . put ( \n                 ScalingMetric . EXPECTED_PROCESSING_RATE , \n                 EvaluatedScalingMetric . of ( cappedTargetCapacity ) ) ; \n         return  newParallelism ; \n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70  \n  \n \n  \n \n  \n beamtop on flink run k8s \n flink k8s operator apiflink1.13k8s api \n  \n WordCount \n flink-beam-examplepomDockfile \n \x3c!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n"License"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n--\x3e \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     \x3c!--<parent>\n        <groupId>org.apache.flink</groupId>\n        <artifactId>flink-kubernetes-operator-parent</artifactId>\n        <version>1.5.0</version>\n        <relativePath>../..</relativePath>\n    </parent>--\x3e \n     < groupId > org.apache.flink </ groupId > \n     < artifactId > flink-beam-example </ artifactId > \n     < version > 1.5.0 </ version > \n     < name > Flink Beam Example </ name > \n\n     \x3c!-- Given that this is an example skip maven deployment --\x3e \n     < properties > \n         < maven.deploy.skip > true </ maven.deploy.skip > \n         < beam.version > 2.47.0 </ beam.version > \n\n         < slf4j.version > 1.7.36 </ slf4j.version > \n         < log4j.version > 2.17.1 </ log4j.version > \n     </ properties > \n\n     < repositories > \n         < repository > \n             < id > confluent </ id > \n             < url > https://packages.confluent.io/maven/ </ url > \n         </ repository > \n     </ repositories > \n\n     < dependencies > \n         \x3c!-- Apache Beam dependencies --\x3e \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-core </ artifactId > \n             < version > ${beam.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-examples-java </ artifactId > \n             < version > ${beam.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > org.springframework </ groupId > \n                     < artifactId > spring-jcl </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-flink-1.14 </ artifactId > \n             < version > ${beam.version} </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-google-cloud-platform </ artifactId > \n             < version > ${beam.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > junit </ groupId > \n                     < artifactId > junit </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         \x3c!-- Add logging framework, to produce console output when running in the IDE. --\x3e \n         \x3c!-- These dependencies are excluded from the application JAR by default. --\x3e \n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-api </ artifactId > \n             < version > ${slf4j.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.logging.log4j </ groupId > \n             < artifactId > log4j-slf4j-impl </ artifactId > \n             < version > ${log4j.version} </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.logging.log4j </ groupId > \n             < artifactId > log4j-api </ artifactId > \n             < version > ${log4j.version} </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.logging.log4j </ groupId > \n             < artifactId > log4j-core </ artifactId > \n             < version > ${log4j.version} </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n     </ dependencies > \n\n     < build > \n         < plugins > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-shade-plugin </ artifactId > \n                 < version > 3.1.1 </ version > \n                 < executions > \n                     \x3c!-- Run shade goal on package phase --\x3e \n                     < execution > \n                         < phase > package </ phase > \n                         < goals > \n                             < goal > shade </ goal > \n                         </ goals > \n                         < configuration > \n                             < artifactSet > \n                                 < excludes > \n                                     < exclude > org.apache.flink:flink-shaded-force-shading </ exclude > \n                                     < exclude > com.google.code.findbugs:jsr305 </ exclude > \n                                     < exclude > org.slf4j:* </ exclude > \n                                     < exclude > org.apache.logging.log4j:* </ exclude > \n                                 </ excludes > \n                             </ artifactSet > \n                             < filters > \n                                 < filter > \n                                     \x3c!-- Do not copy the signatures in the META-INF folder.\n                                    Otherwise, this might cause SecurityExceptions when using the JAR. --\x3e \n                                     < artifact > *:* </ artifact > \n                                     < excludes > \n                                         < exclude > META-INF/versions/9/module-info.class </ exclude > \n                                         < exclude > META-INF/*.SF </ exclude > \n                                         < exclude > META-INF/*.DSA </ exclude > \n                                         < exclude > META-INF/*.RSA </ exclude > \n                                     </ excludes > \n                                 </ filter > \n                             </ filters > \n                             < transformers > \n                                 < transformer   implementation = " org.apache.maven.plugins.shade.resource.ServicesResourceTransformer " /> \n                             </ transformers > \n                         </ configuration > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n         </ plugins > \n     </ build > \n </ project > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 ################################################################################ \n Licensed to the Apache Software Foundation (ASF) under one \n or more contributor license agreements.  See the NOTICE file \n distributed with this work for additional information \n regarding copyright ownership.  The ASF licenses this file \n to you under the Apache License, Version 2.0 (the \n "License"); you may not use this file except in compliance \n with the License.  You may obtain a copy of the License at \n\n     http://www.apache.org/licenses/LICENSE-2.0 \n\n Unless required by applicable law or agreed to in writing, software \n distributed under the License is distributed on an "AS IS" BASIS, \n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n See the License for the specific language governing permissions and \nlimitations under the License. \n ################################################################################ \n #FROM flink:1.14.3 # \n FROM  personalharbor.com/bigdata/flink:1.14.6-scala_2.12 \n\n RUN  mkdir /opt/flink/usrlib \n ADD  target/flink-beam-example-*.jar /opt/flink/usrlib/beam-runner.jar \n\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 beam-example.yaml \n ################################################################################ \n Licensed to the Apache Software Foundation (ASF) under one \n or more contributor license agreements.  See the NOTICE file \n distributed with this work for additional information \n regarding copyright ownership.  The ASF licenses this file \n to you under the Apache License, Version 2.0 (the \n "License"); you may not use this file except in compliance \n with the License.  You may obtain a copy of the License at \n\n     http://www.apache.org/licenses/LICENSE-2.0 \n\n Unless required by applicable law or agreed to in writing, software \n distributed under the License is distributed on an "AS IS" BASIS, \n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n See the License for the specific language governing permissions and \nlimitations under the License. \n ################################################################################ \n\n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   name :  beam - example\n spec : \n   image :  personalharbor.com/bigdata/flink - beam - example : latest\n   flinkVersion :  v1_14\n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "1" \n   serviceAccount :  flink\n   jobManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   taskManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   job : \n     entryClass :  org.apache.beam.examples.WordCount\n     jarURI :  local : ///opt/flink/usrlib/beam - runner.jar\n     args :   [   "--runner=FlinkRunner" ,   "--output=file://opt/output.txt"   ] \n     parallelism :   1 \n     upgradeMode :  stateless\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  \n kubectl apply  - f beam - example.yaml  - n flink \n \n 1 #  -- \n pom \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 " \n          xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     < groupId > com.gordon </ groupId > \n     < artifactId > apache-beam </ artifactId > \n     < version > 1.0-SNAPSHOT </ version > \n\n     < repositories > \n         < repository > \n             < id > central </ id > \n             < url > https://repo1.maven.org/maven2/ </ url > \n         </ repository > \n     </ repositories > \n\n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n         < apache.beam.version > 2.47.0 </ apache.beam.version > \n         < commons.io.version > 2.8.0 </ commons.io.version > \n         < flink.version > 1.14.6 </ flink.version > \n         < spark.version > 3.1.2 </ spark.version > \n         < jackson.version > 2.14.1 </ jackson.version > \n     </ properties > \n     \x3c!----\x3e \n     < dependencyManagement > \n         < dependencies > \n             < dependency > \n                 < groupId > commons-io </ groupId > \n                 < artifactId > commons-io </ artifactId > \n                 < version > ${commons.io.version} </ version > \n             </ dependency > \n             < dependency > \n                 < groupId > com.fasterxml.jackson.core </ groupId > \n                 < artifactId > jackson-databind </ artifactId > \n                 < version > ${jackson.version} </ version > \n             </ dependency > \n             < dependency > \n                 < groupId > com.fasterxml.jackson.core </ groupId > \n                 < artifactId > jackson-core </ artifactId > \n                 < version > ${jackson.version} </ version > \n             </ dependency > \n\n         </ dependencies > \n     </ dependencyManagement > \n\n     < dependencies > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-core </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-direct-java </ artifactId > \n             < version > ${apache.beam.version} </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-flink-1.14 </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-spark-3 </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-jdbc </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > mysql </ groupId > \n             < artifactId > mysql-connector-java </ artifactId > \n             < version > 5.1.48 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > com.mchange </ groupId > \n             < artifactId > c3p0 </ artifactId > \n             < version > 0.9.5.4 </ version > \n         </ dependency > \n\n         \x3c!-- https://mvnrepository.com/artifact/org.apache.beam/beam-sdks-java-io-hcatalog --\x3e \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-hcatalog </ artifactId > \n             < version > ${apache.beam.version} </ version > \n\n         </ dependency > \n\n         \x3c!-- https://mvnrepository.com/artifact/org.apache.hive.hcatalog/hive-hcatalog-core --\x3e \n         < dependency > \n             < groupId > org.apache.hive.hcatalog </ groupId > \n             < artifactId > hive-hcatalog-core </ artifactId > \n             < version > 2.1.0 </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > org.apache.calcite </ groupId > \n                     < artifactId > calcite-avatica </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-hadoop-format </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n\n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-kafka </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.kafka </ groupId > \n             < artifactId > kafka-clients </ artifactId > \n             < version > 2.4.1 </ version > \n         </ dependency > \n\n\n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-examples-java </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         \x3c!-- https://mvnrepository.com/artifact/commons-cli/commons-cli --\x3e \n         < dependency > \n             < groupId > commons-cli </ groupId > \n             < artifactId > commons-cli </ artifactId > \n             < version > 1.4 </ version > \n         </ dependency > \n\n         \x3c!-- https://mvnrepository.com/artifact/commons-io/commons-io --\x3e \n         < dependency > \n             < groupId > commons-io </ groupId > \n             < artifactId > commons-io </ artifactId > \n         </ dependency > \n\n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-clients_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n         \x3c!--1.12--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-runtime_2.12</artifactId>\n            <version>${flink.version}</version>\n            <scope>provided</scope>\n        </dependency>--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-runtime </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-java_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n\n\n\n\n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-core_2.12 </ artifactId > \n             < version > ${spark.version} </ version > \n             < scope > provided </ scope > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.module </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-streaming_2.12 </ artifactId > \n             < version > ${spark.version} </ version > \n             < scope > provided </ scope > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > com.fasterxml.jackson.core </ groupId > \n             < artifactId > jackson-core </ artifactId > \n         </ dependency > \n\n         < dependency > \n             < groupId > com.fasterxml.jackson.core </ groupId > \n             < artifactId > jackson-databind </ artifactId > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-extensions-sql </ artifactId > \n             < version > ${apache.beam.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > jackson-databind </ artifactId > \n                 </ exclusion > \n                 < exclusion > \n                     < artifactId > jackson-annotations </ artifactId > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n\n     </ dependencies > \n\n     < build > \n         < plugins > \n\n             \x3c!--  --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.5.1 </ version > \n                 < configuration > \n                     < source > 1.8 </ source > \n                     < target > 1.8 </ target > \n                     \x3c!--<encoding>${project.build.sourceEncoding}</encoding>--\x3e \n                 </ configuration > \n             </ plugin > \n\n             \x3c!-- () --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-shade-plugin </ artifactId > \n                 < version > 2.3 </ version > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < goals > \n                             < goal > shade </ goal > \n                         </ goals > \n                         < configuration > \n                             < filters > \n                                 < filter > \n                                     < artifact > *:* </ artifact > \n                                     < excludes > \n                                         \x3c!--\n                                        zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --\x3e \n                                         < exclude > META-INF/*.SF </ exclude > \n                                         < exclude > META-INF/*.DSA </ exclude > \n                                         < exclude > META-INF/*.RSA </ exclude > \n                                     </ excludes > \n                                 </ filter > \n                             </ filters > \n                             < transformers > \n                                 < transformer   implementation = " org.apache.maven.plugins.shade.resource.ManifestResourceTransformer " > \n                                     \x3c!-- jar() --\x3e \n                                     < mainClass > com.gordon.docker.HelloDocker </ mainClass > \n                                 </ transformer > \n                             </ transformers > \n                         </ configuration > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n\n             \x3c!--docker--\x3e \n             \x3c!--<plugin>\n                <groupId>com.spotify</groupId>\n                <artifactId>docker-maven-plugin</artifactId>\n                <version>1.0.0</version>\n                <configuration>\n                    <dockerDirectory>src/main/docker</dockerDirectory>\n                    <resources>\n                        <resource>\n                            <targetPath>/</targetPath>\n                            <directory>${project.build.directory}</directory>\n                            <include>${project.build.finalName}.jar</include>\n                        </resource>\n                    </resources>\n                </configuration>\n            </plugin>\n            &lt;!&ndash;,&ndash;&gt;\n            <plugin>\n                <artifactId>maven-antrun-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <phase>package</phase>\n                        <configuration>\n                            <tasks>\n                                <copy todir="src/main/docker" file="target/${project.artifactId}-${project.version}.${project.packaging}"></copy>\n                            </tasks>\n                        </configuration>\n                        <goals>\n                            <goal>run</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>--\x3e \n\n         </ plugins > \n     </ build > \n\n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 AboutText \n import   com . gordon . uniform . common . WriteOneFilePerWindow ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . TextIO ; \n import   org . apache . beam . sdk . io . kafka . KafkaIO ; \n import   org . apache . beam . sdk . options . PipelineOptions ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . Create ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . Values ; \n import   org . apache . beam . sdk . transforms . windowing . FixedWindows ; \n import   org . apache . beam . sdk . transforms . windowing . Window ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . apache . beam . vendor . guava . v26_0_jre . com . google . common . collect . ImmutableMap ; \n import   org . apache . kafka . common . serialization . LongDeserializer ; \n import   org . apache . kafka . common . serialization . StringDeserializer ; \n import   org . joda . time . Duration ; \n import   org . slf4j . Logger ; \n import   org . slf4j . LoggerFactory ; \n\n import   java . util . Arrays ; \n import   java . util . List ; \n\n public   class   AboutText   { \n     private   static   final   Logger   LOG   =   LoggerFactory . getLogger ( AboutText . class ) ; \n     // Create a Java Collection, in this case a List of Strings. \n     static   final   List < String >   LINES   =   Arrays . asList ( \n             "To be, or not to be: that is the question: " , \n             "Whether \'tis nobler in the mind to suffer " , \n             "The slings and arrows of outrageous fortune, " , \n             "Or to take arms against a sea of troubles, " ) ; \n\n     public   static   void   main ( String [ ]  args )   { \n\n\n         //todo 1.pipline \n         //MyOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().as(MyOptions.class); \n         PipelineOptions  options  =   PipelineOptionsFactory . create ( ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         //todo 2. \n         //todo  ListPCollection \n         PCollection < String >  from_list  =  pipeline . apply ( "from List" ,   Create . of ( LINES ) ) ; \n        from_list . apply ( "Print elements" , \n                 MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                     LOG . warn ( "line is {}" , x ) ; \n                     System . out . println ( x ) ; \n                     return  x ; \n                 } ) ) ; \n\n\n         //todo  text file \n\n         PCollection < String >  from_text  =  pipeline . apply ( TextIO . read ( ) . from ( "input/input.txt" ) ) ; \n        from_text\n                 . apply ( "Print elements" , \n                         MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                             System . out . println ( x ) ; \n                             LOG . warn ( "input is {}" , x ) ; \n                             return  x ; \n                         } ) ) ; \n\n         // \n         //from_text.apply("write_text", TextIO.write().to(options.getOutput())); \n\n         //streaming \n\n         /*PCollection<String> from_text = pipeline.apply(TextIO.read()\n                .from(options.getInput())\n                .watchForNewFiles(\n                        // Check for new files every ten seconds\n                        Duration.standardSeconds(10),\n                        // Stop watching the filepattern if no new files appear within an minute\n                        afterTimeSinceNewOutput(Duration.standardMinutes(1)))\n        );\n\n        from_text.apply("Print elements",\n                MapElements.into(TypeDescriptors.strings()).via(x -> {\n                    System.out.println(x);\n                    return x;\n                }));*/ \n\n\n         //Must use windowed writes when applying WriteFiles to an unbounded PCollection \n         //text file \n        pipeline . apply ( KafkaIO . < Long ,   String > read ( ) . withBootstrapServers ( "node1:9092,node2:9092,node3:9092" ) \n                 . withTopic ( "hpa-test" ) \n                 . withKeyDeserializer ( LongDeserializer . class ) \n                 . withValueDeserializer ( StringDeserializer . class ) \n                 . withConsumerConfigUpdates ( ImmutableMap . of ( \n                         "group.id" ,   "beam-kafka" , \n                         "auto.offset.reset" ,   "latest" , \n                         "enable.auto.commit" ,   "true" \n                 ) ) \n                 . withoutMetadata ( ) ) \n                 . apply ( Values . < String > create ( ) ) \n                 . apply ( Window . < String > into ( FixedWindows . of ( Duration . standardMinutes ( 1 ) ) ) ) \n                 . apply ( new   WriteOneFilePerWindow ( "streamsink/part" , 2 ) ) ; \n                 //.apply(TextIO.write().to("streamsink/part").withShardNameTemplate(ShardNameTemplate.DIRECTORY_CONTAINER).withWindowedWrites().withNumShards(2)); \n\n         //TextIO.Read.withHintMatchesManyFiles() \n         // todo from_mysql \n\n         // todo from_hive \n\n         // todo kafka \n\n         // todo hadoop \n\n\n         //todo 4. \n        pipeline . run ( ) . waitUntilFinish ( ) ; \n\n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117  \n import   static   org . apache . beam . vendor . guava . v26_0_jre . com . google . common . base . MoreObjects . firstNonNull ; \n\n import   javax . annotation . Nullable ; \n import   org . apache . beam . sdk . io . FileBasedSink ; \n import   org . apache . beam . sdk . io . FileBasedSink . FilenamePolicy ; \n import   org . apache . beam . sdk . io . FileBasedSink . OutputFileHints ; \n import   org . apache . beam . sdk . io . TextIO ; \n import   org . apache . beam . sdk . io . fs . ResolveOptions . StandardResolveOptions ; \n import   org . apache . beam . sdk . io . fs . ResourceId ; \n import   org . apache . beam . sdk . transforms . DoFn ; \n import   org . apache . beam . sdk . transforms . PTransform ; \n import   org . apache . beam . sdk . transforms . windowing . BoundedWindow ; \n import   org . apache . beam . sdk . transforms . windowing . IntervalWindow ; \n import   org . apache . beam . sdk . transforms . windowing . PaneInfo ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . PDone ; \n\n\n import   java . time . Instant ; \n import   java . time . ZoneId ; \n import   java . time . format . DateTimeFormatter ; \n\n /**\n * A {@link DoFn} that writes elements to files with names deterministically derived from the lower\n * and upper bounds of their key (an {@link IntervalWindow}).\n *\n * <p>This is test utility code, not for end-users, so examples can be focused on their primary\n * lessons.\n */ \n public   class   WriteOneFilePerWindow   extends   PTransform < PCollection < String > ,   PDone >   { \n   private   static   final   DateTimeFormatter   FORMATTER_WINDOW   =   java . time . format . DateTimeFormatter . ofPattern ( "HH-mm" ) . withZone ( ZoneId . of ( "Asia/Shanghai" ) ) ; \n   private   static   final   DateTimeFormatter   FORMATTER_HOUR   =   java . time . format . DateTimeFormatter . ofPattern ( "yyyy-MM-dd/HH" ) . withZone ( ZoneId . of ( "Asia/Shanghai" ) ) ; \n   private   String  filenamePrefix ; \n   @Nullable   private   Integer  numShards ; \n\n   public   WriteOneFilePerWindow ( String  filenamePrefix ,   Integer  numShards )   { \n     this . filenamePrefix  =  filenamePrefix ; \n     this . numShards  =  numShards ; \n   } \n\n   @Override \n   public   PDone   expand ( PCollection < String >  input )   { \n     ResourceId  resource  =   FileBasedSink . convertToFileResourceIfPossible ( filenamePrefix ) ; \n     TextIO . Write  write  = \n         TextIO . write ( ) \n             . to ( new   PerWindowFiles ( resource ) ) \n             . withTempDirectory ( resource . getCurrentDirectory ( ) ) \n             . withWindowedWrites ( ) ; \n     if   ( numShards  !=   null )   { \n      write  =  write . withNumShards ( numShards ) ; \n     } \n     return  input . apply ( write ) ; \n   } \n\n   /**\n   * A {@link FilenamePolicy} produces a base file name for a write based on metadata about the data\n   * being written. This always includes the shard number and the total number of shards. For\n   * windowed writes, it also includes the window and pane index (a sequence number assigned to each\n   * trigger firing).\n   */ \n   public   static   class   PerWindowFiles   extends   FilenamePolicy   { \n\n     private   final   ResourceId  baseFilename ; \n\n     public   PerWindowFiles ( ResourceId  baseFilename )   { \n       this . baseFilename  =  baseFilename ; \n     } \n\n     public   String   filenamePrefixForWindow ( IntervalWindow  window )   { \n       String  prefix  = \n          baseFilename . isDirectory ( )   ?   ""   :   firstNonNull ( baseFilename . getFilename ( ) ,   "" ) ; \n       return   String . format ( \n           "%s/%s-%s" ,    FORMATTER_HOUR . format ( Instant . ofEpochMilli ( window . start ( ) . getMillis ( ) ) ) , prefix ,   FORMATTER_WINDOW . format ( Instant . ofEpochMilli ( window . end ( ) . getMillis ( ) ) ) ) ; \n     } \n\n     @Override \n     public   ResourceId   windowedFilename ( \n         int  shardNumber , \n         int  numShards , \n         BoundedWindow  window , \n         PaneInfo  paneInfo , \n         OutputFileHints  outputFileHints )   { \n       IntervalWindow  intervalWindow  =   ( IntervalWindow )  window ; \n       String  filename  = \n           String . format ( \n               "%s-%s-of-%s%s" , //prefixfilenamePrefixForWindow,, \n               filenamePrefixForWindow ( intervalWindow ) , \n              shardNumber , \n              numShards , \n              outputFileHints . getSuggestedFilenameSuffix ( ) ) ; \n       return  baseFilename\n           . getCurrentDirectory ( ) \n           . resolve ( filename ,   StandardResolveOptions . RESOLVE_FILE ) ; \n     } \n\n     @Override \n     public   ResourceId   unwindowedFilename ( \n         int  shardNumber ,   int  numShards ,   OutputFileHints  outputFileHints )   { \n       throw   new   UnsupportedOperationException ( "Unsupported." ) ; \n     } \n   } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 yaml \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   namespace :  flink\n   name :  beam - example\n spec : \n   image :  personalharbor.com/bigdata/flink : 1.14.6 - scala_2.12\n   flinkVersion :  v1_14\n   imagePullPolicy :  IfNotPresent    #  \n   ingress :     # ingressflink web \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "1" \n   serviceAccount :  flink\n   jobManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   taskManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   podTemplate : \n     spec : \n       containers : \n         -   name :  flink - main - container\n           volumeMounts : \n             -   name :  flink - data - volume\n               mountPath :  /flink - data\n             -   name :  flink - jar   # nfsjar \n               #flink \n               #/opt/flink, \n               mountPath :  /opt/flink/usrlib\n       volumes : \n         -   name :  flink - data - volume\nFlink is designed to run as a specific user with restricted privileges. \nThe owner of the host path should be set to "flink" with the user ID (UID) of 9999. \n           hostPath : \n             #The base directory of the JobResultStore isn\'t accessible. No dirty JobResults can be restored \n             path :  /tmp/flink    # chown 9999:9999 -R /tmp/flink \n             type :  Directory\n         -   name :  flink - jar\n           persistentVolumeClaim : \n             claimName :  flink - jar - pvc\n   job : \n     entryClass :  com.gordon.source_sink.AboutText\n     jarURI :  local : ///opt/flink/usrlib/apache - beam - 1.0 - SNAPSHOT.jar\n     args :   [   "--runner=org.apache.beam.runners.flink.FlinkRunner " , "--input=/flink-data/input/input.txt" ,   "--output=/flink-data/output/output"   ] \n     parallelism :   1 \n     upgradeMode :  stateless\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 About Kafka \n \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . runners . flink . FlinkRunner ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . kafka . KafkaIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . Values ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . apache . beam . vendor . guava . v26_0_jre . com . google . common . collect . ImmutableMap ; \n import   org . apache . flink . api . java . utils . ParameterTool ; \n import   org . apache . kafka . common . serialization . StringDeserializer ; \n import   org . apache . kafka . common . serialization . StringSerializer ; \n import   org . slf4j . Logger ; \n import   org . slf4j . LoggerFactory ; \n\n public   class   AboutKafka   { \n     private   static   final   Logger   LOG   =   LoggerFactory . getLogger ( AboutKafka . class ) ; \n     public   static   void   main ( String [ ]  args )   { \n\n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n        options . setRunner ( FlinkRunner . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         System . out . println ( "options.getRunner() = "   +  options . getRunner ( ) ) ; \n         final   ParameterTool  params  =   ParameterTool . fromArgs ( args ) ; \n         String  kafkaTopic  =  params . get ( "kafka-topic" ,   "hpa-test" ) ; \n         //todo KafkaIO \n         PCollection < String >  from_kafka  =  pipeline\n                 . apply ( KafkaIO . < String ,   String > read ( ) \n                         . withBootstrapServers ( "node1:9092,node2:9092,node3:9092" ) \n                         . withTopic ( kafkaTopic )    // use withTopics(List<String>) to read from multiple topics. \n                         . withKeyDeserializer ( StringDeserializer . class ) \n                         . withValueDeserializer ( StringDeserializer . class ) \n                         // Rest of the settings are optional : \n\n                         // you can further customize KafkaConsumer used to read the records by adding more \n                         // settings for ConsumerConfig. e.g : \n                         . withConsumerConfigUpdates ( ImmutableMap . of ( \n                                 "group.id" ,   "beam-kafka" , \n                                 "auto.offset.reset" ,   "latest" , \n                                 "enable.auto.commit" ,   "true" \n                         ) ) \n                         . withoutMetadata ( ) \n\n                 ) . apply ( Values . create ( ) ) \n                 . apply ( "from_kafka" ,   MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                     LOG . info ( "currnet value is: {}" , x ) ; \n                     System . out . println ( x ) ; \n                     return  x ; \n                 } ) ) ; \n\n\n         /*from_kafka.apply(KafkaIO.<Void, String>write()\n                        .withBootstrapServers("node1:9092,node2:9092,node3:9092")\n                        .withTopic("test01")  // use withTopics(List<String>) to read from multiple topics.\n                        .withValueSerializer(StringSerializer.class) // just need serializer for value\n                        .values());*/ \n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 beam-from-kafka.yaml \n \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   namespace :  flink\n   name :  beam - from - kafka\n spec : \n   image :  personalharbor.com/bigdata/flink : 1.14.6 - scala_2.12\n   flinkVersion :  v1_14\n   imagePullPolicy :  IfNotPresent    #  \n   ingress :     # ingressflink web \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "2" \n     web.cancel.enable :   "true" \n   serviceAccount :  flink\n   jobManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   taskManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   podTemplate : \n     spec : \n \n       hostAliases : \n         -   ip :  192.168.8.102\n           hostnames : \n             -   "node3" \n         -   ip :  192.168.8.103\n           hostnames : \n             -   "node2" \n         -   ip :  192.168.8.104\n           hostnames : \n             -   "node1" \n       containers : \n         -   name :  flink - main - container\n           volumeMounts : \n             -   name :  flink - data - volume\n               mountPath :  /flink - data\n             -   name :  flink - jar   # nfsjar \n               #flink \n               #/opt/flink, \n               mountPath :  /opt/flink/usrlib\n       volumes : \n         -   name :  flink - data - volume\nFlink is designed to run as a specific user with restricted privileges. \nThe owner of the host path should be set to "flink" with the user ID (UID) of 9999. \n           hostPath : \n             #The base directory of the JobResultStore isn\'t accessible. No dirty JobResults can be restored \n             path :  /tmp/flink    # chown 9999:9999 -R /tmp/flink \n             type :  Directory\n         -   name :  flink - jar\n           persistentVolumeClaim : \n             claimName :  flink - jar - pvc\n   job : \n     entryClass :  com.gordon.source_sink.AboutKafka\n     jarURI :  local : ///opt/flink/usrlib/apache - beam - 1.0 - SNAPSHOT.jar\n     args :   [   "--runner=org.apache.beam.runners.flink.FlinkRunner " ] \n     parallelism :   1 \n     upgradeMode :  stateless\n   logConfiguration : \n     "log4j-console.properties" :   | \n      rootLogger.level = INFO\n      rootLogger.appenderRef.file.ref = LogFile\n      rootLogger.appenderRef.console.ref = LogConsole\n      appender.file.name = LogFile\n      appender.file.type = File\n      appender.file.append = false\n      appender.file.fileName = ${sys:log.file}\n      appender.file.layout.type = PatternLayout\n      appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n      appender.console.name = LogConsole\n      appender.console.type = CONSOLE\n      appender.console.layout.type = PatternLayout\n      appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n      logger.akka.name = akka\n      logger.akka.level = WARN\n      logger.kafka.name= org.apache.kafka\n      logger.kafka.level = WARN\n      logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline\n      logger.netty.level = WARN \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 \n \n  \n \n import   com . gordon . create_pipeline . MyOptions ; \n import   com . gordon . uniform . common . MyHDFSSynchronization ; \n import   com . gordon . uniform . config . HDFSSinkConfig ; \n import   com . gordon . uniform . config . HdfsSourceConfig ; \n import   com . gordon . uniform . config . KafkaConfig ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . hadoop . format . HDFSSynchronization ; \n import   org . apache . beam . sdk . io . hadoop . format . HadoopFormatIO ; \n import   org . apache . beam . sdk . io . kafka . KafkaIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . * ; \n import   org . apache . beam . sdk . transforms . windowing . FixedWindows ; \n import   org . apache . beam . sdk . transforms . windowing . Window ; \n import   org . apache . beam . sdk . values . * ; \n import   org . apache . beam . vendor . guava . v26_0_jre . com . google . common . collect . ImmutableMap ; \n import   org . apache . flink . configuration . IllegalConfigurationException ; \n import   org . apache . hadoop . conf . Configuration ; \n import   org . apache . hadoop . io . LongWritable ; \n import   org . apache . hadoop . io . Text ; \n import   org . apache . hadoop . mapreduce . InputFormat ; \n import   org . apache . hadoop . mapreduce . OutputFormat ; \n import   org . apache . hadoop . mapreduce . lib . input . TextInputFormat ; \n import   org . apache . hadoop . mapreduce . lib . output . TextOutputFormat ; \n import   org . apache . kafka . common . serialization . StringDeserializer ; \n import   org . joda . time . Duration ; \n\n /**\n * kafkahdfs hdfs\n */ \n public   class   Computing   { \n     public   static   < V >   void   main ( String [ ]  args )   { \n\n         if ( System . getProperty ( "os.name" ) . toLowerCase ( ) . contains ( "windows" ) ) { \n             String  command_line  =   "--runner=org.apache.beam.runners.flink.FlinkRunner --kafkaConfig_isOpen=true --hdfsSourceConfig_isOpen=false --hdfsSinkConfig_isOpen=true --output=hdfs://node1:8020/data/output/fromKafka" ; \n            args  =  command_line . split ( "\\\\s+" ) ; \n         } \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . as ( MyOptions . class ) ; \n        options . setParallelism ( 1 ) ; \n        options . setCheckpointingInterval ( 2 * 60 * 1000L ) ; \n         System . out . println ( "options.getRunner() = "   +  options . getRunner ( ) ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         PCollection < KV < Text ,   Text > >  res  =   null ; \n\n         //kafka \n         boolean  kafkaConfig_isOpen  =  options . getKafkaConfig_isOpen ( ) ; \n         System . out . println ( "kafkaConfig_isOpen = "   +  kafkaConfig_isOpen ) ; \n         KafkaConfig  kafkaConfig  =   new   KafkaConfig ( ) ; \n        kafkaConfig . setOpen ( kafkaConfig_isOpen ) ; \n         if   ( kafkaConfig . isOpen ( ) )   { \n             PCollection < String >  from_kafka  =  pipeline\n                     . apply ( KafkaIO . < String ,   String > read ( ) \n                             . withBootstrapServers ( "node1:9092,node2:9092,node3:9092" ) \n                             . withTopic ( "hpa-test" )    // use withTopics(List<String>) to read from multiple topics. \n                             . withKeyDeserializer ( StringDeserializer . class ) \n                             . withValueDeserializer ( StringDeserializer . class ) \n                             // Rest of the settings are optional : \n                             // you can further customize KafkaConsumer used to read the records by adding more \n                             // settings for ConsumerConfig. e.g : \n                             . withConsumerConfigUpdates ( ImmutableMap . of ( \n                                     "group.id" ,   "beam-kafka" , \n                                     "auto.offset.reset" ,   "latest" , \n                                     "enable.auto.commit" ,   "true" \n                             ) ) \n                             . withoutMetadata ( ) \n                     ) . apply ( Values . create ( ) ) ; \n            res  =  from_kafka\n                     . apply ( "from_hadoop" ,   MapElements . into ( TypeDescriptors . kvs ( new   TypeDescriptor < Text > ( )   { \n                     } ,   new   TypeDescriptor < Text > ( )   { \n                     } ) ) . via ( x  ->   { \n                         System . out . println ( "kafka line is  = "   +  x ) ; \n                         return   KV . of ( new   Text ( String . valueOf ( System . currentTimeMillis ( ) ) ) ,   new   Text ( x ) ) ; \n                     } ) ) ; \n         } \n\n\n         Configuration  conf  =   new   Configuration ( ) ; \n        conf . set ( "fs.defaultFS" ,   "hdfs://node1:8020" ) ; \n        conf . set ( "fs.hdfs.impl" ,   "org.apache.hadoop.hdfs.DistributedFileSystem" ) ; \n\n         //hdfs  \n         boolean  hdfsSourceConfig_isOpen  =  options . getHdfsSourceConfig_isOpen ( ) ; \n         System . out . println ( "hdfsSourceConfig_isOpen = "   +  hdfsSourceConfig_isOpen ) ; \n         HdfsSourceConfig  hdfsSourceConfig  =   new   HdfsSourceConfig ( ) ; \n        hdfsSourceConfig . setOpen ( hdfsSourceConfig_isOpen ) ; \n\n         if   ( ! kafkaConfig . isOpen ( )   &&  hdfsSourceConfig . isOpen ( ) )   { \n             // Set Hadoop InputFormat, key and value class in configuration \n             //mapreduce map input keyvalue \n\n            conf . setClass ( "key.class" ,   LongWritable . class ,   Object . class ) ; \n            conf . setClass ( "value.class" ,   Text . class ,   Object . class ) ; \n            conf . setClass ( "mapreduce.job.inputformat.class" ,   TextInputFormat . class ,   InputFormat . class ) ; \n             //hdfs://node1:8020/data/input \n            conf . set ( "mapreduce.input.fileinputformat.inputdir" ,  options . getInput ( ) ) ; \n\n             PCollection < KV < LongWritable ,   Text > >  from_hdfs  =  pipeline . apply ( "from_hdfs" ,   HadoopFormatIO . < LongWritable ,   Text > read ( ) . withConfiguration ( conf ) ) ; \n            res  =  from_hdfs\n                     . apply ( "from_hadoop" ,   MapElements . into ( TypeDescriptors . kvs ( new   TypeDescriptor < Text > ( )   { \n                     } ,   new   TypeDescriptor < Text > ( )   { \n                     } ) ) . via ( x  ->   { \n                         System . out . println ( "line = "   +  x . getValue ( ) ) ; \n                         return   KV . of ( new   Text ( String . valueOf ( System . currentTimeMillis ( ) ) ) ,  x . getValue ( ) ) ; \n                     } ) ) ; \n         } \n\n\n         //hdfs \n         boolean  hdfsSinkConfig_isOpen  =  options . getHdfsSinkConfig_isOpen ( ) ; \n         System . out . println ( "hdfsSinkConfig_isOpen = "   +  hdfsSinkConfig_isOpen ) ; \n         HDFSSinkConfig  hdfsSinkConfig  =   new   HDFSSinkConfig ( ) ; \n        hdfsSinkConfig . setOpen ( hdfsSinkConfig_isOpen ) ; \n         if   ( hdfsSinkConfig . isOpen ( )   &&  res  !=   null )   { \n            conf . set ( HadoopFormatIO . JOB_ID ,   "AboutHadoop" ) ; \n            conf . setClass ( HadoopFormatIO . OUTPUT_FORMAT_CLASS_ATTR ,   TextOutputFormat . class ,   OutputFormat . class ) ; \n             //conf.setClass(HadoopFormatIO.OUTPUT_FORMAT_CLASS_ATTR, MyTextOutputFormat.class, OutputFormat.class); \n            conf . setClass ( HadoopFormatIO . OUTPUT_KEY_CLASS ,   Text . class ,   Object . class ) ; \n            conf . setClass ( HadoopFormatIO . OUTPUT_VALUE_CLASS ,   Text . class ,   Object . class ) ; \n            conf . setInt ( HadoopFormatIO . NUM_REDUCES ,   2 ) ; \n             //hdfs://node1:8020/data/output \n            conf . set ( HadoopFormatIO . OUTPUT_DIR ,  options . getOutput ( ) + "/current/" ) ; \n             if   ( res . isBounded ( ) . equals ( PCollection . IsBounded . UNBOUNDED ) \n                     ||   ! res . getWindowingStrategy ( ) . equals ( WindowingStrategy . globalDefault ( ) ) )   { \n\n                 ConfigTransform < Text ,   Text >  configTransform  =   new   ConfigTransform < > ( conf ) ; \n                res . apply ( Window . into ( FixedWindows . of ( Duration . standardMinutes ( 2 ) ) ) ) \n                         . setTypeDescriptor ( TypeDescriptors . kvs ( \n                                 new   TypeDescriptor < Text > ( )   { \n                                 } ,   new   TypeDescriptor < Text > ( )   { \n                                 } ) ) \n                         . apply ( \n                                 "writeStream" , \n                                 HadoopFormatIO . < Text ,   Text > write ( ) \n                                         . withConfigurationTransform ( configTransform ) \n                                         . withExternalSynchronization ( new   MyHDFSSynchronization ( conf . get ( HadoopFormatIO . OUTPUT_DIR ) ) ) ) ; \n                 //.withExternalSynchronization(new HDFSSynchronization(conf.get(HadoopFormatIO.OUTPUT_DIR)))); \n\n             }   else   { \n                res . apply ( \n                         "writeBatch" , \n                         HadoopFormatIO . < Text ,   Text > write ( ) \n                                 . withConfiguration ( conf ) \n                                 . withPartitioning ( ) \n                                 . withExternalSynchronization ( new   HDFSSynchronization ( conf . get ( HadoopFormatIO . OUTPUT_DIR ) ) ) ) ; \n             } \n         } \n\n         if   ( res  ==   null )   { \n             throw   new   IllegalConfigurationException ( "" ) ; \n         }   else   { \n            pipeline . run ( ) . waitUntilFinish ( ) ; \n         } \n\n     } \n\n     /** Simple transform for providing Hadoop {@link Configuration} into {@link HadoopFormatIO}. */ \n     private   static   class   ConfigTransform < KeyT ,   ValueT > \n             extends   PTransform < \n             PCollection < ?   extends  KV < KeyT ,   ValueT > > ,   PCollectionView < Configuration > >   { \n\n         private   final   transient   Configuration  hConf ; \n\n         private   ConfigTransform ( Configuration  hConf )   { \n             this . hConf  =  hConf ; \n         } \n\n         @Override \n         public   PCollectionView < Configuration >   expand ( PCollection < ?   extends  KV < KeyT ,   ValueT > >  input )   { \n             return  input\n                     . getPipeline ( ) \n                     . apply ( Create . < Configuration > of ( hConf ) ) \n                     . apply ( View . < Configuration > asSingleton ( ) . withDefaultValue ( hConf ) ) ; \n         } \n     } \n\n\n     private   static   class   ConvertToHadoopFormatFn < InputT ,   OutputT >   extends   DoFn < InputT ,   OutputT >   { \n\n         private   static   final   long  serialVersionUID  =   - 6841922575497475096L ; \n         private   final   SerializableFunction < InputT ,   OutputT >  transformFn ; \n\n         ConvertToHadoopFormatFn ( SerializableFunction < InputT ,   OutputT >  transformFn )   { \n             this . transformFn  =  transformFn ; \n         } \n\n         @DoFn.ProcessElement \n         public   void   processElement ( @DoFn.Element   InputT  element ,   OutputReceiver < OutputT >  outReceiver )   { \n            outReceiver . output ( transformFn . apply ( element ) ) ; \n         } \n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 \n import   org . apache . beam . runners . flink . FlinkPipelineOptions ; \n import   org . apache . beam . runners . spark . SparkPipelineOptions ; \n import   org . apache . beam . sdk . options . Default ; \n import   org . apache . beam . sdk . options . Description ; \n import   org . apache . beam . sdk . options . PipelineOptions ; \n\n public   interface   MyOptions   extends   SparkPipelineOptions , FlinkPipelineOptions , PipelineOptions   { \n     //You can also specify a description, which appears when a user passes as a command-line argument, and a default value.--help \n     //You set the description and default value using annotations \n     @Description ( "Input for the pipeline" ) \n     @Default.String ( "hdfs://node1:8020/data/input" ) \n     String   getInput ( ) ; \n     void   setInput ( String  input ) ; \n\n     @Description ( "Output for the pipeline" ) \n     @Default.String ( "hdfs://node1:8020/data/output" ) \n     String   getOutput ( ) ; \n     void   setOutput ( String  output ) ; \n\n     @Description ( "kafkaConf for the pipeline" ) \n     @Default.Boolean ( false ) \n     Boolean   getKafkaConfig_isOpen ( ) ; \n     void   setKafkaConfig_isOpen ( Boolean  isOpen ) ; \n\n     @Description ( "HdfsSource for the pipeline" ) \n     @Default.Boolean ( false ) \n     Boolean   getHdfsSourceConfig_isOpen ( ) ; \n     void   setHdfsSourceConfig_isOpen ( Boolean  isOpen ) ; \n\n\n     @Description ( "HdfsSink for the pipeline" ) \n     @Default.Boolean ( false ) \n     Boolean   getHdfsSinkConfig_isOpen ( ) ; \n     void   setHdfsSinkConfig_isOpen ( Boolean  isOpen ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 : \n  \n import   org . apache . beam . sdk . io . hadoop . format . ExternalSynchronization ; \n import   org . apache . beam . sdk . io . hadoop . format . HadoopFormatIO ; \n import   org . apache . beam . vendor . guava . v26_0_jre . com . google . common . base . Preconditions ; \n import   org . apache . hadoop . conf . Configuration ; \n import   org . apache . hadoop . fs . * ; \n import   org . apache . hadoop . hdfs . protocol . AlreadyBeingCreatedException ; \n import   org . apache . hadoop . ipc . RemoteException ; \n import   org . apache . hadoop . mapred . FileAlreadyExistsException ; \n import   org . apache . hadoop . mapreduce . JobID ; \n import   org . apache . hadoop . mapreduce . TaskAttemptID ; \n import   org . apache . hadoop . mapreduce . TaskID ; \n import   org . slf4j . Logger ; \n import   org . slf4j . LoggerFactory ; \n\n import   java . io . IOException ; \n import   java . io . Serializable ; \n import   java . time . Instant ; \n import   java . time . ZoneId ; \n import   java . time . format . DateTimeFormatter ; \n import   java . util . Random ; \n\n /**\n * Implementation of {@link ExternalSynchronization} which registers locks in the HDFS.\n *\n * <p>Requires {@code locksDir} to be specified. This directory MUST be different that directory\n * which is possibly stored under {@code "mapreduce.output.fileoutputformat.outputdir"} key.\n * Otherwise setup of job will fail because the directory will exist before job setup.\n */ \n public   class   MyHDFSSynchronization   implements   ExternalSynchronization   { \n\n     private   static   final   Logger   LOG   =   LoggerFactory . getLogger ( MyHDFSSynchronization . class ) ; \n     // \n     private   static   final   DateTimeFormatter  dateTimeFormatter  =   DateTimeFormatter . ofPattern ( "yyyy-MM-dd--HH-mm" ) . withZone ( ZoneId . of ( "Asia/Shanghai" ) ) ; \n     private   static   final   String   LOCKS_DIR_PATTERN   =   "%s/" ; \n     private   static   final   String   LOCKS_DIR_TASK_PATTERN   =   LOCKS_DIR_PATTERN   +   "%s" ; \n     private   static   final   String   LOCKS_DIR_TASK_ATTEMPT_PATTERN   =   LOCKS_DIR_TASK_PATTERN   +   "_%s" ; \n     private   static   final   String   LOCKS_DIR_JOB_FILENAME   =   LOCKS_DIR_PATTERN   +   "_job" ; \n\n     private   static   final   transient   Random   RANDOM_GEN   =   new   Random ( ) ; \n\n     private   final   String  locksDir ; \n     private   final   ThrowingFunction < Configuration ,   FileSystem ,   IOException >  fileSystemFactory ; \n\n     /**\n     * Creates instance of {@link MyHDFSSynchronization}.\n     *\n     * @param locksDir directory where locks will be stored. This directory MUST be different that\n     *                 directory which is possibly stored under {@code\n     *                 "mapreduce.output.fileoutputformat.outputdir"} key. Otherwise setup of job will fail\n     *                 because the directory will exist before job setup.\n     */ \n     public   MyHDFSSynchronization ( String  locksDir )   { \n         this ( locksDir ,   FileSystem :: newInstance ) ; \n     } \n\n     /**\n     * Creates instance of {@link MyHDFSSynchronization}. Exists only for easier testing.\n     *\n     * @param locksDir          directory where locks will be stored. This directory MUST be different that\n     *                          directory which is possibly stored under {@code\n     *                          "mapreduce.output.fileoutputformat.outputdir"} key. Otherwise setup of job will fail\n     *                          because the directory will exist before job setup.\n     * @param fileSystemFactory supplier of the file system\n     */ \n     MyHDFSSynchronization ( \n             String  locksDir ,   ThrowingFunction < Configuration ,   FileSystem ,   IOException >  fileSystemFactory )   { \n         this . locksDir  =  locksDir ; \n         this . fileSystemFactory  =  fileSystemFactory ; \n     } \n\n     @Override \n     public   boolean   tryAcquireJobLock ( Configuration  conf )   { \n         Path  path  =   new   Path ( locksDir ,   String . format ( LOCKS_DIR_JOB_FILENAME ,   getJobJtIdentifier ( conf ) ) ) ; \n         return   tryCreateFile ( conf ,  path ) ; \n     } \n\n     @Override \n     public   void   releaseJobIdLock ( Configuration  conf )   { \n         Path  path  =   new   Path ( locksDir ,   String . format ( LOCKS_DIR_PATTERN ,   getJobJtIdentifier ( conf ) ) ) ; \n\n         try   ( FileSystem  fileSystem  =  fileSystemFactory . apply ( conf ) )   { \n\n             Path  src  =   new   Path ( conf . get ( HadoopFormatIO . OUTPUT_DIR )   +   "/_temporary/0/" ) ; \n             System . out . println ( "src = "   +  src . toString ( ) ) ; \n             RemoteIterator < LocatedFileStatus >  sourceFiles  =  fileSystem . listFiles ( src ,   true ) ; \n             if   ( sourceFiles  !=   null )   { \n                 String  mid_dir  =  dateTimeFormatter . format ( Instant . ofEpochMilli ( System . currentTimeMillis ( ) ) ) ; \n                 String  storePath  =   new   Path ( conf . get ( HadoopFormatIO . OUTPUT_DIR ) ) . getParent ( ) . toString ( ) ; \n                 String  newPathStr  =  storePath  +   "/"   +  mid_dir  +   "/%s" ; \n                 Path  success  =   new   Path ( String . format ( newPathStr ,   "_SUCCESS" ) ) ; \n                 boolean  is_exist_dir  =  fileSystem . exists ( new   Path ( String . format ( newPathStr ,   "" ) ) ) ; \n                 if   ( is_exist_dir )   { \n                     // \n                    newPathStr  =  storePath  +   "/"   +  mid_dir  +   "/%s-"   +   System . currentTimeMillis ( ) ; \n                    fileSystem . delete ( success ,   false ) ; \n                 } \n                 while   ( sourceFiles . hasNext ( ) )   { \n                     LocatedFileStatus  next  =  sourceFiles . next ( ) ; \n                     FileUtil . copy ( fileSystem ,  next ,  fileSystem , \n                             new   Path ( String . format ( newPathStr ,  next . getPath ( ) . getName ( ) ) ) , \n                             false ,   false ,  conf ) ; \n                 } \n                fileSystem . createNewFile ( success ) ; \n             } \n\n             if   ( fileSystem . delete ( path ,   true ) )   { \n                 LOG . info ( "Delete of lock directory {} was successful" ,  path ) ; \n             }   else   { \n                 LOG . warn ( "Delete of lock directory {} was unsuccessful" ,  path ) ; \n             } \n         }   catch   ( IOException  e )   { \n             String  formattedExceptionMessage  = \n                     String . format ( "Delete of lock directory %s was unsuccessful" ,  path ) ; \n             throw   new   IllegalStateException ( formattedExceptionMessage ,  e ) ; \n         } \n     } \n\n     @Override \n     public   TaskID   acquireTaskIdLock ( Configuration  conf )   { \n         JobID  jobId  =   HadoopFormats . getJobId ( conf ) ; \n         boolean  lockAcquired  =   false ; \n         int  taskIdCandidate  =   0 ; \n\n         while   ( ! lockAcquired )   { \n            taskIdCandidate  =   RANDOM_GEN . nextInt ( Integer . MAX_VALUE ) ; \n             Path  path  = \n                     new   Path ( \n                            locksDir , \n                             String . format ( LOCKS_DIR_TASK_PATTERN ,   getJobJtIdentifier ( conf ) ,  taskIdCandidate ) ) ; \n            lockAcquired  =   tryCreateFile ( conf ,  path ) ; \n         } \n\n         return   HadoopFormats . createTaskID ( jobId ,  taskIdCandidate ) ; \n     } \n\n     @Override \n     public   TaskAttemptID   acquireTaskAttemptIdLock ( Configuration  conf ,   int  taskId )   { \n         String  jobJtIdentifier  =   getJobJtIdentifier ( conf ) ; \n         JobID  jobId  =   HadoopFormats . getJobId ( conf ) ; \n         int  taskAttemptCandidate  =   0 ; \n         boolean  taskAttemptAcquired  =   false ; \n\n         while   ( ! taskAttemptAcquired )   { \n            taskAttemptCandidate ++ ; \n             Path  path  = \n                     new   Path ( \n                            locksDir , \n                             String . format ( \n                                     LOCKS_DIR_TASK_ATTEMPT_PATTERN ,  jobJtIdentifier ,  taskId ,  taskAttemptCandidate ) ) ; \n            taskAttemptAcquired  =   tryCreateFile ( conf ,  path ) ; \n         } \n\n         return   HadoopFormats . createTaskAttemptID ( jobId ,  taskId ,  taskAttemptCandidate ) ; \n     } \n\n     private   boolean   tryCreateFile ( Configuration  conf ,   Path  path )   { \n         try   ( FileSystem  fileSystem  =  fileSystemFactory . apply ( conf ) )   { \n             try   { \n                 return  fileSystem . createNewFile ( path ) ; \n             }   catch   ( FileAlreadyExistsException   |   org . apache . hadoop . fs . FileAlreadyExistsException  e )   { \n                 return   false ; \n             }   catch   ( RemoteException  e )   { \n                 // remote hdfs exception \n                 if   ( e . getClassName ( ) . equals ( AlreadyBeingCreatedException . class . getName ( ) ) )   { \n                     return   false ; \n                 } \n                 throw  e ; \n             } \n         }   catch   ( IOException  e )   { \n             throw   new   IllegalStateException ( String . format ( "Creation of file on path %s failed" ,  path ) ,  e ) ; \n         } \n     } \n\n     private   String   getJobJtIdentifier ( Configuration  conf )   { \n         JobID  job  = \n                 Preconditions . checkNotNull ( \n                         HadoopFormats . getJobId ( conf ) , \n                         "Configuration must contain jobID under key %s." , \n                         HadoopFormatIO . JOB_ID ) ; \n         return  job . getJtIdentifier ( ) ; \n     } \n\n     /**\n     * Function which can throw exception.\n     *\n     * @param <T1> parameter type\n     * @param <T2> result type\n     * @param <X>  exception type\n     */ \n     @FunctionalInterface \n     interface   ThrowingFunction < T1 ,  T2 ,   X   extends   Exception >   extends   Serializable   { \n         T2   apply ( T1  value )   throws   X ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 MyTextOutputFormat \n import   org . apache . hadoop . classification . InterfaceAudience ; \n import   org . apache . hadoop . classification . InterfaceStability ; \n import   org . apache . hadoop . conf . Configuration ; \n import   org . apache . hadoop . fs . FSDataOutputStream ; \n import   org . apache . hadoop . fs . FileSystem ; \n import   org . apache . hadoop . fs . Path ; \n import   org . apache . hadoop . io . NullWritable ; \n import   org . apache . hadoop . io . Text ; \n import   org . apache . hadoop . io . compress . CompressionCodec ; \n import   org . apache . hadoop . io . compress . GzipCodec ; \n import   org . apache . hadoop . mapreduce . OutputFormat ; \n import   org . apache . hadoop . mapreduce . RecordWriter ; \n import   org . apache . hadoop . mapreduce . TaskAttemptContext ; \n import   org . apache . hadoop . mapreduce . lib . output . FileOutputCommitter ; \n import   org . apache . hadoop . mapreduce . lib . output . FileOutputFormat ; \n import   org . apache . hadoop . util . ReflectionUtils ; \n\n import   java . io . DataOutputStream ; \n import   java . io . IOException ; \n import   java . io . UnsupportedEncodingException ; \n import   java . time . Instant ; \n import   java . time . ZoneId ; \n import   java . time . format . DateTimeFormatter ; \n\n /**\n * An {@link OutputFormat} that writes plain text files.\n */ \n @InterfaceAudience.Public \n @InterfaceStability.Stable \n public   class   MyTextOutputFormat < K ,   V >   extends   FileOutputFormat < K ,   V >   { \n     public   static   String   SEPERATOR   =   "mapreduce.output.textoutputformat.separator" ; \n     private   static   final   DateTimeFormatter   FORMATTER_HOUR   =   java . time . format . DateTimeFormatter . ofPattern ( "yyyy-MM-dd/HH-mm" ) . withZone ( ZoneId . of ( "Asia/Shanghai" ) ) ; \n\n     public   MyTextOutputFormat ( )   { \n         super ( ) ; \n     } \n\n     protected   static   class   LineRecordWriter < K ,   V > \n             extends   RecordWriter < K ,   V >   { \n         private   static   final   String  utf8  =   "UTF-8" ; \n         private   static   final   byte [ ]  newline ; \n\n         static   { \n             try   { \n                newline  =   "\\n" . getBytes ( utf8 ) ; \n             }   catch   ( UnsupportedEncodingException  uee )   { \n                 throw   new   IllegalArgumentException ( "can\'t find "   +  utf8  +   " encoding" ) ; \n             } \n         } \n\n         protected   DataOutputStream  out ; \n         private   final   byte [ ]  keyValueSeparator ; \n\n\n         public   LineRecordWriter ( DataOutputStream  out ,   String  keyValueSeparator )   { \n             this . out  =  out ; \n             try   { \n                 this . keyValueSeparator  =  keyValueSeparator . getBytes ( utf8 ) ; \n             }   catch   ( UnsupportedEncodingException  uee )   { \n                 throw   new   IllegalArgumentException ( "can\'t find "   +  utf8  +   " encoding" ) ; \n             } \n         } \n\n         public   LineRecordWriter ( DataOutputStream  out )   { \n             this ( out ,   "\\t" ) ; \n         } \n\n         /**\n         * Write the object to the byte stream, handling Text as a special\n         * case.\n         *\n         * @param o the object to print\n         * @throws IOException if the write throws, we pass it on\n         */ \n         private   void   writeObject ( Object  o )   throws   IOException   { \n             if   ( o  instanceof   Text )   { \n                 Text   to   =   ( Text )  o ; \n                out . write ( to . getBytes ( ) ,   0 ,   to . getLength ( ) ) ; \n             }   else   { \n                out . write ( o . toString ( ) . getBytes ( utf8 ) ) ; \n             } \n         } \n\n         public   synchronized   void   write ( K  key ,   V  value ) \n                 throws   IOException   { \n\n             boolean  nullKey  =  key  ==   null   ||  key  instanceof   NullWritable ; \n             boolean  nullValue  =  value  ==   null   ||  value  instanceof   NullWritable ; \n             if   ( nullKey  &&  nullValue )   { \n                 return ; \n             } \n             if   ( ! nullKey )   { \n                 writeObject ( key ) ; \n             } \n             if   ( ! ( nullKey  ||  nullValue ) )   { \n                out . write ( keyValueSeparator ) ; \n             } \n             if   ( ! nullValue )   { \n                 writeObject ( value ) ; \n             } \n            out . write ( newline ) ; \n         } \n\n         public   synchronized   void   close ( TaskAttemptContext  context )   throws   IOException   { \n            out . close ( ) ; \n         } \n     } \n\n     public   RecordWriter < K ,   V > \n     getRecordWriter ( TaskAttemptContext  job\n     )   throws   IOException ,   InterruptedException   { \n         Configuration  conf  =  job . getConfiguration ( ) ; \n         boolean  isCompressed  =   getCompressOutput ( job ) ; \n         String  keyValueSeparator  =  conf . get ( SEPERATOR ,   "\\t" ) ; \n         CompressionCodec  codec  =   null ; \n         String  extension  =   "" ; \n         if   ( isCompressed )   { \n             Class < ?   extends   CompressionCodec >  codecClass  = \n                     getOutputCompressorClass ( job ,   GzipCodec . class ) ; \n            codec  =   ( CompressionCodec )   ReflectionUtils . newInstance ( codecClass ,  conf ) ; \n            extension  =  codec . getDefaultExtension ( ) ; \n         } \n\n         Path  file  =   getDefaultWorkFile ( job ,  extension ) ; \n         System . out . println ( "file = "   +  file ) ; \n         FileSystem  fs  =  file . getFileSystem ( conf ) ; \n         if   ( ! isCompressed )   { \n             FSDataOutputStream  fileOut  =  fs . create ( file ,   false ) ; \n             return   new   LineRecordWriter < K ,   V > ( fileOut ,  keyValueSeparator ) ; \n         }   else   { \n             FSDataOutputStream  fileOut  =  fs . create ( file ,   false ) ; \n             return   new   LineRecordWriter < K ,   V > ( new   DataOutputStream \n                     ( codec . createOutputStream ( fileOut ) ) , \n                    keyValueSeparator ) ; \n         } \n     } \n\n\n     @Override \n     public   Path   getDefaultWorkFile ( TaskAttemptContext  context ,   String  extension )   throws   IOException   { \n         FileOutputCommitter  committer  = \n                 ( FileOutputCommitter )   getOutputCommitter ( context ) ; \n         return   new   Path ( committer . getWorkPath ( ) ,   getUniqueFile ( context , \n                 FORMATTER_HOUR . format ( Instant . ofEpochMilli ( System . currentTimeMillis ( ) ) )   +   "/" +   getOutputName ( context ) ,  extension ) ) ; \n     } \n\n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 yaml \n apiVersion :  flink.apache.org/v1beta1\n kind :  FlinkDeployment\n metadata : \n   namespace :  flink\n   name :  kafka - 2 - hdfs\n spec : \n   image :  personalharbor.com/bigdata/flink : 1.14.6 - scala_2.12\n   flinkVersion :  v1_14\n   imagePullPolicy :  Never    #  \n   ingress :     # ingressflink web \n     template :   "flink.k8s.io/{{namespace}}/{{name}}(/|$)(.*)" \n     className :   "nginx" \n     annotations : \n       nginx.ingress.kubernetes.io/rewrite-target :   "/$2" \n   flinkConfiguration : \n     taskmanager.numberOfTaskSlots :   "2" \n     web.cancel.enable :   "true" \n   serviceAccount :  flink\n   jobManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   taskManager : \n     resource : \n       memory :   "2048m" \n       cpu :   1 \n   podTemplate : \n     spec : \n \n       hostAliases : \n         -   ip :  192.168.8.102\n           hostnames : \n             -   "node3" \n         -   ip :  192.168.8.103\n           hostnames : \n             -   "node2" \n         -   ip :  192.168.8.104\n           hostnames : \n             -   "node1" \n       containers : \n         -   name :  flink - main - container\n           volumeMounts : \n             -   name :  flink - data - volume\n               mountPath :  /flink - data\n             -   name :  flink - jar   # nfsjar \n               #flink \n               #/opt/flink, \n               mountPath :  /opt/flink/usrlib\n       volumes : \n         -   name :  flink - data - volume\nFlink is designed to run as a specific user with restricted privileges. \nThe owner of the host path should be set to "flink" with the user ID (UID) of 9999. \n           hostPath : \n             #The base directory of the JobResultStore isn\'t accessible. No dirty JobResults can be restored \n             path :  /tmp/flink    # chown 9999:9999 -R /tmp/flink \n             type :  Directory\n         -   name :  flink - jar\n           persistentVolumeClaim : \n             claimName :  flink - jar - pvc\n   job : \n     entryClass :  com.gordon.uniform.Computing\n     jarURI :  local : ///opt/flink/usrlib/apache - beam - 1.0 - SNAPSHOT.jar\n     #hdfs \n     args :   [   "--runner=org.apache.beam.runners.flink.FlinkRunner  --kafkaConfig_isOpen=true --hdfsSinkConfig_isOpen=true --output=hdfs://node1:8020/data/output/fromKafka" ] \n     #hdfs \n     #args: [ "--runner=org.apache.beam.runners.flink.FlinkRunner --hdfsSourceConfig_isOpen=true --hdfsSinkConfig_isOpen=true --output=hdfs://node1:8020/data/output/fromHdfs"] \n     parallelism :   1 \n     upgradeMode :  stateless\n   logConfiguration : \n     "log4j-console.properties" :   | \n      rootLogger.level = INFO\n      rootLogger.appenderRef.file.ref = LogFile\n      rootLogger.appenderRef.console.ref = LogConsole\n      appender.file.name = LogFile\n      appender.file.type = File\n      appender.file.append = false\n      appender.file.fileName = ${sys:log.file}\n      appender.file.layout.type = PatternLayout\n      appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n      appender.console.name = LogConsole\n      appender.console.type = CONSOLE\n      appender.console.layout.type = PatternLayout\n      appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n      logger.akka.name = akka\n      logger.akka.level = WARN\n      logger.kafka.name= org.apache.kafka\n      logger.kafka.level = WARN\n      logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline\n      logger.netty.level = WARN \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 \n \n kafka \n \n hdfs \n \n'},{title:"Helm",frontmatter:{title:"Helm",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["k8s"]},regularPath:"/%E5%85%B6%E4%BB%96/helm.html",relativePath:"/helm.md",key:"v-c38fec2a",path:"/2023/06/10/helm/",headers:[{level:2,title:"",slug:""},{level:2,title:"Helm",slug:"helm"},{level:2,title:"",slug:""},{level:2,title:"Helm ",slug:"helm-"},{level:3,title:"",slug:""},{level:3,title:"'helm search' Charts",slug:"helm-search--charts"},{level:3,title:"'helm install'",slug:"helm-install"},{level:3,title:"'helm upgrade'  'helm rollback' release ",slug:"helm-upgrade--helm-rollback--release-"},{level:3,title:"'helm uninstall' release",slug:"helm-uninstall--release"},{level:3,title:"'helm repo'",slug:"helm-repo-"},{level:2,title:" charts",slug:"-charts"},{level:3,title:"",slug:""}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:'  \n Helm  Kubernetes yum \n Helm \n Helm \n, \n wget  https://get.helm.sh/helm-v3.7.1-linux-amd64.tar.gz  -O  /tmp/helm-v3.7.1-linux-amd64.tar.gz\n \n tar   -xf  /tmp/helm-v3.7.1-linux-amd64.tar.gz  -C  /root/\n \n ln   -s  /root/linux-amd64/helm /usr/local/bin/helm\n \n 1 2 3 4 5 6 #   \n Helm |  \n # \nhelm repo  add  bitnami https://charts.bitnami.com/bitnami\n #charts \nhelm search repo bitnami\n #chart \nhelm show chart bitnami/mysql\n # \nhelm  install  bitnami/mysql --generate-name\n #chart \nhelm  ls \n # \nhelm uninstall mysql-1612624192\n ##Kubernetes mysql-1612624192 servicedeployment pod helm uninstall  --keep-history  Helm  \n #chart \nhelm status mysql-1612624192\n # helm rollback  \n # \nhelm repo update\n # \nhelm get  -h \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #  Helm  \n  \n Chart   Helm  Kubernetes  Apt dpkg Yum RPM Kubernetes  \n Repository   charts  \n Release   Kubernetes  chart  chart   release  MySQL chartchart  release    release name  \n \'helm search\' Charts \n Helm  \n \n helm search hub    Artifact Hub   helm charts Artifact Hub \n helm search repo    helm repo add  helm  \n \n   helm search hub  charts \n $ helm search hub wordpress\nURL                                                 CHART VERSION APP VERSION DESCRIPTION\nhttps://hub.helm.sh/charts/bitnami/wordpress        7.6.7         5.2.4       Web publishing platform for building blogs and ...\nhttps://hub.helm.sh/charts/presslabs/wordpress-...  v0.6.3        v0.6.3      Presslabs WordPress Operator Helm Chart\nhttps://hub.helm.sh/charts/presslabs/wordpress-...  v0.7.1        v0.7.1      A Helm chart for deploying a WordPress site on ...\n \n 1 2 3 4 5  Artifact Hub   wordpress  charts \n  helm search hub   charts \n   helm search repo  chart \n $ helm repo add brigade https://brigadecore.github.io/charts\n"brigade" has been added to your repositories\n$ helm search repo brigade\nNAME                          CHART VERSION APP VERSION DESCRIPTION\nbrigade/brigade               1.3.2         v1.2.1      Brigade provides event-driven scripting of Kube...\nbrigade/brigade-github-app    0.4.1         v0.2.1      The Brigade GitHub App, an advanced gateway for...\nbrigade/brigade-github-oauth  0.2.0         v0.20.0     The legacy OAuth GitHub Gateway for Brigade\nbrigade/brigade-k8s-gateway   0.1.0                     A Helm chart for Kubernetes\nbrigade/brigade-project       1.0.0         v1.0.0      Create a Brigade project\nbrigade/kashti                0.4.0         v0.4.0      A Helm chart for Kubernetes\n \n 1 2 3 4 5 6 7 8 9 10 Helm  \n $ helm search repo kash\nNAME            CHART VERSION APP VERSION DESCRIPTION\nbrigade/kashti  0.4.0         v0.4.0      A Helm chart for Kubernetes\n \n 1 2 3  helm   helm install   \n \'helm install\' \n  helm  \n   helm install   helm releasechart \n $ helm install happy-panda bitnami/wordpress\nNAME: happy-panda\nLAST DEPLOYED: Tue Jan 26 10:27:17 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n** Please be patient while the chart is being deployed **\n\nYour WordPress site can be accessed through the following DNS name from within your cluster:\n\n    happy-panda-wordpress.default.svc.cluster.local (port 80)\n\nTo access your WordPress site from outside the cluster follow the steps below:\n\n1. Get the WordPress URL by running these commands:\n\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        Watch the status with: \'kubectl get svc --namespace default -w happy-panda-wordpress\'\n\n   export SERVICE_IP=$(kubectl get svc --namespace default happy-panda-wordpress --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")\n   echo "WordPress URL: http://$SERVICE_IP/"\n   echo "WordPress Admin URL: http://$SERVICE_IP/admin"\n\n2. Open a browser and access WordPress using the obtained URL.\n\n3. Login with the following credentials below to see your blog:\n\n  echo Username: user\n  echo Password: $(kubectl get secret --namespace default happy-panda-wordpress -o jsonpath="{.data.wordpress-password}" | base64 --decode)\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  wordpress  chart chart  release    happy-panda  Helm --generate-name  \n  helm  release \n Helm \n \n Namespace \n NetworkPolicy \n ResourceQuota \n LimitRange \n PodSecurityPolicy \n PodDisruptionBudget \n ServiceAccount \n Secret \n SecretList \n ConfigMap \n StorageClass \n PersistentVolume \n PersistentVolumeClaim \n CustomResourceDefinition \n ClusterRole \n ClusterRoleList \n ClusterRoleBinding \n ClusterRoleBindingList \n Role \n RoleList \n RoleBinding \n RoleBindingList \n Service \n DaemonSet \n Pod \n ReplicationController \n ReplicaSet \n Deployment \n HorizontalPodAutoscaler \n StatefulSet \n Job \n CronJob \n Ingress \n APIService \n \n Helm  charts  600M  Docker  \n   helm status   release  \n $ helm status happy-panda\nNAME: happy-panda\nLAST DEPLOYED: Tue Jan 26 10:27:17 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n** Please be patient while the chart is being deployed **\n\nYour WordPress site can be accessed through the following DNS name from within your cluster:\n\n    happy-panda-wordpress.default.svc.cluster.local (port 80)\n\nTo access your WordPress site from outside the cluster follow the steps below:\n\n1. Get the WordPress URL by running these commands:\n\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        Watch the status with: \'kubectl get svc --namespace default -w happy-panda-wordpress\'\n\n   export SERVICE_IP=$(kubectl get svc --namespace default happy-panda-wordpress --template "{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}")\n   echo "WordPress URL: http://$SERVICE_IP/"\n   echo "WordPress Admin URL: http://$SERVICE_IP/admin"\n\n2. Open a browser and access WordPress using the obtained URL.\n\n3. Login with the following credentials below to see your blog:\n\n  echo Username: user\n  echo Password: $(kubectl get secret --namespace default happy-panda-wordpress -o jsonpath="{.data.wordpress-password}" | base64 --decode)\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  release  \n  chart \n  chart  chart  \n   helm show values   chart  \n $ helm show values bitnami/wordpress\n## Global Docker image parameters\n## Please, note that this will override the image parameters, including dependencies, configured to use the global value\n## Current available global Docker image parameters: imageRegistry and imagePullSecrets\n##\nglobal:\n  imageRegistry: myRegistryName\n  imagePullSecrets:\n    - myRegistryKeySecretName\n  storageClass: myStorageClass\n\n## Bitnami WordPress image version\n## ref: https://hub.docker.com/r/bitnami/wordpress/tags/\n##\nimage:\n  registry: docker.io\n  repository: bitnami/wordpress\n  tag: 5.6.0-debian-10-r35\n  [..]\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  YAML  \n $ echo \'{mariadb.auth.database: user0db, mariadb.auth.username: user0}\' > values.yaml\n$ helm install -f values.yaml bitnami/wordpress --generate-name\n \n 1 2  MariaDB   user0    user0db  chart  \n  \n \n --values  (  -f ) YAML  \n --set  \n \n   --set    --values    --set   --set   ConfigMap   helm get values   release   --set    helm upgrade    --reset-values    --set   \n --set   \n --set  0 name/value  --set name=value  YAML  \n name :  value\n \n 1   --set a=b,c=d   YAML  \n a :  b\n c :  d\n \n 1 2  --set outer.inner=value   \n outer : \n   inner :  value\n \n 1 2  {}  --set name={a, b, c}   \n name : \n   -  a\n   -  b\n   -  c\n \n 1 2 3 4 name/key null   --set name=[],a=null   \n name : \n   -  a\n   -  b\n   -  c\n a :  b\n \n 1 2 3 4 5  \n name :   [ ] \n a :   null \n \n 1 2  2.5.0   --set servers[0].port=80   \n servers : \n   -   port :   80 \n \n 1 2  --set servers[0].port=80,servers[0].host=example   \n servers : \n   -   port :   80 \n     host :  example\n \n 1 2 3   --set   --set name=value1\\,value2   \n name :   "value1,value2" \n \n 1 \b chart   toYaml   annotationslabels node selectors  --set nodeSelector."kubernetes\\.io/role"=master   \n nodeSelector : \n   kubernetes.io/role :  master\n \n 1 2   --set   Chart   values.yaml    --set    Values   \n  \n helm install   \n \n chart  \n  chart  helm install foo foo-0.1.1.tgz  \n  chart  helm install foo path/to/foo  \n  URL helm install foo https://example.com/charts/foo-1.2.3.tgz  \n \'helm upgrade\'  \'helm rollback\' release  \n  chart  release   helm upgrade   \n  release  Kubernetes  chart Helm  \n $ helm upgrade -f panda.yaml happy-panda bitnami/wordpress\n \n 1  happy-panda   release  chart  YAML  \n mariadb.auth.username :  user1\n \n 1   helm get values   \n $ helm get values happy-panda\nmariadb:\n  auth:\n    username: user1\n \n 1 2 3 4 helm get   release  panda.yaml   \n   helm rollback [RELEASE] [REVISION]   \n $ helm rollback happy-panda 1\n \n 1   happy-panda  release revision revision 1 revision 1  helm history [RELEASE]   release  \n  \n  Helm  cli    helm --help   \n \n --timeout   Go duration    Kubernetes   5m0s  \n --wait  Pods  ready PVC Deployments  ready  Pods  Desired   maxUnavailable  Services  IP  LoadBalancer   Ingress release   --timeout  release   FAILED  Deployment   replicas  1  maxUnavailable  0 --wait   ready Pod  \n --no-hooks  \n --recreate-pods   upgrade    rollback  PoddeploymentPod  Helm 3  \n \'helm uninstall\' release \n   helm uninstall   release \n $ helm uninstall happy-panda\n \n 1  release  helm list   release \n $ helm list\nNAME            VERSION UPDATED                         STATUS          CHART\ninky-cat        1       Wed Sep 28 12:59:46 2016        DEPLOYED        alpine-0.1.0\n \n 1 2 3  happy-panda   release  \n  Helm  release  Helm 3  release    helm uninstall --keep-history   helm list --uninstalled    --keep-history   release \n helm list --all   Helm  release   --keep-history  \n $  helm list --all\nNAME            VERSION UPDATED                         STATUS          CHART\nhappy-panda     2       Wed Sep 28 12:47:54 2016        UNINSTALLED     wordpress-10.4.5.6.0\ninky-cat        1       Wed Sep 28 12:59:46 2016        DEPLOYED        alpine-0.1.0\nkindred-angelf  2       Tue Sep 27 16:16:10 2016        UNINSTALLED     alpine-0.1.0\n \n 1 2 3 4 5  release \n \'helm repo\' \n Helm 3  chart  helm repo   \n   helm repo list   \n $ helm repo list\nNAME            URL\nstable          https://charts.helm.sh/stable\nmumoshu         https://mumoshu.github.io/charts\n \n 1 2 3 4   helm repo add   \n $ helm repo add dev https://example.com/dev-charts\n \n 1  chart   helm repo update   Helm  \n   helm repo remove   \n  charts \n  \n chart   chart   helm create   \n $ helm create deis-workflow\nCreating deis-workflow\n \n 1 2  ./deis-workflow   chart  \n \n Chart.yaml Chart  \n values.yaml  templates  \n Templates  yaml  \n charts chart  chart \n NOTES.txt  Chart  helm install   Chart \n helpers.tpl chart  \n \n  chart   helm lint   \n 2 templatesyaml \n deployment.yaml \n \n service.yaml \n \n 3 mychart \n \n 4  \nhelm upgrade [chart]  \n \n 5 chart \nyamlyamlchartvalues.vamlyaml \nyaml \n image \ntag \nlabel \nport \nreplicas \n1 values.yaml \n \n 2 templatesyaml \n \n ValuesRelease \n  \n  chart   helm package   \n $ helm package deis-workflow\ndeis-workflow-0.1.0.tgz\n \n 1 2  chart   helm install   \n $ helm install deis-workflow ./deis-workflow-0.1.0.tgz\n...\n \n 1 2  chart  chart  \n'},{title:"sqoop",frontmatter:{title:"sqoop",date:"2022-05-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["",""],tags:["",""]},regularPath:"/%E4%B8%AD%E9%97%B4%E4%BB%B6/Sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.html",relativePath:"/Sqoop.md",key:"v-7e69d236",path:"/2022/05/10/sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/",headers:[{level:2,title:"Sqoop ",slug:"sqoop-"},{level:3,title:"1. ",slug:"_1-"},{level:3,title:"2. ",slug:"_2-"},{level:2,title:"Sqoop  MySQL",slug:"sqoop--mysql"},{level:3,title:"1. MySQL",slug:"_1-mysql"},{level:3,title:"2. ",slug:"_2-"},{level:2,title:"Sqoop  HDFS",slug:"sqoop--hdfs"},{level:3,title:"3.1 MySQLHDFS",slug:"_3-1-mysqlhdfs"},{level:3,title:"3.2 HDFSMySQL",slug:"_3-2-hdfsmysql"},{level:2,title:"Sqoop  Hive",slug:"sqoop--hive"},{level:3,title:"4.1 MySQLHive",slug:"_4-1-mysqlhive"},{level:3,title:"4.2 Hive MySQL",slug:"_4-2-hive-mysql"},{level:2,title:"Sqoop  HBase",slug:"sqoop--hbase"},{level:3,title:"5.1 MySQLHBase",slug:"_5-1-mysqlhbase"},{level:2,title:"",slug:""},{level:2,title:"Sqoop ",slug:"sqoop-"},{level:3,title:"7.1 query",slug:"_7-1-query"},{level:3,title:"7.2 ",slug:"_7-2-"},{level:2,title:"",slug:""},{level:2,title:"",slug:""}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:" Sqoop  \n 1.  \nsqoop help \n \n 1 #  2.  \nsqoop help  \n \n 1 #  Sqoop  MySQL \n 1. MySQL \n  Sqoop  MySQL  \n sqoop list-databases  \\ \n --connect  jdbc:mysql://hadoop001:3306/  \\ \n --username  root  \\ \n --password  root\n \n 1 2 3 4 #  2.  \n sqoop list-tables  \\ \n --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n --username  root  \\ \n --password  root\n \n 1 2 3 4 #  Sqoop  HDFS \n 3.1 MySQLHDFS \n 1.  \n  MySQL   help_keyword   HDFS   /sqoop   3   map tasks   \n \n help_keyword  MySQL  \n \n sqoop  import   \\ \n --connect  jdbc:mysql://hadoop001:3306/mysql  \\      \n --username  root  \\ \n --password  root  \\ \n --table  help_keyword  \\             #  \n--delete-target-dir  \\              #  \n--target-dir /sqoop  \\              #  \n--fields-terminated-by  '\\t'    \\     #  \n -m   3                               #  map tasks  \n \n 1 2 3 4 5 6 7 8 9   split    map task   \n \n   -- autoreset-to-one-mapper    map task  \n   --split-by <column-name>   \n 2.  \n \nhadoop fs  -ls    -R  /sqoop\n \nhadoop fs  -text   /sqoop/part-m-00000\n \n 1 2 3 4  HDFS , 3  \n 3.2 HDFSMySQL \n sqoop  export    \\ \n     --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n     --username  root  \\ \n     --password  root  \\ \n     --table  help_keyword_from_hdfs  \\          #  MySQL  help_keyword_from_hdf  \n    --export-dir /sqoop   \\ \n    --input-fields-terminated-by  '\\t' \\ \n     --m   3  \n \n 1 2 3 4 5 6 7 8  \n CREATE   TABLE  help_keyword_from_hdfs  LIKE  help_keyword  ; \n \n 1 #  Sqoop  Hive \n 4.1 MySQLHive \n Sqoop  Hive  HDFS  HDFS   Load   Hive   target-dir   \n 1.  \n sqoop  import   \\ \n   --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n   --username  root  \\ \n   --password  root  \\ \n   --table  help_keyword  \\          #       \n  --delete-target-dir  \\           #  \n  --target-dir /sqoop_hive   \\     #  \n  --hive-database sqoop_test  \\    #  Hive  sqoop_test  default  \n  --hive-import  \\                 #  Hive \n  --hive-overwrite  \\              #  Hive  \n   -m   3                            #  \n \n 1 2 3 4 5 6 7 8 9 10 11  Hive   sqoop_test   Hive   default   \n hive  \n hive >   SHOW DATABASES ; \n sqoop_test  \n hive >   CREATE DATABASE sqoop_test ; \n \n 1 2 3 4 #  2.  \n sqoop_test  \n hive >   SHOW  TABLES  IN  sqoop_test ; \n \n hive >  SELECT * FROM sqoop_test.help_keyword ; \n \n 1 2 3 4 #  3.  \n   java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf  Hive   lib    hive-exec-**.jar   sqoop   lib   \n [ root@hadoop001 lib ] # ll hive-exec-* \n-rw-r--r--.  1   1106   4001   19632031   11    13   21 :45 hive-exec-1.1.0-cdh5.15.2.jar\n [ root@hadoop001 lib ] # cp hive-exec-1.1.0-cdh5.15.2.jar  ${SQOOP_HOME}/lib \n \n 1 2 3 \n 4.2 Hive MySQL \n  Hive  HDFS  Hive  MySQL HDFS  MySQL \n 1. HiveHDFS \n \nhive >  use sqoop_test ; \n \nhive >  desc formatted help_keyword ; \n \n 1 2 3 4 Location   \n  \n 3.2  \n sqoop  export    \\ \n     --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n     --username  root  \\ \n     --password  root  \\ \n     --table  help_keyword_from_hive  \\ \n    --export-dir /user/hive/warehouse/sqoop_test.db/help_keyword   \\ \n    -input-fields-terminated-by  '\\001'   \\               #  hive  \\001 \n     --m   3  \n \n 1 2 3 4 5 6 7 8 MySQL  \n CREATE   TABLE  help_keyword_from_hive  LIKE  help_keyword  ; \n \n 1 #  Sqoop  HBase \n \n  RDBMS  HBase HBase  RDBMS \n 5.1 MySQLHBase \n 1.  \n   help_keyword   HBase   help_keyword_hbase    help_keyword_id    RowKey   keywordInfo   \n sqoop  import   \\ \n     --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n     --username  root  \\ \n     --password  root  \\ \n     --table  help_keyword  \\                #  \n    --hbase-table help_keyword_hbase  \\    # hbase  \n    --column-family keywordInfo  \\         #  keywordInfo   \n    --hbase-row-key help_keyword_id      #  help_keyword_id  RowKey \n \n 1 2 3 4 5 6 7 8  HBase  \n \nhbase >  list\n \nhbase >  create  'help_keyword_hbase' ,  'keywordInfo' \n \nhbase >  desc  'help_keyword_hbase' \n \n 1 2 3 4 5 6 #  2.  \n   scan   \n  \n Sqoop   import-all-tables   HDFS/Hive \n \n   --autoreset-to-one-mapper   map task ; \n  WHERE  \n \n \n  \n \n You must not intend to use non-default splitting column, nor impose any conditions via a  WHERE  clause. \n \n \n  HDFS \n sqoop import-all-tables  \\ \n     --connect  jdbc:mysql://hadoop001:3306/  \\ \n     --username  root  \\ \n     --password  root  \\ \n    --warehouse-dir  /sqoop_all  \\       #  \n    --fields-terminated-by  '\\t'    \\ \n     -m   3 \n \n 1 2 3 4 5 6 7  Hive \n sqoop import-all-tables  -Dorg.apache.sqoop.splitter.allow_text_splitter = true  \\ \n   --connect  jdbc:mysql://hadoop001:3306/  \\ \n   --username  root  \\ \n   --password  root  \\ \n  --hive-database sqoop_test  \\           #  Hive     \n  --hive-import  \\ \n  --hive-overwrite  \\ \n   -m   3 \n \n 1 2 3 4 5 6 7 8 #  Sqoop  \n 7.1 query \n Sqoop   query   SQLsqoopexport +query \n sqoop  import   \\ \n   --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n   --username  root  \\ \n   --password  root  \\ \n   --query   'select * from help_keyword where  $CONDITIONS and  help_keyword_id < 50'   \\   \n  --delete-target-dir  \\             \n  --target-dir /sqoop_hive   \\  \n  --hive-database sqoop_test  \\             #   Hive  default  \n  --hive-table filter_help_keyword  \\       #  \n  --split-by help_keyword_id  \\             #  split        \n  --hive-import  \\                          #  Hive \n  --hive-overwrite  \\                      \n   -m   3                                   \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13   query   \n \n   --hive-table   \n   -m   1   --autoreset-to-one-mapper   --split-by   \n SQL   where    $CONDITIONS  \n 7.2  \n sqoop  import   \\ \n     --connect  jdbc:mysql://hadoop001:3306/mysql  \\ \n     --username  root  \\ \n     --password  root  \\ \n     --table  help_keyword  \\ \n    --target-dir /sqoop_hive   \\ \n    --hive-database sqoop_test  \\          \n     --incremental   append   \\               #  \n    --check-column  help_keyword_id  \\      #  \n    --last-value  300    \\                    #  \n    --hive-import  \\    \n     -m   3   \n \n 1 2 3 4 5 6 7 8 9 10 11 12 incremental   \n \n append   last-value   \n lastmodified   timestamp    last-value   \n \n  Sqoop   query   \n  \n Sqoop   Hive does not support the SQL type for column xxx   \n \n --map-column-java<mapping>     SQL  Java  \n --map-column-hive <mapping>    Hive  Java  \n \n   id   String  value   Integer  \n $ sqoop import ... --map-column-java id=String,value=Integer\n \n 1  \n  \n Sqoop User Guide (v1.4.7) \n"},{title:"linux",frontmatter:{title:"linux",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["linux"]},regularPath:"/%E5%85%B6%E4%BB%96/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93.html",relativePath:"/linux.md",key:"v-cdeeea2e",path:"/2023/06/10/linux%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/",headers:[{level:2,title:"1. ",slug:"_1-"},{level:2,title:"2. ",slug:"_2-"},{level:2,title:"3. ",slug:"_3-"},{level:2,title:"4. ",slug:"_4-"},{level:2,title:"5. ",slug:"_5-"},{level:2,title:"6. ",slug:"_6-"},{level:2,title:"7. ",slug:"_7-"},{level:2,title:"8.   +  - ",slug:"_8-----"},{level:2,title:"9.   +  - ",slug:"_9-----"},{level:2,title:"10. ",slug:"_10-"},{level:2,title:"11. RPM ",slug:"_11-rpm-"},{level:2,title:"12. YUM ",slug:"_12-yum-"},{level:2,title:"13. deb ",slug:"_13-deb-"},{level:2,title:"14. ",slug:"_14-"},{level:2,title:"15. ",slug:"_15-"},{level:2,title:"16. ",slug:"_16-"},{level:2,title:"17. ",slug:"_17-"},{level:2,title:"18. ",slug:"_18-"},{level:2,title:"19. SWAP ",slug:"_19-swap-"},{level:2,title:"20. ",slug:"_20-"},{level:2,title:"21. ",slug:"_21-"},{level:2,title:"22.  WIFI ",slug:"_22---wifi-"},{level:2,title:"23. ",slug:"_23-"},{level:2,title:"24. ",slug:"_24-"},{level:2,title:"25. ",slug:"_25-"},{level:2,title:"26. ",slug:"_26-"},{level:3,title:"26.1",slug:"_26-1"},{level:3,title:"26.2",slug:"_26-2"},{level:3,title:"26.3",slug:"_26-3"},{level:3,title:"26.4",slug:"_26-4"},{level:3,title:"26.5",slug:"_26-5"},{level:3,title:"26.6",slug:"_26-6"},{level:3,title:"26.7ctrl+c :",slug:"_26-7ctrl-c-"},{level:3,title:"26.8who  w ",slug:"_26-8who--w-"},{level:3,title:"26.9dmesg ",slug:"_26-9dmesg-"},{level:3,title:"26.10df ",slug:"_26-10df-"},{level:3,title:"26.11du ",slug:"_26-11du-"},{level:3,title:"26.12free ",slug:"_26-12free-"},{level:2,title:"27. VIM",slug:"_27-vim"},{level:2,title:"28. (RPM)",slug:"_28--rpm"},{level:3,title:"28.1",slug:"_28-1"},{level:3,title:"28.2",slug:"_28-2"},{level:3,title:"28.3",slug:"_28-3"},{level:3,title:"28.4",slug:"_28-4"},{level:3,title:"28.5",slug:"_28-5"},{level:3,title:"29.",slug:"_29-"},{level:2,title:"3json",slug:"_3json"}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:" 1.  \n uname -m \nuname -r \ndmidecode -q \n(SMBIOS / DMI) hdparm -i /dev/hda \nhdparm -tT /dev/sda \narch \nuname -m \nuname -r \ndmidecode -q  - (SMBIOS / DMI)\nhdparm -i /dev/hda \nhdparm -tT /dev/sda \ncat /proc/cpuinfo CPU info\ncat /proc/interrupts \ncat /proc/meminfo \ncat /proc/swaps swap\ncat /proc/version \ncat /proc/net/dev \ncat /proc/mounts \nlspci -tv  PCI \nlsusb -tv  USB \ndate \ncal 2007 2007\ndate 041217002007.00  - .\nclock -w  BIOS\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #  2.  \n shutdown -h now (1)\ninit 0 (2)\ntelinit 0 (3)\nshutdown -h hours:minutes & \nshutdown -c \nshutdown -r now (1)\nreboot (2)\nlogout \n \n 1 2 3 4 5 6 7 8 #  3.  \n cd /home  '/ home' '\ncd .. \ncd ../.. \ncd \ncd ~user1 \ncd - \npwd \nls \nls -F \nls -l \nls -a \nls *[0-9]* \ntree (1)\nlstree (2)\nmkdir dir1  'dir1' '\nmkdir dir1 dir2 \nmkdir -p /tmp/dir1/dir2 \nrm -f file1  'file1' '\nrmdir dir1  'dir1' '\nrm -rf dir1  'dir1' \nrm -rf dir1 dir2 \nmv dir1 new_dir / \ncp file1 file2 \ncp dir/* . \ncp -a /tmp/dir1 . \ncp -a dir1 dir2 \nln -s file1 lnk1 \nln file1 lnk1 \ntouch -t 0712250000 file1  - (YYMMDDhhmm)\nfile file1 outputs the mime type of the file as text\niconv -l \niconv -f fromEncoding -t toEncoding inputFile > outputFile creates a new from the given input file by assuming it is encoded in fromEncoding and converting it to toEncoding.\nfind . -maxdepth 1 -name *.jpg -print -exec convert \"{}\" -resize 80x60 \"thumbs/{}\" \\; batch resize files in the current directory and send them to a thumbnails directory (requires convert from Imagemagick)\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #  4.  \n find / -name file1  '/' \nfind / -user user1  'user1' \nfind /home/user1 -name \\*.bin  '/ home/user1' '.bin' \nfind /usr/bin -type f -atime +100 100\nfind /usr/bin -type f -mtime -10 10\nfind / -name \\*.rpm -exec chmod 755 '{}' \\;  '.rpm' \nfind / -xdev -name \\*.rpm  '.rpm' \nlocate \\*.ps  '.ps'  -  'updatedb' \nwhereis halt man\nwhich halt \n \n 1 2 3 4 5 6 7 8 9 10 #  5.  \n mount /dev/hda2 /mnt/hda2 hda2 -  '/ mnt/hda2' \numount /dev/hda2 hda2 -  '/ mnt/hda2' \nfuser -km /mnt/hda2 \numount -n /mnt/hda2  /etc/mtab - \nmount /dev/fd0 /mnt/floppy \nmount /dev/cdrom /mnt/cdrom cdromdvdrom\nmount /dev/hdc /mnt/cdrecorder cdrwdvdrom\nmount /dev/hdb /mnt/cdrecorder cdrwdvdrom\nmount -o loop file.iso /mnt/cdrom ISO\nmount -t vfat /dev/hda5 /mnt/hda5 Windows FAT32\nmount /dev/sda1 /mnt/usbdisk usb \nmount -t smbfs -o username=user,password=pass //WinClient/share /mnt/share windows\n \n 1 2 3 4 5 6 7 8 9 10 11 12 # \n # \nvmware-hgfsclient\n # \nvmhgfs-fuse .host:/ /mnt/hgfs/\n # \n # \n vim  /etc/rc.local\n # \nvmhgfs-fuse .host:/ /mnt/hgfs\n \n 1 2 3 4 5 6 7 8 9 10 #  6.  \n df -h \nls -lSr |more \ndu -sh dir1  'dir1' '\ndu -sk * | sort -rn \nrpm -q -a --qf '%10{SIZE}t%{NAME}n' | sort -k1,1n rpm (fedora, redhat)\ndpkg-query -W -f='${Installed-Size;10}t${Package}n' | sort -k1,1n deb (ubuntu, debian)\n \n 1 2 3 4 5 6 #  7.  \n groupadd group_name \ngroupdel group_name \ngroupmod -n new_group_name old_group_name \nuseradd -c \"Name Surname \" -g admin -d /home/user1 -s /bin/bash user1  \"admin\" \nuseradd user1 \nuserdel -r user1  ( '-r' )\nusermod -c \"User FTP\" -g system -d /ftp/user1 -s /bin/nologin user1 \npasswd \npasswd user1  (root)\nchage -E 2005-12-31 user1 \npwck  '/etc/passwd' \ngrpck  '/etc/passwd' \nnewgrp group_name \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 #  8.   +  -  \n ls -lh \nls /tmp | pr -T5 -W$COLUMNS 5\nchmod ugo+rwx directory1 (u)(g)(o)r (w)(x)\nchmod go-rwx directory1 (g)(o)\nchown user1 file1 \nchown -R user1 directory1 \nchgrp group1 file1 \nchown user1:group1 file1 \nfind / -perm -u+s SUID\nchmod u+s /bin/file1  SUID  - \nchmod u-s /bin/file1  SUID\nchmod g+s /home/public SGID  - SUID \nchmod g-s /home/public  SGID \nchmod o+t /home/public  STIKY  - \nchmod o-t /home/public  STIKY \nchmod +x  \nchmod -x  \nchmod u+x  \nchmod g+x  \nchmod o+x  \nchmod ug+x  \nchmod =wx  \nchmod ug=wx  \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #  9.   +  -  \n chattr +a file1 \nchattr +c file1 /\nchattr +d file1 dump\nchattr +i file1 \nchattr +s file1 \nchattr +S file1 \nchattr +u file1 \nlsattr \n \n 1 2 3 4 5 6 7 8 #  10.  \n bunzip2 file1.bz2  'file1.bz2'\nbzip2 file1  'file1' \ngunzip file1.gz  'file1.gz'\ngzip file1  'file1'\ngzip -9 file1 \nrar a file1.rar test_file  'file1.rar' \nrar a file1.rar file1 file2 dir1  'file1', 'file2'  'dir1'\nrar x file1.rar rar\nunrar x file1.rar rar\ntar -cvf archive.tar file1  tarball\ntar -cvf archive.tar file1 file2 dir1  'file1', 'file2'  'dir1'\ntar -tf archive.tar \ntar -xvf archive.tar \ntar -xvf archive.tar -C /tmp  /tmp\ntar -cvfj archive.tar.bz2 dir1 bzip2\ntar -xvfj archive.tar.bz2 bzip2\ntar -cvfz archive.tar.gz dir1 gzip\ntar -xvfz archive.tar.gz gzip\nzip file1.zip file1 zip\nzip -r file1.zip file1 file2 dir1 zip\nunzip file1.zip zip\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #  11. RPM  \n rpm -ivh package.rpm rpm\nrpm -ivh --nodeeps package.rpm rpm\nrpm -U package.rpm rpm\nrpm -F package.rpm rpm\nrpm -e package_name.rpm rpm\nrpm -qa rpm\nrpm -qa | grep httpd  \"httpd\" rpm\nrpm -qi package_name \nrpm -qg \"System Environment/Daemons\" rpm\nrpm -ql package_name rpm\nrpm -qc package_name rpm\nrpm -q package_name --whatrequires rpm\nrpm -q package_name --whatprovides rpm\nrpm -q package_name --scripts /l\nrpm -q package_name --changelog rpm\nrpm -qf /etc/httpd/conf/httpd.conf rpm\nrpm -qp package.rpm -l rpm\nrpm --import /media/cdrom/RPM-GPG-KEY \nrpm --checksig package.rpm rpm\nrpm -qa gpg-pubkey rpm\nrpm -V package_name  MD5\nrpm -Va rpm- \nrpm -Vp package.rpm rpm\nrpm2cpio package.rpm | cpio --extract --make-directories *bin* rpm\nrpm -ivh /usr/src/redhat/RPMS/`arch`/package.rpm rpm\nrpmbuild --rebuild package_name.src.rpm rpm rpm \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #  12. YUM  \n yum install package_name rpm\nyum localinstall package_name.rpm rpm\nyum update package_name.rpm rpm\nyum update package_name rpm\nyum remove package_name rpm\nyum list \nyum search package_name rpm\nyum clean packages rpm\nyum clean headers \nyum clean all \n \n 1 2 3 4 5 6 7 8 9 10 #  13. deb  \n dpkg -i package.deb / deb \ndpkg -r package_name  deb \ndpkg -l  deb \ndpkg -l | grep httpd  \"httpd\" deb\ndpkg -s package_name \ndpkg -L package_name deb\ndpkg --contents package.deb \ndpkg -S /bin/ping deb\nAPT  (Debian, Ubuntu )\napt-get install package_name / deb \napt-cdrom install package_name / deb \napt-get update \napt-get upgrade \napt-get remove package_name deb\napt-get check \napt-get clean \napt-cache search searched-package \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #  14.  \n cat file1 \ntac file1 \nmore file1 \nless file1  'more' \nhead -2 file1 \ntail -2 file1 \ntail -f /var/log/messages \n \n 1 2 3 4 5 6 7 #  15.  \n cat file1 file2 ... | command <> file1_in.txt_or_file1_out.txt general syntax for text manipulation using PIPE, STDIN and STDOUT\ncat file1 | command( sed, grep, awk, grep, etc...) > result.txt \ncat file1 | command( sed, grep, awk, grep, etc...) >> result.txt \ngrep Aug /var/log/messages  '/var/log/messages'\"Aug\"\ngrep ^Aug /var/log/messages  '/var/log/messages'\"Aug\"\ngrep [0-9] /var/log/messages  '/var/log/messages' \ngrep Aug -R /var/log/*  '/var/log' \"Aug\"\nsed 's/stringa1/stringa2/g' example.txt example.txt \"string1\"  \"string2\"\nsed '/^$/d' example.txt example.txt\nsed '/ *#/d; /^$/d' example.txt example.txt\necho 'esempio' | tr '[:lower:]' '[:upper:]' \nsed -e '1d' result.txt example.txt \nsed -n '/stringa1/p'  \"string1\"\nsed -e 's/ *$//' example.txt \nsed -e 's/stringa1//g' example.txt  \"string1\" \nsed -n '1,5p;5q' example.txt 5\nsed -n '5p;5q' example.txt 5\nsed -e 's/00*/0/g' example.txt \ncat -n file1 \ncat example.txt | awk 'NR%2==1' example.txt\necho a b c | awk '{print $1}' \necho a b c | awk '{print $1,$3}' \npaste file1 file2 \npaste -d '+' file1 file2 \"+\"\nsort file1 file2 \nsort file1 file2 | uniq ()\nsort file1 file2 | uniq -u \nsort file1 file2 | uniq -d ()\ncomm -1 file1 file2  'file1' \ncomm -2 file1 file2  'file2' \ncomm -3 file1 file2 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #  16.  \n dos2unix filedos.txt fileunix.txt MSDOSUNIX\nunix2dos fileunix.txt filedos.txt UNIXMSDOS\nrecode ..HTML < page.txt > page.html html\nrecode -l | more \n \n 1 2 3 4 #  17.  \n badblocks -v /dev/hda1 hda1\nfsck /dev/hda1 /hda1linux\nfsck.ext2 /dev/hda1 /hda1ext2\ne2fsck /dev/hda1 /hda1ext2\ne2fsck -j /dev/hda1 /hda1ext3\nfsck.ext3 /dev/hda1 /hda1ext3\nfsck.vfat /dev/hda1 /hda1fat\nfsck.msdos /dev/hda1 /hda1dos\ndosfsck /dev/hda1 /hda1dos\n \n 1 2 3 4 5 6 7 8 9 #  18.  \n mkfs /dev/hda1 hda1\nmke2fs /dev/hda1 hda1linux ext2\nmke2fs -j /dev/hda1 hda1linux ext3()\nmkfs -t vfat 32 -F /dev/hda1  FAT32 \nfdformat -n /dev/fd0 \nmkswap /dev/hda3 swap\n \n 1 2 3 4 5 6 #  19. SWAP  \n mkswap /dev/hda3 swap\nswapon /dev/hda3 swap\nswapon /dev/hda2 /dev/hdb3 swap\n \n 1 2 3 #  20.  \n dump -0aj -f /tmp/home0.bak /home  '/home' \ndump -1aj -f /tmp/home0.bak /home  '/home' \nrestore -if /tmp/home0.bak \nrsync -rogpav --delete /home /tmp \nrsync -rogpav -e ssh --delete /home ip_address:/tmp SSHrsync\nrsync -az -e ssh --delete ip_addr:/home/public /home/local ssh\nrsync -az -e ssh --delete /home/local ip_addr:/home/public ssh\ndd bs=1M if=/dev/hda | gzip | ssh user@ip_addr 'dd of=hda.gz' ssh\ndd if=/dev/sda of=/tmp/file1 \ntar -Puf backup.tar /home/user  '/home/user' \n( cd /tmp/local/ && tar c . ) | ssh -C user@ip_addr 'cd /home/share/ && tar x -p' ssh\n( tar c /home ) | ssh -C user@ip_addr 'cd /home/backup-home && tar x -p' ssh\ntar cf - . | (cd /tmp/backup ; tar xf - ) \nfind /home/user1 -name '*.txt' | xargs cp -av --target-directory=/home/backup/ --parents  '.txt' \nfind /var/log -name '*.log' | tar cv --files-from=- | bzip2 > log.tar.bz2  '.log' bzip\ndd if=/dev/hda of=/dev/fd0 bs=512 count=1  MBR (Master Boot Record)\ndd if=/dev/fd0 of=/dev/hda bs=512 count=1 MBR\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #  21.  \n cdrecord -v gracetime=2 dev=/dev/cdrom -eject blank=fast -force \nmkisofs /dev/cdrom > cd.iso iso\nmkisofs /dev/cdrom | gzip > cd_iso.gz iso\nmkisofs -J -allow-leading-dots -R -V \"Label CD\" -iso-level 4 -o ./cd.iso data_cd iso\ncdrecord -v dev=/dev/cdrom cd.iso ISO\ngzip -dc cd_iso.gz | cdrecord dev=/dev/cdrom - ISO\nmount -o loop cd.iso /mnt/iso ISO\ncd-paranoia -B CD wav \ncd-paranoia -- \"-3\" CD wav -3\ncdrecord --scanbus scsi\ndd if=/dev/hdc | md5sum md5sum CD\n \n 1 2 3 4 5 6 7 8 9 10 11 #  22.  WIFI  \n ifconfig eth0 \nifup eth0  'eth0' \nifdown eth0  'eth0' \nifconfig eth0 192.168.1.1 netmask 255.255.255.0 IP\nifconfig eth0 promisc  'eth0'  (sniffing)\ndhclient eth0 dhcp 'eth0'\nroute -n show routing table\nroute add -net 0/0 gw IP_Gateway configura default gateway\nroute add -net 192.168.0.0 netmask 255.255.0.0 gw 192.168.1.1 configure static route to reach network '192.168.0.0/16'\nroute del 0/0 gw IP_gateway remove static route\necho \"1\" > /proc/sys/net/ipv4/ip_forward activate ip routing\nhostname show hostname of system\nhost www.example.com lookup hostname to resolve name to ip address and viceversa(1)\nnslookup www.example.com lookup hostname to resolve name to ip address and viceversa(2)\nip link show show link status of all interfaces\nmii-tool eth0 show link status of 'eth0'\nethtool eth0 show statistics of network card 'eth0'\nnetstat -tup show all active network connections and their PID\nnetstat -tupl show all network services listening on the system and their PID\ntcpdump tcp port 80 show all HTTP traffic\niwlist scan show wireless networks\niwconfig eth1 show configuration of a wireless network card\nhostname show hostname\nhost www.example.com lookup hostname to resolve name to ip address and viceversa\nnslookup www.example.com lookup hostname to resolve name to ip address and viceversa\nwhois www.example.com lookup on Whois database\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #  23.  \n ls -a\nls -l\nls -R\nls -ld\nctrl+r\nLinux.\npwd:\n \n 1 2 3 4 5 6 7 #  24.  \n file:\n \n 1 #  25.  \n 1 cp  cp  \n \n -r:-v \n  cp -r  \n cp -r   \n \n 2 touch+  \n \n / \n touch *  \n touch d 20040210 test test  20040210 \n touch abc  abc  \n \n 3 mv    \n \n mv []<> <> \n mva.txt ../ a.txt  \n mv a.txt b.txt a.txt  b.txt \n mvdir2 ../ dir2  \n \n 4 rm  \n \n -i -r \n \n 5 mkdir +  \n 6 rm -r +  \n \n rmdir  \n \n 7 mkdir -p dir1/dir2  dir1  dir1  dir2  dir1/ dir1/dir2 \n 8 rmdir p dir1/dir2  dir1  dir2  dir1  \n 9 rm *  \n 10 -f   rm f *.txt txt  \n 11 -i   \n \n rm i *  \n rm:backup:is a directory   \n rm: remove myfiles.txt ? Y \n , Y  N  \n \n 12 -r   \n \n rm -r test  test  test  \n rm -r *  r  f  \n rm -rf test  \n \n 13 grep  \n \n grep[] <> < > \n greplinux test.txt test.txt  linux  \n \n 14 ln  \n \n  \n ln [] <> <> \n  \n 1.: \n lns /usr/share/do doc  doc,/usr/share/do \n 2.: \n ln /usr/share/test hard hard test   \n 26.  \n 26.1 \n date:+%Y--%m--%d \n date -s: \n hwclock(clock)() \n cal \n  cal []  \n cal cal4 2004  2004  4  \n cal- y 2003 2003  \n uptime \n 26.2 \n echo  echo \"liuyazhuang\" >> liuyazhuang.txt \n cat, \n cat[]<> \n cat test.txt test.txt  \n cat test.txt | more  test.txt  \n cat test.txt >> test1.txt  test.txt  test1.txt  \n cat test.txt test2.txt >readme.txt :  test.txt  test2.txt  readme.txt  \n head: 10  -n:head -n  \n tail 10 -n -f  \n tail[]<> \n tail-10 /etc/passwd /etc/passwd/ 10  \n tail+10 /etc/passwd /etc/passwd/ 10  \n more \n more   more  Q  \n ls -al |more etc   Q  \n less q  \n less  more   less  less  Q  \n ls -al | less/etc  Q  \n 26.3 \n Ispci PCI  -v \n Isusb USB  -v \n Ismod() \n 26.4 \n shutdown  \n shutdown[] -h  -r \n shutdown -h now \n 10 shutdown -h +10 \n 23:30 shutdown -h 23:30 \n shutdown -r now \n poweroff \n reboot \n 26.5 \n zip: zip liuyazhuang.zip myfile zip  zip  \n unzip unzip liuyazhuang.zip \n gzip gzip  \n tar \n tar -cvf out.tar liuyazhuang \"liuyazhuang\" \n tar -xvf liuyazhuang.tar  liuyazhuang.tar  \n tar -cvzf backup.tar.gz/etc \n -z  gzip  \n -c tar  \n -v \n -f \n -z gzip  \n -t \n -x tar  \n tar -cvf test.tar * test.tar,.tar  \n tar -zcvf test.tar.gz * test.tar, gzip  \n tar -tf test.tar  test.tar  \n tar -xvf test.tar  test.tar  \n tar -zxvf foo.tar.gz  \n gzip  gunzip  \n gziptest.txt  \n gizpl test.txt.gz \n 26.6 \n locatelocate keyword \n  updatedb  \n find  \n  \n find . -name liuyazhuang \"liuyazhuang\" \n find / -name *.conf .conf  \n find / -perm 777  777  \n find / -type d  \n find . -name \"a*\"-exec ls -l {} ; \n find  \n find [<>][] \n find / -name httpd.conf  httpd.conf  \n 26.7ctrl+c : \n 26.8who  w  \n  \n who/w[] \n 26.9dmesg  \n  \n 26.10df  \n  \n 26.11du  \n  \n du [] <> \n 26.12free  \n  \n 27. VIM \n VIM  Linux  vim  vim  \n  vim +   vim \n  vim  vim  \n :q vim  \n VIM  \n vim  \n 1 \n vim  esc  \n  \n i :  \n o: \n dd: \n yy \n n+yy : n  n  \n p: \n u \n r: \n /  \n 2 \n  \" i \" esc  \n 3ex  \n \" : \" ex  vim. \n ext  \n :w  \n :q  \n :q!  \n :x ::wq \n :set number  \n :!   \n :sh  ctrl+d  vim \n 28. (RPM) \n 28.1 \n  RPM :rpm ivh wu-ftpd-2.6.2-8.i386.rpm \n i rpm  v:  h:  \n 28.2 \n  RPM rpm e wu-ftpd \n wu-ftpdwu-ftpd-2.6.2-8 .wu-ftpd-2.6.2-8.i386.rpm \n 28.3 \n rpm Uvh wu-ftpd-2.6.2-8.i386.rpm Uvh \n 28.4 \n rpm  rpm rpm rpm  \n rpm Fvhwu-ftpd-2.6.2-8.i386.rpm -Fvh \n 28.5 \n  RPM -q   \n rpm q wu-ftpd \n rpm ql package-name \n rpm ql xv (l ) \n 29. \n 1GET \n curl  URL\n curl  URL?a = 1 & b = nihao\n \n 1 2 #  2POST \n curl   -X  POST  -d   'a=1&b=nihao'  URL\n \n 1 #  3json \n curl   -H   \"Content-Type: application/json\"   -X  POST  -d   '{\"abc\":123,\"bcd\":\"nihao\"}'  URL\n curl   -H   \"Content-Type: application/json\"   -X  POST  -d  @test.json URL\n   \n \n 1 2 3  -H header -X (POST/GET/HEAD/DELETE/PUT/PATCH) -d  \n curl man curlcurl -h \n"},{title:"gitlabCICD",frontmatter:{title:"gitlabCICD",date:"2022-10-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["CICD"],tags:["CICD","","pipeline"]},regularPath:"/%E5%85%B6%E4%BB%96/gitlab%E9%80%9A%E8%BF%87CICD%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2.html",relativePath:"/gitlabCICD.md",key:"v-10e8b782",path:"/2022/10/08/gitlab%E9%80%9A%E8%BF%87cicd%E6%B5%81%E6%B0%B4%E7%BA%BF%E9%83%A8%E7%BD%B2/",headers:[{level:2,title:"",slug:""},{level:3,title:"1.JDK",slug:"_1-jdk"},{level:3,title:"2.maven",slug:"_2-maven"},{level:3,title:"3.Git",slug:"_3-git"},{level:3,title:"4.Docker ()",slug:"_4-docker--"},{level:2,title:"GitLab",slug:"gitlab"},{level:3,title:"1.",slug:"_1-"},{level:3,title:"2.gitlab-runner",slug:"_2-gitlab-runner"},{level:3,title:"3.gitlab-runner ",slug:"_3-gitlab-runner-"},{level:3,title:"4.",slug:"_4-"},{level:3,title:"5.",slug:"_5-"},{level:2,title:"gitlab CICD ",slug:"gitlab-cicd-"},{level:2,title:"",slug:""}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:"  \n 1.JDK \n \n  \n tar -zxvf jdk-8u161-linux-x64.tar.gz  \n vi /etc/profile \n \n export   JAVA_HOME = /home/soft/java8/jdk\n export   PATH = $JAVA_HOME /bin: $PATH \n export   CLASSPATH = .: $JAVA_HOME /lib/dt.jar: $JAVA_HOME /lib/tools.jar\n \n 1 2 3 \n source /etc/profile  \n java -version \n 2.maven \n \n jdk \n localRepository,conf/setting.xml \n mirror: \n \n < mirror > \n       < id > alimaven </ id > \n       < name > aliyun maven </ name > \n       < url > http://maven.aliyun.com/nexus/content/groups/public/ </ url > \n       < mirrorOf > central </ mirrorOf > \n </ mirror > \n\n \n 1 2 3 4 5 6 7 \n  \n \n export   MAVEN_HOME = /home/soft/maven/maven\n export   PATH = $PATH : $MAVEN_HOME /bin  \n \n 1 2 \n source /etc/proflie \n mvn -version \n 3.Git \n \n jdk \n  \n \n yum  install  curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker\n\n \n 1 2 \n git make prefix=/home/soft/git/git all (/home/soft/git) \n git make prefix=/home/socf/git/git install \n vi /etc/profile \n \n export   PATH = $PATH :/home/soft/git/git\n\n \n 1 2 #  4.Docker () \n  yum install -y docker \nsystemctl status docker \ndockersystemctl start docker/systemctl start docker.service \nservice docker start \ndockersystemctl restart docker \nsudo service docker restart \ndockersystemctl stop docker \nservice docker stop \ndocker images \ndocker build -t  \n:docker rmi  \ndocker run -p 192.168.211.128:8081:8081  \ndocker run -d -p --name   \n GitLab \n  \n systemctl status firewalld.service\nsystemctl stop firewalld \n \n 1 2 #  1. \n \n yum vi /etc/yum.repos.d/gitlab-ce.repo \n \n [ gitlab-ce ] \n name = Gitlab CE Repository\n baseurl = https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el $releasever /\n gpgcheck = 0 \n enabled = 1 \n\n \n 1 2 3 4 5 6 \n yum makecache \n gitlabyum install gitlab-ce \n  \n \n gitlab/var/log/gitlab\ngitlab/etc/gitlab/  gitlab.rb\n/opt/gitlab\n /var/opt/gitlab/\n   /var/opt/gitlab/git-data/repositories\n/var/opt/gitlab/backups/\nnginx/var/opt/gitlab/nginx/\nredis/var/opt/gitlab/redis\n\n \n 1 2 3 4 5 6 7 8 9 \n ip vi /etc/gitlab/gitlab.rb \n \n \n \n  \n postfix \n :yum install -y postfix \n systemctl enable postfix \n systemctl start psotfix \n \n gitlab.rb \n\n \n 1 2 # \ngitlab_rails [ 'gitlab_email_enabled' ]   =   true \ngitlab_rails [ 'gitlab_email_from' ]   =   '' \ngitlab_rails [ 'gitlab_email_display_name' ]   =   'xxx' \n \ngitlab_rails [ 'smtp_enable' ]   =   true \ngitlab_rails [ 'smtp_address' ]   =   \"smtp.163.com\" \ngitlab_rails [ 'smtp_port' ]   =   465 \ngitlab_rails [ 'smtp_user_name' ]   =   \"\" \ngitlab_rails [ 'smtp_password' ]   =   \"smtp\" \ngitlab_rails [ 'smtp_domain' ]   =   \"163.com\" \ngitlab_rails [ 'smtp_authentication' ]   =   \"login\" \ngitlab_rails [ 'smtp_enable_starttls_auto' ]   =   true \ngitlab_rails [ 'smtp_tls' ]   =   true \ngitlab_rails [ 'smtp_openssl_verify_mode' ]   =   'none' \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n gitlab-ctl reconfigure \n gitlab-rails console \n  \n \n \n gitlab.rb \n \n puma [ 'worker_processes' ]   =   2                           #CPU+1gitLab21 \npuma [ 'work_timeout' ]   =   60                              # \npuma [ 'worker_memory_limit_min' ]   =   \"200 * 1 << 20\"      # \npuma [ 'worker_memory_limit_max' ]   =   \"300 * 1 << 20\"      # \npostgresql [ 'shared_buffers' ]   =   \"128MB\"                    # \npostgresql [ 'max_worker_processes' ]   =   6                    # \nsidekiq [ 'concurrency' ]   =   15                               #sidekiq \n\nGitLab\nprometheus [ 'enable' ]   =   false \nprometheus [ 'monitor_kubernetes' ]   =   false \nalertmanager [ 'enable' ]   =   false   \nnode_exporter [ 'enable' ]   =   false  \nredis_exporter [ 'enable' ]   =   false  \npostgres_exporter [ 'enable' ]   =   false \ngitlab_exporter [ 'probe_sidekiq' ]   =   false \nprometheus_monitoring [ 'enable' ]   =   false \ngrafana [ 'enable' ]   =   false \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \n sudo gitlab-ctl restart \n  \n \n gitlab-ctl start  # \ngitlab-ctl restart # \ngitlab-ctl stop  # \ngitlab-ctl restart nginx  #nginx \ngitlab-ctl status  # \ngitlab-ctl reconfigure  # \ngitlab-ctl show-config  # \ngitlab-ctl uninstall  #gitlab \ngitlab-ctl cleanse  # \ngitlab-ctl  tail   < service name > \ngitlab-ctl  tail  nginx   #gitlabnginx \ngitlab-rails console   # \ngitlab-ctl  help                    #gitlab \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 root, cat /etc/gitlab/initial_root_password \n 2.gitlab-runner \n \n gitlab-runner:https://mirrors.tuna.tsinghua.edu.cn/gitlab-runner/yum/el7/ \n  \n rpm -ivh gitlab-runner-12.9.1-1.x86_64.rpm --nodeps --force  \n systemctl start gitlab-runner \n systemctl status gitlab-runner \n gitlab-runner -h \n 3.gitlab-runner  \n \n runner:gitlab-runner register ()\n \n \n gitlabPlease enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/) \n \n \n \n runnerPlease enter the gitlab-ci token for this runner \n \n \n \n gitlab-runnerPlease enter the gitlab-ci description for this runner \n \n \n gitlab-runnerPlease enter the gitlab-ci tags for this runner (comma separated) \n \n \n Please enter the executor: custom, docker, parallels, docker+machine, kubernetes, docker-ssh, shell, ssh, virtualbox, docker-ssh+machine \n \n \n \n \n \n \n runnergitlab-runner list \n gitlab-runner unregister --all-runners \n 4.   \n git \n \n sshssh-keygen -t rsa \n cat ~/.ssh/id_rsa \nCICDSSH_PRIVATE_KEY \n \n \n \n  \n \n \n \n .gitlab-ci.yml \n,  \n variables : \n   server_ip :  192.168.44.129\n   jar_name :  demo - 0.0.1 - SNAPSHOT.jar\n   java_path :  /home/java/java1.8/bin\n   upload_path :  /home/gitlab - project/\n   ssh_password :   111111 \n   repo_path :  /home/soft/maven/repo\n   jar_path :  com/example/demo/0.0.1 - SNAPSHOT\n \n stages : \n   -  build\n   -  upload\n   -  deploy\n maven  \n maven-build : \n   stage :  build\n   script : \n     -  mvn clean install  - Dmaven.test.skip=true\n     -  echo \"\"\njar \n upload-jar : \n   stage :  upload\n   script : \n     -  echo \"jar/home/gitlab - project\"\n     -  cd $repo_path/$jar_path\n     -  cp  - r $jar_name $upload_path\n SpringBoot jar \n deploy-test : \n   stage :  deploy\n   script : \n     -  echo \"$jar_name\"\n     -  cd $upload_path\n     -  nohup java  - jar $jar_name  > log.out &\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 \n stages: \n script \n 5. \n 1.build,deploy, \n fatal:  git  fetch-pack: expected shallow list\nfatal: The remote end hung up unexpectedly\nERROR: Job failed:  exit  status  1 \n \n 1 2 3 : \n TSgitlab-cifatal: git fetch-pack: expected shallow list,fatal: -  (zhihu.com) \n gitlabcicd \n \n  git clone . \n cicd \n \n  \n 1.job \n 2.stage \n  \n gitlab CICD  \n (183) gitlabCICD_&Mr.Li-CSDN \n  \n gitlab \n gitlab-ctl start  # \ngitlab-ctl restart # \ngitlab-ctl stop  # \ngitlab-ctl restart nginx  #nginx \ngitlab-ctl status  # \ngitlab-ctl reconfigure  # \ngitlab-ctl show-config  # \ngitlab-ctl uninstall  #gitlab \ngitlab-ctl cleanse  # \ngitlab-ctl  tail   < service name > \ngitlab-ctl  tail  nginx   #gitlabnginx \ngitlab-rails console   # \ngitlab-ctl  help                    #gitlab \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 gitlab-runnner \n - systemctl start gitlab-runner\n- systemctl status gitlab-runner\n- gitlab-runner  -h \n \n 1 2 3 .gitlab-ci.yml \n https://gitlab.com/gitlab-org/gitlab-foss/-/blob/master/lib/gitlab/ci/templates \n,  \n variables : \n   server_ip :  192.168.44.129\n   jar_name :  demo - 0.0.1 - SNAPSHOT.jar\n   java_path :  /home/java/java1.8/bin\n   upload_path :  /home/gitlab - project/\n   ssh_password :   111111 \n   repo_path :  /home/soft/maven/repo\n   jar_path :  com/example/demo/0.0.1 - SNAPSHOT\n \n stages : \n   -  build\n   -  upload\n   -  deploy\n maven  \n maven-build : \n   stage :  build\n   script : \n     -  mvn clean install  - Dmaven.test.skip=true\n     -  echo \"\"\njar \n upload-jar : \n   stage :  upload\n   script : \n     -  echo \"jar/home/gitlab - project\"\n     -  cd $repo_path/$jar_path\n     -  cp  - r $jar_name $upload_path\n SpringBoot jar \n deploy-test : \n   stage :  deploy\n   script : \n     -  echo \"$jar_name\"\n     -  cd $upload_path\n     -  nohup java  - jar $jar_name  > log.out &\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # job  \n stages : \n   -  build\n   -  cleanup\n   -  deploy\nbuildjob \n build :   #  \n   stage :  build  #  \n   only :   #  \n     -  master  #  \n     -  pre  #  \n     -  testN  #  test1 test2 ... testN \n     -  devN  #  dev1 dev2 ... devN \n   variables : \n     VERSION :  1.0.10  #   \n   before_script : \nSSH key \n- ... \n NODE \n     -  NODE_ENV=`if  [ [  $ { CI_COMMIT_REF_NAME : 0 : 3 }  = \"dev\"  | |  $ { CI_COMMIT_REF_NAME : 0 : 4 }  = \"test\"  ] ] ; then echo \"development\"; else echo \"production\"; fi`;\n   script : \nnode modules npm install \n     -  PACKAGE_HASH=$(md5sum package.json  |  awk ' { print $1 } ');\n     -  mkdir  - p ~/builds/cache/node_modules  # dockervolume  runner \n     -  NPM_CACHE=~/builds/cache/node_modules/$ { PACKAGE_HASH } .tar\n     -  if  [   - f $NPM_CACHE  ] ;\n      then\n        echo \"Use Cache\";\n        tar xf $NPM_CACHE;\n      else\n        npm install;\n        tar cf  -  ./node_modules  >  $NPM_CACHE;\n      fi\nnpm build \n     -  echo \"NODE_ENV=$NODE_ENV node build/build.js\"\n     -  NODE_ENV=$NODE_ENV node build/build.js\nupload to CDN \n- ... \ndocker build \n     -  echo `docker build  - t \"$CI_PIPELINE_ID\" .  |  awk  - F \"Successfully built \" ' { print $2 } '`\ndocker push \n     -  if  [  $NODE_ENV = \"development\"  ] ;  # serverrunnerpush \n      then\n        docker login dev.hub.xxx.com  - u$USERNAME  - p$PASSWORD;\n        docker tag $CI_PIPELINE_ID dev.hub.xxx.com/namespace/webapp : $CI_COMMIT_REF_NAME;\n        docker push dev.hub.xxx.com/namespace/webapp : $CI_COMMIT_REF_NAME;\n        docker rmi dev.hub.xxx.com/namespace/webapp : $CI_COMMIT_REF_NAME;\n        echo \" --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- - - \";\n        echo \"dev.hub.xxx.com/namespace/webapp : $CI_COMMIT_REF_NAME\";\n      else\nthen \n        \" --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- - - \";\n        echo \"hub.prd.xxx.com/namespace/webapp : $DATE.$CI_BUILD_ID\";\n      fi\n \n clean_testN : \n   stage :  cleanup\n   only : \n     -  testN\n   tags : \n     -  dc2fe - deploy - testN\n   script : \n     -  docker stop webapp\n     -  docker rm webapp\n   allow_failure :   true \n\n deploy_testN : \n   stage :  deploy\n   only : \n     -  testN\n   tags : \n     -  dc2fe - deploy - testN\n   script : \n- ssh root@IP \"docker stop webapp; docker rm webapp; docker run --name webapp -d -p 8000:8000 dev.hub.xxx.com/namespace/webapp-testN:latest\" \n \n     -  docker pull dev.hub.xxx.com/namespace/webapp : testN\n     -  docker run  - - name webapp  - d  - p 8000 : 8000 dev.hub.xxx.com/namespace/webapp : testN\nclean_dev: # devtest \ndeploy_dev: \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 "},{title:"hash",frontmatter:{title:"hash",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["redis","hash",""]},regularPath:"/%E5%85%B6%E4%BB%96/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95.html",relativePath:"/hash.md",key:"v-851571e2",path:"/2023/06/10/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/",headers:[{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"",slug:""}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:'  \n  \n  \n  \n \n  \n  3  3  1  \n \n  \n  \n  \n   KVkey-valu  key   \n  \n  \n    key  \n  3   hash(key) % 3   \n  key  \n hash ( key )   %   3 \n \n 1  0 key  \n    \n  ABC  KV   hash(key) % 3   \n \n  3  key  key-01key-02key-03  key  hash()  hash( key-01) = 6hash( key-02) = 7hash(key-03) = 8 \n  key  \n \n  3  3  4   \n \n  hash(key-01) %  3  = 0 hash(key-01) %  4  = 2 key-01  C key-01  A  C \n  \n    3  4  hash(key) % 4  \n  M  O(M)  \n  \n  \n  \n (Consistent Hash)1997  2^32   \n  \n  \n  \n memcached  \n  \n \n RPCDubbo \n  \n LVS \n  \n  \n  2^32  60  2^32    \n \n  \n \n  IP  \n  \n \n    \n  \n    \n  3  \n \n  key-01  key-01  key-01  \n  key-01  A \n \n  key  2  \n \n  key  key  \n  key  \n \n  \n  3  4 D  \n \n key-01key-03  key-02  D \n  3  2 A  \n \n key-02  key-03  key-01  B \n    \n  3  \n    \n  3  \n \n  A A  \n  \n  A  A  B  B  B  B  \n    \n  \n  \n    \n   \n  3  \n \n  A A-01A-02A-03 \n  B B-01B-02B-03 \n  C C-01C-02C-03 \n \n  3  9  3  \n \n   A-01A-01 A A  \n  3  Nginx  1 160  \n    \n    \n  \n    \n  \n hash(key) \n  \n Node \n import   com . gordon . cicd . uitl . HashUtils ; \n\n import   java . util . * ; \n\n public   class   Node   { \n     private   static   final   int   VIRTUAL_NODE_NO_PER_NODE   =   3 ; // \n     private   final   String  ip ; \n     private   final   List < Integer >  virtualNodeHashes  =   new   ArrayList < > ( VIRTUAL_NODE_NO_PER_NODE ) ; \n     private   final   Map < Object ,   Object >  cacheMap  =   new   HashMap < > ( ) ; \n\n     public   Node ( String  ip )   { \n         Objects . requireNonNull ( ip ) ; \n         this . ip  =  ip ; \n         initVirtualNodes ( ) ; \n     } \n\n\n     private   void   initVirtualNodes ( )   { \n         String  virtualNodeKey ; \n         for   ( int  i  =   1 ;  i  <=   VIRTUAL_NODE_NO_PER_NODE ;  i ++ )   { \n            virtualNodeKey  =  ip  +   "#"   +  i ; \n            virtualNodeHashes . add ( HashUtils . hashcode ( virtualNodeKey ) ) ; \n         } \n     } \n\n     public   void   addCacheItem ( Object  key ,   Object  value )   { \n        cacheMap . put ( key ,  value ) ; \n     } \n\n\n     public   Object   getCacheItem ( Object  key )   { \n         return  cacheMap . get ( key ) ; \n     } \n\n\n     public   void   removeCacheItem ( Object  key )   { \n        cacheMap . remove ( key ) ; \n     } \n\n\n     public   List < Integer >   getVirtualNodeHashes ( )   { \n         return  virtualNodeHashes ; \n     } \n\n     public   String   getIp ( )   { \n         return  ip ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 hash \n public   class   HashUtils   { \n\n     /**\n     * FNV1_32_HASH\n     *\n     * @param obj\n     *         object\n     * @return hashcode\n     */ \n     public   static   int   hashcode ( Object  obj )   { \n         final   int  p  =   16777619 ; \n         int  hash  =   ( int )   2166136261L ; \n         String  str  =  obj . toString ( ) ; \n         for   ( int  i  =   0 ;  i  <  str . length ( ) ;  i ++ ) \n            hash  =   ( hash  ^  str . charAt ( i ) )   *  p ; \n        hash  +=  hash  <<   13 ; \n        hash  ^=  hash  >>   7 ; \n        hash  +=  hash  <<   3 ; \n        hash  ^=  hash  >>   17 ; \n        hash  +=  hash  <<   5 ; \n\n         if   ( hash  <   0 ) \n            hash  =   Math . abs ( hash ) ; \n         //System.out.println("hash computer:" + hash); \n         return  hash ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 hash \n import   com . gordon . cicd . entity . Node ; \n import   com . gordon . cicd . uitl . HashUtils ; \n\n import   java . util . * ; \n\n public   class   ConsistentHash   { \n     private   final   TreeMap < Integer ,   Node >  hashRing  =   new   TreeMap < > ( ) ; \n\n     public   List < Node >  nodeList  =   new   ArrayList < > ( ) ; \n\n     /**\n     * \n     * \n     * 2Node\n     * @param ip\n     */ \n     public   void   addNode ( String  ip )   { \n         Objects . requireNonNull ( ip ) ; \n         Node  node  =   new   Node ( ip ) ; \n        nodeList . add ( node ) ; \n         for   ( Integer  virtualNodeHash  :  node . getVirtualNodeHashes ( ) )   { \n            hashRing . put ( virtualNodeHash ,  node ) ; \n             //System.out.println("[" + node + "] hash:" + virtualNodeHash + ""); \n         } \n     } \n\n     /**\n     * \n     * @param node\n     */ \n     public   void   removeNode ( Node  node ) { \n        nodeList . remove ( node ) ; \n     } \n\n     /**\n     * \n     * \n     * @param key\n     * @return\n     */ \n     public   Object   get ( Object  key )   { \n         Node  node  =   findMatchNode ( key ) ; \n         System . out . println ( ":"   +  node . getIp ( ) ) ; \n         return  node . getCacheItem ( key ) ; \n     } \n\n     /**\n     * \n     * hash\n     * @param key\n     * @param value\n     */ \n     public   void   put ( Object  key ,   Object  value )   { \n         Node  node  =   findMatchNode ( key ) ; \n\n        node . addCacheItem ( key ,  value ) ; \n     } \n\n     /**\n     * \n     */ \n     public   void   evict ( Object  key )   { \n         findMatchNode ( key ) . removeCacheItem ( key ) ; \n     } \n\n\n     /**\n     *  \n     * @param key Hash\n     *      * @return \n     * @return\n     */ \n     private   Node   findMatchNode ( Object  key )   { \n         Map . Entry < Integer ,   Node >  entry  =  hashRing . ceilingEntry ( HashUtils . hashcode ( key ) ) ; \n         if   ( entry  ==   null )   { \n            entry  =  hashRing . firstEntry ( ) ; \n         } \n         return  entry . getValue ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80  \n import   com . gordon . cicd . entity . Node ; \n\n import   java . util . ArrayList ; \n import   java . util . Comparator ; \n import   java . util . HashMap ; \n import   java . util . List ; \n\n public   class   ConsistentHashTest   { \n     public   static   final   int   NODE_SIZE   =   3 ; \n     public   static   final   int   STRING_COUNT   =   10 ; \n     private   static   ConsistentHash  consistentHash  =   new   ConsistentHash ( ) ; \n     private   static   List < String >  sList  =   new   ArrayList < > ( ) ; \n\n     public   static   void   main ( String [ ]  args )   { \n         //  \n         for   ( int  i  =   0 ;  i  <   NODE_SIZE ;  i ++ )   { \n             String  ip  =   new   StringBuilder ( "10.2.1." ) . append ( i ) \n                     . toString ( ) ; \n            consistentHash . addNode ( ip ) ; \n         } \n\n         // ; \n         for   ( int  i  =   0 ;  i  <   10 ;  i ++ )   { \n            sList . add ( i + "" ) ; \n         } \n\n         //  \n         for   ( String  s  :  sList )   { \n            consistentHash . put ( s ,  s ) ; \n         } \n\n\n\n         //  \n         HashMap < String ,   List < Object > >  map  =   new   HashMap < > ( ) ; \n         for   ( int  i  =   0 ;  i  <  sList . size ( ) ;  i ++ )   { \n             String  key  =  sList . get ( i ) ; \n             Node  matchNode  =  consistentHash . findMatchNode ( key ) ; \n             List < Object >  list  =  map . getOrDefault ( matchNode . getIp ( ) ,   new   ArrayList < > ( ) ) ; \n            list . add ( i ) ; \n            map . put ( matchNode . getIp ( ) ,  list ) ; \n         } \n         for   ( String  ip  :  map . keySet ( ) )   { \n             System . out . println ( "ip:" + ip + ":" + map . get ( ip ) ) ; \n         } \n\n         ArrayList < Ip_Vm >  ip_vms  =   new   ArrayList < > ( ) ; \n\n         for   ( Node  node  :  consistentHash . nodeList ) { \n             System . out . println ( "ip:" + node . getIp ( ) + ":" + node . getVirtualNodeHashes ( ) ) ; \n             for   ( Integer  virtualNodeHash  :  node . getVirtualNodeHashes ( ) )   { \n                ip_vms . add ( new   Ip_Vm ( node . getIp ( ) , virtualNodeHash ) ) ; \n             } \n         } \n        ip_vms . sort ( ( Comparator . comparingInt ( Ip_Vm :: getVirtualNodeHashCode ) ) ) ; \n         System . out . println ( "hash" ) ; \n         for   ( Ip_Vm  ip_vm  :  ip_vms )   { \n             System . out . print ( ip_vm . getIp ( ) + "\\t" ) ; \n         } \n         System . out . println ( ) ; \n\n         System . out . println ( "================================================" ) ; \n\n         //  \n        consistentHash . addNode ( "10.2.1.3" ) ; \n\n         //  \n\n         HashMap < String ,   List < Object > >  map_n  =   new   HashMap < > ( ) ; \n         for   ( int  i  =   0 ;  i  <  sList . size ( ) ;  i ++ )   { \n             String  key  =  sList . get ( i ) ; \n             Node  matchNode  =  consistentHash . findMatchNode ( key ) ; \n             List < Object >  list  =  map_n . getOrDefault ( matchNode . getIp ( ) ,   new   ArrayList < > ( ) ) ; \n            list . add ( i ) ; \n            map_n . put ( matchNode . getIp ( ) ,  list ) ; \n         } \n         for   ( String  ip  :  map_n . keySet ( ) )   { \n             System . out . println ( "ip:" + ip + ":" + map_n . get ( ip ) ) ; \n         } \n\n\n         ArrayList < Ip_Vm >  ip_vms_n  =   new   ArrayList < > ( ) ; \n         for   ( Node  node  :  consistentHash . nodeList ) { \n             System . out . println ( "ip:" + node . getIp ( ) + ":" + node . getVirtualNodeHashes ( ) ) ; \n             for   ( Integer  virtualNodeHash  :  node . getVirtualNodeHashes ( ) )   { \n                ip_vms_n . add ( new   Ip_Vm ( node . getIp ( ) , virtualNodeHash ) ) ; \n             } \n         } \n        ip_vms_n . sort ( ( Comparator . comparingInt ( Ip_Vm :: getVirtualNodeHashCode ) ) ) ; \n         System . out . println ( "hash" ) ; \n         for   ( Ip_Vm  ip_vm  :  ip_vms_n )   { \n             System . out . print ( ip_vm . getIp ( ) + "\\t" ) ; \n         } \n         System . out . println ( ) ; \n\n\n     } \n\n     private   static   class   Ip_Vm { \n         String  ip ; \n         Integer  virtualNodeHashCode ; \n         List < Object >  keys ; \n\n         public   List < Object >   getKeys ( )   { \n             return  keys ; \n         } \n\n         public   void   addKey ( Object  key )   { \n            keys . add ( key ) ; \n         } \n\n\n         public   Ip_Vm ( String  ip ,   Integer  virtualNodeHashCode )   { \n             this . ip  =  ip ; \n             this . virtualNodeHashCode  =  virtualNodeHashCode ; \n            keys = new   ArrayList < > ( ) ; \n         } \n\n         public   String   getIp ( )   { \n             return  ip ; \n         } \n\n         public   void   setIp ( String  ip )   { \n             this . ip  =  ip ; \n         } \n\n         public   Integer   getVirtualNodeHashCode ( )   { \n             return  virtualNodeHashCode ; \n         } \n\n         public   void   setVirtualNodeHashCode ( Integer  virtualNodeHashCode )   { \n             this . virtualNodeHashCode  =  virtualNodeHashCode ; \n         } \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 \n  \n public   class   ConsistentHashTest   { \n     public   static   final   int   NODE_SIZE   =   10 ; \n     public   static   final   int   STRING_COUNT   =   100   *   100 ; \n     private   static   ConsistentHash  consistentHash  =   new   ConsistentHash ( ) ; \n     private   static   List < String >  sList  =   new   ArrayList < > ( ) ; \n\n     public   static   void   main ( String [ ]  args )   { \n         //  \n         for   ( int  i  =   0 ;  i  <   NODE_SIZE ;  i ++ )   { \n             String  ip  =   new   StringBuilder ( "10.2.1." ) . append ( i ) \n                     . toString ( ) ; \n            consistentHash . addNode ( ip ) ; \n         } \n\n         // ; \n         for   ( int  i  =   0 ;  i  <   STRING_COUNT ;  i ++ )   { \n            sList . add ( RandomStringUtils . randomAlphanumeric ( 10 ) ) ; \n         } \n\n         //  \n         for   ( String  s  :  sList )   { \n            consistentHash . put ( s ,  s ) ; \n         } \n\n         for ( int  i  =   0   ;  i  <   10   ;  i  ++ )   { \n             int  index  =   RandomUtils . nextInt ( 0 ,   STRING_COUNT ) ; \n             String  key  =  sList . get ( index ) ; \n             String  cache  =   ( String )  consistentHash . get ( key ) ; \n             System . out . println ( "Random:" + index + ",key:"   +  key  +   ",consistentHash get value:"   +  cache  + ",value is:"   +  key . equals ( cache ) ) ; \n         } \n\n         //  \n         for   ( Node  node  :  consistentHash . nodeList ) { \n             System . out . println ( node ) ; \n         } \n\n         //  \n        consistentHash . addNode ( "10.2.1.110" ) ; \n         for ( int  i  =   0   ;  i  <   10   ;  i  ++ )   { \n             int  index  =   RandomUtils . nextInt ( 0 ,   STRING_COUNT ) ; \n             String  key  =  sList . get ( index ) ; \n             String  cache  =   ( String )  consistentHash . get ( key ) ; \n             System . out . println ( "Random:" + index + ",key:"   +  key  +   ",consistentHash get value:"   +  cache  + ",value is:"   +  key . equals ( cache ) ) ; \n         } \n\n         //  \n         for   ( Node  node  :  consistentHash . nodeList ) { \n             System . out . println ( node ) ; \n         } \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 '},{title:"",frontmatter:{title:"",date:"2023-02-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["",""]},regularPath:"/%E5%85%B6%E4%BB%96/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8.html",relativePath:"/.md",key:"v-7350f07e",path:"/2023/02/08/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E5%B8%83%E8%B0%B7%E9%B8%9F%E8%BF%87%E6%BB%A4%E5%99%A8/",headers:[{level:3,title:"",slug:""},{level:3,title:"BitMap",slug:"bitmap"},{level:3,title:"",slug:""},{level:3,title:"",slug:""}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:'  \n  (Bloom Filter)Burton Howard Bloom1970space efficient  (Crawler)1/81/4false positive rate()false negative () \n HashMap \n  URL  OK  \n  HashMap  Key O(1)   HashMap  1000  HashMapKey=String 16 Value=Integer1.2  G \n  bitmap1000  int  40M 10 000 000 * 4/1024/1024 =40M 3%1000  Integer 161M  13.3% \n  HashMap  \n  100  URL URL  64B 640GB \n BitMap \n bit1  8  \n Bit-map  bit  value key  bit  \n  Java int  4 1  = 81 byte = 8 bit 32  bit  32   32  int  32  \n \n 1 Byte = 8 Bit1 KB = 1024 Byte1 MB = 1024 KB1GB = 1024 MB \n \n  1  5  BitMap  \n  id  int 4 32  50 000 000 * 4/1024/1024 = 200M \n BitMap.5  5  50 000 000/8/1024/1024 = 6M \n  BitMap  \n  bit  value key  BitMap    0  10 1  BitMap  \n  {1,3,5,7}  \n \n  65  int[N/32+1]  int  N  \n int[0]  0~31 \n int[1]  32~63 \n int[2]  64~95 \n \n   M/32   M%32  \n 65/32 = 2  65%32=1  65  int[2]   1  \n Java BitArraylonglong864long[Math.ceil(n/64)]long\n \n 1 #   \n  \n        \n  Bloom Filters (jasondavies.com) \n  \n  ListSetMap  \n       Google  Bigtable  IO Google Chrome  \n  Key-Value  HbaseRedisValue  Key  Value  IO  Hash Bit Array 1  \n  \n \n 1\n \n 1 2  \n \n 1 1(hash) 1\n \n 1 2  \n  hash   Counting Bloom filter  \n  \n 1. \n 2. \n 3. \n   \n 1 10  32  2G \n 2 URL ( URL  64 ) \n 3 \n 4- URL  \n 5 \n 6 IO \n 7 \n 8 \n \n\n\n\n\n key \n\n 4  id\n\n id \n\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 9. \n \n  \n \n HTT0A36000(10) \n  \n \n C+D+C+D+4000id50redis set--- \n Bloom Filter()Bloom Filterfalse positiveBloom FilterBloom Filter \n MurmurHashbitsetrediskey-value4000id8KBget&set \n   redis100id50100id \n  \n 1.RedisBloom\n Redis (bitmap) Redis4.0 Redis  Redis Server  RedisBloom  Redis  Module\nhttps://github.com/RedisBloom/RedisBloom\nhttps://oss.redislabs.com/redisbloom/\nredis:\n.bitmapclient\n.clientbitmapredis\n.bitmapredis\n\n\n2.Guava  BloomFilter\nGuava 11.0BloomFilter\nGuava redis\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 #   \n  AA  n  k A   a  B 1 \n  k k   1 A A  \n  3  URL  {URL1,URL2,URL3} hash  16  \n \n   Hash1()  Hash1(URL1) = 3  Hash1(URL2) = 6  Hash1(URL3) = 6  \n \n  URL1  Hash(1)  1  \n  Hash  URL2,URL3  Hash  m   1%    m/100    space-efficient \n  Hash  \n Hash1(URL1) = 3,Hash2(URL1) = 5,Hash3(URL1) = 6\nHash1(URL2) = 5,Hash2(URL2) = 8,Hash3(URL2) = 14\nHash1(URL3) = 4,Hash2(URL3) = 7,Hash3(URL3) = 10\n \n 1 2 3 \n  k  k  bit  \n  \n  1  bit  bit  1  \n   URL1000  bit  \n Hash1(URL1000) = 7,Hash2(URL1000) = 8,Hash3(URL1000) = 14\n \n 1 \n  bit  URL1,URL2,URL3  1   URL1000   \n   \n  100%  \n    \n Bloom Filter \n Coding \n 1.spark \n 2.redis \n 3.Hbase \n  \n  \n  \n  Hash  CPU  \n Counting Bloom Filter3  4  Bloom Filter  \n  \n  \n Cuckoo Filter: Practically Better Than Bloom \n  \n \n \n 1. \n 2. 95%  \n 3.quotient filter \n 4.3% \n  \n  \n 1.hash \n""hashbit \n hash  hash  \nhash \n fp = fingerprint(x)\nh1 = hash(x)\nh2 = h1 ^ hash(fp)  // \n \n 1 2 3 h2h1hash \n hash(fp) !=0 h2!=h1 \nhashhash 2 2 10000000...n0 \n \n 2. \n  \n \n hash \n \n  \nhashhashfalse \n \n Data(11)h1h2h133 \n \n  \n  hash  2  50% 248  84%95%98% \n  kb+1  k  hash b  \n  2  hash  4  8  \n  \n \n why  8  why false \n  \n  \n  \n  \n  \n 1.   \n \n  \n \n 2.   \n \n  \n3% \n40% \n \n 3.  \n \n  \n \n  \n 1. \n \n  \n \n \n 2. \n \n  \n \n \n 3. \n \n 2 \n2 \n \n 4. \n \n  \n1.; \n2.  ; \n3.. \n \n Redishttps://github.com/kristoff-it/redis-cuckoofilter \n Redissqlhttps://github.com/RedBeardLab/rediSQL \n  hash \n hash \n \n  \n  hash  T1T2 \n  hash  h1h2 \n  h1  T1  \n  h2  T2  \n  \n  \n  \n \n  \n  x hash  T1  2  T2  1  \n  T1  2  y  \n  y  z  \n  y  z  \n z  v  \n  x  \n \n  x  \n  hash  hash  \n  MaxLoop rehash  \n  \n  50% \n   hash     95% \n    hash  85% CPU  \n   4  hash  2    99%  \n Coding \n 1.redis4.0module,module. \n'},{title:"",frontmatter:{title:"",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["",""]},regularPath:"/%E5%85%B6%E4%BB%96/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95.html",relativePath:"/.md",key:"v-22a43ff6",path:"/2023/06/10/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/",headers:[{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"2. ",slug:"_2-"},{level:2,title:"3. ",slug:"_3-"}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:'  \n  \n   \n \n \n Process \n \n \n  \n \n \n  \n Chandy-Lamport algorithm \n 2.  \n 2.1  \n \n ProcessesChannel \n Channel(Channel); \n  ProcessChannel \n Send/Recv \n () \n \n 2.2 Channel \n \n  Snapshot Markermsg \n  \n \n 2.3 ChannelState \n \n Channel(Subsequence) \n Channel    -  \n \n \n The state of a channel is the sequence of messages sent along the channel, excluding the messages received along the channel \n \n \n Channel \n \n 2.4 ProcessState \n \n 2.5 Global State \n \n 2.6  -  \n \n \n  tokenp, q, cc\' tokentoken \n 3.  \n 3.1  \n 3.1.1  \n \n Process \n ChannelProcess() \n process(instant)**""** \n (SuperImpose) \n \n 3.1.2  \n Example 3.1  \n Channel() pctoken  \n \n token \n \n \n \n 3.2  \n 3.2.1  \n \n 3.2.2  \n \n  ProcessChan; \n  \n  \n \n \n \n \n \n 3.3 token \n 3.3.1  pqtokenpqMarkerq \n \n \n qMarkerpqp \n \n \n qMarkerpqpMarkerchanchanprecord(p recordchanq) \n \n \n ptokenqp(A) In-p; (B). In-chan; (C). In-q token \n \n 3.3.2 pqtoken, qp  \n \n qtokenqtokentoken; \n qtoken(in-chan)qtokenmarkerpqchanp marker qqchantokenin-chan \n \n \n qin-chan?  tokenqMarkerMaker \n  \n \n pq \n qMarkertokenq() \n  qMarkertoken chan  \n \n  \n Flink checkpoint \n https://zhuanlan.zhihu.com/p/96045864 \n Paxos \n  \n zk \n'},{title:"",frontmatter:{title:"",date:"2022-10-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["","bisdiff/bispatch"]},regularPath:"/%E5%85%B6%E4%BB%96/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95.html",relativePath:"/.md",key:"v-0ddfb2e2",path:"/2022/10/08/%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95/",headers:[{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"bsdiff",slug:"bsdiff"},{level:3,title:"bsdiff",slug:"bsdiff"},{level:3,title:"BSDiff",slug:"bsdiff"},{level:3,title:"BSPatch",slug:"bspatch"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:"-"},{level:2,title:"ZSTD",slug:"zstd"}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:'  \n   \n   \n  \n        bsdiffXdelta3 \n  Courgette \n CourgettebsdiffXdelta3 \n bsdiffXdelta3 \n bsdiffXdelta3 \n \n bsdiffPatchXdelta3Patch \n bsdiff \n  \n Linuxcmp OKtestDiffUpdate_OldtestDiffUpdate_New0 \n xiaoyzhang$  cat  testDiffUpdate_Old\n\n 123456789 \n\nxiaoyzhang$  cat  testDiffUpdate_New\n\n0123456789\n \n 1 2 3 4 5 6 7 CMPtestDiffUpdate_Delta \n xiaoyzhang$  cmp   -l  testDiffUpdate_New testDiffUpdate_Old  >  testDiffUpdate_Delta\n\ncmp: EOF on testDiffUpdate_Old\n\nxiaoyzhang$  cat  testDiffUpdate_Delta\n\n 1   60   61 \n\n 2   61   62 \n\n 3   62   63 \n\n 4   63   64 \n\n 5   64   65 \n\n 6   65   66 \n\n 7   66   67 \n\n 8   67   70 \n\n 9   70   71 \n\n 10   71   12 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  \n -rw-r--r--  1  xiaoyzhang  1085706827  **110**  12   23   12 :33 testDiffUpdate_Delta\n\n-rw-r--r--  1  xiaoyzhang  1085706827  **11**  12   23   12 :29 testDiffUpdate_New\n\n-rw-r--r--  1  xiaoyzhang  1085706827  **10**  12   23   12 :28 testDiffUpdate_Old\n \n 1 2 3 4 5 . \n BSDiffdiff stringpatch \n bsdiff \n --Colin PercivalBsDiff/BsPatch50%50% \n  BSDiff \n bsdiff \n oldnlogn,O(n)hash \n len,len,diff string,extra string. \n diff string+extra string \n **.** \n O(nlogn),O(n)noldBSDiff Faster suffix sortingbucketIVIold \n  \n  \n n * n * logn \n  \n ABCA \n 1.suffix[i] \n \n 2.sa[i] (),rank[i] ()sa \n \n O(N^2) \nNO(N * logN)O(1)O(N * logN)O(N * logN * N) \n \n diffqusort,n * lognSA-ISn, \n diff stringextra string \n 50% \n BSDiffpatch \n oldababanewabacd14327 \n 1.searchlen5528 \n 2.ababaabacddiff string0001101 \n 3.oldnew14327extra \n 4.blockbsdiff()streammainbzip2 \n Iscannewposoldlenlastscan=scan-lenb,lastpos=pos-lenblastoffset=scan-poslastoffsetnewold,oldAnewA+lastoffset=newAoldnewAoldscorelen,scscnewoldoldscsc+lastoffsetlenflenb \n \n \n \n  \n 1. \n   longest common sequencelongest common substring\n \n \n     {a,b,c,d,e,f,g,h} {a,c,e,f} b,d,g,h{a,h},{c,d,e}\n   {c,d,e,f} c,d,e,f{a,b,c,d},{g,h}\n LCS\n \n  \n \n \n {a,d} \n c[i][j]O(mn) \n suffix_search O(nlogn) \n ****patch \n diff string extrastring zippatch \n \n zippatchdiff stringsBSDiff \n  BSPatch \n patch \n 1patch \n 2patch \n 3new \n Ompatchm \n  \n BSDiffBSPatch \n BSDiff \n  O(nlogn)  O(n) \n BSPatch \n  O(n+m)   O(n+m) \n  \n bsdiff \n 1.divsufsort \n bsdiff \n \n 10~35MB \n 3 \n \n bsdiffChromiumbsdiffbsidffsufsortdivsufsortPatch \n \n \n 2. \n PatchBzip2Bzip2 \n bsdiffPatch01Bzip2bsdiff \n (tar.bz2zip)Bzip2 \n  \n RLE(Run-length encoding)Bzip2RLE \n Bzip2RLEPatch, Bzip2,  \n  \n 1.javajbsdiff \n < dependency > \n   < groupId > io.sigpipe </ groupId > \n   < artifactId > jbsdiff </ artifactId > \n   < version > 1.0 </ version > \n </ dependency > \n \n 1 2 3 4 5  \n #jar  \n java   -jar  jbsdiff.jar diff/patch  oldfile newfile patchfile\n #Apache Commons Compress bzip2gzpack200  xz \n java   -Djbsdiff.compressor = gz  -jar  jbsdiff.jar  diff  a.bin b.bin patch.gz\n \n 1 2 3 4 library \n //FileUI \n // \n FileUI . diff ( oldFile ,  newFile ,  patchFile ,  compression ) ; \n //diff(oldFile, newFile, patchFile, CompressorStreamFactory.BZIP2);//bzip2 \n // \n FileUI . patch ( oldFile ,  newFile ,  patchFile ) ; \n \n 1 2 3 4 5 6  \n \n \n \n library \n \n import   io . sigpipe . jbsdiff . InvalidHeaderException ; \n import   io . sigpipe . jbsdiff . ui . FileUI ; \n import   org . apache . commons . compress . compressors . CompressorException ; \n\n import   java . io . File ; \n import   java . io . IOException ; \n\n public   class   BsDiffTest   { \n\n     public   static   void   main ( String [ ]  args )   throws   IOException ,   InvalidHeaderException ,   CompressorException   { \n         File  oldFile  =   new   File ( "E:\\\\BigData\\\\learning\\\\bsdiff\\\\1.zip" ) ; \n         File  newFile  =   new   File ( "E:\\\\BigData\\\\learning\\\\bsdiff\\\\2.zip" ) ; \n         File  patchFile_bzip2  =   new   File ( "E:\\\\BigData\\\\learning\\\\bsdiff\\\\delta_bzip2" ) ; \n         File  patchFile_gz  =   new   File ( "E:\\\\BigData\\\\learning\\\\bsdiff\\\\delta_gz" ) ; \n         File  outFile = new   File ( "E:\\\\BigData\\\\learning\\\\bsdiff\\\\out.zip" ) ; \n         String  compression_bzip2 = "bzip2" ; \n         String  compression_gz = "gz" ; \n         FileUI . diff ( oldFile ,  newFile ,  patchFile_bzip2 ,  compression_bzip2 ) ; \n         FileUI . diff ( oldFile ,  newFile ,  patchFile_gz ,  compression_gz ) ; \n         FileUI . patch ( oldFile ,  outFile ,  patchFile_gz ) ; \n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2.JNIC \n windowmaclinuxbsdiff native Java \nlinuxbsdifflinuxsojava \n bsdiff bsdiff bzip2 \n\nbsdiffbzip2 \n\nbsdiff: http://www.daemonology.net/bsdiff/bsdiff-4.3.tar.gz(\nbzip2: https://www.sourceware.org/bzip2/downloads.html\n\n\n \n 1 2 3 4 5 6 7 8 #   \n       \n  \n class   PrefixSum   { \n   //  \n   private   int [ ]  prefix ; \n   //   \n   public   PrefixSum ( int [ ]  nums )   { \n    prefix  =   new   int [ nums . length  +   1 ] ; \n     //  nums  \n     for   ( int  i  =   1 ;  i  <  prefix . length ;  i ++ )   { \n      prefix [ i ]   =  prefix [ i  -   1 ]   +  nums [ i  -   1 ] ; \n     } \n   } \n   //  [i, j]   \n   public   int   query ( int  i ,   int  j )   { \n     return  prefix [ j  +   1 ]   -  prefix [ i ] ; \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n prefix[i]  nums[0..i-1]  nums[i..j]  prefix[j+1] - prefix[i]  \n        \n  nums  nums[2..6]  1 nums[3..9]  3 nums[0..4]  2 \n  nums  \n  nums[i..j]  val   for    O(N)  nums  \n  prefix  nums  diff  diff[i]  nums[i]  nums[i-1]   \n int [ ]  diff  =   new   int [ nums . length ] ; \n //  \ndiff [ 0 ]   =  nums [ 0 ] ; \n for   ( int  i  =   1 ;  i  <  nums . length ;  i ++ )   { \n    diff [ i ]   =  nums [ i ]   -  nums [ i  -   1 ] ; \n } \n \n 1 2 3 4 5 6 \n  diff  nums  \n int [ ]  res  =   new   int [ diff . length ] ; \n //  \nres [ 0 ]   =  diff [ 0 ] ; \n for   ( int  i  =   1 ;  i  <  diff . length ;  i ++ )   { \n    res [ i ]   =  res [ i  -   1 ]   +  diff [ i ] ; \n } \n \n 1 2 3 4 5 6  diff   nums[i..j]  3 diff[i] += 3  diff[j+1] -= 3  \n \n  diff  nums  diff[i] += 3  nums[i..]  3 diff[j+1] -= 3  nums[j+1..]  3 nums[i..j]  3   \n   O(1)   diff  nums  diff  diff  nums  \n  increment  result  \n //  \n class   Difference   { \n     //  \n     private   int [ ]  diff ; \n\n     /*  */ \n     public   Difference ( int [ ]  nums )   { \n         assert  nums . length  >   0 ; \n        diff  =   new   int [ nums . length ] ; \n         //  \n        diff [ 0 ]   =  nums [ 0 ] ; \n         for   ( int  i  =   1 ;  i  <  nums . length ;  i ++ )   { \n            diff [ i ]   =  nums [ i ]   -  nums [ i  -   1 ] ; \n         } \n     } \n\n     /*  [i,j]  val*/ \n     public   void   increment ( int  i ,   int  j ,   int  val )   { \n        diff [ i ]   +=  val ; \n         if   ( j  +   1   <  diff . length )   { \n            diff [ j  +   1 ]   -=  val ; \n         } \n     } \n\n     /*  */ \n     public   int [ ]   result ( )   { \n         int [ ]  res  =   new   int [ diff . length ] ; \n         //  \n        res [ 0 ]   =  diff [ 0 ] ; \n         for   ( int  i  =   1 ;  i  <  diff . length ;  i ++ )   { \n            res [ i ]   =  res [ i  -   1 ]   +  diff [ i ] ; \n         } \n         return  res ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  increment  if  \n public   void   increment ( int  i ,   int  j ,   int  val )   { \n    diff [ i ]   +=  val ; \n     if   ( j  +   1   <  diff . length )   { \n        diff [ j  +   1 ]   -=  val ; \n     } \n } \n \n 1 2 3 4 5 6  j+1 >= diff.length  nums[i]  diff  val  \n 1 \n \n int [ ]   corpFlightBookings ( int [ ] [ ]  bookings ,   int  n )   { \n     // nums  0 \n     int [ ]  nums  =   new   int [ n ] ; \n     //  \n     Difference  df  =   new   Difference ( nums ) ; \n\n     for   ( int [ ]  booking  :  bookings )   { \n         //  \n         int  i  =  booking [ 0 ]   -   1 ; \n         int  j  =  booking [ 1 ]   -   1 ; \n         int  val  =  booking [ 2 ] ; \n         //  nums[i..j]  val \n        df . increment ( i ,  j ,  val ) ; \n     } \n     //  \n     return  df . result ( ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 -- \n s1={1,3,4,5,6,7,7,8},s2={3,5,7,4,8,6,7,8,2}s1s2LCS \ns1s2 {3,4,6,7,8} \n LCS \n  \n \n \n  \n \n LCS = {3,4,6,7,8} \n \n LCS ={3,5,7,7,8} \n  \n     c[i][j]O(mn)1LCSO(m+n) \n n * logn*logn \n rank[] \n  \n rank[] \n (0) \n 0 \n 0 \n rank[] \n 4816 \n 1  s(aabaaaab) 1  s[i]-\'a\'+1 \n \n 2  2  \n  rank  i   i+1  2  2  \n \n 3  2^2  \n  rank  i  i+2  2^2  2^2  \n \n 4  2^3  \n  rank  i  i+4  2^3  2^3  \n \n  4  3   rank   \n 1324354056617782SA[]={3,4,5,0,6,1,7,2} \n public   abstract   class   SuffixArray   { \n\n   // Length of the suffix array \n   protected   final   int   N ; \n\n   // T is the text \n   protected   int [ ]   T ; \n\n   // The sorted suffix array values. \n   protected   int [ ]  sa ; \n\n   // Longest Common Prefix array \n   protected   int [ ]  lcp ; \n\n   private   boolean  constructedSa  =   false ; \n   private   boolean  constructedLcpArray  =   false ; \n\n   public   SuffixArray ( int [ ]  text )   { \n     if   ( text  ==   null )   throw   new   IllegalArgumentException ( "Text cannot be null." ) ; \n     this . T   =  text ; \n     this . N   =  text . length ; \n   } \n\n   public   int   getTextLength ( )   { \n     return   T . length ; \n   } \n\n   // Returns the suffix array. \n   public   int [ ]   getSa ( )   { \n     buildSuffixArray ( ) ; \n     return  sa ; \n   } \n\n   // Returns the LCP array. \n   public   int [ ]   getLcpArray ( )   { \n     buildLcpArray ( ) ; \n     return  lcp ; \n   } \n\n   // Builds the suffix array by calling the construct() method. \n   protected   void   buildSuffixArray ( )   { \n     if   ( constructedSa )   return ; \n     construct ( ) ; \n    constructedSa  =   true ; \n   } \n\n   // Builds the LCP array by first creating the SA and then running the kasai algorithm. \n   protected   void   buildLcpArray ( )   { \n     if   ( constructedLcpArray )   return ; \n     buildSuffixArray ( ) ; \n     kasai ( ) ; \n    constructedLcpArray  =   true ; \n   } \n\n   protected   static   int [ ]   toIntArray ( String  s )   { \n     if   ( s  ==   null )   return   null ; \n     int [ ]  t  =   new   int [ s . length ( ) ] ; \n     for   ( int  i  =   0 ;  i  <  s . length ( ) ;  i ++ )  t [ i ]   =  s . charAt ( i ) ; \n     return  t ; \n   } \n\n   // The suffix array construction algorithm is left undefined \n   // as there are multiple ways to do this. \n   protected   abstract   void   construct ( ) ; \n\n   // Use Kasai algorithm to build LCP array \n   // http://www.mi.fu-berlin.de/wiki/pub/ABI/RnaSeqP4/suffix-array.pdf \n   private   void   kasai ( )   { \n    lcp  =   new   int [ N ] ; \n     int [ ]  inv  =   new   int [ N ] ; \n     for   ( int  i  =   0 ;  i  <   N ;  i ++ )  inv [ sa [ i ] ]   =  i ; \n     for   ( int  i  =   0 ,  len  =   0 ;  i  <   N ;  i ++ )   { \n       if   ( inv [ i ]   >   0 )   { \n         int  k  =  sa [ inv [ i ]   -   1 ] ; \n         while   ( ( i  +  len  <   N )   &&   ( k  +  len  <   N )   &&   T [ i  +  len ]   ==   T [ k  +  len ] )  len ++ ; \n        lcp [ inv [ i ] ]   =  len ; \n         if   ( len  >   0 )  len -- ; \n       } \n     } \n   } \n\n } \n\n public   class   SuffixArrayMed   extends   SuffixArray   { \n\n   // Wrapper class to help sort suffix ranks \n   static   class   SuffixRankTuple   implements   Comparable < SuffixRankTuple >   { \n\n     int  firstHalf ,  secondHalf ,  originalIndex ; \n\n     // Sort Suffix ranks first on the first half then the second half \n     @Override \n     public   int   compareTo ( SuffixRankTuple  other )   { \n       int  cmp  =   Integer . compare ( firstHalf ,  other . firstHalf ) ; \n       if   ( cmp  ==   0 )   return   Integer . compare ( secondHalf ,  other . secondHalf ) ; \n       return  cmp ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n       return  originalIndex  +   " -> ("   +  firstHalf  +   ", "   +  secondHalf  +   ")" ; \n     } \n   } \n\n   public   SuffixArrayMed ( String  text )   { \n     super ( toIntArray ( text ) ) ; \n   } \n\n   public   SuffixArrayMed ( int [ ]  text )   { \n     super ( text ) ; \n   } \n\n   // Construct a suffix array in O(nlog^2(n)) \n   @Override \n   protected   void   construct ( )   { \n    sa  =   new   int [ N ] ; \n\n     // Maintain suffix ranks in both a matrix with two rows containing the \n     // current and last rank information as well as some sortable rank objects \n     int [ ] [ ]  suffixRanks  =   new   int [ 2 ] [ N ] ; \n     SuffixRankTuple [ ]  ranks  =   new   SuffixRankTuple [ N ] ; \n\n     // Assign a numerical value to each character in the text \n     for   ( int  i  =   0 ;  i  <   N ;  i ++ )   { \n      suffixRanks [ 0 ] [ i ]   =   T [ i ] ; \n      ranks [ i ]   =   new   SuffixRankTuple ( ) ; \n     } \n\n     // O(log(n)) \n     for   ( int  pos  =   1 ;  pos  <   N ;  pos  *=   2 )   { \n\n       for   ( int  i  =   0 ;  i  <   N ;  i ++ )   { \n         SuffixRankTuple  suffixRank  =  ranks [ i ] ; \n        suffixRank . firstHalf  =  suffixRanks [ 0 ] [ i ] ; \n        suffixRank . secondHalf  =  i  +  pos  <   N   ?  suffixRanks [ 0 ] [ i  +  pos ]   :   - 1 ; \n        suffixRank . originalIndex  =  i ; \n       } \n\n       // O(nlog(n)) \n       java . util . Arrays . sort ( ranks ) ; \n\n       int  newRank  =   0 ; \n      suffixRanks [ 1 ] [ ranks [ 0 ] . originalIndex ]   =   0 ; \n\n       for   ( int  i  =   1 ;  i  <   N ;  i ++ )   { \n\n         SuffixRankTuple  lastSuffixRank  =  ranks [ i  -   1 ] ; \n         SuffixRankTuple  currSuffixRank  =  ranks [ i ] ; \n\n         // If the first half differs from the second half \n         if   ( currSuffixRank . firstHalf  !=  lastSuffixRank . firstHalf\n             ||  currSuffixRank . secondHalf  !=  lastSuffixRank . secondHalf )  newRank ++ ; \n\n        suffixRanks [ 1 ] [ currSuffixRank . originalIndex ]   =  newRank ; \n       } \n\n       // Place top row (current row) to be the last row \n      suffixRanks [ 0 ]   =  suffixRanks [ 1 ] ; \n\n       // Optimization to stop early \n       if   ( newRank  ==   N   -   1 )   break ; \n     } \n\n     // Fill suffix array \n     for   ( int  i  =   0 ;  i  <   N ;  i ++ )   { \n      sa [ i ]   =  ranks [ i ] . originalIndex ; \n      ranks [ i ]   =   null ; \n     } \n\n     // Cleanup \n    suffixRanks [ 0 ]   =  suffixRanks [ 1 ]   =   null ; \n    suffixRanks  =   null ; \n    ranks  =   null ; \n   } \n\n   public   static   void   main ( String [ ]  args )   { \n\n     // String[] strs = { "AAGAAGC", "AGAAGT", "CGAAGC" }; \n     // String[] strs = { "abca", "bcad", "daca" }; \n     // String[] strs = { "abca", "bcad", "daca" }; \n     // String[] strs = { "AABC", "BCDC", "BCDE", "CDED" }; \n     // String[] strs = { "abcdefg", "bcdefgh", "cdefghi" }; \n     // String[] strs = { "xxx", "yyy", "zzz" }; \n     // TreeSet <String> lcss = SuffixArrayMed.lcs(strs, 2); \n     // System.out.println(lcss); \n\n     // SuffixArrayMed sa = new SuffixArrayMed("abracadabra"); \n     // System.out.println(sa); \n     // System.out.println(java.util.Arrays.toString(sa.sa)); \n     // System.out.println(java.util.Arrays.toString(sa.lcp)); \n\n     SuffixArrayMed  sa  =   new   SuffixArrayMed ( "ABBABAABAA" ) ; \n     // SuffixArrayMed sa = new SuffixArrayMed("GAGAGAGAGAGAG"); \n     System . out . println ( sa ) ; \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 DC3n--dc3 \n SA-ISn \n  \n (*$) \n Si<i+1S,Li+1 ) \n \n LMSLMS \n LL \n \n LMS \n \n S \n \n  \n LSM \n \n class   SAIS   { \n\t static   char  saisyo  =   \'!\' ; \n\n\t public   static   int [ ]   SAISENGINE ( String  s )   { \n\t\t int [ ]  str  =   new   int [ s . length ( ) ] ; \n\t\t for   ( int  i  =   0 ;  i  <  s . length ( ) ;  i ++ )   { \n\t\t\tstr [ i ]   =  s . charAt ( i )   -  saisyo  +   1 ; \n\t\t } \n\t\t int [ ]  result  =   SAIS ( str ,   127 ) ; \n\t\t return  result ; \n\t } \n\n\t public   static   int [ ]   SAIS ( int [ ]  s ,   int  spe )   { \n\t\t if   ( s . length  ==   1 )   { \n\t\t\t return   new   int [ ]   {   0   } ; \n\t\t }   else   { \n\t\t\t int  len  =  s . length  +   1 ;   // \n\t\t\t int [ ]  str  =   new   int [ len ] ; \n\t\t\t for   ( int  i  =   0 ;  i  <  s . length ;  i ++ )   { \n\t\t\t\tstr [ i ]   =  s [ i ] ;   // \n\t\t\t } \n\t\t\tstr [ str . length  -   1 ]   =   0 ; \n\t\t\t int [ ]   LS   =   new   int [ len ] ;   // L  S  L=1 S=0  \n\t\t\t int [ ]   LMS   =   new   int [ len ] ;   // LMS  LMS  -1  \n\t\t\t int [ ]   LMSsublen   =   new   int [ len ] ;   // LMS  \n\t\t\t int [ ]   LMSlist ; //  LMS  LMS  \n\t\t\t int   LMScount   =   0 ;   //LMS  LMS  \n\t\t\t int [ ]  used  =   new   int [ len ] ;   // \n\t\t\t Arrays . fill ( used ,   0 ) ;   //  1  0 \n\t\t\t SA [ ]  sa  =   new   SA [ spe ] ;   // SA  \n\t\t\t int [ ]  dict  =   new   int [ spe ] ; //  \n\t\t\t for   ( int  i  =   0 ;  i  <  len ;  i ++ )   { \n\t\t\t\tdict [ str [ i ] ] ++ ; \n\t\t\t } \n\t\t\t for   ( int  i  =   0 ;  i  <  spe ;  i ++ )   { \n\t\t\t\tsa [ i ]   =   new   SA ( dict [ i ] ) ; //  \n\t\t\t } \n\t\t\t Arrays . fill ( LMS ,   - 1 ) ; \n\t\t\t Arrays . fill ( LMSsublen ,   0 ) ; \n\t\t\t LS [ len  -   1 ]   =   0 ;   //LS \n\t\t\t for   ( int  i  =  len  -   2 ;  i  >=   0 ;  i -- )   {   // \n\t\t\t\t if   ( str [ i ]   ==  str [ i  +   1 ] )   { \n\t\t\t\t\t LS [ i ]   =   LS [ i  +   1 ] ;   // \n\t\t\t\t }   else   if   ( str [ i ]   >  str [ i  +   1 ] )   { \n\t\t\t\t\t LS [ i ]   =   1 ;   // L \n\t\t\t\t }   else   if   ( str [ i ]   <  str [ i  +   1 ] )   { \n\t\t\t\t\t LS [ i ]   =   0 ;   // S \n\t\t\t\t } \n\t\t\t } \n\t\t\t for   ( int  i  =   1 ;  i  <  len ;  i ++ )   {   // \n\t\t\t\t if   ( LS [ i ]   ==   0   &&   LS [ i  -   1 ]   ==   1 )   { \n\t\t\t\t\t LMS [ i ]   =   1 ;   // LMS \n\t\t\t\t\t LMScount ++ ;   // LMS  \n\t\t\t\t }   else   { \n\t\t\t\t\t LMS [ i ]   =   - 1 ;   // LMS-1  \n\t\t\t\t } \n\t\t\t } \n\t\t\t LMS [ 0 ]   =   - 1 ;   // LMS \n\t\t\t LMSlist   =   new   int [ LMScount ] ; //  LMS  \n\t\t\t int  tmpindex  =   0 ; \n\t\t\t for   ( int  i  =   0 ;  i  <  len ;  i ++ )   { \n\t\t\t\t if   ( LMS [ i ]   ==   1 )   { \n\t\t\t\t\t LMSlist [ tmpindex ]   =  i ;   // LMS  \n\t\t\t\t\ttmpindex ++ ; \n\t\t\t\t } \n\t\t\t } \n\t\t\t // Coco 5/13  1  \n\t\t\tsa  =   resetindex ( sa ) ; \n\t\t\t for   ( int  i  =   0 ;  i  <   LMSlist . length ;  i ++ )   {   // LMS  \n\t\t\t\t if   ( LMSlist [ i ]   !=  len  -   1 )   { \n\t\t\t\t\t LMSsublen [ LMSlist [ i ] ]   =   LMSlist [ i  +   1 ]   -   LMSlist [ i ] ;   //LMS  LMS  LMS  \n\t\t\t\t }   else   { \n\t\t\t\t\t LMSsublen [ LMSlist [ i ] ]   =   1 ; //  LMS  1 \n\t\t\t\t } \n\t\t\t } \n\t\t\tsa  =   resetindex ( sa ) ; //4  \n\t\t\t for   ( int  i  =   0 ;  i  <   LMSlist . length ;  i ++ )   {   // 1  \n\t\t\t\t int  index  =   LMSlist [ i ] ; \n\t\t\t\tsa [ str [ index ] ] . addusirokara ( index ) ;   // =  \n\t\t\t\tused [ index ]   =   1 ;   // \n\t\t\t }   // 1  \n\t\t\tsa  =   resetindex ( sa ) ;   // \n\n\t\t\t for   ( int  i  =   0 ;  i  <  sa . length ;  i ++ )   {   // 2  L  \n\t\t\t\t for   ( int  j  =   0 ;  j  <  sa [ i ] . size ( ) ;  j ++ )   { \n\t\t\t\t\t int  value  =  sa [ i ] . get ( j )   -   1 ; \n\t\t\t\t\t if   ( value  >=   0   &&  used [ value ]   ==   0   &&   LS [ value ]   ==   1 )   { \n\t\t\t\t\t\tused [ value ]   =   1 ; \n\t\t\t\t\t\tsa [ str [ value ] ] . addmaekara ( value ) ;   // \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t } \n\t\t\tsa  =   resetindex ( sa ) ; //3  \n\t\t\t for   ( int  i  =   1 ;  i  <  sa . length ;  i ++ )   {   //3  \n\t\t\t\t for   ( int  j  =   0 ;  j  <  sa [ i ] . size ( ) ;  j ++ )   { \n\t\t\t\t\t int  value  =  sa [ i ] . get ( j ) ; \n\t\t\t\t\t if   ( value  >=   0   &&   LMS [ value ]   ==   1 )   { \n\t\t\t\t\t\tsa [ i ] . set ( j ,   - 1 ) ;   // LMS \n\t\t\t\t\t\tused [ value ]   =   0 ;   //LMS \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t }   //3  \n\t\t\tsa  =   resetindex ( sa ) ; //4  \n\t\t\t for   ( int  i  =  sa . length  -   1 ;  i  >=   0 ;  i -- )   { // 4  \n\t\t\t\t for   ( int  j  =  sa [ i ] . size ( )   -   1 ;  j  >=   0 ;  j -- )   { \n\t\t\t\t\t if   ( sa [ i ] . get ( j )   !=   - 1 )   { \n\t\t\t\t\t\t int  index  =  sa [ i ] . get ( j )   -   1 ; \n\t\t\t\t\t\t if   ( index  >=   0   &&  used [ index ]   ==   0   &&   LS [ index ]   ==   0 )   { \n\t\t\t\t\t\t\tsa [ str [ index ] ] . addusirokara ( index ) ;   // \n\t\t\t\t\t\t\tused [ index ]   =   1 ; \n\t\t\t\t\t\t } \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t } \n\t\t\tsa  =   resetindex ( sa ) ; //4  \n\t\t\t int  counter  =   0 ; \n\t\t\t int [ ]  nowsa  =   new   int [ 1 ] ; //  LMS  \n\t\t\t int [ ]  oldsa  =   new   int [ 1 ] ;   // LMS  \n\t\t\t Arrays . fill ( oldsa ,   - 1 ) ; \n\t\t\t int [ ]   LMScounter   =   new   int [ len ] ; \n\t\t\t for   ( int  i  =   0 ;  i  <  sa . length ;  i ++ )   {   // \n\t\t\t\t for   ( int  j  =   0 ;  j  <  sa [ i ] . size ( ) ;  j ++ )   { \n\t\t\t\t\t if   ( LMS [ sa [ i ] . get ( j ) ]   !=   - 1 )   { //  LMS LMS  \n\t\t\t\t\t\tcounter ++ ; \n\t\t\t\t\t\t LMScounter [ sa [ i ] . get ( j ) ]   =  counter ; \n\t\t\t\t\t\tnowsa  =   new   int [ LMSsublen [ sa [ i ] . get ( j ) ]   +   1 ] ; // LMS  LMS  \n\t\t\t\t\t\t Arrays . fill ( nowsa ,   - 1 ) ; \n\t\t\t\t\t\ttmpindex  =   0 ;   //LMS  \n\t\t\t\t\t\t for   ( int  k  =  sa [ i ] . get ( j ) ;  k  <  len ;  k ++ )   { \n\t\t\t\t\t\t\tnowsa [ tmpindex ]   =  str [ k ] ; \n\t\t\t\t\t\t\ttmpindex ++ ; \n\t\t\t\t\t\t\t if   ( k  !=  sa [ i ] . get ( j )   &&   LMS [ k ]   !=   - 1 )   { \n\t\t\t\t\t\t\t\t break ;   // LMS LMS  \n\t\t\t\t\t\t\t } \n\t\t\t\t\t\t } \n\t\t\t\t\t\t if   ( equals ( oldsa ,  nowsa ) /* equals(oldsa, nowsa) */ )   {   // LMS  \n\t\t\t\t\t\t\tcounter -- ; \n\t\t\t\t\t\t\t LMScounter [ sa [ i ] . get ( j ) ]   =  counter ; \n\t\t\t\t\t\t } \n\t\t\t\t\t\toldsa  =   clone ( nowsa ) ; \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t } \n\t\t\t int [ ]  new_str  =   new   int [ LMSlist . length ] ; \n\t\t\t int [ ]   LMSindex   =   new   int [ LMSlist . length  +   1 ] ; \n\n\t\t\t for   ( int  i  =   0 ;  i  <   LMSlist . length ;  i ++ )   {   // SAIS  LMS  \n\t\t\t\tnew_str [ i ]   =   LMScounter [ LMSlist [ i ] ] ; \n\t\t\t\t LMSindex [ i ]   =   LMSlist [ i ] ; \n\t\t\t } \n\t\t\t int [ ]  newLMSindex  =   SAIS ( new_str ,  counter  +   1 ) ; \n\n\t\t\tsa  =   resetindex ( sa ) ; \n\t\t\t Arrays . fill ( used ,   0 ) ; //  \n\t\t\tsa  =   clear ( sa ) ; //  LMS  \n\n\t\t\t for   ( int  i  =  newLMSindex . length  -   1 ;  i  >=   0 ;  i -- )   {   // 1 \n\t\t\t\t int  value  =   LMSindex [ newLMSindex [ i ] ] ; \n\t\t\t\tsa [ str [ value ] ] . addusirokara ( value ) ; \n\t\t\t\tused [ value ]   =   1 ; \n\t\t\t } \n\t\t\tsa  =   resetindex ( sa ) ; \n\t\t\t //2   3  4  \n\t\t\t for   ( int  i  =   0 ;  i  <  sa . length ;  i ++ )   { \n\t\t\t\t for   ( int  j  =   0 ;  j  <  sa [ i ] . size ( ) ;  j ++ )   { \n\t\t\t\t\t int  value  =  sa [ i ] . get ( j )   -   1 ; \n\t\t\t\t\t if   ( value  >=   0   &&  used [ value ]   ==   0   &&   LS [ value ]   ==   1 )   { \n\t\t\t\t\t\tused [ value ]   =   1 ; \n\t\t\t\t\t\tsa [ str [ value ] ] . addmaekara ( value ) ; \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t } \n\t\t\tsa  =   resetindex ( sa ) ; //2  \n\t\t\t for   ( int  i  =   1 ;  i  <  sa . length ;  i ++ )   { // 2.5  \n\t\t\t\t for   ( int  j  =   0 ;  j  <  sa [ i ] . size ( ) ;  j ++ )   { \n\t\t\t\t\t int  value  =  sa [ i ] . get ( j ) ; \n\t\t\t\t\t if   ( value  >=   0   &&   LMS [ value ]   >=   1 )   { \n\t\t\t\t\t\tsa [ i ] . set ( j ,   - 1 ) ;   // \n\t\t\t\t\t\tused [ value ]   =   0 ; \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t }   //2.5  \n\t\t\t //3  \n\t\t\t for   ( int  i  =  sa . length  -   1 ;  i  >=   0 ;  i -- )   { \n\t\t\t\t for   ( int  j  =  sa [ i ] . size ( )   -   1 ;  j  >=   0 ;  j -- )   { \n\t\t\t\t\t if   ( sa [ i ] . get ( j )   !=   - 1 )   { \n\t\t\t\t\t\t int  index  =  sa [ i ] . get ( j )   -   1 ; \n\t\t\t\t\t\t if   ( index  >=   0   &&  used [ index ]   ==   0   &&   LS [ index ]   ==   0 )   { \n\t\t\t\t\t\t\tsa [ str [ index ] ] . addusirokara ( index ) ; \n\t\t\t\t\t\t\tused [ index ]   =   1 ; \n\t\t\t\t\t\t } \n\t\t\t\t\t } \n\t\t\t\t } \n\t\t\t } \n\t\t\tsa  =   resetindex ( sa ) ; //3  \n\t\t\t int [ ]  ret  =   convert ( sa ,  len  -   1 ) ;   // \n\t\t\t return  ret ; \n\t\t } \n\t } \n\n\t static   boolean   equals ( int [ ]  a1 ,   int [ ]  a2 )   { \n\t\t if   ( a1 . length  !=  a2 . length )   { \n\t\t\t return   false ; \n\t\t }   else   { \n\t\t\t for   ( int  i  =   0 ;  i  <  a1 . length ;  i ++ )   { \n\t\t\t\t if   ( a1 [ i ]   !=  a2 [ i ] )   { \n\t\t\t\t\t return   false ; \n\t\t\t\t } \n\t\t\t } \n\t\t\t return   true ; \n\t\t } \n\t } \n\n\t static   SA [ ]   resetindex ( SA [ ]  a )   { \n\t\t for   ( int  i  =   0 ;  i  <  a . length ;  i ++ )   { \n\t\t\ta [ i ] . resetindex ( ) ; \n\t\t } \n\t\t return  a ; \n\t } \n\n\t static   SA [ ]   clear ( SA [ ]  a )   { \n\t\t for   ( int  i  =   0 ;  i  <  a . length ;  i ++ )   { \n\t\t\ta [ i ] . reset ( ) ; \n\t\t } \n\t\t return  a ; \n\t } \n\n\t static   class   SA   { \n\t\t private   int [ ]  ary ; \n\t\t private   int  mae ,  ato ; \n\n\t\t SA ( int  volume )   { \n\t\t\tary  =   new   int [ volume ] ; \n\t\t\t Arrays . fill ( ary ,   - 1 ) ; \n\t\t\tmae  =   0 ; \n\t\t\tato  =  ary . length  -   1 ; \n\t\t } \n\n\t\t void   addmaekara ( int  value )   { \n\t\t\t if   ( mae  <  ary . length )   { \n\t\t\t\tary [ mae ]   =  value ; \n\t\t\t\tmae ++ ; \n\t\t\t } \n\t\t } \n\n\t\t void   addusirokara ( int  value )   { \n\t\t\t if   ( ato  >=   0 )   { \n\t\t\t\tary [ ato ]   =  value ; \n\t\t\t\tato -- ; \n\t\t\t } \n\t\t } \n\n\t\t void   resetindex ( )   { \n\t\t\tmae  =   0 ; \n\t\t\tato  =  ary . length  -   1 ; \n\t\t } \n\n\t\t void   set ( int  index ,   int  element )   { \n\t\t\tary [ index ]   =  element ; \n\t\t } \n\n\t\t void   reset ( )   { \n\t\t\t Arrays . fill ( ary ,   - 1 ) ; \n\t\t\t resetindex ( ) ; \n\t\t } \n\n\t\t int   get ( int  index )   { \n\t\t\t return  ary [ index ] ; \n\t\t } \n\n\t\t int   size ( )   { \n\t\t\t return  ary . length ; \n\t\t } \n\t } \n\n\t static   int [ ]   convert ( SA [ ]  s ,   int  len )   { \n\t\t int [ ]  ret  =   new   int [ len ] ; \n\t\t int  counter  =   0 ; \n\t\t for   ( int  i  =   1 ;  i  <  s . length ;  i ++ )   { \n\t\t\t for   ( int  j  =   0 ;  j  <  s [ i ] . size ( ) ;  j ++ )   { \n\t\t\t\tret [ counter ]   =  s [ i ] . get ( j ) ; \n\t\t\t\tcounter ++ ; \n\t\t\t } \n\t\t } \n\t\t return  ret ; \n\t } \n\n\t public   static   int [ ]   clone ( int [ ]  a )   { \n\t\t int [ ]  ret  =   new   int [ a . length ] ; \n\t\t for   ( int  i  =   0 ;  i  <  a . length ;  i ++ )   { \n\t\t\tret [ i ]   =  a [ i ] ; \n\t\t } \n\t\t return  ret ; \n\t } \n\n\t public   static   void   main ( String [ ]  args )   { \n\t\t String  s  =   "aabaaaab" ; \n\t\t int [ ]  res  =   SAISENGINE ( s ) ; \n\t\t for   ( int  i  =   0 ;  i  <  res . length ;  i ++ )   { \n\t\t\t System . out . println ( res [ i ] ) ; \n\t\t } \n\t } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 #  ZSTD \n  \n'},{title:"",frontmatter:{title:"",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:[""]},regularPath:"/%E5%85%B6%E4%BB%96/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F.html",relativePath:"/.md",key:"v-41ccc7e2",path:"/2023/06/10/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E5%BC%8F/",headers:[{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"1.Java",slug:"_1-java"},{level:2,title:"2.Hessian ",slug:"_2-hessian-"},{level:2,title:"3.Json",slug:"_3-json"},{level:2,title:"4.ProtoBuf",slug:"_4-protobuf"},{level:2,title:"5.Kryo",slug:"_5-kryo"}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:'  \n  \n  \n  \n \n  \n  \n 1.Java \n 5JavaJsonkryoprotobufHessian \n JavaSerializable \n public   class   TestBean   implements   Serializable   { \n \n     private   Integer  id ; \n \n     private   String  name ; \n \n     private   Date  date ; \n  //gettersettertoString \n } \n \n 1 2 3 4 5 6 7 8 9 Java  \n Serializable serialVersionUID  serialVersionUID  \n Transient \n Transient transient  int  0null \n serial VersionUID \n Serializable serialVersionUID  \n import   java . io . Serializable ; \n \n public   class   Person   implements   Serializable { \n    private   static   final   long  serialVersionUID  =   1234567890L ; \n    private   int  id ; \n    private   String  name ; \n    public   Person ( int  id ,   String  name ) { \n        this . id  =  id ; \n        this . name  =  name ; \n    } \n \n    public   String   toString ( ) { \n        return   "Person: "   +  id  +   " "   +  name ; \n    } \n \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #  2.Hessian  \n Hessian  \n Hessian  \n \n  \n  \n Java \n hessian1hessian2Java50%Java30%Java20% \n \n HessianmavenHessian \n < dependency > \n   < groupId > com.caucho </ groupId > \n   < artifactId > hessian </ artifactId > \n   < version > 4.0.62 </ version > \n </ dependency > \n \n 1 2 3 4 5 jdkhessianSerializable \n  \n \n public   class   Test   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         ByteArrayOutputStream  os  =   new   ByteArrayOutputStream ( ) ; \n         Hessian2Output  output  =   new   Hessian2Output ( os ) ; \n        output . writeObject ( Person . hehe ( 123L ,   "wangyong" ) ) ; \n        output . close ( ) ; \n \n         ByteArrayInputStream  in  =   new   ByteArrayInputStream ( os . toByteArray ( ) ) ; \n         Hessian2Input  input  =   new   Hessian2Input ( in ) ; \n         System . out . println ( input . readObject ( ) ) ; \n     } \n } \n\n class   Person   implements   Serializable   { \n     private   Long  id ; \n     private   String  name ; \n     private   Person ( long  id ,   String  name )   { \n         this . id  =  id ; \n         this . name  =  name ; \n         System . out . println ( "call dd" ) ; \n     } \n \n     public   static   Person   hehe ( Long  id ,   String  name )   { \n         Person  p  =   new   Person ( id ,  name ) ; \n         return  p ; \n     } \n \n     @Override \n     public   String   toString ( )   { \n         return   "id="   +  id  +   ", name="   +  name ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 http://hessian.caucho.com/doc/hessian-serialization.html \n 3.Json \n Json web JSON   \n JSON  JSON  JSON   \n Json \n \n  \n  \n  \n  \n \n  \n { \n "employees" :   [ \n {   "firstName" : "Mike"   ,   "lastName" : "Chen"   } , \n {   "firstName" : "Anna"   ,   "lastName" : "Smith"   } , \n {   "firstName" : "Peter"   ,   "lastName" : "Jones"   } \n ] \n } \n \n 1 2 3 4 5 6 7 #  4.ProtoBuf \n  \n Protobuf \n  Protobuf     pojo  \n Protobuf \n Protobufprotoprotobuf \n  \n syntax   =   "proto3" ; \n \n message   SendRequest   { \n   string  query  =   1 ; \n   int32  page_number  =   2 ; \n   repeated   int32  result_per_page  =   3 ; \n } \n \n 1 2 3 4 5 6 7 \n .protoproto3proto3protocol bufferproto2 \n message \n SendRequest(name/value) \n repeated \n 5.Kryo \n Kryo  Java  Java Kryo  \n Kryo  \n \n   \n   \n API \n \n  JVM   Kryo   \n Kryo \n \n Apache Hive \n Apache Spark \n Twitters Chill \n Storm \n akka-kryo-serialization \n \n Kryo \n static   void   quickStart ( )   throws   FileNotFoundException   { \n     Kryo  kryo  =   new   Kryo ( ) ; \n     Output  output  =   new   Output ( new   FileOutputStream ( "file.bin" ) ) ; \n     SomeClass  someObject  =   new   SomeClass ( ) ; \n    someObject . setValue ( "this is someObject." ) ; \n    kryo . writeObject ( output ,  someObject ) ; \n    output . close ( ) ; \n \n     Input  input  =   new   Input ( new   FileInputStream ( "file.bin" ) ) ; \n     SomeClass  deSomeObject  =  kryo . readObject ( input ,   SomeClass . class ) ; \n    input . close ( ) ; \n    \n     Assert . assertEquals ( someObject . getValue ( ) ,  deSomeObject . getValue ( ) ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 '},{title:"",frontmatter:{title:"",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["",""]},regularPath:"/%E5%85%B6%E4%BB%96/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98.html",relativePath:"/.md",key:"v-29f87cb0",path:"/2023/06/10/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98/",headers:[{level:2,title:"",slug:""},{level:3,title:"JVM",slug:"jvm"},{level:2,title:"Guava Cache",slug:"guava-cache"},{level:3,title:"",slug:""},{level:3,title:"",slug:"--"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"Caffeine Cache",slug:"caffeine-cache"},{level:3,title:"",slug:""},{level:3,title:" ",slug:"-"},{level:3,title:"",slug:""},{level:3,title:"Coding",slug:"coding"}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:'  \n JVM \n JVM ListSetMap \n \n \n   LRULFUFIFO  \n \n  CurrentHashMap \n  IO  Guava  Caffeine \n Guava Cache \n guava cachegoogleConcurrentHashMapsegments     \n guava cache  com.google.common.cache  CacheBuilder  Cache  \n  \n \x3c!--guava  --\x3e \n         < dependency > \n             < groupId > com.google.guava </ groupId > \n             < artifactId > guava </ artifactId > \n             < version > 19.0 </ version > \n         </ dependency > \n \n 1 2 3 4 5 6 #  1CacheLoader \n LoadingCache < Key ,   Graph >  graphs  =   CacheBuilder . newBuilder ( ) \n        . maximumSize ( 1000 ) \n        . build ( \n            new   CacheLoader < Key ,   Graph > ( )   { \n              public   Graph   load ( Key  key )   throws   AnyException   { \n                return   createExpensiveGraph ( key ) ; \n              } \n            } ) ; \n . . . \n try   { \n   return  graphs . get ( key ) ; \n }   catch   ( ExecutionException  e )   { \n   throw   new   OtherException ( e . getCause ( ) ) ; \n } \n \n . . . \n // \n return  graphs . getUnchecked ( key ) ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 cacheget(K)loadload \n 1get(K)getUnchecked(K) \n CacheLoaderLoadingCache.getKExecutionExceptiongetUncheckedK \n 2getload \n getAll(Iterable<? extendsK>)getAllCacheLoader.loadCacheLoader.loadAllgetAll(Iterable) \n public   LoadingCache < String ,   String >  caches  =   CacheBuilder \n\t . newBuilder ( ) . maximumSize ( 100 ) \n\t . expireAfterWrite ( 100 ,   TimeUnit . SECONDS )   //  \n\t . build ( new   CacheLoader < String ,   String > ( )   { \n\t\t @Override \n\t\t public   String   load ( final   String  key )   { \n\t\t\t return   getSchema ( key ) ; \n\t\t } \n\t\t\t\t\n\t\t @Override \n\t\t public   Map < String , String >   loadAll ( final   Iterable < ?   extends   String >  keys )   throws   Exception   { \n             //com.google.common.collect.Lists \n\t\t\t ArrayList < String >  keysList  =   Lists . newArrayList ( keys ) ; \n\t\t\t return   getSchemas ( keysList ) ; \n\t\t } \n } ) ; \n \n private   static   Map < String , String >   getSchemas ( List < String >  keys )   { \n\t\t Map < String , String >  map  =   new   HashMap < > ( ) ; \n\t\t\n\t\t //... \n\t\t System . out . println ( "loadall..." ) ; \n\t\t return  map ; \n } \n \n List < String >  keys  =   new   ArrayList < > ( ) ; \nkeys . add ( "key2" ) ; \nkeys . add ( "key3" ) ; \n \n try   { \n\tcaches . getAll ( keys ) ; \n }   catch   ( ExecutionException  e1 )   { \n\te1 . printStackTrace ( ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 expireAfterWriteguavaloadloadAll \n  2callable \n LoadingCache < String ,   String >  cache  =   CacheBuilder \n\t . newBuilder ( ) . maximumSize ( 100 ) \n\t . expireAfterWrite ( 100 ,   TimeUnit . SECONDS )   //  \n\t . build ( new   CacheLoader < String ,   String > ( )   { \n\t\t @Override \n\t\t public   String   load ( String  key )   { \n\t\t\t return   getSchema ( key ) ; \n\t\t } \n } ) ; \n \n private   static   String   getSchema ( String  key )   { \n     System . out . println ( "load..." ) ; \n     return  key + "schema" ; \n } \n \n try   { \n\t String  value  =  cache . get ( "key4" ,   new   Callable < String > ( )   { \n\t\t\t @Override \n\t\t\t public   String   call ( )   throws   Exception   { \n\t\t\t\t System . out . println ( "i am callable..." ) ; \n\t\t\t\t return   "i am callable..." ; \n\t\t\t } \n\t   } ) ; \n\t System . out . println ( value ) ; \n }   catch   ( ExecutionException  e1 )   { \n\te1 . printStackTrace ( ) ; \n } \n //i am callable...  \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  \n Cache < String ,   String >  cache2  =   CacheBuilder . newBuilder ( ) \n\t\t . maximumSize ( 1000 ) \n\t\t . expireAfterWrite ( 100 ,   TimeUnit . SECONDS )   //  \n\t\t . build ( ) ;   // look Ma, no CacheLoader \n \n try   { \n\t String  value  =  cache2 . get ( "key4" ,   new   Callable < String > ( )   { \n\t\t\t @Override \n\t\t\t public   String   call ( )   throws   Exception   { \n\t\t\t\t System . out . println ( "i am callable..." ) ; \n\t\t\t\t return   "i am callable..." ; \n\t\t\t } \n\t   } ) ; \n\t System . out . println ( value ) ; \n }   catch   ( ExecutionException  e1 )   { \n\te1 . printStackTrace ( ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 callable get-if-absent-computeLoadingCacheCachecallable \n 1CacheCallablegetKCallable \n 2LoadingCachegetKgetKCallablegetKCallableKCallableload \n 3 \nput(K,V) get-if-absent-compute \n  \n guavaguava \n 1size \n CacheBuilder.maximumSize(long)  Guava  LRU  CacheBuilder.weigher(Weigher)Guava Cache \n LoadingCache < Key ,   Graph >  graphs  =   CacheBuilder . newBuilder ( )   \n\t . maximumWeight ( 100000 ) \n\t . weigher ( new   Weigher < Key ,   Graph > ( )   { \n\t\t public   int   weigh ( Key  k ,   Graph  g )   { \n\t\t\t return  g . vertices ( ) . size ( ) ; \n\t\t } \n\t } ) \n\t . build ( new   CacheLoader < Key ,   Graph > ( )   { \n\t\t\t public   Graph   load ( Key  key )   {   // no checked exception \n\t\t\t\t return   createExpensiveGraph ( key ) ; \n\t\t\t } \n\t } ) ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 #  2 \n guava Cache \n expireAfterAccess(long, TimeUnit)Key/Keyload()sizeLRU \nexpireAfterWrite(long, TimeUnit)KeyKeyload() \nGuava Cache/loadcache \n 3 \n Guava Cache \n CacheBuilder.weakKeys() \nCacheBuilder.weakValues() \nCacheBuilder.softValues() \nguavacache \n  \n 1Key \nKeyguava \n KeyCache.invalidate(key) \nCache.invalidateAll(keys) \nCache.invalidateAll() \n2 \nCacheBuilder.removalListener(RemovalListener)RemovalListenerRemovalNotificationRemovalCause \n  \n 1 \nCacheBuilder \n CacheCacheBuilder \n Cache.cleanUp \n cacheguavaguava \n 2 \n1 \n keyCacheLoaderreloadloadKeyloadKey \n 2 \n LoadingCache.refresh(K) Key  LoadingCacherefresh \n public   static   ThreadPoolExecutor  threadPool  =   new   ThreadPoolExecutor ( 5 ,   50 ,   300 ,   TimeUnit . SECONDS ,  \n\t new   ArrayBlockingQueue < Runnable > ( 50 ) ,   \n\t new   ThreadFactory ( ) {   public   Thread   newThread ( Runnable  r )   { \n\t\t return   new   Thread ( r ,   "pool_"   +  r . hashCode ( ) ) ; \n\t } } ,   new   ThreadPoolExecutor . DiscardOldestPolicy ( ) ) ; \n \n \n public   static   LoadingCache < String ,   String >  cache  =   CacheBuilder \n\t . newBuilder ( ) . maximumSize ( 100 ) \n\t . expireAfterWrite ( 100 ,   TimeUnit . SECONDS )   //  \n\t . build ( new   CacheLoader < String ,   String > ( )   { \n\t\t @Override \n\t\t public   String   load ( String  key )   { \n\t\t\t return   getSchema ( key ) ; \n\t\t } \n\t\t\n\t\t public   ListenableFuture < String >   reload ( String  key ,   String  oldValue )   throws   Exception   { \n\t\t\t ListenableFutureTask < String >  task  =    ListenableFutureTask . create ( new   Callable < String > ( )   { \n\t\t\t\t @Override \n\t\t\t\t public   String   call ( )   throws   Exception   { \n\t\t\t\t\t Thread . sleep ( 1000 ) ; \n\t\t\t\t\t System . out . println ( "async...." ) ; \n\t\t\t\t\t return   getSchema ( key ) ; \n\t\t\t\t } \n\t\t\t } ) ; \n\t\t\tthreadPool . submit ( task ) ; \n\t\t\t return  task ; \n\t\t } \n\t } ) ; \n \n // \ncache . refresh ( key1 ) ; //void \n System . out . println ( "after refresh" ) ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  \n after refresh \n async... \n refreshreloadreloadload \n \n 3+ \n  \n 1 \n expireAfterWriteexpireAfterAccessget \n Guava cachekeyload \n 2 \n guavaGuavaloadkey \n Guava cacherefreshAfterWrite \n LoadingCache < String ,   Object >  caches  =   CacheBuilder . newBuilder ( ) \n                 . maximumSize ( 100 ) \n                 . refreshAfterWrite ( 10 ,   TimeUnit . MINUTES ) \n                 . build ( new   CacheLoader < String ,   Object > ( )   { \n                     @Override \n                     public   Object   load ( String  key )   throws   Exception   { \n                         return   generateValueByKey ( key ) ; \n                     } \n                 } ) ; \n try   { \n     System . out . println ( caches . get ( "key-zorro" ) ) ; \n }   catch   ( ExecutionException  e )   { \n    e . printStackTrace ( ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Guava cache \n 3 \n keykeykey \n ListeningExecutorService  backgroundRefreshPools  =  \n\t\t\t\t MoreExecutors . listeningDecorator ( Executors . newFixedThreadPool ( 20 ) ) ; \n         LoadingCache < String ,   Object >  caches  =   CacheBuilder . newBuilder ( ) \n                 . maximumSize ( 100 ) \n                 . refreshAfterWrite ( 10 ,   TimeUnit . MINUTES ) \n                 . build ( new   CacheLoader < String ,   Object > ( )   { \n                     @Override \n                     public   Object   load ( String  key )   throws   Exception   { \n                         return   generateValueByKey ( key ) ; \n                     } \n                    \n                     @Override \n                     public   ListenableFuture < Object >   reload ( String  key , \n                    \t\t Object  oldValue )   throws   Exception   { \n                    \t return  backgroundRefreshPools . submit ( new   Callable < Object > ( )   { \n \n\t\t\t\t\t\t\t @Override \n\t\t\t\t\t\t\t public   Object   call ( )   throws   Exception   { \n\t\t\t\t\t\t\t\t return   generateValueByKey ( key ) ; \n\t\t\t\t\t\t\t } \n\t\t\t\t\t\t } ) ; \n                     } \n                 } ) ; \n try   { \n     System . out . println ( caches . get ( "key-zorro" ) ) ; \n }   catch   ( ExecutionException  e )   { \n    e . printStackTrace ( ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 CacheLoaderreload \n CacheBuilder.refreshAfterWrite(long, TimeUnit)expireAfterWrite(long, TimeUnit)load() \n CacheBuilder.refreshAfterWrite(long, TimeUnit)expireAfterWrite(long, TimeUnit)  \n  \n 1. \n lru \n 2. \n expireAfterWrite(long, TimeUnit) \n 3.load() \n CacheBuilder.refreshAfterWrite(long, TimeUnit) \n refresh \n Caffeine Cache \n Caffeine  Google Guava Cache  Guava  Guava Plus \n guava caffeine \n  \n Guava  put  \n Caffeine  Guava Caffeine  Disruptor RingBuffer ForkJoinPool.commonPool() \n  \n 18  \n \n 262 \n \n 38  \n   \n ,Guava  S-LRU Caffeine  LRULFU  W-TinyLFU \n 2.1 LRU \n Least Recently Used**** \n    \n 2.2 LFU \n Least Frequently Used \n  \n   LRU  LFU  \n LFUXXXXXX    XXX   \n LFU    \n 2.3 TinyLFU \n TinyLFU LFU LFU  \n TinyLFU    LFU    New ItemCache Victim  \n \n tiny-lfu-arch \n   TinyLFU  Count-Min Sketch      Count-Min Sketch pproximating Data with the Count-Min Data Structure \n TinyLFU  resetSketch1WSketch2 reset   \n    \n 2.4 W-TinyLFU \n W-TinyLFU  Caffeine  \n W-TinyLFU  TinyLFUTinyLFUW-TinyLFU   Window Cache   TinLFU  Main CacheLRU--\x3eLFU \n  \n  \n   maxSizerefreshAfterWrite expireAfterWrite \n  get  refreshAfterWrite  \n   \n \n * *  \n  \n   maxSizeexpireAfterWrite refreshAfterWrite \n  get  expireAfterWrite  key load \n   \n \n * * *  redis  \n  \n   maxSizerefreshAfterWriteexpireAfterWriterefreshAfterWrite < expireAfterWrite \n   \n \n get  refreshAfterWrite  expireAfterWrite * get  expireAfterWrite key load \n \n   \n \n * * *  redis  \n \n \n caffeineguava \n Caffeine  Guava API Guava  Caffeine CacheBuilder.newBuilder() Caffeine.newBuilder()  \n < dependency > \n     < groupId > com.github.ben-manes.caffeine </ groupId > \n     < artifactId > caffeine </ artifactId > \n </ dependency > \n \n 1 2 3 4  Guava  get() load() null  ExecutionException Caffeine get() null \n Coding \n Caffeine \n 1. \n 1. \n get keykey \n /**\n     * \n     * @param key\n     * @return\n     */\npublic Object manulOperator(String key) {\n    Cache&lt;String, Object> cache = Caffeine.newBuilder()\n        .expireAfterWrite(1, TimeUnit.SECONDS)\n        .expireAfterAccess(1, TimeUnit.SECONDS)\n        .maximumSize(10)\n        .build();\n    //keyvalue\n    Object value = cache.get(key, t -> setValue(key).apply(key));\n    cache.put("hello",value);\n\n    //null\n    Object ifPresent = cache.getIfPresent(key);\n    //key\n    cache.invalidate(key);\n    return value;\n}\n\npublic Function&lt;String, Object> setValue(String key){\n    return t -> key + "value";\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 2.  \n CachebuildCacheLoaderloadkeyvalue \n /**\n     * \n     * @param key\n     * @return\n     */\npublic Object syncOperator(String key){\n    LoadingCache&lt;String, Object> cache = Caffeine.newBuilder()\n        .maximumSize(100)\n        .expireAfterWrite(1, TimeUnit.MINUTES)\n        .build(k -> setValue(key).apply(key));\n    return cache.get(key);\n}\n\npublic Function&lt;String, Object> setValue(String key){\n    return t -> key + "value";\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 3.  \n AsyncLoadingCacheLoadingCacheExecutorCompletableFuture \n CacheLoaderAsyncCacheLoaderCompletableFuture \n /**\n     * \n     *\n     * @param key\n     * @return\n     */\npublic Object asyncOperator(String key){\n    AsyncLoadingCache&lt;String, Object> cache = Caffeine.newBuilder()\n        .maximumSize(100)\n        .expireAfterWrite(1, TimeUnit.MINUTES)\n        .buildAsync(k -> setAsyncValue(key).get());\n\n    return cache.get(key);\n}\n\npublic CompletableFuture&lt;Object> setAsyncValue(String key){\n    return CompletableFuture.supplyAsync(() -> {\n        return key + "value";\n    });\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #  2  \n Caffeine3 \n 1.  \n  \n // \nLoadingCache&lt;String, Object> cache = Caffeine.newBuilder()\n    .maximumSize(10000)\n    .build(key -> function(key));\n\n// \nLoadingCache&lt;String, Object> cache1 = Caffeine.newBuilder()\n    .maximumWeight(10000)\n    .weigher(key -> function1(key))\n    .build(key -> function(key));\nmaximumWeightmaximumSize\n \n 1 2 3 4 5 6 7 8 9 10 11 2. \n // \nLoadingCache&lt;String, Object> cache = Caffeine.newBuilder()\n    .expireAfterAccess(5, TimeUnit.MINUTES)\n    .build(key -> function(key));\nLoadingCache&lt;String, Object> cache1 = Caffeine.newBuilder()\n    .expireAfterWrite(10, TimeUnit.MINUTES)\n    .build(key -> function(key));\n\n// \nLoadingCache&lt;String, Object> cache2 = Caffeine.newBuilder()\n    .expireAfter(new Expiry&lt;String, Object>() {\n        @Override\n        public long expireAfterCreate(String key, Object value, long currentTime) {\n            return TimeUnit.SECONDS.toNanos(seconds);\n        }\n\n        @Override\n        public long expireAfterUpdate(@Nonnull String s, @Nonnull Object o, long l, long l1) {\n            return 0;\n        }\n\n        @Override\n        public long expireAfterRead(@Nonnull String s, @Nonnull Object o, long l, long l1) {\n            return 0;\n        }\n    }).build(key -> function(key));\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Caffeine \n \n expireAfterAccess(long, TimeUnit):key \n expireAfterWrite(long, TimeUnit):  \n expireAfter(Expiry): Expiry \n \n O(1) \n 3.  \n Java \n \n // keyvalue\nLoadingCache&lt;String, Object> cache = Caffeine.newBuilder()\n    .weakKeys()\n    .weakValues()\n    .build(key -> function(key));\n\n// \nLoadingCache&lt;String, Object> cache1 = Caffeine.newBuilder()\n    .softValues()\n    .build(key -> function(key));\n \n 1 2 3 4 5 6 7 8 9 10 AsyncLoadingCache \n \n Caffeine.weakKeys()keykey(identity) (==)  key equals() \n Caffeine.weakValues() valuevalue(identity) (==)  key equals() \n Caffeine.softValues() value(least-recently-used ) softValues() (identity) (==) equals()  \n \n Caffeine.weakValues()Caffeine.softValues() \n  3.  \n Cache&lt;String, Object> cache = Caffeine.newBuilder()\n    .removalListener((String key, Object value, RemovalCause cause) ->\n                     System.out.printf("Key %s was removed (%s)%n", key, cause))\n    .build();\n \n 1 2 3 4 #   4.  \n CacheWriter  \n LoadingCache&lt;String, Object> cache2 = Caffeine.newBuilder()\n    .writer(new CacheWriter&lt;String, Object>() {\n        @Override public void write(String key, Object value) {\n            // \n        }\n        @Override public void delete(String key, Object value, RemovalCause cause) {\n            // \n        }\n    })\n    .build(key -> function(key));\n \n 1 2 3 4 5 6 7 8 9 10  \n CacheWriterAsyncLoadingCache \n  5.  \n Guava Cache \n Cache&lt;String, Object> cache = Caffeine.newBuilder()\n    .maximumSize(10_000)\n    .recordStats()\n    .build();\n \n 1 2 3 4 Caffeine.recordStats(), .  Cache.stats() CacheStatsCacheStats \n \n hitRate():  \n evictionCount():  \n averageLoadPenalty():  \n \n'},{title:"",frontmatter:{title:"",date:"2022-1-08",author:"Gordon",sidebar:"auto",categories:[""],tags:["id"]},regularPath:"/%E5%85%B6%E4%BB%96/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95.html",relativePath:"/.md",key:"v-2e09059c",path:"/2022/01/08/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/",headers:[{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:'  \n snowflake \n id \n \n twitter ID  \n  \n  \n \n  64  long  id 4 \n 11 () \n 1longJava01id00 \n 241 \n  41 412^41-1:(2^411)/(1000606024365)=69  \n 41 69 69  \n 41     \n id  \n   id  \n 310 \n 1010245datacenterId5workerId 2^10=1024  \n 52^51=310123.3132datecenterIdworkerId \n 4 12bit \n id12()4096ID \n QPS409.6w/sIDCID \n  \n public   class   IdWorker   { \n \n\t // bit  1 id  bit  0 \n\n\t //    2^41 - 1   69 \n\t private   long  twepoch  =   1585644268888L ; \n\n\t //ID  25  321 31 \n\t private   long  workerId ; \n\t //ID 25  321 31 \n\t private   long  datacenterId ; \n\t //id  12 4096 -1 = 4095  \n\t private   long  sequence ; \n\t //5id \n\t private   long  workerIdBits  =   5L ; \n\t //5id \n\t private   long  datacenterIdBits  =   5L ; \n\t /**\n\t *\n\t *         -1   10000001\n\t *         -1   11111110\n\t *         -1   11111111\n\t *         -112= 1111 1111 0000 0000 0000\n\t *         -1       = 1111 1111 1111 1111 1111\n\t *           = 0000 0000 1111 1111 1111=4095\n\t *         sequenceMask4095\n\t *\n\t */ \n\t // 5 bit31id32 \n\t private   long  maxWorkerId  =   - 1L   ^   ( - 1L   <<  workerIdBits ) ; \n\t // 5 bit31id32 \n\t private   long  maxDatacenterId  =   - 1L   ^   ( - 1L   <<  datacenterIdBits ) ; \n\n\t //id 2  12 \n\t private   long  sequenceBits  =   12L ; \n\t private   long  sequenceMask  =   - 1L   ^   ( - 1L   <<  sequenceBits ) ; \n \n\t private   long  workerIdShift  =  sequenceBits ; \n\t private   long  datacenterIdShift  =  sequenceBits  +  workerIdBits ; \n\t private   long  timestampLeftShift  =  sequenceBits  +  workerIdBits  +  datacenterIdBits ; \n\n\t //1 \n\t private   long  lastTimestamp  =   - 1L ; \n\t public   long   getWorkerId ( ) { \n\t\t return  workerId ; \n\t } \n\t public   long   getDatacenterId ( )   { \n\t\t return  datacenterId ; \n\t } \n\t public   long   getTimestamp ( )   { \n\t\t return   System . currentTimeMillis ( ) ; \n\t } \n \n \n \n\t public   IdWorker ( long  workerId ,   long  datacenterId ,   long  sequence )   { \n \n\t\t // idid31 0 \n\t\t if   ( workerId  >  maxWorkerId  ||  workerId  <   0 )   { \n\t\t\t throw   new   IllegalArgumentException ( \n\t\t\t\t\t String . format ( "worker Id can\'t be greater than %d or less than 0" , maxWorkerId ) ) ; \n\t\t } \n \n\t\t if   ( datacenterId  >  maxDatacenterId  ||  datacenterId  <   0 )   { \n \n\t\t\t throw   new   IllegalArgumentException ( \n\t\t\t\t\t String . format ( "datacenter Id can\'t be greater than %d or less than 0" , maxDatacenterId ) ) ; \n\t\t } \n\t\t this . workerId  =  workerId ; \n\t\t this . datacenterId  =  datacenterId ; \n\t\t this . sequence  =  sequence ; \n\t } \n \n\t // nextId()snowflakeid \n\t public   synchronized   long   nextId ( )   { \n\t\t //  \n\t\t long  timestamp  =   timeGen ( ) ; \n\t\t if   ( timestamp  <  lastTimestamp )   { // \n \n\t\t\t System . err . printf ( \n\t\t\t\t\t "clock is moving backwards. Rejecting requests until %d." ,  lastTimestamp ) ; \n\t\t\t throw   new   RuntimeException ( \n\t\t\t\t\t String . format ( "Clock moved backwards. Refusing to generate id for %d milliseconds" , \n\t\t\t\t\t\t\tlastTimestamp  -  timestamp ) ) ; \n\t\t } \n \n\t\t // id \n\t\t // seqence14096 \n\t\t if   ( lastTimestamp  ==  timestamp )   { \n \n\t\t\t // 4096 \n\t\t\t //4096sequence4096 \n\t\t\tsequence  =   ( sequence  +   1 )   &  sequenceMask ; \n\t\t\t //id 4095ID \n\t\t\t if   ( sequence  ==   0 )   { \n\t\t\t\ttimestamp  =   tilNextMillis ( lastTimestamp ) ; \n\t\t\t } \n \n\t\t }   else   { \n\t\t\tsequence  =   0 ; \n\t\t } \n\t\t // id \n\t\tlastTimestamp  =  timestamp ; \n\t\t // 64bitid \n\t\t // 41 bitid5 bitid5 bit12 bit \n\t\t // 64 bit10long \n\t\t return   ( ( timestamp  -  twepoch )   <<  timestampLeftShift )   | \n\t\t\t\t ( datacenterId  <<  datacenterIdShift )   | \n\t\t\t\t ( workerId  <<  workerIdShift )   |  sequence ; \n\t } \n \n\t /**\n\t * id 4095ID\n\t * @param lastTimestamp\n\t * @return\n\t */ \n\t private   long   tilNextMillis ( long  lastTimestamp )   { \n \n\t\t long  timestamp  =   timeGen ( ) ; \n \n\t\t while   ( timestamp  <=  lastTimestamp )   { \n\t\t\ttimestamp  =   timeGen ( ) ; \n\t\t } \n\t\t return  timestamp ; \n\t } \n\t // \n\t private   long   timeGen ( ) { \n\t\t return   System . currentTimeMillis ( ) ; \n\t } \n \n\t /**\n\t *  main \n\t * @param args\n\t */ \n\t public   static   void   main ( String [ ]  args )   { \n\t\t System . out . println ( 1 & 4596 ) ; \n\t\t System . out . println ( 2 & 4596 ) ; \n\t\t System . out . println ( 6 & 4596 ) ; \n\t\t System . out . println ( 6 & 4596 ) ; \n\t\t System . out . println ( 6 & 4596 ) ; \n\t\t System . out . println ( 6 & 4596 ) ; \n //\t\tIdWorker worker = new IdWorker(1,1,1); \n //\t\tfor (int i = 0; i < 22; i++) { \n //\t\t\tSystem.out.println(worker.nextId()); \n //\t\t} \n\t } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 #   \n 4 \n 1.ID \n IDID \n 2. \n  id id \n 3. \n  \n 4. \n  \n  \n  id \n  \n  \n  \n 1What ID ID  \n 2Why key \n  ID   -1  ID  \n 2097167233578045440\n\n0001 1101 0001 1010 1010 0010 0111 1100 1101 1000 0000 0010 0001 0000 0000 0000\n \n 1 2 3  ID  \n \n       -1 () \n \n if (timestamp < this.lastTimestamp) {\n   return -1;\n}\n \n 1 2 3 \n   timestamp   lastTimeStamp   -1  key \n  \n        \n  \n  9:00:00 ID  lastTimestamp=10:00:00 timestamp=09:00:00   \n      \n   NTP  \n  \n  \n \n  \n \n   ntpdate   NTP   10    NTP   \n  1   2  10    1  2    \n \n  10  \n */10 * * * * /usr/sbin/ntpdate <ip>\n \n 1   NTP Pool NTP Pool Linuxntppool.org \n  NTP  \n  \n   \n  NTP  \n ntpdate  < IP>\n \n 1  1  ID  \n  NTP     1s  \n  \n  \n 1 Leaf \n 2 UidGenerator \n 3 Redis  ID ID  \n Leaf \n   Leaf    ZooKeeper  Leaf    \n  \n  UidGenerator  \n  UidGenerator  ID  ID RingBuffer  ID  incrementAndGet()  \n  ID \n Github \n https://github.com/baidu/uid-generator\n \n 1 '},{frontmatter:{},regularPath:"/%E5%85%B6%E4%BB%96/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84.html",relativePath:"/.md",key:"v-7ea9e72a",path:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%B4%A2%E5%BC%95%E6%A0%91%E7%BB%93%E6%9E%84/",lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:""},{title:"Spring",frontmatter:{},regularPath:"/%E5%85%B6%E4%BB%96/spring.html",relativePath:"/spring.md",key:"v-63f06f0b",path:"/1970/01/01/spring/",headers:[{level:2,title:"Spring",slug:"spring-2"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"IOC",slug:"ioc"},{level:3,title:"IOC",slug:"ioc"},{level:3,title:"IOC",slug:"ioc"},{level:3,title:"IOC",slug:"ioc"},{level:2,title:"Spring",slug:"spring"},{level:3,title:"",slug:""},{level:3,title:"Bean",slug:"bean"},{level:3,title:"import",slug:"import"},{level:2,title:"DI",slug:"di"},{level:3,title:"",slug:""},{level:3,title:"set ",slug:"set-"},{level:3,title:"pc ",slug:"-pc-"},{level:3,title:"Bean",slug:"bean"},{level:2,title:"Bean",slug:"bean"},{level:3,title:"byNamebyType",slug:"bynamebytype"},{level:3,title:"",slug:""},{level:3,title:"@Autowired@Resource",slug:"autowired-resource"},{level:3,title:"5.4 ",slug:"_5-4-"},{level:3,title:"5.5 javaSpring",slug:"_5-5-javaspring"},{level:2,title:"6 ",slug:"_6-"},{level:3,title:"6.1",slug:"_6-1"},{level:3,title:"6.2 ",slug:"_6-2-"},{level:2,title:"7.AOP",slug:"_7-aop"},{level:3,title:"7.1 AOP",slug:"_7-1-aop"},{level:3,title:"7.2 AopSpring",slug:"_7-2-aopspring"},{level:3,title:"7.3 SpringAop",slug:"_7-3-springaop"},{level:2,title:"8.Mybatis",slug:"_8-mybatis"},{level:3,title:"8.1Mybatis",slug:"_8-1mybatis"},{level:3,title:"8.1Mybatis",slug:"_8-1mybatis"},{level:2,title:"9.Spring",slug:"_9-spring"},{level:2,title:"10.",slug:"_10-"},{level:2,title:"11.",slug:"_11-"},{level:3,title:"Maven",slug:"-maven"},{level:3,title:"entity  pom.xml ",slug:"entity--pom-xml-"},{level:3,title:"dao  pom.xml ",slug:"dao--pom-xml-"},{level:3,title:"service  pom.xml ",slug:"service--pom-xml-"},{level:3,title:"web pom.xml ",slug:"web-pom-xml-"},{level:3,title:"",slug:""}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:' Spring \n Spring \n  \n      Spring  \n      2002Springinterface21 \n      Springinterface2120043241.0 \n       Rod Johnson  Spring Framework Rod C/C++Rod1996Java \n      Spring \n      SSH :Struct2 + Spring + Hibernate \n      SSMSpringMVC + SPring +Mybatis \n      https://spring.io/ \n      http://repo.spring.io/release/org/springframework/spring \n      GitHubhttps://github.com/spring-projects/spring-framework \n      Springhttps://docs.spring.io/spring-framework/docs/current/reference/html/core.html#beans-annotation-config \n  \n \n Spring \n Spring \n IOCAOP \n  \n \n SpringIOCAOP \n  \n IOC \n IOC \n mavenspring-study,src,,module \n  \n < dependencies > \n         < dependency > \n             < groupId > org.springframework </ groupId > \n             < artifactId > spring-webmvc </ artifactId > \n             < version > 5.2.0.RELEASE </ version > \n         </ dependency > \n     </ dependencies > \n \n 1 2 3 4 5 6 7 spring-01-ioc1 \n daoservice \n \n  \n1.UserDao \n public   interface   UserDao   { \n     void   getUser ( ) ; \n } \n \n 1 2 3 2.UserDaoImpl \n public   class   UserDaoImpl   implements   UserDao { \n     public   void   getUser ( )   { \n         System . out . println ( "" ) ; \n     } \n } \n \n 1 2 3 4 5 3.UserService \n public interface UserService {\n    void getUser();\n}\n \n 1 2 3 4.UserServiceImpl \n public class UserServiceImpl implements UserService{\n \n    private UserDao userDao = new UserDaoImpl();\n \n    public void getUser() {\n        userDao.getUser();\n    }\n}\n \n 1 2 3 4 5 6 7 8  \n public   class   MyTest   { \n     public   static   void   main ( String [ ]  args )   { \n         UserService  userService  =   new   UserServiceImpl ( ) ; \n        userService . getUser ( ) ; \n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 Userdao \n public   class   UserDaoMySqlImpl   implements   UserDao   { \n    @Override \n    public   void   getUser ( )   { \n        System . out . println ( "MySql" ) ; \n   } \n } \n \n 1 2 3 4 5 6 MySql , service \n public   class   UserServiceImpl   implements   UserService { \n \n     //private UserDao userDao = new UserDaoImpl(); \n     private   UserDao  userDao  =   new   UserDaoMySqlImpl ( ) ; \n\n     public   void   getUser ( )   { \n        userDao . getUser ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 , Userdao \n public   class   UserDaoOracleImpl   implements   UserDao   { \n    @Override \n    public   void   getUser ( )   { \n        System . out . println ( "Oracle" ) ; \n   } \n } \n \n 1 2 3 4 5 6 UserDao \n public   class   UserServiceImpl   implements   UserService { \n \n     //private UserDao userDao = new UserDaoImpl(); \n     //private UserDao userDao = new UserDaoMySqlImpl(); \n     private   UserDao  userDao  =   new   UserDaoOracleImpl ( ) ; \n\n     public   void   getUser ( )   { \n        userDao . getUser ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 UesrDaoSet \n public   class   UserServiceImpl   implements   UserService { \n \n     //private UserDao userDao = new UserDaoImpl(); \n     //private UserDao userDao = new UserDaoMySqlImpl(); \n     //private UserDao userDao = new UserDaoOracleImpl(); \n     private   UserDao  userDao ; \n     // set \n     public   void   setUserDao ( UserDao  userDao )   { \n         this . userDao  =  userDao ; \n     } \n\n     public   void   getUser ( )   { \n        userDao . getUser ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  \n public   class   MyTest   { \n     public   static   void   main ( String [ ]  args )   { \n        /* UserService userService = new UserServiceImpl();\n        userService.getUser();*/ \n         UserServiceImpl  userService  =   new   UserServiceImpl ( ) ; \n         // \n        userService . setUserDao ( new   UserDaoImpl ( ) ) ; \n        userService . getUser ( ) ; \n         //mysql \n        userService . setUserDao ( new   UserDaoMySqlImpl ( ) ) ; \n        userService . getUser ( ) ; \n         //oracle \n        userService . setUserDao ( new   UserDaoOracleImpl ( ) ) ; \n        userService . getUser ( ) ; \n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \n new(UserServiceImpl) \n set \n  \n IOC \n IOCInversion of ControlDIIOC  DIIOCIOC \n \n IoCSpring IoCXMLSpringIoC \n SpringIoc \n \n XMLBeanBeanBean \n xmlspringIOCDependency InjectionDI \n spring-02-hellospring module \n \n  \n public   class   Hello   { \n     private   String  name ; \n \n     public   String   getName ( )   { \n         return  name ; \n     } \n \n     public   void   setName ( String  name )   { \n         this . name  =  name ; \n     } \n \n     @Override \n     public   String   toString ( )   { \n         return   "Hello{"   + \n                 "name=\'"   +  name  +   \'\\\'\'   + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 beans.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     \x3c!--SpringSpringBean--\x3e \n     < bean   id = " hello "   class = " com.learning.pojo.Hello " > \n         < property   name = " name "   value = " spring " /> \n     </ bean > \n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12  \n public   class   MyTest   { \n \n     public   static   void   main ( String [ ]  args )   { \n         //spring \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         //spring \n         //Hello hello = (Hello) context.getBean("hello"); \n         //getBean \n         Hello  hello  =  context . getBean ( "hello" ,   Hello . class ) ; \n         System . out . println ( hello . toString ( ) ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 newxmlxml \n spring-01-ioc1 Springbeans.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " MysqlImpl "   class = " com.learning.dao.UserDaoMySqlImpl " /> \n     < bean   id = " OracleImpl "   class = " com.learning.dao.UserDaoOracleImpl " /> \n\n     < bean   id = " ServiceImpl "   class = " com.learning.service.UserServiceImpl " > \n         \x3c!--: name , set , --\x3e \n         \x3c!--bean , value  ref--\x3e \n         < property   name = " userDao "   ref = " OracleImpl " /> \n     </ bean > \n\n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  \n public   class   MyTest   { \n     public   static   void   main ( String [ ]  args )   { \n         // \n        /* UserService userService = new UserServiceImpl();\n        userService.getUser();*/ \n\n        //ioc \n         /*UserServiceImpl userService = new UserServiceImpl();\n        //\n        userService.setUserDao(new UserDaoImpl());\n        userService.getUser();\n        //mysql\n        userService.setUserDao(new UserDaoMySqlImpl());\n        userService.getUser();\n        //oracle\n        userService.setUserDao(new UserDaoOracleImpl());\n        userService.getUser();*/ \n\n         //xml \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         UserServiceImpl  serviceImpl  =  context . getBean ( "ServiceImpl" ,   UserServiceImpl . class ) ; \n        serviceImpl . getUser ( ) ; \n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  \n \n   SpringSpring \n  ***  \n   set \n IOC  IOC ** Spring ** \n IOC \n  spring-03-ioc2 module \n \n \n  \n  User \n public   class   User   { \n\n    private   String  name ; \n\n    public   User ( )   { \n        System . out . println ( "user" ) ; \n   } \n\n     public   User ( String  name )   { \n         this . name  =  name ; \n         System . out . println ( "user" ) ; \n     } \n\n     public   void   setName ( String  name )   { \n        this . name  =  name ; \n   } \n\n    public   void   show ( ) { \n        System . out . println ( "name=" +  name  ) ; \n   } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " user "   class = " com.learning.pojo.User " > \n         < property   name = " name "   value = " gordon " /> \n     </ bean > \n </ beans >  \n \n 1 2 3 4 5 6 7 8 9 10  \n public   class   Mytest   { \n     public   static   void   main ( String [ ]  args )   { \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         //getBean, user ,  \n         User  user  =  context . getBean ( "user" ,   User . class ) ; \n        user . show ( ) ; \n     } \n } \n\n //  \nuser\nname = gordon\n \n 1 2 3 4 5 6 7 8 9 10 11 12 showUser \n \n \n  \n \n \n public   class   UserT   { \n \n     private   String  name ; \n     private   int  age ; \n\n     public   UserT ( )   { \n         System . out . println ( "user" ) ; \n     } \n     public   UserT ( String  name ,   int  age )   { this . name  =  name ; this . age  =  age ; \n         System . out . println ( "user" ) ; } \n     public   String   getName ( )   { return  name ; } \n     public   void   setName ( String  name )   { this . name  =  name ; } \n     public   int   getAge ( )   { return  age ; } \n     public   void   setAge ( int  age )   { this . age  =  age ; } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " user "   class = " com.learning.pojo.User " > \n         < property   name = " name "   value = " gordon " /> \n     </ bean > \n     \x3c!-- index --\x3e \n     < bean   id = " user1 "   class = " com.learning.pojo.UserT " > \n         \x3c!-- index , 0 --\x3e \n         < constructor-arg   index = " 0 "   value = "  " /> \n         < constructor-arg   index = " 1 "   value = " 18 " /> \n     </ bean > \n     \x3c!--  --\x3e \n     < bean   id = " user2 "   class = " com.learning.pojo.UserT " > \n         < constructor-arg   type = " int "   value = " 18 " /> \n         < constructor-arg   type = " java.lang.String "   value = "  " /> \n     </ bean > \n     \x3c!--  --\x3e \n     < bean   id = " user3 "   class = " com.learning.pojo.UserT " > \n         \x3c!-- name --\x3e \n         < constructor-arg   name = " name "   value = "  " /> \n         < constructor-arg   name = " age "   value = " 18 " /> \n     </ bean > \n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  \n public   class   Mytest   { \n     public   static   void   main ( String [ ]  args )   { \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         //getBean, user ,  \n        /* User user = context.getBean("user", User.class);\n        user.show();*/ \n\n        // \n         //index \n         UserT  user1  =  context . getBean ( "user1" ,   UserT . class ) ; \n         System . out . println ( "user1 = "   +  user1 ) ; \n         // \n         UserT  user2  =  context . getBean ( "user2" ,   UserT . class ) ; \n         System . out . println ( "user2 = "   +  user2 ) ; \n         // \n         UserT  user3  =  context . getBean ( "user3" ,   UserT . class ) ; \n         System . out . println ( "user3 = "   +  user3 ) ; \n\n     } \n } \n\n\n // \nuser   -- \nuser  \nuser\nuser\nuser1  =   UserT { name = \'\' ,  age = 18 } \nuser2  =   UserT { name = \'\' ,  age = 18 } \nuser3  =   UserT { name = \'\' ,  age = 18 } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 spring new ClassPathXmlApplicationContext(beans.xml); spring \n Spring \n  \n \x3c!----\x3e \n     < alias   name = " user "   alias = " userAlias " /> \n \n 1 2 #  Bean \n \n         \x3c!--beanjava,Spring--\x3e \n\n \x3c!--\n   id bean,,id,name\n   id,name,name\n   name,,,\n   idname,applicationContext.getBean(.class);\n\nclassbean=+\n--\x3e \n     < bean   id = " user4 "   class = " com.learning.pojo.UserT "   name = " userAlias4 userAlias5,userAlias6;userAlias7 " > \n         \x3c!-- name --\x3e \n         < constructor-arg   name = " name "   value = "  " /> \n         < constructor-arg   name = " age "   value = " 18 " /> \n     </ bean > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  \n public   class   Mytest   { \n     public   static   void   main ( String [ ]  args )   { \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         // \n         //alias \n         UserT  userAlias3  =  context . getBean ( "userAlias3" ,   UserT . class ) ; \n         System . out . println ( "userAlias3 = "   +  userAlias3 ) ; \n         //bean name \n         Object  userAlias4  =  context . getBean ( "userAlias4" ) ; \n         System . out . println ( "userAlias4 = "   +  userAlias4 ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 #  import \n import1 \nbeanimportbeans.xml \n dao \n \n \n beans.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " defaultImpl "   class = " com.learning.pojo.UserDaoImpl " /> \n\n     < bean   id = " default "   class = " com.learning.service.UserServiceImpl " > \n         \x3c!--: name , set , --\x3e \n         \x3c!--bean , value  ref--\x3e \n         < property   name = " userDao "   ref = " defaultImpl " /> \n     </ bean > \n\n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n \n beans1.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " OracleImpl "   class = " com.learning.pojo.UserDaoOracleImpl " /> \n\n     < bean   id = " ServiceImpl "   class = " com.learning.service.UserServiceImpl " > \n         \x3c!--: name , set , --\x3e \n         \x3c!--bean , value  ref--\x3e \n         < property   name = " userDao "   ref = " OracleImpl " /> \n     </ bean > \n\n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n \n beans2.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " mysqlImpl "   class = " com.learning.pojo.UserDaoMySqlImpl " /> \n\n     < bean   id = " mysql "   class = " com.learning.service.UserServiceImpl " > \n         \x3c!--: name , set , --\x3e \n         \x3c!--bean , value  ref--\x3e \n         < property   name = " userDao "   ref = " mysqlImpl " /> \n     </ bean > \n\n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n \n applicationContext.xml \n \n \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     \x3c!-- <import resource="{path}/beans.xml"/> --\x3e \n     < import   resource = " beans.xml " /> \n     < import   resource = " beans1.xml " /> \n     < import   resource = " beans2.xml " /> \n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11  \n public   class   MyTest   { \n     public   static   void   main ( String [ ]  args )   { \n\n         //import xml \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "applicationContext.xml" ) ; \n         UserServiceImpl  aDefault  =  context . getBean ( "default" ,   UserServiceImpl . class ) ; \n        aDefault . getUser ( ) ; \n         UserServiceImpl  oracle  =  context . getBean ( "oracle" ,   UserServiceImpl . class ) ; \n        oracle . getUser ( ) ; \n         UserServiceImpl  mysql  =  context . getBean ( "mysql" ,   UserServiceImpl . class ) ; \n        mysql . getUser ( ) ; \n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #  DI \n  \n  \n set  \n  \nbean \nbean \n  \n module spring-04-di \n 1. \n public   class   Address   { \n \n     private   String  address ; \n \n     public   String   getAddress ( )   { \n         return  address ; \n     } \n \n     public   void   setAddress ( String  address )   { \n         this . address  =  address ; \n     } \n     @Override \n     public   String   toString ( )   { \n         return   "Address{"   + \n                 "address=\'"   +  address  +   \'\\\'\'   + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 2. \n package   com . learning . pojo ; \n\n import   java . util . * ; \n\n public   class   Student   { \n \n     private   String  name ; \n     private   Address  address ; \n     private   String   [ ]  books ; \n     private   List < String >  hobbies ; \n     private   Map < String ,   String >  card ; \n     private   Set < String >  games ; \n     private   String  wife ; \n     private   Properties  info ; \n\n     public   String   getName ( )   { \n         return  name ; \n     } \n\n     public   void   setName ( String  name )   { \n         this . name  =  name ; \n     } \n\n     public   Address   getAddress ( )   { \n         return  address ; \n     } \n\n     public   void   setAddress ( Address  address )   { \n         this . address  =  address ; \n     } \n\n     public   String [ ]   getBooks ( )   { \n         return  books ; \n     } \n\n     public   void   setBooks ( String [ ]  books )   { \n         this . books  =  books ; \n     } \n\n     public   List < String >   getHobbies ( )   { \n         return  hobbies ; \n     } \n\n     public   void   setHobbies ( List < String >  hobbies )   { \n         this . hobbies  =  hobbies ; \n     } \n\n     public   Map < String ,   String >   getCard ( )   { \n         return  card ; \n     } \n\n     public   void   setCard ( Map < String ,   String >  card )   { \n         this . card  =  card ; \n     } \n\n     public   Set < String >   getGames ( )   { \n         return  games ; \n     } \n\n     public   void   setGames ( Set < String >  games )   { \n         this . games  =  games ; \n     } \n\n     public   String   getWife ( )   { \n         return  wife ; \n     } \n\n     public   void   setWife ( String  wife )   { \n         this . wife  =  wife ; \n     } \n\n     public   Properties   getInfo ( )   { \n         return  info ; \n     } \n\n     public   void   setInfo ( Properties  info )   { \n         this . info  =  info ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "Student{"   + \n                 "name=\'"   +  name  +   \'\\\'\'   + \n                 ", address="   +  address . toString ( )   + \n                 ", books="   +   Arrays . toString ( books )   + \n                 ", hobbies="   +  hobbies  + \n                 ", card="   +  card  + \n                 ", games="   +  games  + \n                 ", wife=\'"   +  wife  +   \'\\\'\'   + \n                 ", info="   +  info  + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 3.beans.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " addr "   class = " com.learning.pojo.Address " > \n         < property   name = " address "   value = "  " /> \n     </ bean > \n\n\n     < bean   id = " student "   class = " com.learning.pojo.Student " > \n         \x3c!--  value--\x3e \n         < property   name = " name "   value = " gordon " /> \n         \x3c!-- Bean ref--\x3e \n         < property   name = " address "   ref = " addr " /> \n         \x3c!----\x3e \n         < property   name = " books " > \n             < array > \n                 \x3c!--value --\x3e \n                 < value >  </ value > \n                 < value >  </ value > \n                 < value >  </ value > \n                 < value >  </ value > \n             </ array > \n         </ property > \n         \x3c!--List--\x3e \n         < property   name = " hobbies " > \n             < list > \n                 < value >  </ value > \n                 < value >  </ value > \n                 < value >  </ value > \n             </ list > \n         </ property > \n         \x3c!--map --\x3e \n         < property   name = " card " > \n             < map > \n                 < entry   key = "  "   value = " 456456456465456 " /> \n                 < entry   key = "  "   value = " 1456682255511 " /> \n             </ map > \n         </ property > \n         \x3c!--set --\x3e \n         < property   name = " games " > \n             < set > \n                 < value > LOL </ value > \n                 < value > COC </ value > \n                 < value > BOB </ value > \n             </ set > \n         </ property > \n         \x3c!--Null--\x3e \n         < property   name = " wife " > < null /> </ property > \n         \x3c!--Properties--\x3e \n         < property   name = " info " > \n             < props > \n                 < prop   key = "  " > 20190604 </ prop > \n                 < prop   key = "  " >  </ prop > \n                 < prop   key = "  " >  </ prop > \n             </ props > \n         </ property > \n\n     </ bean > \n\n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 4. \n public   class   MyTest   { \n     public   static   void   main ( String [ ]  args )   { \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         Student  student  =  context . getBean ( "student" ,   Student . class ) ; \n         System . out . println ( student ) ; \n     } \n } \n /*\nStudent{name=\'gordon\', address=Address{address=\'\'}, books=[, , , ], hobbies=[, , ], card={=456456456465456, =1456682255511}, games=[LOL, COC, BOB], wife=\'null\', info={=20190604, =, =}}\n*/ \n\n \n 1 2 3 4 5 6 7 8 9 10 11 #  pc  \n  \n \n p****beans xmlns:p="http://www.springframework.org/schema/p" \n c**** xmlns:c="http://www.springframework.org/schema/c" \n xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xmlns: p = " http://www.springframework.org/schema/p " \n        xmlns: c = " http://www.springframework.org/schema/c " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     \x3c!--cconstruct-args--\x3e \n     \x3c!----\x3e \n     < bean   id = " userc "   class = " com.learning.pojo.UserT "   c: name = " gordon "   c: age = " 18 " /> \n     \x3c!--P(: properties) , set--\x3e \n     < bean   id = " userp "   class = " com.learning.pojo.UserT "   p: name = " gordon "   p: age = " 18 " /> \n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14  \n public   class   Mytest   { \n     public   static   void   main ( String [ ]  args )   { \n         //p c \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans1.xml" ) ; \n         UserT  userc  =  context . getBean ( "userc" ,   UserT . class ) ; \n         System . out . println ( "userc = "   +  userc ) ; \n         UserT  userp  =  context . getBean ( "userp" ,   UserT . class ) ; \n         System . out . println ( "userp = "   +  userp ) ; \n     } \n } \n // \n /*\nuser\nuser\nuserc = UserT{name=\'gordon\', age=18}\nuserp = UserT{name=\'gordon\', age=18}\n*/ \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #  Bean \n **1.** Spring \n <bean id="accountService" class="com.something.DefaultAccountService" scope="singleton"/>\n \n 1 \n**2.** get \n <bean id="accountService" class="com.something.DefaultAccountService" scope="prototype"/>\n \n 1 \n**3.** requestsessionapplicationwebsocketweb \n Bean \n \n springbean \n Springbean \n \n Spring \n \n xml \n java \n bean \n \n Spring \n 1.(component scanning)springbean \n 2.(autowiring)springbeanIoC/DI \n  \n xml ,  . \n  \n 1.spring-05-autowired \n 2.Cat  Dog   \n public   class   Cat   { \n     public   void   shout ( )   { \n         System . out . println ( "miao~" ) ; \n     } \n } \n \n 1 2 3 4 5 public   class   Dog   { \n     public   void   shout ( )   { \n         System . out . println ( "wang~" ) ; \n     } \n } \n \n 1 2 3 4 5 3. \n public   class   User   { \n     private   Cat  cat ; \n     private   Dog  dog ; \n     private   String  str ; \n\n     public   Cat   getCat ( )   { \n         return  cat ; \n     } \n\n     public   void   setCat ( Cat  cat )   { \n         this . cat  =  cat ; \n     } \n\n     public   Dog   getDog ( )   { \n         return  dog ; \n     } \n\n     public   void   setDog ( Dog  dog )   { \n         this . dog  =  dog ; \n     } \n\n     public   String   getStr ( )   { \n         return  str ; \n     } \n\n     public   void   setStr ( String  str )   { \n         this . str  =  str ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "User{"   + \n                 "cat="   +  cat  + \n                 ", dog="   +  dog  + \n                 ", str=\'"   +  str  +   \'\\\'\'   + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 4.beans.xml \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n        http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " dog "   class = " com.learning.pojo.Dog " /> \n     < bean   id = " cat "   class = " com.learning.pojo.Cat " /> \n     < bean   id = " user "   class = " com.learning.pojo.User " > \n         < property   name = " cat "   ref = " cat " /> \n         < property   name = " dog "   ref = " dog " /> \n         < property   name = " str "   value = " gordon " /> \n     </ bean > \n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #  byNamebyType \n autowire byName () \n xml \n  \n  \n 1bean  autowire="byName" \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n        http://www.springframework.org/schema/beans/spring-beans.xsd " > \n\n     < bean   id = " dog "   class = " com.learning.pojo.Dog " /> \n     < bean   id = " cat0 "   class = " com.learning.pojo.Cat " /> \n     < bean   id = " user "   class = " com.learning.pojo.User " > \n         < property   name = " cat "   ref = " cat0 " /> \n         < property   name = " dog "   ref = " dog " /> \n         < property   name = " str "   value = " gordon " /> \n     </ bean > \n     \x3c!--byName cat --\x3e \n     < bean   id = " cat "   class = " com.learning.pojo.Cat " /> \n     < bean   id = " userByName "   class = " com.learning.pojo.User "   autowire = " byName " > \n         < property   name = " str "   value = " gordon " /> \n     </ bean > \n     </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 2 \n public   class   MyTest   { \n     @Test \n     public   void   testMethodAutowire ( )   { \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         User  user  =  context . getBean ( "userByName" ,   User . class ) ; \n        user . getCat ( ) . shout ( ) ; \n        user . getDog ( ) . shout ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 3 cat bean id catXXX \n 4 java.lang.NullPointerExceptionbyNamesetsetCat \n  \n bean autowire byName \n setsetCatsetcat \n springid \n  \n autowire byType () \n autowire byTypespringNoUniqueBeanDefinitionException \n  \n 1userbean  autowire="byType" \n 2 \n 3cat bean \n < bean   id = " dog "   class = " com.learning.pojo.Dog " /> \n < bean   id = " cat "   class = " com.learning.pojo.Cat " /> \n < bean   id = " cat2 "   class = " com.learning.pojo.Cat " /> \n < bean   id = " user "   class = " com.learning.pojo.User "   autowire = " byType " > \n     < property   name = " str "   value = " gordon " /> \n </ bean > \n \n 1 2 3 4 5 6 4NoUniqueBeanDefinitionException \n 5cat2catbeanid \n  \n  \nbyNamebeanidbeanset \nbyTypebeanclassbean \n  \n jdk1.5 Spring2.5 \nThe introduction of annotation-based configuration raised the question of whether this approach is better than XML \n  \n beans2.xml \n  \n2.1.context \nxmlns:context=http://www.springframework.org/schema/context \nhttp://www.springframework.org/schema/context \nhttps://www.springframework.org/schema/context/spring-context.xsd \n2.2.context:annotation-config \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n       xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n       xmlns: context = " http://www.springframework.org/schema/context " \n       xmlns: aop = " http://www.springframework.org/schema/aop " \n       xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       https://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context\n       https://www.springframework.org/schema/context/spring-context.xsd\n       http://www.springframework.org/schema/aop\n       https://www.springframework.org/schema/aop/spring-aop.xsd " > \n \n    \x3c!----\x3e \n    < context: annotation-config /> \n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Autowired \n \n @Autowiredid \n  spring-aop \n \n  \n 1Userset@Autowired \n public   class   User   { \n     @Autowired \n     private   Cat  cat ; \n     @Autowired \n     private   Dog  dog ; \n     private   String  str ; \n\n     public   Cat   getCat ( )   { \n         return  cat ; \n     } \n\n     public   Dog   getDog ( )   { \n         return  dog ; \n     } \n\n     public   String   getStr ( )   { \n         return  str ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "User{"   + \n                 "cat="   +  cat  + \n                 ", dog="   +  dog  + \n                 ", str=\'"   +  str  +   \'\\\'\'   + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 2 \n <?xml version="1.0" encoding="UTF-8"?> \n < beans   xmlns = " http://www.springframework.org/schema/beans " \n        xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n        xmlns: context = " http://www.springframework.org/schema/context " \n        xmlns: aop = " http://www.springframework.org/schema/aop " \n        xsi: schemaLocation = " http://www.springframework.org/schema/beans\n       https://www.springframework.org/schema/beans/spring-beans.xsd\n       http://www.springframework.org/schema/context\n       https://www.springframework.org/schema/context/spring-context.xsd\n       http://www.springframework.org/schema/aop\n       https://www.springframework.org/schema/aop/spring-aop.xsd " > \n\n     \x3c!----\x3e \n     < context: annotation-config /> \n     \x3c!--autowire--\x3e \n     \x3c!--\n    <bean id="dog" class="com.learning.pojo.Dog"/>\n    <bean id="cat" class="com.learning.pojo.Cat"/>\n    <bean id="user" class="com.learning.pojo.User">\n        <property name="cat" ref="cat"/>\n        <property name="dog" ref="dog"/>\n        <property name="str" value="gordon"/>\n    </bean>\n    --\x3e \n     \x3c!--autowire--\x3e \n     \x3c!--\n     byName cat \n    <bean id="userByName" class="com.learning.pojo.User" autowire="byName">\n        <property name="str" value="gordon"/>\n    </bean>\n\n    byType\n    <bean id="userByType" class="com.learning.pojo.User" autowire="byType">\n        <property name="str" value="gordon"/>\n    </bean>\n    --\x3e \n     < bean   id = " dog "   class = " com.learning.pojo.Dog " /> \n     < bean   id = " cat "   class = " com.learning.pojo.Cat " /> \n     < bean   id = " user "   class = " com.learning.pojo.User " /> \n\n </ beans > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 3 \n public   class   MyTest   { \n     @Test \n     public   void   annoTest ( ) { \n         ApplicationContext  context  =   new   ClassPathXmlApplicationContext ( "beans.xml" ) ; \n         User  user  =  context . getBean ( "user" ,   User . class ) ; \n        user . getCat ( ) . shout ( ) ; \n        user . getDog ( ) . shout ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 #  @Autowired@Resource \n @Autowired \nset \nAutowiredsetIOCSpringbyName \n : \n @Autowired(required=false)  falsenulltruenull \n      //Autowiredrequiredfalsenull \n     @Autowired ( required  =   false ) \n     private   Cat  cat ; \n     @Autowired \n     private   Dog  dog ; \n     private   String  name ; \n \n 1 2 3 4 5 6 @Autowired@Autowired@Qualifiervalue = xxx@Autowiredbean \n public   class   User   { \n     @Autowired ( required  =   false ) \n     private   Cat  cat ; \n     @Autowired \n     @Qualifier ( value  =   "dog2" ) \n     private   Dog  dog ; \n     private   String  str ; \n\n     public   Cat   getCat ( )   { \n         return  cat ; \n     } \n\n     public   Dog   getDog ( )   { \n         return  dog ; \n     } \n\n     public   String   getStr ( )   { \n         return  str ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "User{"   + \n                 "cat="   +  cat  + \n                 ", dog="   +  dog  + \n                 ", str=\'"   +  str  +   \'\\\'\'   + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 < bean   id = " dog "   class = " com.learning.pojo.Dog " /> \n     < bean   id = " cat "   class = " com.learning.pojo.Cat " /> \n     < bean   id = " user "   class = " com.learning.pojo.User " /> \n     \x3c!--dog type @Qualifiervalue = xxx@Autowiredbean--\x3e \n     < bean   id = " dog2 "   class = " com.learning.pojo.Dog " /> \n \n 1 2 3 4 5 @Resource****namebyNamebyType \n public   class   People   { \n     @Resource ( name  =   "xxxx" ) \n     private   Cat  cat ; \n \n 1 2 3 @Resource@Autowired \n \n  \n @AutowiredbyType \n @ResourcebyNamebyType \n @AutowiredbyType@ResourcebyName \n 5.4  \n Spring4aop \n \ncontext \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:context="http://www.springframework.org/schema/context"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/context\n        https://www.springframework.org/schema/context/spring-context.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n \n    \x3c!----\x3e\n    <context:annotation-config/>\n    \x3c!----\x3e\n    <context:component-scan base-package="com.yang"/>\n</beans>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1. bean****@Componet \n //@Component <bean id="user" class="com.yang.entity.User"/>\n@Component\npublic class User {\n    String name;\n}\n \n 1 2 3 4 5 2.   @Value \n //@Component <bean id="user" class="com.yang.entity.User"/>\n@Component\npublic class User {\n \n    String name;\n    //@Value("yang") <property name="name" value="yang"/>\n    @Value("yang")\n    public void setName(String name) {\n        this.name = name;\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 3.    \n@Componetwebmvc \n \n dao @Repository \n service @Service \n controller @Controller \nSpringBean \n \n **4.**  \n @Autowired  \n                  Autowired@Qualifier(value="xxx")\n@Nullable   null\n@Resource \n \n 1 2 3 4 5.    \n@Scope(singleton) \n 6.    \nXML   \n \n xml \n   \n \n XML   \n \n xmlbean \n  \n  \n \n \x3c!----\x3e\n   <context:annotation-config/>\n   \x3c!----\x3e\n   <context:component-scan base-package="com.yang"/>\n \n 1 2 3 4 #  5.5 javaSpring \n \n \n package com.yang.entity;\n \nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.stereotype.Component;\n \n@Component\npublic class User {\n \n    private String name;\n \n    public String getName() {\n        return name;\n    }\n    @Value("Yang")\n    public void setName(String name) {\n        this.name = name;\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  \n import org.springframework.context.annotation.Configuration;\nimport org.springframework.context.annotation.Import;\n \n//@Configuration,Spring@Component\n//@ConfigurationapplicationContext.xml\n@Configuration\n@ComponentScan("com.yang.entity")\n@Import(YangConfig2.class)\npublic class YangConfig {\n \n    //Beanbean\n    // == beanid\n    // == beanclass\n \n    @Bean\n    public User getUser () {\n        return new User();//bean\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  \n public class MyTest {\n \n    public static void main(String[] args) {\n \n        ApplicationContext annotationConfigApplicationContext = new AnnotationConfigApplicationContext(YangConfig.class);\n        User getUser = (User)annotationConfigApplicationContext.getBean("getUser");\n        System.out.println(getUser.getName());\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 #  6  \n 6.1 \n  \n \n  \n  \n  \n  \n \n  \n \n  \n  \n  \n  \n   \n \n  \n \n  \n  \n  \n \n  \n \n   \n 6.2  \n \n  \n  \n  \nJDK \n cglib \njava javasist \n \n Proxy InvocationHandler \n Proxy \n java.lang.reflect.Proxy \nProxy() \n      InvocationHandler  invoke java.lang.reflect.Methodjava.lang.reflect.MethodObject Object  \n  \n \n  \n   \n  "$Proxy" \n java.lang.reflect.Proxy  \n  \n    \n  getInterfacesgetInterfacesgetMethodsgetMethod \nProxy.isProxyClasstrue - Proxy.getProxyClassProxy.newProxyInstance - false \n java.security.ProtectionDomainjava.lang.Object  java.security.AllPermission  \n InvocationHandler  APIProxy.newProxyInstanceProxy.getProxyClass \n \n //Foo  \n  InvocationHandler handler = new MyInvocationHandler(...);\n     Class<?> proxyClass = Proxy.getProxyClass(Foo.class.getClassLoader(), Foo.class);\n     Foo f = (Foo) proxyClass.getConstructor(InvocationHandler.class).\n                     newInstance(handler); \n  // \n  Foo f = (Foo) Proxy.newProxyInstance(Foo.class.getClassLoader(),\n                                          new Class<?>[] { Foo.class },\n                                          handler);\npublic static <?> getProxyClass(ClassLoader loader,\n                                     <?>... interfaces)\n                              throws IllegalArgumentException\n/*java.lang.Class   ;  \nProxy.getProxyClassProxy.getProxyClass  \n \ninterfaces \ninterfaces \n cli  \n  Class.forName(i.getName(), false, cl) == i ;  \n \nvoid \n \n VM65535;  interfaces65535 \n Proxy.getProxyClassIllegalArgumentException  interfacesnull NullPointerException  \n \n \n \n \nloader -  \ninterfaces -  \n \n */\n \npublic static Object newProxyInstance(ClassLoader loader,\n                                      <?>[] interfaces,\n                                      InvocationHandler h)\n                               throws IllegalArgumentException\n/* \nProxy.newProxyInstanceIllegalArgumentExceptionProxy.getProxyClass  \n \n \nloader -  \ninterfaces -  \nh -  \n \n */\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 InvocationHandler \n InvocationHandler  \n invoke \ninvoke(Object proxy,  method, Object[] args)  \n Object invoke(Object proxy,\n               method,\n              Object[] args)\n       throws Throwable  \n/* \nproxy -  \nmethod -  \nargs -null java.lang.Integerjava.lang.Boolean  \n \n ;  nullNullPointerException ClassCastException \n \nThrowable -  throwsjava.lang.RuntimeExceptionjava.lang.RuntimeExceptionjava.lang.Error  throwsUndeclaredThrowableException */\n \n 1 2 3 4 5 6 7 8 9 10 11 12  \n  \n package com.yang.demo;\npublic interface UserServiceInterface {\n    public void add();\n    public void delete();\n    public void update();\n    public void select();\n}\n \n 1 2 3 4 5 6 7  \n package com.yang.demo;\npublic class UserServiceImpl implements UserServiceInterface{\n    public void add() {\n        System.out.println("");\n    }\n    public void delete() {\n        System.out.println("");\n    }\n    public void update() {\n        System.out.println("");\n    }\n    public void select() {\n        System.out.println("");\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  \n package com.yang.demo;\nimport java.lang.reflect.InvocationHandler;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Proxy;\n//\npublic class ProxyInvocationHandler implements InvocationHandler {\n \n    //\n    private Object target;\n \n    public void setTarget (Object target) {\n        this.target = target;\n    }\n    //InvocationHandlerinvoke\n    //\n    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n        log(method.getName());\n        Object invoke = method.invoke(target, args);\n        return invoke;\n    }\n    //\n    public Object getProxy () {\n        return Proxy.newProxyInstance(this.getClass().getClassLoader(),\n                target.getClass().getInterfaces(), this);\n    }\n    public void log (String msg) {\n        System.out.println("" + msg + "");\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  \n package com.yang.demo;\n \npublic class Client {\n    public static void main(String[] args) {\n        //\n        UserServiceImpl userService = new UserServiceImpl();\n        //\n        ProxyInvocationHandler proxyInvocationHandler = new ProxyInvocationHandler();\n        //\n        proxyInvocationHandler.setTarget(userService);\n        //\n        UserServiceInterface proxy = (UserServiceInterface)proxyInvocationHandler.getProxy();\n        proxy.add();\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #  7.AOP \n 7.1 AOP \n AOPAspect Oriented ProgrammingAOPOOPSpringAOP \n 7.2 AopSpring \n  \n \n  \n ASPECT   \n Advice  \n target \n Proxy \n PointCut "" \n jointPoint \n \n ![](file:///C:/Users/GORDON~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.png) \nSpringAopAdviceSpring5Advice \n 7.3 SpringAop \n AOP \n <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.9.4</version>\n</dependency>\n \n 1 2 3 4 5 SpringAPI \neg:UserService \n UserServer \n package com.yang.service;\npublic interface UserService {\n    public void add();\n    public void update();\n    public void delete();\n    public void select();\n}\n \n 1 2 3 4 5 6 7 UserServer \n package com.yang.service;\npublic class UserServiceImpl implements UserService{\n    public void add() {\n        System.out.println("");\n    }\n    public void update() {\n        System.out.println("");\n    }\n    public void delete() {\n        System.out.println("");\n    }\n    public void select() {\n        System.out.println("");\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Log \n import org.springframework.aop.AfterReturningAdvice;\nimport org.springframework.aop.MethodBeforeAdvice;\nimport java.lang.reflect.Method;\npublic class Log implements MethodBeforeAdvice, AfterReturningAdvice {\n    //method:method being invoked\n    //object:args: arguments to the method\n    //o: targettarget of the method invocation\n    public void before(Method method, Object[] args, Object target) throws Throwable {\n        System.out.println(target.getClass().getName() + "" + method.getName() + "");\n    }\n    //returnValue:\n    public void afterReturning(Object returnValue, Method method, Object[] args, Object target) throws Throwable {\n        System.out.println("" + method.getName() + "" + returnValue);\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n \n    \x3c!--bean--\x3e\n    <bean id="userService" class="com.yang.service.UserServiceImpl"/>\n    <bean id="log" class="com.yang.log.Log"/>\n    \x3c!--Spring Api--\x3e\n    \x3c!--aop--\x3e\n    <aop:config>\n        \x3c!--execution:execution(*() *() *() *() *())  ..--\x3e\n        <aop:pointcut id="pointcut" expression="execution(* com.yang.service.UserServiceImpl.*(..))"/>\n \n        \x3c!----\x3e\n        <aop:advisor advice-ref="log" pointcut-ref="pointcut"/>\n    </aop:config>\n</beans>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  \n import com.yang.service.UserService;\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.support.ClassPathXmlApplicationContext;\npublic class MyTest {\n    public static void main(String[] args) {\n        ApplicationContext classPathXmlApplicationContext = new ClassPathXmlApplicationContext("applicationContext.xml");\n        UserService userService = classPathXmlApplicationContext.getBean("userService", UserService.class);\n        userService.add();\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 AOP \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n \n    \x3c!--bean--\x3e\n    <bean id="userService" class="com.yang.service.UserServiceImpl"/>\n    <bean id="log" class="com.yang.log.Log"/>\n    <bean id="diy" class="com.yang.diy.DiyPointCut"/>\n    \x3c!--2--\x3e\n    <aop:config>\n       \x3c!--<aop:aspect ref="diy"> : --\x3e\n        <aop:aspect ref="diy">\n            \x3c!----\x3e\n            <aop:pointcut id="point" expression="execution(* com.yang.service.UserServiceImpl.*(..))"/>\n            \x3c!----\x3e\n            <aop:before method="beforeMethod" pointcut-ref="point"/>\n        </aop:aspect>\n    </aop:config>\n</beans>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  \n package com.yang.diy;\npublic class DiyPointCut {\n    public void beforeMethod () {\n        System.out.println("");\n    }\n}\n \n 1 2 3 4 5 6 AOP \nXML \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n \n    \x3c!--bean--\x3e\n    <bean id="userService" class="com.yang.service.UserServiceImpl"/>\n    \x3c!--3--\x3e\n    \x3c!--  JDKproxy-target-class="false"cglibproxy-target-class="true"--\x3e\n    <aop:aspectj-autoproxy proxy-target-class="false"/>\n    <bean id="annotationPointCut" class="com.yang.diy.AnnotationPointcut"/>\n<beans/>\npackage com.yang.diy;\n \nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.Signature;\nimport org.aspectj.lang.annotation.After;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Before;\n \n@Aspect\npublic class AnnotationPointcut {\n    @Before("execution(* com.yang.service.UserServiceImpl.*(..))")\n    public void before () {\n        System.out.println("========");\n    }\n    @After("execution(* com.yang.service.UserServiceImpl.*(..))")\n    public void after () {\n        System.out.println("========");\n    }\n    //\n    @Around("execution(* com.yang.service.UserServiceImpl.*(..))")\n    public void around (ProceedingJoinPoint pjp) throws Throwable {\n        System.out.println("");\n        Signature signature = pjp.getSignature();//\n        System.out.println("signature" + signature);\n \n        Object proceed = pjp.proceed();//\n        System.out.println("");\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \n 8.Mybatis \n 8.1Mybatis \n  \n \n  jar  \njunit \nmybatis \nmysql \nspring \naop \nmybatis-spring \n \n <?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n    <parent>\n        <artifactId>spring-study</artifactId>\n        <groupId>com.yang</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n \n    <artifactId>spring-10-mybatis</artifactId>\n \n    <dependencies>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.13</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>5.1.47</version>\n        </dependency>\n        <dependency>\n            <groupId>org.mybatis</groupId>\n            <artifactId>mybatis</artifactId>\n            <version>3.5.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-webmvc</artifactId>\n            <version>5.2.0.RELEASE</version>\n        </dependency>\n        \x3c!--Springspring-jdbc--\x3e\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-jdbc</artifactId>\n            <version>5.1.9.RELEASE</version>\n        </dependency>\n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.9.4</version>\n        </dependency>\n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjrt</artifactId>\n            <version>1.8.13</version>\n        </dependency>\n        <dependency>\n            <groupId>org.mybatis</groupId>\n            <artifactId>mybatis-spring</artifactId>\n            <version>2.0.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.projectlombok</groupId>\n            <artifactId>lombok</artifactId>\n            <version>1.16.10</version>\n        </dependency>\n    </dependencies>\n \n    <build>\n        <resources>\n            <resource>\n                <directory>src/main/java</directory>\n                <includes>\n                    <include>**/*.xml</include>\n                </includes>\n                <filtering>true</filtering>\n            </resource>\n        </resources>\n    </build>\n</project>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 \n  \nspring-dao.xmlDataSourcesqlSession \n \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n    \x3c!--DataSourceSpringMybatis  c3p0  dbcp  druid\n    SpringJDBCorg.springframework.jdbc.datasource--\x3e\n    <bean id="dataSource" class="org.springframework.jdbc.datasource.DriverManagerDataSource">\n        <property name="driverClassName" value="com.mysql.jdbc.Driver"/>\n        <property name="url" value="jdbc:mysql://localhost:3306/mybatis?useSSL=true&amp;useUnicode=false&amp;characterEncoding=utf-8"/>\n        <property name="username" value="root"/>\n        <property name="password" value="root"/>\n    </bean>\n    \x3c!--sqlSessionFactory--\x3e\n    <bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean">\n        <property name="dataSource" ref="dataSource" />\n        \x3c!--Mybatis--\x3e\n        <property name="configLocation" value="classpath:mybatis-config.xml"/>\n        <property name="mapperLocations" value="classpath:com/yang/mapper/*.xml"/>\n    </bean>\n \n    <bean id="sqlSession" class="org.mybatis.spring.SqlSessionTemplate">\n        \x3c!--sqlSessionFactoryset--\x3e\n        <constructor-arg index="0" ref="sqlSessionFactory"/>\n    </bean>\n</beans>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 mybatis-config.xmlmybatis \n <?xml version="1.0" encoding="UTF-8" ?>\n<!DOCTYPE configuration\n        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"\n        "http://mybatis.org/dtd/mybatis-3-config.dtd">\n<configuration>\n    <typeAliases>\n        <package name="com.yang.entity"/>\n    </typeAliases>\n</configuration>\n \n 1 2 3 4 5 6 7 8 9 applicationContext.xmlbean \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd">\n    <import resource="spring-dao.xml"/>\n    <bean id="userMapper" class="com.yang.mapper.UserMapperImpl">\n        <property name="sqlSessionTemplate" ref="sqlSession"/>\n    </bean>\n</beans>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 \n  \n \n \n package com.yang.entity;\nimport lombok.Data;\n@Data\npublic class User {\n    private int id;\n    private String name;\n    private int pwd;\n}\n \n 1 2 3 4 5 6 7 8 mapper \n package com.yang.mapper;\nimport com.yang.entity.User;\nimport java.util.List;\n \npublic interface UserMapper {\n    public List<User> selectUser();\n}\n \n 1 2 3 4 5 6 7 mapper \n package com.yang.mapper;\n \nimport com.yang.entity.User;\nimport org.mybatis.spring.SqlSessionTemplate;\n \nimport java.util.List;\n \npublic class UserMapperImpl implements UserMapper{\n \n    //sqlSessionsqlSessionTemplate\n    private SqlSessionTemplate sqlSessionTemplate;\n \n    public void setSqlSessionTemplate (SqlSessionTemplate sqlSessionTemplate) {\n        this.sqlSessionTemplate = sqlSessionTemplate;\n    }\n    public List<User> selectUser() {\n        UserMapper mapper = sqlSessionTemplate.getMapper(UserMapper.class);\n        return mapper.selectUser();\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mapper \n <?xml version="1.0" encoding="UTF-8" ?>\n<!DOCTYPE mapper\n        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"\n        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">\n<mapper namespace="com.yang.mapper.UserMapper">\n    <select id="selectUser" resultType="user">\n        select * from mybatis.user;\n    </select>\n</mapper>\n \n 1 2 3 4 5 6 7 8 9  \n import com.yang.entity.User;\nimport com.yang.mapper.UserMapper;\nimport org.junit.Test;\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.support.ClassPathXmlApplicationContext;\n \nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.util.List;\n \npublic class MyTest {\n    @Test\n    public void test () throws IOException {\n        ApplicationContext classPathXmlApplicationContext = new ClassPathXmlApplicationContext("applicationContext.xml");\n \n        UserMapper userMapper = classPathXmlApplicationContext.getBean("userMapper", UserMapper.class);\n        List<User> userList = userMapper.selectUser();\n \n        for (User user : userList) {\n            System.out.println(user);\n        }\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #  8.1Mybatis \n SqlSessionDaoSupport \n SqlSessionDaoSupport  SqlSession getSqlSession()  SqlSessionTemplate SQL : \n package com.yang.mapper;\n \nimport com.yang.entity.User;\nimport org.apache.ibatis.session.SqlSession;\nimport org.mybatis.spring.support.SqlSessionDaoSupport;\n \nimport java.util.List;\npublic class UserMapperImpl2 extends SqlSessionDaoSupport implements UserMapper {\n    public List<User> selectUser() {\n        SqlSession sqlSession = getSqlSession();\n        UserMapper mapper = sqlSession.getMapper(UserMapper.class);\n        List<User> users = mapper.selectUser();\n        //return getSqlSession().getMapper(UserMapper.class).selectUser();\n        return users;\n    }\n}\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 mybatismybatisSqlSessionDaoSupport getSqlSession()setSqlSessionTemplate \n 9.Spring \n <?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:aop="http://www.springframework.org/schema/aop"\n       xmlns:tx="http://www.springframework.org/schema/tx"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n        https://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/aop\n        https://www.springframework.org/schema/aop/spring-aop.xsd\n        http://www.springframework.org/schema/tx\n        http://www.springframework.org/schema/tx/spring-tx.xsd">\n \n    <import resource="spring-dao.xml"/>\n \n    <bean id="userMapper" class="com.yang.mapper.UserMapperImpl2">\n        <property name="sqlSessionTemplate" ref="sqlSession"/>\n    </bean>\n    \x3c!----\x3e\n    \x3c!-- Spring  Spring  DataSourceTransactionManager --\x3e\n    <bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">\n        <property name="dataSource" ref="dataSource"/>\n        \x3c!----\x3e\n        \x3c!--<constructor-arg ref="dataSource" />--\x3e\n    </bean>\n \n    \x3c!--AOP--\x3e\n    \x3c!----\x3e\n    <tx:advice id="txAdvice" transaction-manager="transactionManager">\n        \x3c!----\x3e\n        \x3c!-- propagation\n                PROPAGATION_REQUIRED:\n                PROPAGATION_SUPPORTS:\n                PROPAGATION_MANDATORY:\n                PROPAGATION_REQUIRES_NEW:\n                PROPAGATION_NOT_SUPPORTED:\n                PROPAGATION_NEVER:\n                PROPAGATION_NESTED:      PROPAGATION_REQUIRED \n        --\x3e\n        <tx:attributes>\n            <tx:method name="add" propagation="REQUIRED"/>\n            <tx:method name="delete" propagation="REQUIRED"/>\n            <tx:method name="update" propagation="REQUIRED"/>\n            <tx:method name="select" read-only="true"/>\n            \x3c!----\x3e\n            <tx:method name="*" propagation="REQUIRED"/>\n        </tx:attributes>\n    </tx:advice>\n \n    \x3c!----\x3e\n    <aop:config>\n       \x3c!----\x3e\n        <aop:pointcut id="txPointCut" expression="execution(* com.yang.mapper.*.*(..))"/>\n        <aop:advisor advice-ref="txAdvice" pointcut-ref="txPointCut"/>\n    </aop:config>\n</beans>\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #  10. \n \n Spring5 \n https://space.bilibili.com/95256449?from=search&seid=18072733147691362652 \n springbootspringcloud \n 11. \n IDEA \n          SprngBoot 2.0.4 \n           Maven  2.19.1 \n  \n                father \n                            dao     \n                            entity    \n                            service  \n                            web     \n                      web servicedaoentity \n                           service daoentity \n                           dao entity \n                           entity \n Maven \n      File new project Spring Initializr      Next \n \n \n \n  \n \n src \n    new   Module  Spring Initaializr  \n \n \n daoserviceweb \n serviceentity \n daowebmysqlmybatisredisweb \n \n \n .mvn.gitignoredaoimlmvnwmvnw.cmdpom.xml \n webApplicatinresourcesapplication.properties \n \n Maven Projects \n \n  \n  \n  \npom.xmljarpombuildspring-boot-maven-plugin \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n     \x3c!--pom.xml--\x3e \n     < groupId > com.miu </ groupId > \n     < artifactId > father </ artifactId > \n     < version > 0.0.1-SNAPSHOT </ version > \n     < packaging > pom </ packaging > \n \n     < name > father </ name > \n     < description > Demo project for Spring Boot </ description > \n \n     < parent > \n         < groupId > org.springframework.boot </ groupId > \n         < artifactId > spring-boot-starter-parent </ artifactId > \n         < version > 2.0.4.RELEASE </ version > \n         < relativePath />   \x3c!-- lookup parent from repository --\x3e \n     </ parent > \n \n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n     </ properties > \n \n     \x3c!-- --\x3e \n     < modules > \n         < module > entity </ module > \n         < module > dao </ module > \n         < module > service </ module > \n         < module > web </ module > \n     </ modules > \n \n     < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter </ artifactId > \n         </ dependency > \n \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n     </ dependencies > \n \n     < build > \n         < plugins > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.1 </ version > \n                 < configuration > \n                     < source > ${java.version} </ source > \n                     < target > ${java.version} </ target > \n                 </ configuration > \n             </ plugin > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-surefire-plugin </ artifactId > \n                 < version > 2.19.1 </ version > \n                 < configuration > \n                     < skipTests > true </ skipTests >      \x3c!-- --\x3e \n                 </ configuration > \n             </ plugin > \n         </ plugins > \n     </ build > \n \n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 daoserviceentitypom.xmlbuild  \n entity  pom.xml  \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n     < groupId > com.miu </ groupId > \n     < artifactId > entity </ artifactId > \n     < version > 0.0.1-SNAPSHOT </ version > \n     < packaging > jar </ packaging > \n     < name > entity </ name > \n     < description > Demo project for Spring Boot </ description > \n     \x3c!----\x3e \n     < parent > \n         < groupId > com.miu </ groupId > \n         < artifactId > father </ artifactId > \n         < version > 0.0.1-SNAPSHOT </ version > \n         < relativePath > ../pom.xml </ relativePath > \n     </ parent > \n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n     </ properties > \n     < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter </ artifactId > \n         </ dependency > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n     </ dependencies > \n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #  dao  pom.xml  \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n     \x3c!--dao  pom.xml--\x3e \n     < groupId > com.miu </ groupId > \n     < artifactId > dao </ artifactId > \n     < version > 0.0.1-SNAPSHOT </ version > \n     < packaging > jar </ packaging > \n     < name > dao </ name > \n     < description > Demo project for Spring Boot </ description > \n     \x3c!----\x3e \n     < parent > \n         < groupId > com.miu </ groupId > \n         < artifactId > father </ artifactId > \n         < version > 0.0.1-SNAPSHOT </ version > \n         < relativePath > ../pom.xml </ relativePath > \n     </ parent > \n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n     </ properties > \n     < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-data-redis </ artifactId > \n         </ dependency > \n         < dependency > \n             < groupId > org.mybatis.spring.boot </ groupId > \n             < artifactId > mybatis-spring-boot-starter </ artifactId > \n             < version > 1.3.2 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > mysql </ groupId > \n             < artifactId > mysql-connector-java </ artifactId > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n         \x3c!--dao  entity--\x3e \n         < dependency > \n             < groupId > com.miu </ groupId > \n             < artifactId > entity </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n     </ dependencies > \n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #  service  pom.xml  \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n \n     < groupId > com.miu </ groupId > \n     < artifactId > service </ artifactId > \n     < version > 0.0.1-SNAPSHOT </ version > \n     < packaging > jar </ packaging > \n \n     < name > service </ name > \n     < description > Demo project for Spring Boot </ description > \n     \x3c!----\x3e \n     < parent > \n         < groupId > com.miu </ groupId > \n         < artifactId > father </ artifactId > \n         < version > 0.0.1-SNAPSHOT </ version > \n         < relativePath > ../pom.xml </ relativePath > \n     </ parent > \n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n     </ properties > \n     < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter </ artifactId > \n         </ dependency > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n         \x3c!--service entity--\x3e \n         < dependency > \n             < groupId > com.miu </ groupId > \n             < artifactId > entity </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n         \x3c!--service dao--\x3e \n         < dependency > \n             < groupId > com.miu </ groupId > \n             < artifactId > dao </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n     </ dependencies > \n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #  web pom.xml  \n      buildwebMain Class \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 "   xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n     < groupId > com.miu </ groupId > \n     < artifactId > web </ artifactId > \n     < version > 0.0.1-SNAPSHOT </ version > \n     < packaging > jar </ packaging > \n     < name > web </ name > \n     < description > Demo project for Spring Boot </ description > \n     \x3c!----\x3e \n     < parent > \n         < groupId > com.miu </ groupId > \n         < artifactId > father </ artifactId > \n         < version > 0.0.1-SNAPSHOT </ version > \n         < relativePath > ../pom.xml </ relativePath > \n     </ parent > \n     < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n     </ properties > \n     < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-data-redis </ artifactId > \n         </ dependency > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-web </ artifactId > \n         </ dependency > \n         < dependency > \n             < groupId > org.mybatis.spring.boot </ groupId > \n             < artifactId > mybatis-spring-boot-starter </ artifactId > \n             < version > 1.3.2 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > mysql </ groupId > \n             < artifactId > mysql-connector-java </ artifactId > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n         \x3c!--web entity--\x3e \n         < dependency > \n             < groupId > com.miu </ groupId > \n             < artifactId > entity </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n         \x3c!--web service--\x3e \n         < dependency > \n             < groupId > com.miu </ groupId > \n             < artifactId > service </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n         \x3c!--web dao--\x3e \n         < dependency > \n             < groupId > com.miu </ groupId > \n             < artifactId > dao </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n     </ dependencies > \n     < build > \n         < plugins > \n             < plugin > \n                 < groupId > org.springframework.boot </ groupId > \n                 < artifactId > spring-boot-maven-plugin </ artifactId > \n                 < configuration > \n                     \x3c!-- Main Class --\x3e \n                     < mainClass > com.miu.web.WebApplication </ mainClass > \n                     < layout > ZIP </ layout > \n                 </ configuration > \n                 < executions > \n                     < execution > \n                         < goals > \n                             < goal > repackage </ goal > \x3c!--Jar--\x3e \n                         </ goals > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n         </ plugins > \n     </ build > \n </ project > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86  \n  \n entity EntiyTest \n \n dao DaoTest \n \n serviceServiceTest \n \n WebWebTest \n \n webapplication.propertiesOKmysqlredis \n server.port = 8080 \n #--------------------------------------------------------------------------- \nspring.datasource.driver-class-name = com.mysql.jdbc.Driver\n spring.datasource.url = jdbc:mysql://127.0.0.1:3306/test?characterEncoding = utf8\n spring.datasource.username = root\n spring.datasource.password = 123 \n #------------------------------------redis--------------------------------------- \n spring.redis.database = 0 \n spring.redis.host = 127.0 .0.1\n spring.redis.port = 6379 \n spring.redis.password = \nspring.redis.jedis.pool.max-active = 8 \nspring.redis.jedis.pool.max-idle = 8 \nspring.redis.jedis.pool.max-wait = -1ms\nspring.redis.jedis.pool.min-idle = 0 \n spring.redis.timeout = 10000ms\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 web \n \n jar \n @Autowired  pom.xml packagecleancleanjarwar \n \n packageBUILD SUCCESS pom.xml \n \n webtargetjar \n \n jar java -jar web-0.0.1-SNAPSHOT.jar jar \n \n  \n \n  \n  \n webpom.xmlbuild \n  \n  \n1.pom.xml jarpombuild  \n 2.pom.xml commonutilsentityservice   \n 3. ../pom.xml \n 4.applicatin.properties \n 5.jarwarwarwartomcatSpringBoottomcatwartomcatjarjava -jar  \n 6.@Autowired  new \n @ComponentScan(basePackages = "")Mybatismapper \n@MapperScan("mapper") \n \n'},{title:"gitlabCICD",frontmatter:{title:"gitlabCICD",date:"2022-10-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["kafka",""]},regularPath:"/%E5%85%B6%E4%BB%96/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86.html",relativePath:"/.md",key:"v-61c5b94b",path:"/2022/10/08/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%8E%9F%E7%90%86/",headers:[{level:3,title:"",slug:""},{level:3,title:" DMA ?",slug:"-dma-"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"PageCache ",slug:"pagecache-"},{level:3,title:"",slug:""},{level:3,title:"",slug:""}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:"  \n  10  I/O I/O  \n  I/O  \n  DMA ? \n  DMA I/O  \n \n CPU  \n    \n CPU  CPU  \n \n  \n \n  CPU CPU  \n  CPU  \n  DMA  *Direct Memory Access*   \n  DMA   I/O  DMA  CPU  CPU   \n  DMA  \n \n  \n \n  read  I/O  \n  I/O  DMA CPU  \n DMA  I/O  \n  DMA  I/O  DMA  \n DMA  CPUCPU   \n  DMA  CPU \n CPU  DMA  \n \n  CPU  DMA  CPU  CPU  DMA  \n  DMA  I/O  I/O  DMA  \n  \n  \n  I/O  I/O  \n  \n read ( file ,  tmp_buf ,  len ) ; \n write ( socket ,  tmp_buf ,  len ) ; \n \n 1 2  \n \n   4    read()    write()  \n  \n   4   DMA  CPU  \n \n   DMA  \n   CPU  \n   socket  CPU  \n   socket  DMA  \n \n  4  CPU  \n  \n    \n  \n  2  \n \n mmap + write \n sendfile \n \n  \n  \n \n \n  \n  \n \n  I/O  \n  \n mmap + write \n  read()    mmap()    read()   \n buf  =   mmap ( file ,  len ) ; \n write ( sockfd ,  buf ,  len ) ; \n \n 1 2 mmap()     \n \n  \n \n   mmap()  DMA  \n   write()  socket  CPU  \n  socket  DMA  \n \n   mmap()    read()   \n  CPU  socket  4  2  \n sendfile \n  Linux  2.1   sendfile()  \ninclude   <sys/socket.h> \n ssize_t   sendfile ( int  out_fd ,   int  in_fd ,   off_t   * offset ,   size_t  count ) ; \n \n 1 2  \n   read()    write()   2  \n  socket  2  3  \n \n  CPU COPY  \n  sg sendfile \n Linux 2.4  scatter/gather  sendfile  CPU COPY  Read BUffer  Socket Buffer  Read Buffer  Socket Buffer  \n \n  *Zero-copy* CPU  DMA   \n  2   2  2  CPU2  DMA  \n    \n  \n Kafka  I/O  Kafka  \n  Kafka  Java NIO   transferTo   \n @Overridepublic  \n long   transferFrom ( FileChannel  fileChannel ,   long  position ,   long  count )   throws   IOException   {  \n     return  fileChannel . transferTo ( position ,  count ,  socketChannel ) ; \n } \n \n 1 2 3 4  Linux   sendfile()    transferTo()    sendfile()   \n   65%   \n \n https://developer.ibm.com/articles/j-zerocopy/ \n Nginx  \n http  { \n ... \n    sendfile on\n ... \n } \n \n 1 2 3 4 5 sendfile : \n \n  on sendfile  2  2  \n  off read + write 4  4  \n \n  sendfileLinux  2.1  \n PageCache  \n  *PageCache*  \n  PageCache  PageCache  \n  DMA  \n  \n  \n   PageCache   \n  PageCache  PageCache  \n  PageCache   \n  read   32 KB   read  0  32 KB  3264 KB  PageCache 3264 KB  3264 KB  PageCache  \n PageCache  \n \n  \n  \n \n  \n GB PageCache  DMA  PageCache  \n  GB  PageCache  PageCache  \n  2  \n \n PageCache  PageCache \n PageCache  DMA  PageCache  \n \n  PageCache PageCache  PageCache \n  \n  \n  read  read  \n \n  \n \n  read  I/O  I/O  \n  I/O  PageCache  \n  PageCache  read  \n \n  I/O  \n \n  \n \n    \n    \n \n  I/O  PageCache I/O  PageCache \n  PageCache  I/O  I/O PageCache  I/O  I/O I/O  I/O \n  PageCache PageCache  PageCache \n   I/O +  I/O  \n  I/O  \n \n  PageCache  MySQL  I/O \n  PageCache  PageCache  I/O \n \n  I/O  PageCache \n \n  I/O  I/O  PageCache    I/O  \n    I/O  PageCache  \n \n  I/O +  I/O \n  \n \n  I/O +  I/O \n  \n \n  nginx  \n location  / video /   {  \n     sendfile   on ;  \n     aio   on ;  \n    directio  1024m ;  \n } \n \n 1 2 3 4 5   directio   I/O +  I/O \n  \n  I/O  CPU  CPU  CPU  \n DMA  I/O  DMA  DMA CPU  DMA  DMA CPU  \n  IO  4  4  2  DMA  2  CPU  \n  sendfile   \n Kafka  Nginx  \n  PageCache PageCache  I/O  IO  \n  \n  PageCache  PageCache IO +  IO  \n  Nginx  IO  IO \n"},{title:"",frontmatter:{title:"",date:"2023-05-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["","spring"]},regularPath:"/%E5%85%B6%E4%BB%96/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0.html",relativePath:"/.md",key:"v-5692179e",path:"/2023/05/08/%E7%BE%8E%E5%9B%A2%E5%BC%80%E6%BA%90%E5%8A%A8%E6%80%81%E7%BA%BF%E7%A8%8B%E6%B1%A0/",headers:[{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:3,title:"1.",slug:"_1-"},{level:3,title:"2.nacos",slug:"_2-nacos"}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:'  \n  \n \n  https://dynamictp.cn \n gitee https://gitee.com/dromara/dynamic-tp \n github https://github.com/dromara/dynamic-tp \n  \n \n maven  \n  \n  @EnableDynamicTp  \n  @Resource  @Autowired  DtpRegistry.getDtpExecutor("name")  \n  4  \n   \n 1. \n 1.usermodule \n          < dependencies > \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-web </ artifactId > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-test </ artifactId > \n             < scope > test </ scope > \n         </ dependency > \n     </ dependencies > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 \n 2.bean \n import   org . springframework . boot . SpringApplication ; \n import   org . springframework . boot . autoconfigure . SpringBootApplication ; \n import   org . springframework . context . annotation . Bean ; \n\n import   java . util . concurrent . * ; \n\n @SpringBootApplication \n public   class   MyApplication   { \n\n     @Bean   //bean \n     public   ExecutorService   executorService ( ) { \n         return   new   ThreadPoolExecutor ( 10 ,   100 , \n                 0L ,   TimeUnit . MILLISECONDS , \n                 new   LinkedBlockingQueue < Runnable > ( ) ) ; \n     } \n\n     public   static   void   main ( String [ ]  args )   { \n         SpringApplication . run ( MyApplication . class , args ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 3.Controller \n import   org . springframework . web . bind . annotation . GetMapping ; \n import   org . springframework . web . bind . annotation . RestController ; \n\n import   javax . annotation . Resource ; \n import   java . util . concurrent . ExecutorService ; \n\n @RestController \n public   class   CommonExecutorsController   { \n\n     @Resource   // \n     private   ExecutorService  executorService ; \n\n     @GetMapping ( "/test" ) \n     public   Integer   test ( ) { \n        executorService . execute ( ( ) -> doTask ( ) ) ; \n         return   1 ; \n     } \n\n     public   void   doTask ( ) { \n\n\n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  \n application.yml \n dtp : \n   core-pool-size :   10 \n   maximum-pool-size :   100 \n \n 1 2 3  \n package   com . gordon . common ; \n\n import   org . springframework . beans . factory . annotation . Autowired ; \n import   org . springframework . boot . SpringApplication ; \n import   org . springframework . boot . autoconfigure . SpringBootApplication ; \n import   org . springframework . context . annotation . Bean ; \n import   org . springframework . core . env . Environment ; \n\n import   java . util . concurrent . ExecutorService ; \n import   java . util . concurrent . LinkedBlockingQueue ; \n import   java . util . concurrent . ThreadPoolExecutor ; \n import   java . util . concurrent . TimeUnit ; \n\n @SpringBootApplication \n public   class   MyApplication   { \n\n     @Autowired   // \n     public   Environment  environment ; \n\n     // CommonExecutorsController  \n     @Bean   //bean \n     public   ExecutorService   executorService ( ) { \n         Integer  corePoolSize  =   Integer . valueOf ( environment . getProperty ( "dtp.core-pool-size" ) ) ; \n         Integer  maximumPoolSize  =   Integer . valueOf ( environment . getProperty ( "dtp.maximum-pool-size" ) ) ; \n\n         // \n         System . out . println ( "corePoolSize = "   +  corePoolSize ) ; \n\n         return   new   ThreadPoolExecutor ( corePoolSize ,  maximumPoolSize , \n                 0L ,   TimeUnit . MILLISECONDS , \n                 new   LinkedBlockingQueue < Runnable > ( ) ) ; \n     } \n\n     public   static   void   main ( String [ ]  args )   { \n         SpringApplication . run ( MyApplication . class , args ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #  2.nacos \n nacosspringboot \n starternacos \n  \n < dependency > \n             < groupId > com.alibaba.boot </ groupId > \n             < artifactId > nacos-config-spring-boot-starter </ artifactId > \n             < version > 0.2.7 </ version > \n         </ dependency > \n \n 1 2 3 4 5  \n package   com . gordon . singleDtp ; \n\n import   java . util . concurrent . LinkedBlockingQueue ; \n import   java . util . concurrent . ThreadPoolExecutor ; \n import   java . util . concurrent . TimeUnit ; \n\n public   class   DtpExecutor   extends   ThreadPoolExecutor   { \n\n     public   DtpExecutor ( int  corePoolSize ,   int  maximumPoolSize )   { \n         super ( corePoolSize ,  maximumPoolSize , 0L ,   TimeUnit . MILLISECONDS , \n                 new   LinkedBlockingQueue < Runnable > ( ) ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14  \n package   com . gordon . singleDtp ; \n\n import   org . springframework . beans . factory . annotation . Autowired ; \n import   org . springframework . context . annotation . Bean ; \n import   org . springframework . context . annotation . Configuration ; \n import   org . springframework . core . env . Environment ; \n\n @Configuration \n public   class   AutoSingleDtpConfiguration    { //AutoSingleDtpConfiguration spring.factories \n\n     @Autowired   // \n     public   Environment  environment ; \n\n     // CommonExecutorsController  \n     @Bean   //bean \n     public   DtpExecutor   executor ( ) { \n         Integer  corePoolSize  =   Integer . valueOf ( environment . getProperty ( "dtp.core-pool-size" ) ) ; \n         Integer  maximumPoolSize  =   Integer . valueOf ( environment . getProperty ( "dtp.maximum-pool-size" ) ) ; \n\n         return   new   DtpExecutor ( corePoolSize , maximumPoolSize ) ; \n     } \n\n     @Bean \n     public   NacosListener   nacosListener ( ) { \n         return   new   NacosListener ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  \n 1.@Configuration \n 2.META-INFspring.factories \n org.springframework.boot.autoconfigure.EnableAutoConfiguration = com.gordon.singleDtp.AutoSingleDtpConfiguration \n \n 1  \n package   com . gordon . singleDtp ; \n\n import   com . alibaba . nacos . api . annotation . NacosInjected ; \n import   com . alibaba . nacos . api . config . ConfigService ; \n import   com . alibaba . nacos . api . config . listener . Listener ; \n import   org . springframework . beans . factory . InitializingBean ; \n import   org . springframework . beans . factory . annotation . Autowired ; \n import   org . springframework . beans . factory . config . YamlPropertiesFactoryBean ; \n import   org . springframework . core . io . ByteArrayResource ; \n\n import   java . util . Properties ; \n import   java . util . concurrent . Executor ; \n import   java . util . concurrent . Executors ; \n\n public   class   NacosListener   implements   Listener ,   InitializingBean   { \n\n     @NacosInjected \n     private   ConfigService  configService ; \n\n     @Autowired \n     private   DtpExecutor  executor ; \n\n     @Override   // \n     public   Executor   getExecutor ( )   { \n         return   Executors . newFixedThreadPool ( 1 ) ; \n     } \n\n     @Override \n     public   void   receiveConfigInfo ( String  content )   { \n\n         YamlPropertiesFactoryBean  bean  =   new   YamlPropertiesFactoryBean ( ) ; \n        bean . setResources ( new   ByteArrayResource ( content . getBytes ( ) ) ) ; \n         Properties  properties  =  bean . getObject ( ) ; \n         Integer  corePoolSize  =   Integer . valueOf ( properties . getProperty ( "dtp.core-pool-size" ) ) ; \n         Integer  maximumPoolSize  =   Integer . valueOf ( properties . getProperty ( "dtp.maximum-pool-size" ) ) ; \n        executor . setCorePoolSize ( corePoolSize ) ; \n        executor . setMaximumPoolSize ( maximumPoolSize ) ; \n\n     } \n\n     @Override \n     public   void   afterPropertiesSet ( )   throws   Exception   { \n        configService . addListener ( "dtp.yaml" , "DEFAULT_GROUP" , this ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 nacos \n nacos : \n   config : \n     server-addr :  127.0.0.1 : 8848 \n     data-id :  dtp.yaml\n     type :  yaml\n     auto-refresh :   true \n     bootstrap : \n       enable :   true \n \n 1 2 3 4 5 6 7 8 \n userstarter \n < dependency > \n             < groupId > com.example </ groupId > \n             < artifactId > dtp-spring-boot-stater-nacos </ artifactId > \n             < version > 0.0.1-SNAPSHOT </ version > \n         </ dependency > \n \n 1 2 3 4 5 usersingleDtp \n  \n package   com . gordon . singleDtp ; \n\n import   org . springframework . boot . SpringApplication ; \n import   org . springframework . boot . autoconfigure . SpringBootApplication ; \n\n @SpringBootApplication \n public   class   MyApplication   { \n\n     public   static   void   main ( String [ ]  args )   { \n         ConfigurableApplicationContext  ct  =   SpringApplication . run ( MyApplication . class ,  args ) ; \n         DtpExecutor  executor  =  ct . getBean ( DtpExecutor . class ) ; \n         // \n         System . out . println ( "SpringBootApplication = "   +  executor . getCorePoolSize ( ) ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 controller \n package   com . gordon . singleDtp . controller ; \n\n import   com . gordon . singleDtp . DtpExecutor ; \n import   org . springframework . beans . factory . annotation . Autowired ; \n import   org . springframework . web . bind . annotation . GetMapping ; \n import   org . springframework . web . bind . annotation . RestController ; \n\n @RestController \n public   class   SingleDtpExecutorsController   { \n     @Autowired \n     private   DtpExecutor  executor ; \n\n     @GetMapping ( "/test" ) \n     public   String   test ( ) { \n        executor . execute ( ( ) -> doTask ( ) ) ; \n\n         return  executor . getCorePoolSize ( ) + "" ; \n     } \n\n     public   void   doTask ( ) { \n\n\n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  \n'},{title:"IO",frontmatter:{title:"IO",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["IO"]},regularPath:"/%E5%85%B6%E4%BB%96/%E7%BD%91%E7%BB%9CIO%E6%A8%A1%E5%9E%8B.html",relativePath:"/IO.md",key:"v-227baaf0",path:"/2023/06/10/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/",headers:[{level:3,title:"",slug:""},{level:3,title:"IO",slug:"io"},{level:3,title:"IOBIO",slug:"iobio"},{level:3,title:"IONIO",slug:"ionio"},{level:3,title:"IO",slug:"io-"},{level:3,title:"IO",slug:"io"},{level:3,title:"IO IO(AIO)",slug:"io-io-aio"},{level:3,title:"Reactor",slug:"reactor"},{level:3,title:"",slug:""}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:'  \n IO \n IO \n IOBIONIOIOIOIO \n  \n        \n IOBIO \n \n  \n IONIO \n \n CPU \n IO \n \n IO   selectpollepoll  fd  recvfrom  \n fd(File Descriptor),\n \n 1 #  IOselect \n select fd  select  fd  select  recvfrom  \n  select  \n \n IOLinux1024 \n select  fdset  fd I/O \n IO poll \n    poll select poll   selectpoll socket      \n  epoll  \n IOepoll \n  I/O __bilibili \n  select/poll  epoll  \n epoll  epoll_ctl()  fd  fd  fd  epoll_wait()      \n \n \n  \n 1 epoll_create  epoll  epoll  fd \n 2 epoll_ctl  fd  epoll fd  \n 3 epoll_wait  epoll  \n 4 \n 5 DMA ring_buffer CPU  \n 6CPU  \n \n \n ipportsocketsocket \n \n \n  socket  socket  eventpoll  eventpoll RUNNING \n \n \n 7 epoll_wait  eventpoll  events  \n 8 events  \n 9 \n selectpollepoll \n \n \n \n \n select \n poll \n epoll \n \n \n \n \n  \n  \n  \n  \n \n \n fd \n  \n  \n  \n \n \n  \n O(n) \n O(n) \n O(1) \n \n \n  \n 1024 \n  \n  \n \n \n fd \n selectfd \n pollfd \n fd \n \n \n \n epoll IO epoll_wait()  IO  \n IO \n IO sigaction  SIGIO  SIGIO  recvfrom  \n \n IO  BIONIO AIO IO \n IO IO(AIO) \n  BIONIO    AIO IO  IO \n  \n \n IO \n  Reactor \n  \n  \n \n  Reactor  Reactor  I/O  \n \n CPU  \n  \n  I/O Select  \n  I/O  \n \n  Reactor Reactor  Handler  Handler  Handler  \n  Handler  Acceptor  \n Reactor \n 1.1  \nread requestbyte \ndecode requestbyte \nprocess (compute) service \nencode replybyte \nsend replybyte \n 1.2 Reactor Doug Lea \n \n Reactor - IOReactorThread PoolReactoracceptorHandler \nAcceptor - Handler \nHandler - readdecodecomputeencodesendhandler \nThread Pool - Thread Poolthreadworker threadHandlerdecodecomputeencodeworker threadHandlerreadsendReactorworker threadsocket. \n 1.3 Reactor \n \n readsend \nreadsendReactorReactorreadsend \n Readsend \nreadsendreadsendreadsendReactor \n  \nreaddecodeprocessencodesendreadsendReactordecodeprocessencodeworker thread \n Reactor \n \n  Reactor  \n  \n    IO  \n  Reactor  Reactor   Reactor    IO/  \n \n  Reactor  Reactor IO \n IO CPU IO \n \n  Reactor  Reactor  Selector MainReactor  Accept  SubReactor SubReactor  I/O  I/O  \n  SubReactor  \n  \n Doug LeaReactor \nReactorReactorReactor PoolReactorselectordispatch loopreadsendReactordecodeprocessencodeworker thread \n ReactoracceptorEvent Loop Event Loop PoolEvent LoopSelectorAcceptorEventLoopPoolEvent LoopEvent LoopHandler-readdecodeprocessencodesend-Handler \n 2.1  \n \n reactornettykafka \n  \n BIOBlocking IO \n 1 \n @Slf4j \n public   class   BioClient   { \n\n\t public   static   void   main ( String [ ]  args )   throws   IOException   { \n\t\t Socket  socket  =   new   Socket ( "127.0.0.1" ,   9099 ) ; \n\t\t //  \n\t\tsocket . getOutputStream ( ) . write ( "Hello BioServer" . getBytes ( ) ) ; \n\t\tsocket . getOutputStream ( ) . flush ( ) ; \n\t\tlog . info ( "" ) ; \n\t\t byte [ ]  bytes  =   new   byte [ 1024 ] ; \n\t\t // \n\t\tsocket . getInputStream ( ) . read ( bytes ) ; \n\t\tlog . info ( "{}" ,   new   String ( bytes ) ) ; \n\t\tsocket . close ( ) ; \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 2 \n @Slf4j \n public   class   BioServer   { \n\t public   static   void   main ( String [ ]  args )   throws   IOException   { \n\t\t ServerSocket  serverSocket  =   new   ServerSocket ( 9099 ) ; \n\t\t while   ( true )   { \n\t\t\tlog . info ( "" ) ; \n\t\t\t // \n\t\t\t Socket  socket  =  serverSocket . accept ( ) ; \n\t\t\tlog . info ( "" ) ; \n\t\t\t new   Thread ( ( )   ->   { \n\t\t\t\t try   { \n\t\t\t\t\t handler ( socket ) ; \n\t\t\t\t }   catch   ( IOException  e )   { \n\t\t\t\t\tlog . error ( e . getMessage ( ) ,  e ) ; \n\t\t\t\t } \n\t\t\t } ) . start ( ) ; \n\n\t\t } \n\t } \n\n\t private   static   void   handler ( Socket  socket )   throws   IOException   { \n\t\t byte [ ]  bytes  =   new   byte [ 1024 ] ; \n\n\t\tlog . info ( "" ) ; \n\t\t //  \n\t\t int  read  =  socket . getInputStream ( ) . read ( bytes ) ; \n\t\tlog . info ( "" ) ; \n\t\t if   ( read  !=   - 1 )   { \n\t\t\tlog . info ( "{}" ,   new   String ( bytes ,   0 ,  read ) ) ; \n\t\t } \n\t\t // outputStream \n\t\tsocket . getOutputStream ( ) . write ( "Hello BioClient" . getBytes ( ) ) ; \n\t\tsocket . getOutputStream ( ) . flush ( ) ; \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 3 \nBIO \n  \n \n  \n \n 4 \nBIO \n  \n 0 \n int  read  =  socket . getInputStream ( ) . read ( bytes ) ; \n \n 1 BIO \n 10W10W \n  \n BIO \n  \n BIO    \n NIONonBlocking IO \n 1 \n @Slf4j \n public   class   NioClient   { \n\t public   static   void   main ( String [ ]  args )   throws   IOException   { \n\n\n\t\t SocketChannel  sc  =   SocketChannel . open ( ) ; \n\t\tsc . connect ( new   InetSocketAddress ( "localhost" ,   8099 ) ) ; \n\n\t\tsc . write ( Charset . defaultCharset ( ) . encode ( "Hello NioServer" ) ) ; \n\t\tlog . info ( "" ) ; \n\t\t ByteBuffer  buffer  =   ByteBuffer . allocate ( 16 ) ; \n\n\t\t int  read  =  sc . read ( buffer ) ; \n\t\t if   ( read  >   0 )   { \n\t\t\tlog . info ( " {} " ,  read ) ; \n\t\t } \n\n\t\tsc . write ( Charset . defaultCharset ( ) . encode ( "Hello NioServer2" ) ) ; \n\t\tlog . info ( "" ) ; \n\n\t\t System . in . read ( ) ;   //  \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 2 \n @Slf4j \n public   class   NioServer   { \n\n\t public   static   void   main ( String [ ]  args )   throws   IOException ,   InterruptedException   { \n\t\t ByteBuffer  buffer  =   ByteBuffer . allocate ( 16 ) ; \n\t\t ServerSocketChannel  ssc  =   ServerSocketChannel . open ( ) ; \n\t\tssc . configureBlocking ( false ) ; // accept \n\t\tssc . bind ( new   InetSocketAddress ( 8099 ) ) ; \n\t\t List < SocketChannel >  channelList  =   new   ArrayList < > ( ) ; \n\t\t while   ( true )   { \n\t\t\tlog . info ( "" ) ; \n\t\t\t SocketChannel  accept  =  ssc . accept ( ) ; \n\t\t\t if   ( accept  !=   null )   { \n\t\t\t\tlog . info ( ""   +  accept ) ; \n\t\t\t\taccept . configureBlocking ( false ) ;   // read \n\t\t\t\tchannelList . add ( accept ) ; \n\t\t\t } \n\n\t\t\t for   ( SocketChannel  channel  :  channelList )   { \n\t\t\t\tlog . info ( "" ,  channel ) ; \n\t\t\t\t int  read  =  channel . read ( buffer ) ; \n\t\t\t\t if   ( read  >   0 )   { \n\t\t\t\t\tbuffer . flip ( ) ; \n\t\t\t\t\tlog . info ( " {} " ,  read ) ; \n\t\t\t\t\t ByteBuffer  retBuf  =   ByteBuffer . wrap ( "Hi NioClient" . getBytes ( ) ) ; \n\t\t\t\t\t TimeUnit . SECONDS . sleep ( 5 ) ;   //  \n\t\t\t\t\tchannel . write ( retBuf ) ; \n\t\t\t\t\tbuffer . clear ( ) ; \n\t\t\t\t\tlog . info ( "" ,  channel ) ; \n\t\t\t\t } \n\t\t\t } \n\t\t } \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 3 \nNIOBIOJavaNIONIONIO \n  \n \n  \n \n 4 \n NIOBIOCPU \n Multiplexing \n 1 \n  \n channelSelectorchannel \nselectorReactor \n @Slf4j \n public   class   MultioClient   { \n\n\t private   Selector  selector ; \n\n\t public   static   void   main ( String [ ]  args )   throws   IOException   { \n\t\t MultioClient  client  =   new   MultioClient ( ) ; \n\t\tclient . initClient ( "127.0.0.1" ,   9099 ) ; \n\t\tclient . connect ( ) ; \n\t } \n\n\t /**\n\t * Socket\n\t *\n\t * @param ip   ip\n\t * @param port \n\t */ \n\t public   void   initClient ( String  ip ,   int  port )   throws   IOException   { \n\t\t // Socket \n\t\t SocketChannel  channel  =   SocketChannel . open ( ) ; \n\t\t //  \n\t\tchannel . configureBlocking ( false ) ; \n\t\t //  \n\t\t this . selector  =   Selector . open ( ) ; \n\t\t // ,listen \n\t\t // channel.finishConnect()  \n\t\tlog . info ( "1" ) ; \n\t\tchannel . connect ( new   InetSocketAddress ( ip ,  port ) ) ; \n\t\t // SelectionKey.OP_CONNECT \n\n\t\tchannel . register ( selector ,   SelectionKey . OP_CONNECT ) ; \n\t } \n\n\t /**\n\t * selector\n\t */ \n\t public   void   connect ( )   throws   IOException   { \n\t\t // selector \n\t\t while   ( true )   { \n\t\t\tselector . select ( ) ; \n\t\t\t // selector \n\t\t\t Iterator < SelectionKey >  it  =   this . selector . selectedKeys ( ) . iterator ( ) ; \n\t\t\t while   ( it . hasNext ( ) )   { \n\t\t\t\t SelectionKey  key  =  it . next ( ) ; \n\t\t\t\t // key, \n\t\t\t\tit . remove ( ) ; \n\t\t\t\t //  \n\t\t\t\t if   ( key . isConnectable ( ) )   { \n\t\t\t\t\t SocketChannel  channel  =   ( SocketChannel )  key . channel ( ) ; \n\t\t\t\t\t //  \n\t\t\t\t\t if   ( channel . isConnectionPending ( ) )   { \n\t\t\t\t\t\tchannel . finishConnect ( ) ; \n\t\t\t\t\t } \n\t\t\t\t\t //  \n\t\t\t\t\tchannel . configureBlocking ( false ) ; \n\t\t\t\t\t //  \n\t\t\t\t\t ByteBuffer  buffer  =   ByteBuffer . wrap ( "Hello MultServer" . getBytes ( ) ) ; \n\n\t\t\t\t\tlog . info ( "3" ) ; \n\t\t\t\t\tchannel . write ( buffer ) ; \n\t\t\t\t\t //  \n\t\t\t\t\tchannel . register ( this . selector ,   SelectionKey . OP_READ ) ;    //  \n\t\t\t\t }   else   if   ( key . isReadable ( ) )   { \n\t\t\t\t\tlog . info ( "6" ) ; \n\t\t\t\t\t read ( key ) ; \n\t\t\t\t } \n\n\t\t\t } \n\t\t } \n\t } \n\n\t /**\n\t * \n\t *\n\t * @param key\n\t */ \n\t public   void   read ( SelectionKey  key )   throws   IOException   { \n\t\t //read \n\t\t // :Socket \n\t\t SocketChannel  channel  =   ( SocketChannel )  key . channel ( ) ; \n\t\t //  \n\t\t ByteBuffer  buffer  =   ByteBuffer . allocate ( 1024 ) ; \n\t\t int  len  =  channel . read ( buffer ) ; \n\t\t if   ( len  !=   - 1 )   { \n\t\t\tlog . info ( ""   +   new   String ( buffer . array ( ) ,   0 ,  len ) ) ; \n\t\t } \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89  \n \n channelselectorchannelSocketChannelchannelServerSocketChannel \nReactor \n @Slf4j \n public   class   MultioServer   { \n\n\n\t public   static   void   main ( String [ ]  args )   throws   IOException   { \n\t\t // Socket. \n\t\t ServerSocketChannel  ssc  =   ServerSocketChannel . open ( ) ; \n\t\t // selectorselector \n\t\tssc . configureBlocking ( false ) ; \n\t\tssc . socket ( ) . bind ( new   InetSocketAddress ( 9099 ) ) ; \n\t\t // selector \n\t\t Selector  selector  =   Selector . open ( ) ; \n\t\t // ServerSocketChannelselectorselectoraccept \n\t\tssc . register ( selector ,   SelectionKey . OP_ACCEPT ) ; \n\n\t\t while   ( true )   { \n\t\t\tlog . info ( "" ) ; \n\t\t\t // channelkeyselectaccept() \n\t\t\t int  select  =  selector . select ( ) ; \n\n\t\t\tlog . info ( "" ) ; \n\t\t\t //  \n\t\t\t Iterator < SelectionKey >  it  =  selector . selectedKeys ( ) . iterator ( ) ; \n\t\t\t while   ( it . hasNext ( ) )   { \n\t\t\t\t SelectionKey  key  =  it . next ( ) ; \n\t\t\t\t //keyselect \n\t\t\t\tit . remove ( ) ; \n\t\t\t\t handle ( key ) ; \n\t\t\t } \n\t\t } \n\t } \n\n\t /**\n\t * @param key key\n\t */ \n\t private   static   void   handle ( SelectionKey  key )   throws   IOException   { \n\t\t if   ( key . isAcceptable ( ) )   { \n\n\t\t\tlog . info ( "2[]" ) ; \n\t\t\t ServerSocketChannel  ssc  =   ( ServerSocketChannel )  key . channel ( ) ; \n\t\t\t // NIOaccept \n\t\t\t //  \n\t\t\t SocketChannel  sc  =  ssc . accept ( ) ; \n\t\t\tsc . configureBlocking ( false ) ; \n\t\t\t // SelectorChannel \n\t\t\tsc . register ( key . selector ( ) ,   SelectionKey . OP_READ ) ; \n\t\t }   else   if   ( key . isReadable ( ) )   { \n\n\t\t\tlog . info ( "4[]" ) ; \n\t\t\t SocketChannel  sc  =   ( SocketChannel )  key . channel ( ) ; \n\t\t\t ByteBuffer  buffer  =   ByteBuffer . allocate ( 1024 ) ; \n\t\t\t // NIOreadread \n\t\t\t int  len  =  sc . read ( buffer ) ; \n\t\t\t if   ( len  !=   - 1 )   { \n\t\t\t\tlog . info ( "" , new   String ( buffer . array ( ) ,   0 ,  len ) ) ; \n\t\t\t } \n\t\t\t //  \n\t\t\t ByteBuffer  bufferToWrite  =   ByteBuffer . wrap ( "Hello MultClient" . getBytes ( ) ) ; \n\t\t\tsc . write ( bufferToWrite ) ; \n\t\t\t // key.isWritable()  key.isReadable() \n\t\t\tkey . interestOps ( SelectionKey . OP_READ   |   SelectionKey . OP_WRITE ) ; \n\t\t }   else   if   ( key . isWritable ( ) )   { \n\n\t\t\tlog . info ( "5[]" ) ; \n\t\t\t SocketChannel  sc  =   ( SocketChannel )  key . channel ( ) ; \n\t\t\t // NIO \n\t\t\t // JavaNIO \n\t\t\t //  \n\t\t\tkey . interestOps ( SelectionKey . OP_READ ) ; \n\t\t } \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 3 \n \n  \n \n \nkey.isReadable() \n \n \n \n \n  \n \n 4 \n Reactor \n NettyIO \n NettyIO \n 1 \nNetty \n 1.group \n2.channel \n3.handlerhandlerChannelInboundHandler4.ChannelOutboundHandler \n5. \n6.sync \n @Slf4j \n public   class  netty  { \n\t public   static   void   main ( String [ ]  args )   throws   InterruptedException ,   IOException   { \n\n\t\t new   Bootstrap ( ) \n\t\t\t\t . group ( new   NioEventLoopGroup ( ) ) \n\t\t\t\t . channel ( NioSocketChannel . class ) \n\t\t\t\t . handler ( new   ChannelInitializer < NioSocketChannel > ( )   { \n\t\t\t\t\t @Override \n\t\t\t\t\t protected   void   initChannel ( NioSocketChannel  ch )   throws   Exception   { \n\t\t\t\t\t\tch . pipeline ( ) . addLast ( new   StringEncoder ( ) ) ; \n\t\t\t\t\t\tch . pipeline ( ) . addLast ( new   StringDecoder ( ) ) ;                          // inputStream \n\t\t\t\t\t\tch . pipeline ( ) . addLast ( new   ChannelInboundHandlerAdapter ( )   { \n\t\t\t\t\t\t\t //  \n\t\t\t\t\t\t\t @Override \n\t\t\t\t\t\t\t public   void   channelRead ( ChannelHandlerContext  ctx ,   Object  msg )   { \n\t\t\t\t\t\t\t\tlog . info ( "4netty{}" ,  msg ) ; \n\t\t\t\t\t\t\t } \n\n\t\t\t\t\t\t\t @Override \n\t\t\t\t\t\t\t public   void   channelActive ( ChannelHandlerContext  ctx )   { \n\t\t\t\t\t\t\t\tlog . info ( "1" ) ; \n\t\t\t\t\t\t\t\tctx . writeAndFlush ( "Hello NettyServer" ) ; \n\t\t\t\t\t\t\t } \n\t\t\t\t\t\t } ) ; \n\t\t\t\t\t } \n\t\t\t\t } ) \n\t\t\t\t . connect ( new   InetSocketAddress ( "localhost" ,   8099 ) ) \n\t\t\t\t . sync ( ) ; \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  \nBootstrapServerBootstrapinitChannel \n StringEncoderStringDecoderwriteAndFlush \n @Slf4j \n public   class  netty  { \n\t public   static   void   main ( String [ ]  args )   { \n\n\t\t new   ServerBootstrap ( ) \n\t\t\t\t . group ( new   NioEventLoopGroup ( ) ) \n\t\t\t\t . channel ( NioServerSocketChannel . class ) \n\t\t\t\t . childHandler ( new   ChannelInitializer < NioSocketChannel > ( )   { \n\t\t\t\t\t @Override \n\t\t\t\t\t protected   void   initChannel ( NioSocketChannel  ch )   throws   Exception   { \n\t\t\t\t\t\tch . pipeline ( ) . addLast ( new   StringEncoder ( ) ) ; \n\t\t\t\t\t\tch . pipeline ( ) . addLast ( new   StringDecoder ( ) ) ; \n\t\t\t\t\t\tch . pipeline ( ) . addLast ( new   ChannelInboundHandlerAdapter ( )   { \n\n\t\t\t\t\t\t\t @Override \n\t\t\t\t\t\t\t public   void   channelRead ( ChannelHandlerContext  ctx ,   Object  msg )   throws   Exception   { \n\t\t\t\t\t\t\t\tlog . info ( "2{}" ,  msg ) ; \n\t\t\t\t\t\t\t } \n\n\t\t\t\t\t\t\t @Override \n\t\t\t\t\t\t\t public   void   channelReadComplete ( ChannelHandlerContext  ctx )   throws   Exception   { \n\t\t\t\t\t\t\t\tlog . info ( "3" ) ; \n\t\t\t\t\t\t\t\tctx . channel ( ) . writeAndFlush ( "Hello NettyClient" ) ; \n\t\t\t\t\t\t\t } \n\t\t\t\t\t\t } ) ; \n\t\t\t\t\t } \n\t\t\t\t } ) \n\t\t\t\t . bind ( 8099 ) ; \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 3 \n  \n \n  \n \n 4 \n NettyChannelHandlerNetty \n AIOAsynchronous IO \n 1 \n \n ChannelAsynchronousSocketChannel \n \n @Slf4j \n public   class   AIOClient   { \n\n\t public   static   void   main ( String . . .  args )   throws   Exception   { \n\t\t AsynchronousSocketChannel  socketChannel  =   AsynchronousSocketChannel . open ( ) ; \n\t\tsocketChannel . connect ( new   InetSocketAddress ( "127.0.0.1" ,   9000 ) ) . get ( ) ; \n\t\tsocketChannel . write ( ByteBuffer . wrap ( "Hello AioServer" . getBytes ( ) ) ) ; \n\t\t ByteBuffer  buffer  =   ByteBuffer . allocate ( 512 ) ; \n\t\t Integer  len  =  socketChannel . read ( buffer ) . get ( ) ; \n\t\t if   ( len  !=   - 1 )   { \n\t\t\tlog . info ( ""   +   new   String ( buffer . array ( ) ,   0 ,  len ) ) ; \n\t\t } \n\t } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 2. \n @Slf4j \n public   class   AIOServer   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         final   AsynchronousServerSocketChannel  serverChannel  = \n                 AsynchronousServerSocketChannel . open ( ) . bind ( new   InetSocketAddress ( 9000 ) ) ; \n\n        serverChannel . accept ( null ,   new   CompletionHandler < AsynchronousSocketChannel ,   Object > ( )   { \n             @Override \n             public   void   completed ( AsynchronousSocketChannel  socketChannel ,   Object  attachment )   { \n                 try   { \n                     //  \n                    serverChannel . accept ( attachment ,   this ) ; \n                    log . info ( socketChannel . getRemoteAddress ( ) . toString ( ) ) ; \n                     ByteBuffer  buffer  =   ByteBuffer . allocate ( 1024 ) ; \n                    socketChannel . read ( buffer ,  buffer ,   new   CompletionHandler < Integer ,   ByteBuffer > ( )   { \n                         @Override \n                         public   void   completed ( Integer  result ,   ByteBuffer  buffer )   { \n                            buffer . flip ( ) ; \n                            log . info ( new   String ( buffer . array ( ) ,   0 ,  result ) ) ; \n                            socketChannel . write ( ByteBuffer . wrap ( "Hello AioClient" . getBytes ( ) ) ) ; \n                         } \n\n                         @Override \n                         public   void   failed ( Throwable  exc ,   ByteBuffer  buffer )   { \n                            exc . printStackTrace ( ) ; \n                         } \n                     } ) ; \n                 }   catch   ( IOException  e )   { \n                    e . printStackTrace ( ) ; \n                 } \n             } \n\n             @Override \n             public   void   failed ( Throwable  exc ,   Object  attachment )   { \n                exc . printStackTrace ( ) ; \n             } \n         } ) ; \n\n         Thread . sleep ( Integer . MAX_VALUE ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 3 \n  \n  \n \n  \n \n 4 \n  JDK7  \n LinuxAIOEpollAIOJDKLinuxAIONettyNettyJDKNIO \n \n'},{frontmatter:{},regularPath:"/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%84%9A%E6%9C%AC/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html",relativePath:"/git.md",key:"v-e7d27b94",path:"/1970/01/01/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/",headers:[{level:2,title:"git config",slug:"git-config"},{level:2,title:"git clone",slug:"git-clone"},{level:2,title:"git init",slug:"git-init"},{level:2,title:"git status",slug:"git-status"},{level:2,title:"git remote",slug:"git-remote"},{level:2,title:"git branch",slug:"git-branch"},{level:2,title:"git checkout",slug:"git-checkout"},{level:2,title:"git cherry-pick",slug:"git-cherry-pick"},{level:2,title:"git add",slug:"git-add"},{level:2,title:"git commit",slug:"git-commit"},{level:2,title:"git fetch",slug:"git-fetch"},{level:2,title:"git merge",slug:"git-merge"},{level:2,title:"git diff",slug:"git-diff"},{level:2,title:"git pull",slug:"git-pull"},{level:2,title:"git push",slug:"git-push"},{level:2,title:"git log",slug:"git-log"},{level:2,title:"git reset",slug:"git-reset"},{level:2,title:"git revert",slug:"git-revert"},{level:2,title:"git tag",slug:"git-tag"},{level:2,title:"git mv",slug:"git-mv"},{level:2,title:"git rm",slug:"git-rm"},{level:2,title:"Git",slug:"git"},{level:3,title:"1. ",slug:"_1-"}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:' git  init   # \n git  clone https://gitlab.com/GordonChanFZ/recommendsystem.git  # \n git  remote  add  origin https://gitlab.com/GordonChanFZ/recommendsystem.git    # \n git  branch  #  "*"  \n git  branch chenguobin  # \n git   add  \n git  commit\n git  push origin chenguobin:chenguobin  # \n \n 1 2 3 4 5 6 7 8 #  git config \n \n--local--global--system \n$ git config  < - - local  |   - - global  |   - - system >   - l\n \n$ git config  - l\n \n--local--global--system \n$ git config  < - - local  |   - - global  |   - - system >   - e\n \n--local--global--system \n$ git config  < - - local  |   - - global  |   - - system >   - - add  < name >   < value > \n \n$ git config  < - - local  |   - - global  |   - - system >   - - get  < name > \n \n$ git config  < - - local  |   - - global  |   - - system >   - - unset  < name > \n \n$ git config  - - global user . name  <  > \n$ git config  - - global user . email  <  > \nGit \n \nB524288000500MB \n$ git config  - - global http . postBuffer  <  > \n git status/git diff  \n$ git config  - - global color . ui  true \n15 \n$ git config  - - global credential . helper cache\n \n \n$ git config  - - global credential . helper  \'cache --timeout=<>\' \n \n$ git config  - - global credential . helper store\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 #  git clone \n  \n \n$  git  clone  <  > \n \n$  git  clone  <  >   <  > \n-b master \n$  git  clone  <  >   -b   <  >   <  > \n \n 1 2 3 4 5 6 7 8 #  git init \n  .git  \n .git  \n$ git init\n \n 1 2 #  git status \n  \n \n$ git status\n \n \nA M D ?? Git \n$ git status  - s\n \n 1 2 3 4 5 6 7 #  git remote \n  \n \n$ git remote\nURL \n$ git remote  - v\n$ git remote  - - verbose\n \n$ git remote add  <  >   <  URL  > \n \n$ git remote rename  <  >   <  > \n \n$ git remote remove  <  > \n URL  \n$ git remote set - url  <  >   <  URL  > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #  git branch \n  Git  \n "*"  \n$ git branch\n "*"  \n$ git branch  - v\n \n$ git branch  <  > \n \n \n$ git branch  - m  [ <  > ]   <  > \n \n$ git branch  - M   [ <  > ]   <  > \n \n$ git branch  - d  <  > \n \n$ git branch  - D   <  > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #  git checkout \n  \n \n$ git checkout  <  > \n \n "git branch"  "git checkout"  \n$ git checkout  - b  <  > \n \n$ git checkout  - - orphan  <  > \n \n$ git checkout  <  > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 #  git cherry-pick \n  \n \n$ git cherry - pick  < commit  ID > \n \n 1 2 #  git add \n  git commit  \n \n$ git  add   <  > \n \n$ git  add   - u  [ <  > ] \n$ git  add   -- update  [ <  > ] \n <>  \n$ git  add   - A  [ <  > ] \n$ git  add   -- all  [ <  > ] \n \n$ git  add   - i  [ <  > ] \n$ git  add   -- interactive  [ <  > ] \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #  git commit \n  \n \n$ git commit\n \n$ git commit  - m  "<>" \n \n "git add -u" \n$ git commit  - a  - m  "<>" \n \n$ git commit  - - amend\n \n 1 2 3 4 5 6 7 8 9 10 11 12 #  git fetch \n  tmp  \n \n$ git fetch  <  > \n \n$ git fetch  <  >   <  > \n \n 1 2 3 4 5 #  git merge \n  \n \n$ git merge  <  > \n \n 1 2 #  git diff \n  \n \n$ git diff\n \n$ git diff  - - cached\n$ git diff  - - staged\n \n$ git diff  HEAD \n \n$ git diff  < commit  ID > \n \n$ git diff  <  >   <  > \n \n$ git diff  <  > ... <  > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #  git pull \n  \n  git fetch   git merge  HEAD  \n \n$ git pull\n \n 1 2 #  git push \n  \n \n$ git push  <  >   <  > : <  > \n 87 cb0b967095a6c07d49505768137528\n \n$ git push  <  >   : <  > \n$ git push  <  >   - - delete  <  > \n \n 1 2 3 4 5 6 #  git log \n  \n \n$  git  log\n \n$  git  log  < commit ID > \n \n$  git  log - <  > \n \n 1 2 3 4 5 6 7 8 #  git reset \n  \n \n "git add"  \n commit ID  HEAD \n$ git reset  [ <  > ] \n$ git reset  - - mixed  [ <  > ] \n HEAD  \n$ git reset  < commit  ID > \n$ git reset  - - mixed  < commit  ID > \n HEAD  \n "git reset --mixed"  "git add" \n$ git reset  - - soft  < commit  ID > \n HEAD  \n$ git reset  - - hard  < commit  ID > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #  git revert \n  \n \n$ git revert  < commit  ID > \n \n 1 2 #  git tag \n  \n \n$ git tag\n \n$ git tag  <  >   [ < commit  ID > ] \n \n$ git tag  - a  <  >   - m  <  >   [ < commit  ID > ] \n \n$ git checkout  <  > \n \n$ git show  <  > \n \n$ git tag  - d  <  > \n \n$ git push  <  >   <  > \n \n$ git push  <  >  tags\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #  git mv \n  \n \n$ git mv  <  /  >   <  /  > \n \n 1 2 #  git rm \n  \n \n$ git rm  <  > \n \n$ git rm  - r  <  > \n \n$ git rm  - - cached\n \n 1 2 3 4 5 6 7 8 #  Git \n 1.  \n   git branch --all   \n pull  -p  \n$ git pull  - p\n \n$ git fetch  - p\n$ git fetch  - - prune origin\n \n 1 2 3 4 5 6 df -h  \n du -h --max-depth=1 \n yum clean all yum \n find -type f -size +50M \n conda clean -p \n'},{frontmatter:{},regularPath:"/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/redis.html",relativePath:"/redis.md",key:"v-7f30b557",path:"/1970/01/01/redis/",headers:[{level:2,title:"",slug:""},{level:2,title:"redis",slug:"redis"},{level:3,title:"Redis",slug:"redis"},{level:3,title:"String",slug:"string"},{level:3,title:"Hash",slug:"hash"},{level:3,title:"List",slug:"list"},{level:3,title:"Set",slug:"set"},{level:3,title:"ZSet",slug:"zset"},{level:2,title:"Redis",slug:"redis"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"Redis",slug:"redis"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"redis",slug:"redis"},{level:3,title:"RDB",slug:"rdb"},{level:3,title:"AOF",slug:"aof"},{level:3,title:"",slug:""},{level:2,title:"redis",slug:"redis"},{level:3,title:"Redis",slug:"redis"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"Redis",slug:"redis"},{level:3,title:"LuaRedis",slug:"luaredis"},{level:2,title:"Redis",slug:"redis"},{level:2,title:"",slug:""},{level:3,title:"1.big key",slug:"_1-big-key"}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:' title: redis \ndate: 2022-09-08 \nauthor: Gordon \nsidebar: \'auto\' \ncategories: \n \n  \n nosql \ntags: \n kv \n  \n  \n Redisc \n RedisRemote Dictionary ServerRedis Redis  GET/SETINCR/DECRHGET/HSETLPUSH/RPUSHSADD/SMEMBERSZADD/ZRANGE Redis / \n  \n redis \n  Redis  \n Redis   redisObject  keyvalueredisObject StringHashListSetZSet  \n redisObject  redis.h credisObjectredisObject \n \nredisObjecttypeencodingencoding \nencoding \n \n Redis \n  \n Redis key 234 intembstr \n String \n StringRedisRediscRedisc \n String intrawembstr  \n int \n Redis   set num 123  intredisObject ptr  \n \n SDSembstr->raw \n  44  SDSsimple dynamic string encodingraw 44 encodingembstr \n (3.239) \n SDS  SDSRedis int lenint freechar buf[]  \n lenfreebufbuf \n RedsiHelloRedisSDSredisObject \n \n SDS       \n SDS free  \n  len1MBlenlen=freelen1MBfree1MB  \n SDSc \n String \n RedisStringString \n 1 Base64  \n /**\n\n   * Base64\n\n   * @param file\n\n   * @return\n\n   */ \n\n   public   static   String   encodeImg ( MultipartFile  file )   { \n\n     byte [ ]  imgBytes  =   null ; \n\n    try   { \n\n      imgBytes  =  file . getBytes ( ) ; \n\n     }   catch   ( IOException  e )   { \n\n      e . printStackTrace ( ) ; \n\n    } \n\n     BASE64Encoder  encoder  =   new   BASE64Encoder ( ) ; \n\n     return  imgBytes == null ? null : encoder . encode ( imgBytes  ) ; \n\n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 2Redis \n /**\n\n   * Redis\n\n   * @param file\n\n   * @return\n\n   */ \n\n   public   void   uploadImageServiceImpl ( MultipartFile  image )   { \n\n     String  imgId  =   UUID . randomUUID ( ) . toString ( ) ; \n\n    String  imgStr =   ImageUtils . encodeImg ( image ) ; \n\n    redisUtils . set ( imgId  ,  imgStr ) ; \n\n     // imgIdredisredis \n\n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 String   \n  Hash  \n Hash ziplisthashtable hashtablekeyStringvalue key value  \n redis.conf \n \n hashziplisthashtable \n a64byte \n b512 \n hashtablehashtablehashtableHashMap \n  \n  key  HashMaphash  hashtablehashsizemask   \n hashhashhashhashtablekeyindex   \n \n rehash \n valuedictEntryhashhash \n HashMaprehash ht[0]  ht[1]  \n hash dictEntry **tableunsigned long sizeunsigned long sizemaskunsigned long used  hashsize-1hash  \n ht[0]ht[0]ht[1]ht[0]ht[1] \n ht[1] ht[0].used  2 ht[0].used  2  \n ht[0]rehashht[1]ht[0]ht[1]ht[0]ht[1] \n rehash \n rehashRedisrehashRedisRedis rehash  \n Redisrehashrehash rehashindex  rehashindex -1rehash  \n rehash0rehash ht[0]ht[1] ht[0]ht[1] \n ht[1]ht[0] ht[0] rehash \n ziplist \n  ziplist  \n    \n \n  \n \n \n zlbytes 4 \n \n \n zltail 4 \n \n \n zllen 2 \n \n \n entry  \n \n \n zlend  \'0xFF\'  \n \n \n entry previous_entry_lengthencodingcontent  \n \n \n previous_entry_length entry \n \n \n encodingcontent \n \n \n contentcontent \n \n \n hash \n  ht \n   IDkeyvalue \n  \n \n    \n Redishashvaluek v \n \n ID \n IDredis \n   // offsetid \n\n   public   Long   getId ( String  key , String  hashKey , Long  offset )   throws   BusinessException { \n\n     try   { \n\n       if   ( null   ==  offset )   { \n\n        offset = 1L ; \n\n       } \n\n       // id \n\n       return  redisUtil . increment ( key ,  hashKey ,  offset ) ; \n\n     }   catch   ( Exception  e )   { \n\n       //uuidid \n\n       int  randNo = UUID . randomUUID ( ) . toString ( ) . hashCode ( ) ; \n\n       if   ( randNo  <   0 )   { \n\n        randNo = - randNo ; \n\n       } \n\n      return   Long . valueOf ( String . format ( "%16d" ,  randNo ) ) ; \n\n     } \n\n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 UUID \n UUID \n \n  \n  \n \n Redis \n redisincr   +  +  +  + redis \n  \n \n  \n \n  \n \n redis \n ID \n \n   redis \n  List  \n Redis3.2 ziplist  linkedlist 3.2 quicklist ziplist+linkedlist \n redis.conf \n \n  (List)  \nZipList)LinkedList \n ziplist \n 1.64 \n 2.512 \n ziplistlinkedlistquicklist \n linkedlistO(1)O(n) \n linkedlistquicklistcRedis \n \n Redis \n \n  \n prevnextnull \n O(1) \n \n  \n Set \n Redis SetList SetZSet \n Set htintset  \n intsetht \n 1. \n 2.512  \n htinset \n inset int16_t  int32_t   int64_t   \n  encodinglengthcontents[] lengthcontents \n  \n \n \n  \n \n \n  \n \n \n  \n \n \n SRANDMEMBER key [count] \n \n SINTER key1 [key2] \n \n  \n Set   \n  ZSet  \n ZSetZSet ziplist  skiplist ziplistskiplist \n redis.conf \n \n ziplistskiplist \n 1.128 \n 2.64 \n skiplist    \n skiplist \n \n \n  \n \n \n  \n \n \n  \n \n \n  \n \n \n  \n \n headtaillevellenBW \n BWscore \n  \n  \n \n \n \n \n \n \n \n \n \n ZREVRANGE key start stop [WITHSCORES]     \n PV  ZREVRANGE pv_zset 0 -1 \n \n \n \n  \n ZSetscoreZSet10 \n  Redis  \n Redis Redis maxmemory  Redis  \n RedisRedis   keykey  \n Redis \n   \n Redis 8  noeviction 6 \n \n noeviction (  ) \n allkeys-lru key LRU  \n volatile-lru  keyLRU  \n volatile-lfu \n allkeys-lfu \n allkeys-random key   \n volatile-random  key  \n volatile-ttl key   \n \n Redis     allkeys-lru  \n  allkeys-random  \n  redis.conf  \n \n  \n // maxmemory-policy \n 127.0.0.1:6379> config get maxmemory-policy \n // maxmemory-policyallkeys-lru \n 127.0.0.1:6379> config set maxmemory-policy allkeys-lru \n 8LRU LRU \n LRU \n LRU(Least Recently Used) key \n  key  \n RedisLRULRULRU \n RedisLRU key5keykey  \n 5key \n \n LRULRU   \n LRURediskeykey3 \n Redis 3.0LRURedis 16  \n  \n    \n keykey \n keykey \n LRU \n 1.key \n 2. \n Redis 4.0LRULFU LFU \n LFU \n LFU(Least Frequently Used) key \n keykey \n LFUkeyLRU \n LFU volatile-lfu  allkeys-lfu  \n Redis88Redis \n   \n Redis \n \n \n  key \n \n \n  keykey \n \n \n  key \n \n \n     cpu cpu \n   cpu cpu  key \n **keykey** \n   key cpu  \n RDBAOF  \n Redis RDB  AOF  \n RDBRDB save  bgsave RDB keyRDBkey \n RedisRDB Master key Slave key \n AOFRedisRewrite REWRITEAOF  BGREWRITEAOF  keyAOFkey  \n  Redis  \n  \n        /   [] \n      /  \n  NoSQL        \n Redis  NoSQL  Redis    \n      \n        \n   \n   key   key   \n    \n \n \n   \n \n \n reids  \n \n \n  \n \n      \n keyvalue load      synchronized  Lock  \n redis \n // \n public   String   getProduceNum ( String  key )   { \n     //  \n     RLock  lock  =  redissonClient . getLock ( key ) ; \n     try   { \n         //  \n         int  num =   Integer . parseInt ( redisTemplate . opsForValue ( ) . get ( key ) ) ;   \n         //             \n        lock . lock ( ) ; \n         if   ( num >   0 )   { \n             // \n            redisTemplate . opsForValue ( ) . set ( key ,   ( num  -   1 )   +   "" ) ; \n             System . out . println ( "num"   +   ( num -   1 ) ) ; \n         }   else   { \n             System . out . println ( "0" ) ; \n         } \n     }   catch   ( NumberFormatException  e )   { \n        e . printStackTrace ( ) ; \n     }   finally   { \n         // \n        lock . unlock ( ) ; \n     } \n     return   "OK" ; \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #    \n    \n \n \n   \n \n \n   \n \n \n  \n  \n      \n // \n public   class   UserServiceImpl   { \n      @Autowired \n      UserDAO  userDAO ; \n      @Autowired \n      RedisCache  redisCache ; \n \n      public   User   findUser ( Integer  id )   { \n           Object  object  =  redisCache . get ( Integer . toString ( id ) ) ; \n           //  \n           if ( object  !=   null )   { \n                // null \n                if ( object  instanceof   NullValueResultDO )   { \n                     return   null ; \n                } \n                return   ( User ) object ; \n           }   else   {   \n                //  \n                User  user  =  userDAO . getUser ( id ) ; \n                //  \n                if ( user  !=   null )   { \n                    redisCache . put ( Integer . toString ( id ) , user ) ; \n                }   else   { \n                     //  \n                     //redisCache.put(Integer.toString(id), new NullValueResultDO()); \n                    // \n // 60 \nredisCache . put ( Integer . toString ( id ) ,   new   NullValueResultDO ( ) , 60 ) ; \n                } \n                return  user ; \n           } \n      }           \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  \n                  \n    \n    \n \n \n    01 \n \n \n   \n \n \n    \n \n \n   False Negative \n \n \n   False Positive \n \n \n  \n \n \n 01 \n \n \n  \n \n      01  \n  \n \n      \n  \n []1 \n \n  \n \n  \n \n \n m \n \n \n m \n \n \n m1 \n \n \n  \n xyz \n \n aa2a213 \n \n 2131213xza     \n    \n \n \n   \n \n \n   \n \n \n  \n  \n \n z10130xy. \n  \n \x3c!--guava--\x3e \n         < dependency > \n             < groupId > com.google.guava </ groupId > \n             < artifactId > guava </ artifactId > \n             < version > 31.0.1-jre </ version > \n         </ dependency > \n \n 1 2 3 4 5 6 //100w10w \n import   com . google . common . hash . BloomFilter ; \n import   com . google . common . hash . Funnels ; \n import   org . junit . jupiter . api . Test ; \n import   org . springframework . boot . test . context . SpringBootTest ; \n \n import   java . math . BigDecimal ; \n \n @SpringBootTest \n class   RetailUserApplicationTests   { \n \n     @Test \n     void   contextLoads ( )   { \n         this . BloomTest ( ) ; \n     } \n \n     public   void   BloomTest ( )   { \n         //  \n         long  startTime  =   System . currentTimeMillis ( ) ; \n         //  \n         BigDecimal  count  =   new   BigDecimal ( "0" ) ; \n         //  \n         BigDecimal  one  =   new   BigDecimal ( "1" ) ; \n         // 10W   \n         BigDecimal  testCount  =   new   BigDecimal ( "100000" ) ; \n         //  \n         BigDecimal  mult  =   new   BigDecimal ( "100" ) ; \n \n         //  \n         BloomFilter < Integer >  bloomFilter  =   BloomFilter . create ( Funnels . integerFunnel ( ) ,   1000000L ,   0.01 ) ; \n \n         // 100w \n         for   ( int  i  =   1 ;  i  <=   1000000 ;  i ++ )   { \n            bloomFilter . put ( i ) ; \n         } \n \n         // 10W \n         for   ( int  i  =   2000000 ;  i  <=   2100000 ;  i ++ )   { \n             boolean  mightContain  =  bloomFilter . mightContain ( i ) ; \n             if   ( mightContain )   { \n                count  =  count . add ( one ) ; \n             } \n         } \n         System . out . println ( ""   +   ( System . currentTimeMillis ( )   -  startTime )   +   "MS" ) ; \n         System . out . println ( ":"   +  count ) ; \n         System . out . println ( ":"   +   ( count . divide ( testCount ) ) . multiply ( mult )   +   "%" ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 \n hashmap0.75 10.5 \n   \n   \n  \n \\1.  reids \n \\2.   \n 11112324 \n  \n  \n key \n \n key \n \n  \n \n redis+ \n Ehcache+ \n redisRedis \n \n  \n redis \n \n Redis Redis RDB(Redis DataBase)  AOF(Append Only File)  \n Redis6.0Redisio io Thread  worker Thread  \n Redis  \n Redis RDB(Redis DataBase)  AOF(Append Only File)  \n  RDB  \n RDBRDB \n RDBforkRDB \n \n \n  \n \n \n  \n \n \n RDBRedis dump.rdb  \n RedisRedis redis.conf  dbfilename RDB. \n \n  \n RDB \n 1 save \n savefork Redis RDB \n redis.conf dir RDBrdb ./ redis \n \n \n 2 bgsave \n bgsave   forkRedis \n fork   \n bgsave fork bgsave \n \n dbfilename  \n config set dir{newDir} \n config set dbfilename{newFileName} \n 3  \n savebgsave redis.conf  \n \n redissave save 900 1 9001key \n save 300 10 30010key save 60 10000  \n savebgsave \n \n \n save  bgsave   \n \n \n save fork     bdsave  fork  \n \n \n save  bgsave   \n \n \n RDB \n   RDB RDB  \n    \n  AOF  \n AOFRedis   \n AOFAOF \n \n  appendonly no  appendonly yes AOF appendfilename  appendonly.aof dirRDB \n \n AOF**** \n AOF   \n \n \n no  \n \n \n always  \n \n \n everysec  \n \n \n AOF   append  \n  \n RedisAOF \n AOF \n  \n k++k--k \n Redis rewrite  \n redisbgrewriteaoffork \n AOFAOF \n  fork  \n Redisappend Redis \n AOFRedisbgrewriteaofAOF100% \n  \n \n AOF  \n   AOFforkio \n  \n AOF flushall aofflushall \n     AOFRDB RDB  AOF AOFRDB \n   \n redis4.0RDB+AOF4.0 \n \n  bgrewriteaof forkRDBaofAOF \n RDBAOFAOFAOFAOFRDBAOF \n    RDB  AOF RDBAOF \n RDB AOF \n rdbrdbaof \n aof \n  redis \n Redis \n  Redis \n RedisRedis \n \n Redis  Redis  \n Redis \n Redis \n  \n RedisRedis \n \n \n MULTI \n \n \n  \n \n \n EXECDISCARD  \n \n \n Redis \n \n \n \n  \n  \n \n \n \n \n MULTI \n  Redis  EXEC \n \n \n DISCARD \n  Mysql   \n \n \n EXEC \n     keyWATCH \n \n \n WATCH key \n key  MULTI  key EXEC**** \n \n \n UNWATCH \n ****WATCH   ****key EXEC DISCARD key \n   \n Redis \n  Redis \n Redis \n \n  \n \n   RedisRollbackRedisRedis \n \n  \n \n Redis Lua  \n \n  \n \n Redis  Redis \n  LuaRedis \n RedisRedisLuaLuaRedis \n \n LuaRedis \n \n RedisLua \n \n Lua  Redis \n Lua  Redis \n Lua \n \n Lua \n \n LuaLuaRedis \n Lua \n  Redis RedisLua \n \n \n redis lua \n \n Redis Eval  -  Lua  \n redis 127.0.0.1:6379> eval "return {KEYS[1],KEYS[2],ARGV[1],ARGV[2]}" 2 key1 key2 first second\n1) "key1"\n2) "key2"\n3) "first"\n4) "second"\n \n 1 2 3 4 5  \nscript  Lua 5.1 () Lua  \nnumkeys  \nkey [key ]  EVAL  Redis (key) Lua  KEYS  1 ( KEYS[1]  KEYS[2] ) \narg [arg ]  Lua  ARGV  KEYS ( ARGV[1]  ARGV[2] ) \n Luaredis.call()()redis.pcall()()redis \n redis.call()redis.pcall() \n redis-master:637 9 >   eval   "return redis.call(\'set\',\'foo\',\'bar\')"   0 \nOK\nredis-master:637 9 >   eval   "return redis.call(\'get\',KEYS[1])"   1  foo\n "bar" \n \n 1 2 3 4  redis-cli --evaltest.lua \n  redis-cli -a  --eval Lua  key [key ...] , arg [arg ...]  \n key [key ]   numkeys  key  key [key ]  arg [arg ]    \n \n cas,cas.lua \n local   old = ARGV [ 1 ] \n local   new = ARGV [ 2 ] \n local   expire = ARGV [ 3 ] \n local   current = redis.call ( \'get\' ,KEYS [ 1 ] ) \n if  current  then \n\t if  current~ = ARGV [ 1 ]   then \n\t\t return  -1 ; \n\tend ; \n\tredis.call ( \'setex\' ,KEYS [ 1 ] ,ARGV [ 2 ] ,ARGV [ 3 ] ) \n\t return   1 ; \nend ; \n return   0 ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 redis-cli  -h  redis-master  -a   123456   --eval  cas.lua cas , old  60  new\n \n 1 \n Java \n \x3c!--redis--\x3e \n         < dependency > \n             < groupId > org.springframework.boot </ groupId > \n             < artifactId > spring-boot-starter-data-redis </ artifactId > \n             < version > 2.3.1.RELEASE </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.commons </ groupId > \n             < artifactId > commons-pool2 </ artifactId > \n             < version > 2.8.1 </ version > \n         </ dependency > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 import   org . springframework . data . redis . core . RedisTemplate ; \n import   org . springframework . data . redis . core . script . RedisScript ; \n import   org . springframework . data . redis . serializer . GenericToStringSerializer ; \n import   org . springframework . data . redis . serializer . StringRedisSerializer ; \n import   org . springframework . stereotype . Component ; \n\n import   javax . annotation . Resource ; \n import   java . util . Arrays ; \n import   java . util . List ; \n import   java . util . Objects ; \n import   java . util . stream . Collectors ; \n\n @Component \n public   class   RedisUtil   { \n     @Resource \n     private   RedisTemplate < String ,   Object >  redisTemplate ; \n\n     /**\n     *  lua \n     * @author hengyumo\n     * @since 2021-06-05\n     *\n     * @param luaScript  lua \n     * @param returnType \n     * @param keys       KEYS\n     * @param argv       ARGV\n     * @param <T>        \n     *\n     * @return \n     */ \n     public   < T >   T   executeLuaScript ( String  luaScript ,   Class < T >  returnType ,   String [ ]  keys ,   Object . . .  argv )   { \n         Object  execute  =  redisTemplate . execute ( RedisScript . of ( luaScript ,  returnType ) , \n                 new   StringRedisSerializer ( ) , \n                 new   GenericToStringSerializer < > ( returnType ) , \n                 Arrays . asList ( keys ) , \n                 ( Object [ ] )  argv ) ; \n         return   ( T )  execute ; \n     } \n\n\n     // xxx*key \n     private   final   static   String   LUA_SCRIPT_CLEAR_WITH_KEY_PRE   = \n             "local redisKeys = redis.call(\'keys\',KEYS[1]..\'*\');"   + \n                     "for i,k in pairs(redisKeys) do redis.call(\'del\',k);end;"   + \n                     "return redisKeys;" ; \n\n     /**\n     * @author hengyumo\n     * @since 2021-06-05\n     *\n     * key\n     * @param keyPre \n     * @return  key\n     */ \n     public   List < String >   deleteKeysWithPre ( String  keyPre )   { \n         @SuppressWarnings ( "unchecked" ) \n         List < Object >  result  =   executeLuaScript ( LUA_SCRIPT_CLEAR_WITH_KEY_PRE ,   List . class ,   new   String [ ]   { keyPre } ) ; \n         return  result . stream ( ) . map ( x  ->   { \n             if   ( x  instanceof   List )   { \n                 @SuppressWarnings ( "unchecked" ) \n                 List < String >  list  =   ( List < String > )  x ; \n                 if   ( list . size ( )   >   0 )   { \n                     return  list . get ( 0 ) ; \n                 } \n             } \n             return   null ; \n         } ) . filter ( Objects :: nonNull ) . collect ( Collectors . toList ( ) ) ; \n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 \n import   org . junit . Test ; \n import   org . junit . runner . RunWith ; \n import   org . springframework . boot . test . context . SpringBootTest ; \n import   org . springframework . test . context . junit4 . SpringRunner ; \n\n import   javax . annotation . Resource ; \n import   java . util . List ; \n\n\n @SpringBootTest \n @RunWith ( SpringRunner . class ) \n public   class   RedisUtilTest   { \n     @Resource \n     private   RedisUtil  redisUtil ; \n\n     @Test \n     @SuppressWarnings ( "unchecked" ) \n     public   void   executeLuaScript ( )   { \n         String  script_get  =   "redis.call(\'get\',KEYS[1])" ; \n         List < Object >  list  =  redisUtil . executeLuaScript ( script_get , \n                 List . class ,   new   String [ ]   { "cas" } ) ; \n            list . forEach ( x  ->   { \n                 if ( x == null ) { \n                     System . out . println ( "keynil" ) ; \n                 } else { \n                     System . out . println ( x . toString ( ) ) ; \n                 } \n\n             } ) ; \n\n         String  script_cas = "local old=ARGV[1]\\n"   + \n                 "local new=ARGV[2]\\n"   + \n                 "local expire=ARGV[3]\\n"   + \n                 "local current=redis.call(\'get\',KEYS[1])\\n"   + \n                 "if current then\\n"   + \n                 "\\tif current~=ARGV[1] then\\n"   + \n                 "\\t\\treturn -1;\\n"   + \n                 "\\tend;\\n"   + \n                 "\\tredis.call(\'setex\',KEYS[1],60,ARGV[2])\\n"   + \n                 "\\treturn 1;\\n"   + \n                 "end;\\n"   + \n                 "return 0;\\n" ; \n\n        list  =  redisUtil . executeLuaScript ( script_cas , \n                 List . class ,   new   String [ ]   { "cas" } , "old" , "new" ) ; \n         System . out . println ( "" ) ; \n        list . forEach ( System . out :: println ) ; \n\n         String  script_set  =   "redis.call(\'set\',KEYS[1],ARGV[1]);return redis.call(\'get\',KEYS[1]);" ; \n        list  =  redisUtil . executeLuaScript ( script_set , \n                 List . class ,   new   String [ ]   { "cas" } , "old" ) ; \n         System . out . println ( "keyold" ) ; \n\n        list  =  redisUtil . executeLuaScript ( script_cas , \n                 List . class ,   new   String [ ]   { "cas" } , "old" , "new" ) ; \n         System . out . println ( "" ) ; \n        list . forEach ( System . out :: println ) ; \n        list  =  redisUtil . executeLuaScript ( script_cas , \n                 List . class ,   new   String [ ]   { "cas" } , "old" , "new" ) ; \n         System . out . println ( "" ) ; \n        list . forEach ( System . out :: println ) ; \n     } \n\n     @Test \n     public   void   deleteKeysWithPre ( )   { \n         List < String >  list  =  redisUtil . deleteKeysWithPre ( "ca" ) ; \n        list . forEach ( System . out :: println ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 \n  \n Redis \n RedisRedis   \n  \n  \n  masterread/writeonly read  \n    only write  \n ,,( Redis , Redis ) \n   HAProxyTCPNginxF5HAProxy \n  \n RedisL4 \n \n Master \n Slave \n \n RedisRedis \nTCP \n \n  \n  \n  \n  \n \n  \n \\1.    masterslave \n \\2.    masterslavemaster \n  \n master \n master \n  \n   master   \n  \n Cluster \n Cluster \n Redisredis \n  \n  \n Redisredis16384 0 -16383 \n master0 -163830-50005001-1100011001-16383 \n \n keyCRC16  16384 CRC16(key)%16383key \n \n redis \n  \n 1.big key \n 1.1 big key \n big keyskeyvalue  big keys  key  key  big keys \n 1.2 big key \n  big keys \n  \n Redis  big keys  big keys  big keys  \n  \n  big keys  big keys  1MB 1000 1000MB Redis big keys  \n  \n Redis keyhash big keys  \n 1.3 big key \n big key \n \n \n  \n  --bigkeys \n --bigkeys   redis  Key  stringlistset zset hash  key \n string  value  4  value  --bigkeys   string  key   \n \n  value  value  \n \n redis - cli  - h  127.0 .0 .1   - p  6379   - a  "password"   -- bigkeys\n \n 1 --bigkeys   scan  key redis keys  slave  \n -bigkeys   key key  key key  10kb  key \n key (>10kb) top1  key 10kb  key rdb  \n \n \n  \n  Rdbtools  \n Rdbtools  python  Redis  rdb  key  \n 1 \n git clone https : / / github . com / sripathikrishnan / redis - rdb - tools\n\n\n\ncd redis - rdb - tools sudo  &&  python setup . py install\n \n 1 2 3 4 5 2 \n   dump.rdb  ,  > 10kb  key  csv  \n rdb dump . rdb  - c memory  -- bytes  10240   - f live_redis . csv\n \n 1  rdb_bigkeys  \n redis 3.2rdb \n redis CPU bigkey pythonredis RDBbig key 150MBRDB  goRDBbig keyrdb_bigkeys 150MBRDB 12 \n #5  \n./rdb_bigkeys  --bytes   1024   --file  bigkeys_6379.csv  --sep   0   --sorted   --threads   4  dump6379.rdb \n #dump6379.rdb1024bytesKEY  CSVbigkeys_6379.csv \n \n 1 2 3 \n  \n yum  install  go  -y  \n export   GOBIN = $GOPATH /bin\n export   PATH = $PATH : $GOBIN \n mkdir  /home/gocode/\n export   GOPATH = /home/gocode/\n cd   $GOPATH \n git  clone https://github.com/weiyanwei412/rdb_bigkeys.git\n cd  rdb_bigkeys\ngo  install \ngo mod init rdb_bigkeys\ngo mod tidy\n #dial tcp 142.251.42.241:443: connect: connection refused \ngo  env   -w   GOPROXY = https://goproxy.cn,direct\ngo get \ngo build\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 rdb \n \n RDR \n redis 3.2rdb \n go clean  --modcache \ngo mod init rdr-linux\ngo mod tidy\ngo mod vendor \n #,vendorcheck \ngo build\n \n 1 2 3 4 5 6 RDR  redis rdbfile redis-rdb-toolsRDR golang  \n \n  Redis  Key  \n  Redis  Key  \n Redis Key  Dashboard  \n \n  \n ./rdr-linux show  -p   8080  dump.rdb\n \n 1 \n  \n \n redis keystop10 \n ./rdr-linux keys dump.rdb  |   head   -n   10 \n \n 1 \n redis-rdb-toolskeyrdrkeykey \n \n \n \n rdrrdb_bigkeys474MB1minrdb_bigkeyskeyrdrkeytop100key \n 1.4 big key \n  big keys \n  big keys big keys del  big keys Redis4.0  unlink  del unlink  \n  big keys hscansscanzscan big keys ( 200  zset 1del) \n big key  \n  key  key \n \n  key \n key20220101key20220102. \n \n  big keys  \n'},{title:"k8s",frontmatter:{title:"k8s",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["",""]},regularPath:"/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%84%9A%E6%9C%AC/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html",relativePath:"/k8s.md",key:"v-4f7f9be4",path:"/2023/06/10/k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/",headers:[{level:2,title:"Kubernetes",slug:"kubernetes"},{level:2,title:"k8s",slug:"k8s-"}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:'  Kubernetes \n 1.1 Evicted  Pod \n kubectl get pods --all-namespaces  -o  wide  |   grep  Evicted  |   awk   \'{print $1,$2}\'   |   xargs   -L1  kubectl delete pod  -n \n \n 1 1.2 ErrorPod \n kubectl get pods --all-namespaces  -o  wide  |   grep  Error  |   awk   \'{print $1,$2}\'   |   xargs   -L1  kubectl delete pod  -n \n \n 1 1.3  Completed  Pod \n kubectl get pods --all-namespaces  -o  wide  |   grep  Completed  |   awk   \'{print $1,$2}\'   |   xargs   -L1  kubectl delete pod  -n \n \n 1 1.4  PV \n kubectl describe  -A  pvc  |   grep   -E   "^Name:.*$|^Namespace:.*$|^Used By:.*$"   |   grep   -B   2   "<none>"   |   grep   -E   "^Name:.*$|^Namespace:.*$"   |   cut   -f2  -d:  |   paste   -d   " "  - -  |   xargs   -n2   bash   -c   \'kubectl -n ${1} delete pvc ${0}\' \n \n 1 1.5  PVC \n kubectl describe  -A  pvc  |   grep   -E   "^Name:.*$|^Namespace:.*$|^Used By:.*$"   |   grep   -B   2   "<none>"   |   grep   -E   "^Name:.*$|^Namespace:.*$"   |   cut   -f2  -d:  |   paste   -d   " "  - -  |   xargs   -n2   bash   -c   \'kubectl -n ${1} delete pvc ${0}\' \n \n 1 1.6  PV \n kubectl get  pv   |   tail   -n  +2  |   grep   -v  Bound  |   awk   \'{print $1}\'   |   xargs   -L1  kubectl delete  pv \n \n 1 #  k8s \n  \n df   -h \n \n 1  \n du   -h  --max-depth = 1 \n \n 1  \n   #10 \n du   -hm  *  |   sort  -rn | head   -10  \n #100M \n find  ./  -type  f  -size  +100M\n #du -h du -h  ls -l  \n find  ./  -type  f  -size  +100M   -print0   |   xargs   -0   du   -h   |   sort   -nr \n \n 1 2 3 4 5 6  \n lsof   |   grep  deleted\n lsof   |   grep  deleted | awk   \'{print "kill -9",$2}\' | sh \n \n 1 2  \n /var/log/journal \n #journalctl  \n #1 \njournalctl --vacuum-time = 1w\n #2500MB \njournalctl --vacuum-size = 500M\n \n 1 2 3 4 5   /var/log/journal/  rm -rf /var/log/journal/* \n /var/log/messages \n #/var/log/messages 441 \n vim  /etc/logrotate.conf\n \n systemctl restart rsyslog logrotate  -f  /etc/logrotate.conf\n \n 1 2 3 4 \n  \n cat  /dev/null  >  /var/log/messages\n \n 1 yum \n 1 . ( /var/cache/yum )  \n\nyum clean packages\n\n 2 . ( /var/cache/yum )  headers\n\nyum clean headers\n\n 3 . ( /var/cache/yum )  headers\n\nyum clean oldheaders\n\n 4 . ( /var/cache/yum )  headers\n\nyum clean, yum clean all  ( =  yum clean packages ;  yum clean oldheaders ) \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 docker \n /var/lib/containerd \n crictl\n \n 1 /var/lib/docker/overlay2 \n cpu \n kubectl describe nodes node1\n \n 1 Kubernetesetcd \n (210) Kubernetesetcd_k8s etcd_-CSDN \n'},{title:"hbase",frontmatter:{title:"hbase",date:"2022-08-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:["","nosql"],tags:["",""]},regularPath:"/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/hbase.html",relativePath:"/hbase.md",key:"v-24ffb3db",path:"/2022/08/08/hbase/",headers:[{level:2,title:"hbase",slug:"hbase"},{level:2,title:"hbase",slug:"hbase"},{level:2,title:"hbase",slug:"hbase"},{level:2,title:"hbase",slug:"hbase"},{level:3,title:"hbase_shell",slug:"hbase-shell"},{level:3,title:"hbasejavaAPI",slug:"hbasejavaapi"},{level:2,title:"hbase",slug:"hbase"},{level:3,title:"hbase",slug:"hbase"},{level:3,title:"region server",slug:"region-server"},{level:3,title:"master",slug:"master"},{level:3,title:"HBase",slug:"hbase"},{level:2,title:"Hbase",slug:"hbase"},{level:3,title:"HBaseflush",slug:"hbaseflush"},{level:3,title:"HBasestoreFile",slug:"hbasestorefile"},{level:3,title:"Hbasesplit(region)",slug:"hbasesplit-region"},{level:2,title:"HBaseBulk Load ",slug:"hbasebulk-load-"},{level:3,title:"MR-bulkload",slug:"mr-bulkload"},{level:3,title:"Spark bulkLoad",slug:"spark-bulkload"},{level:2,title:"HBaseHive",slug:"hbasehive"},{level:2,title:"Apache Phoenix",slug:"apache-phoenix"},{level:3,title:"phoenixhbasehiveimpala",slug:"phoenixhbasehiveimpala"},{level:3,title:"HBase",slug:"hbase"},{level:3,title:"Apache Phoenix",slug:"apache-phoenix"},{level:3,title:"Apache Phoenix",slug:"apache-phoenix"},{level:3,title:"apache  Phoenix",slug:"apache-phoenix"},{level:3,title:"Apache Phoenix",slug:"apache-phoenix"},{level:2,title:"hbase",slug:"hbase"},{level:3,title:"hbase()",slug:"hbase-"},{level:3,title:"hbase",slug:"hbase"},{level:3,title:"hbase",slug:"hbase"},{level:3,title:"hbaseTTL",slug:"hbasettl"},{level:3,title:"hbaserowkey",slug:"hbaserowkey"},{level:3,title:"hbase",slug:"hbase"},{level:2,title:"HBase",slug:"hbase"}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:" hbase \n HBase: \n         HAOOP , ,  \n HBase java , HBaseHDFS ,   noSQL  \n HBase: \n \n 1)  \n 2)  \n 3)  \n \n HBase, , , SQL,  \n HBase, hadoop, , HBase \n HBase: \n \n \n \n : ,   \n \n \n \n \n : (  ) \n \n \n \n \n : HBase, null, ,  \n \n \n \n : \n \n \n \n  \n \n \n \n \n  \n \n \n \n \n  \n \n \n \n hbase \n hbaseRDBMS \n \n HBase:   , SQL , ,  ,  ,join \n RDBMS:, SQL , , ,join \n \n hbase  HDFS \n \n HBASE:  HDFS ,   HDFS,    \n HDFS: , ,  \n \n : \n \n  HBASE HDFS  ,  HBASEHDFS , HDFS, HBASE \n \n hbasehive \n \n \n HBASE: hadoop , hbasenosql, HBASE ,  \n \n \n HIVE:  hadoop,  , hive,   ,  OLAP \n hbase \n hbase \n HBase: \n \n \n \n hbase-site.xml,  zookeeper \n \n \n \n \n  htrace-core-3.1.0-incubating.jar hbaselib \n \n \n \n \n conf/regionserveslocalhost,  \n \n \n \n HBase: \n \n \n \n  zookeeper \n \n \n \n    #: :  \n   cd  /export/server/zookeeper-3.4.6/bin\n  ./zkServer.sh start \n  \n   #:  jps   \n     QuorumPeerMain\n   #: , ,   follower    leader \n \n 1 2 3 4 5 6 7 \n \n \n \n  hadoop: \n \n \n \n   1)   node1: \n     start-all.sh\n   2)     \n      node1 : \n         namenode\n         datanode\n         resourceManager\n         nodemanager\n     node2 : \n       seconderyNamenode\n       datanode\n       nodemanager\n     node3 : \n        datanode\n        nodemanager\n   3)    50070   8088   \n       50070    \n      datanode\n      \n       8088,    active node  3 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \n \n \n  HBASE \n \n \n \n   1)   node1, : \n      start-hbase.sh\n  \n   2)   :   jps \n      node1 : \n         HMaster\n         HRegionServer\n      node2 : \n        HRegionServer\n      node3 : \n        HRegionServer\n        \n      ,   , , , 2, ,  \n     \n      ,   : \n           :   /export/server/hbase-2.1.0/logs \n           : \n             hbase-root-regionserver-node[N].log\n             hbase-root-regionserver-node[N].out\n           : \n             tail   -200f  xxx. \n  \n   3)    hbase:   16010 \n        :   http://node1:16010 \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \n  hbase \n \n \n \n \n Table:  hbase, hbase, , hbase \n \n \n \n \n rowkey: () RDBMSPK ,   , rowkey    \n \n \n \n \n :   + () \n \n \n :   , , \n \n :\n \n ,  \n , , ,  \n \n \n \n \n :  , ,  \n \n \n \n \n : \n \n \n , ,  \n \n \n \n \n :  ,  \n \n \n :  1   \n  \n \n \n \n \n :   \n \n \n :  rowkey +  +  +  \n \n \n \n : ,      \n hbase \n hbase_shell \n hbaseshell \n \n \n \n HBase \n \n \n \n    [ root@node2 ~ ] # hbase shell \n \n 1 \n \n \n \n  \n \n \n \n   hbase ( main ) :001: 0 >   help \n \n 1 \n   :\n  \t: help  ''\n  \t\n  \thbase(main):012:0> help 'create'\n \n 1 2 3 4 \n \n \n \n : status \n \n \n \n \n \n Hbase: list \n \n \n \n \n \n  \n \n \n \n   :\n       create  ''  ,  '1' , '2'   .. .. \n \n 1 2 \n  \n \n \n \n : put \n \n \n \n   :\n     put   '' , 'rowkey' , ':' , '' \n \n 1 2 \n \n \n \n rowkey? \n \n \n \n   :\n    get '','rowkey',['1','2' ...],['1:'],['1','2:' ...]\n \n 1 2 \n \n \n \n  scan \n \n \n \n   :\n      +, [ { COLUMNS   = > [ '1' , '2' ] }   |   { COLUMNS   = > [ '1:' , '2' ] ,VERSIONS = > N }   ]  ,  [ { FORMATTER  = > 'toString' } ]  ,  [ { LIMIT  = > N } ] \n  \n  :\n     scan  '' ,  { COLUNMS = > [ '1' , '2' ] }   |   { COLUMNS   = > [ '1:' , '2' ] , STARTROW  = > 'rowkey' ,  ENDROW = > 'rowkey' } \n     : \n  \n  :\n       { FORMATTER  = > 'toString' }   :  \n       { LIMIT  = > N }   :   N\n \n 1 2 3 4 5 6 7 8 9 10 \n \n \n \n \n \n \n  \n \n \n \n    ,  rowkey  \n \n 1 \n \n \n : delete \n \n \n \n   :\n     delete  '' , 'rowkey' , ':' \n \n 1 2 \n   :\n      deleteall  '' , 'rowkey' , [ ':' ] \n \n 1 2 \n \n   delete    deleteall: \n     \n    : \n   1)   delete   deleteall \n   2)   delete, , deleteall \n \n 1 2 3 4 5 \n \n \n  \n \n \n \n   :\n    truncate  '' \n \n 1 2 \n \n \n \n  \n \n \n \n   :\n     drop  '' \n \n 1 2 \n : \n      , ,  \n \n \n \n \n  \n \n \n \n   :\n      count  '' \n \n 1 2 #  hbaseshell \n \n \n \n HBase \n \n \n \n    : \n       scan   '',{FILTER=>\"(,)\" } \n   \n     : \n        rowkey : \n            RowFilter   :    \n            PrefixFilter   :     rowkey   \n         : \n            FamilyFilter :    \n         : \n            QualifierFilter   :   ,  \n         : \n           ValueFilter :    ,  \n           SingleColumnValueFilter :     , () \n              \n           SingleColumnVlaueExcludeFilter :   , () \n          : \n              PageFilter   :    \n             \n     \n       :    = >  < >= <= !=      \n  \n    : \n       BinaryComparator   :      \n       BinaryPrefixComparator   :    \n       NullComparator   :     Null \n       SubstringComparator   :      \n  \n    : \n       BinaryComparator   :     binary: \n       BinaryPrefixComparator   :    binaryprefix: \n       NullComparator   :    null \n       SubstringComparator :   substring: \n      \n   ,   : \n      http : //hbase.apache.org/2.2/devapidocs/index.html \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 \n \n \n \n \n \n shell: \n \n \n \n 2.1:  whoami : Hbase \n \n \n \n 2.2:  describe:  \n \n \n \n 2.3: exists   \n \n \n \n 2.4:  is_enabled   is_disabled   \n \n \n \n 2.5 : alter:   \n \n \n : \n alter '' ,NAME=>'',[VERSIONS=>N] \n \n \n \n : \n alter '','delete'=>'' \n  hbasejavaAPI \n \n \n \n  \n \n \n \n maven: \n \n \n 1.1) :  bigdata_parent_01 \n 1.2)  src  \n 1.3) :  day01_hbase \n \n \n \n \n :pom \n \n \n \n \n \n          < repositories > \x3c!----\x3e \n             < repository > \n                 < id > aliyun </ id > \n                 < url > http://maven.aliyun.com/nexus/content/groups/public/ </ url > \n                 < releases > < enabled > true </ enabled > </ releases > \n                 < snapshots > \n                     < enabled > false </ enabled > \n                     < updatePolicy > never </ updatePolicy > \n                 </ snapshots > \n             </ repository > \n         </ repositories > \n    \n         < dependencies > \n             < dependency > \n                 < groupId > org.apache.hbase </ groupId > \n                 < artifactId > hbase-client </ artifactId > \n                 < version > 2.1.0 </ version > \n             </ dependency > \n             < dependency > \n                 < groupId > commons-io </ groupId > \n                 < artifactId > commons-io </ artifactId > \n                 < version > 2.6 </ version > </ dependency > \n             < dependency > \n                 < groupId > junit </ groupId > \n                 < artifactId > junit </ artifactId > \n                 < version > 4.12 </ version > \n                 < scope > test </ scope > \n             </ dependency > \n             < dependency > \n                 < groupId > org.testng </ groupId > \n                 < artifactId > testng </ artifactId > \n                 < version > 6.14.3 </ version > \n                 < scope > test </ scope > \n             </ dependency > \n         </ dependencies > \n    \n         < build > \n             < plugins > \n                 < plugin > \n                     < groupId > org.apache.maven.plugins </ groupId > \n                     < artifactId > maven-compiler-plugin </ artifactId > \n                     < version > 3.1 </ version > \n                     < configuration > \n                         < target > 1.8 </ target > \n                         < source > 1.8 </ source > \n                     </ configuration > \n                 </ plugin > \n             </ plugins > \n         </ build > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \n \n \n : \n \n \n : com.gordon.hbase \n \n \n \n  \n : \n 1)   HBase:  \n\n 2)   : Admin()  Table() \n\n 3)    \n\n 4)     --  \n\n 5)    \n \n 1 2 3 4 5 6 7 8 9 \n  \n \n   //  \n     @Test \n     public   void   test01 ( )   throws   Exception { \n\n         // 1) HBase: \n\n         //Configuration conf = new Configuration(); \n         Configuration  conf  =   HBaseConfiguration . create ( ) ; \n        conf . set ( \"hbase.zookeeper.quorum\" , \"node1:2181,node2:2181,node3:2181\" ) ; \n         Connection  hbConn  =   ConnectionFactory . createConnection ( conf ) ; \n\n         // 2) : Admin()  Table() \n\n         Admin  admin  =  hbConn . getAdmin ( ) ; \n\n         // 3)  \n\n         boolean  flag  =  admin . tableExists ( TableName . valueOf ( \"WATER_BILL\" ) ) ;   // true ,  \n\n         if ( ! flag ) { \n             //   \n             //3.1:   \n             TableDescriptorBuilder  descBuilder  =   TableDescriptorBuilder . newBuilder ( TableName . valueOf ( \"WATER_BILL\" ) ) ; \n\n\n             //3.2:   \n             ColumnFamilyDescriptorBuilder  familyDesc  =   ColumnFamilyDescriptorBuilder . newBuilder ( \"C1\" . getBytes ( ) ) ; \n            descBuilder . setColumnFamily ( familyDesc . build ( ) ) ; \n\n             //3.3:    \n             TableDescriptor  desc  =  descBuilder . build ( ) ; \n\n            admin . createTable ( desc ) ; \n         } \n\n         // 4)   --  \n\n         // 5)  \n        admin . close ( ) ; \n        hbConn . close ( ) ; \n\n     } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  \n \n  \n \n 1)   Hbase \n\n 2)    : admin   table \n\n 3)   :   \n\n 4)   :  --  \n\n 5)    \n \n 1 2 3 4 5 6 7 8 9 \n  \n \n //  \n     @Test \n     public   void   test02 ( )   throws   Exception { \n\n         //1) Hbase \n         Configuration  conf  =   HBaseConfiguration . create ( ) ; \n\n        conf . set ( \"hbase.zookeeper.quorum\" , \"node1:2181,node2:2181,node3:2181\" ) ; \n\n         Connection  hbConn  =   ConnectionFactory . createConnection ( conf ) ; \n         //2)  : admin   table \n\n         Table  table  =  hbConn . getTable ( TableName . valueOf ( \"WATER_BILL\" ) ) ; \n         //3) :   \n\n         Put  put  =   new   Put ( Bytes . toBytes ( 4944191 ) ) ; \n\n        put . addColumn ( \"C1\" . getBytes ( ) , \"NAME\" . getBytes ( ) , \"\" . getBytes ( ) ) ; \n        put . addColumn ( \"C1\" . getBytes ( ) , \"ADDRESS\" . getBytes ( ) , \"7267\" . getBytes ( ) ) ; \n        put . addColumn ( \"C1\" . getBytes ( ) , \"SEX\" . getBytes ( ) , \"\" . getBytes ( ) ) ; \n        put . addColumn ( \"C1\" . getBytes ( ) , \"PAY_TIME\" . getBytes ( ) , \"2020-05-10\" . getBytes ( ) ) ; \n       \n\n        table . put ( put ) ; \n\n         //4) :  --  \n\n         //5)  \n        table . close ( ) ; \n        hbConn . close ( ) ; \n\n\n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  \n      private   Connection  hbConn ; \n     private   Table  table ; \n     private   Admin  admin ; \n     private   String  tableName  =   \"WATER_BILL\" ; \n     @Before \n     public   void   before ( )   throws   Exception { \n\n         //1) Hbase \n         Configuration  conf  =   HBaseConfiguration . create ( ) ; \n\n        conf . set ( \"hbase.zookeeper.quorum\" , \"node1:2181,node2:2181,node3:2181\" ) ; \n\n        hbConn  =   ConnectionFactory . createConnection ( conf ) ; \n         //2)  : admin   table \n        admin  =  hbConn . getAdmin ( ) ; \n        table  =  hbConn . getTable ( TableName . valueOf ( tableName ) ) ; \n\n\n     } \n\n     @Test \n     public   void   test03 ( ) { \n        \n         // 3)  \n        \n        \n         //4)  \n        \n     } \n\n\n     @After \n     public    void   after ( )   throws   Exception { \n         //5.  \n        admin . close ( ) ; \n        table . close ( ) ; \n        hbConn . close ( ) ; \n        \n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  \n \n  \n \n // rowkey  \n     @Test \n     public   void   test03 ( )   throws   Exception { \n\n         // 3)  get \n\n         Get  get  =   new   Get ( Bytes . toBytes ( 4944191 ) ) ; \n         Result  result  =  table . get ( get ) ;   // Result   \n\n         //4)  \n\n         //4.1:  \n         List < Cell >  listCells  =  result . listCells ( ) ; \n\n         //4.2: ,  \n         for   ( Cell  cell  :  listCells )   { \n\n             // : rowkey     \n\n             //4.2.1 rowkey \n             /*byte[] rowArray = cell.getRowArray();\n\n            int rowkey = Bytes.toInt(rowArray, cell.getRowOffset(), cell.getRowLength());*/ \n\n             byte [ ]  rowBytes  =   CellUtil . cloneRow ( cell ) ; \n             int  rowkey  =   Bytes . toInt ( rowBytes ) ; \n\n             // 4.2.2   \n             byte [ ]  familyBytes  =   CellUtil . cloneFamily ( cell ) ; \n             String  family  =   Bytes . toString ( familyBytes ) ; \n             // 4.2.3  () \n             byte [ ]  qualifierBytes  =   CellUtil . cloneQualifier ( cell ) ; \n             String  qualifier  =   Bytes . toString ( qualifierBytes ) ; \n             // 4.2.4   \n             byte [ ]  valueBytes  =   CellUtil . cloneValue ( cell ) ; \n             String  value  =   Bytes . toString ( valueBytes ) ; \n\n\n             System . out . println ( \"rowkey:\" + rowkey  + \"; :\" + family + \"; :\" + qualifier + \"; :\" + value ) ; \n\n         } \n\n\n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  \n \n  \n \n // : \n     @Test \n     public    void    test04 ( )   throws   Exception { \n\n         // 3)  : delete   deleteall \n\n\n         Delete  delete  =   new   Delete ( Bytes . toBytes ( 4944191 ) ) ; \n\n         //delete.addColumn(\"C1\".getBytes(),\"NAME\".getBytes()); \n        delete . addFamily ( \"C1\" . getBytes ( ) ) ; \n        table . delete ( delete ) ; \n\n\n         //4)  --  \n\n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  \n \n  \n \n // : \n     @Test \n     public    void    test05 ( )   throws   Exception { \n         // 3)  \n\n         boolean  flag  =  admin . isTableEnabled ( TableName . valueOf ( tableName ) ) ;   //  \n         if ( flag ) { \n\n            admin . disableTable ( TableName . valueOf ( tableName ) ) ;   //,  \n         } \n\n        admin . deleteTable ( TableName . valueOf ( tableName ) ) ; \n\n         //4)  --  \n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  \n \n \n  \n \n : \n \n \n \n   hbase  org.apache.hadoop.hbase.mapreduce.Import  HDFS\n \n 1 \n \n : shell, hbase \n \n \n : \n \n \n \n  \n \n \n   hbase  org.apache.hadoop.hbase.mapreduce.Import WATER_BILL /water_bill/input\n \n 1 scan \n //scan  \n @Test \n     public   void   scan ( )   throws   Exception { \n         Configuration  conf  =   HBaseConfiguration . create ( ) ; \n        conf . set ( \"hbase.zookeeper.quorum\" ,   \"node1:2181,node2:2181,node3:2181\" ) ; \n         Connection  hbConn  =   ConnectionFactory . createConnection ( conf ) ; \n         Table  table  =  hbConn . getTable ( TableName . valueOf ( \"WATER_BILL\" ) ) ; \n\n         // scan  ,{FILTER=>\"QualifierFilter(=,'substring:a')\"} \n\n         Scan  scan  =   new   Scan ( ) ; \n         FilterList  filter  =   new   FilterList ( ) ; \n         SingleColumnValueFilter  startValueFilter  =   new   SingleColumnValueFilter ( \"C1\" . getBytes ( ) , \"RECORD_DATE\" . getBytes ( ) ,   CompareOperator . GREATER_OR_EQUAL , \"2020-06-01\" . getBytes ( ) ) ; \n         SingleColumnValueFilter  endValueFilter  =   new   SingleColumnValueFilter ( \"C1\" . getBytes ( ) , \"RECORD_DATE\" . getBytes ( ) ,   CompareOperator . LESS_OR_EQUAL , \"2020-07-01\" . getBytes ( ) )   ; \n        filter . addFilter ( startValueFilter ) ; \n        filter . addFilter ( endValueFilter ) ; \n        scan . setFilter ( filter ) ; \n         ResultScanner  results  =  table . getScanner ( scan ) ; \n         for   ( Result  result  :  results )   { \n             List < Cell >  cells  =  result . listCells ( ) ; \n             for   ( Cell  cell  :  cells )   { \n                 String  qulifier  =   Bytes . toString ( CellUtil . cloneQualifier ( cell ) ) ; \n                 if ( \"NAME\" . equals ( qulifier ) ) { \n                     String  name  =   Bytes . toString ( CellUtil . cloneValue ( cell ) ) ; \n                     System . out . println ( name ) ; \n                 } \n             } \n\n         } \n        table . close ( ) ; \n        hbConn . close ( ) ; \n\n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #  hbase \n hbase \n hbase, HBase, , ,  \n ? \n \n \n \n node1  hbaseconf,  backup-masters \n \n \n \n    cd  /export/server/hbase-2.1.0/conf/\n   vim  backup-masters\n  \n  :\n  node2.itcast.cn\n  node3.itcast.cn\n \n 1 2 3 4 5 6 \n \n \n  backup-master  \n \n \n \n    cd  /export/server/hbase-2.1.0/conf/\n   scp  backup-masters node2: $PWD \n   scp  backup-masters node3: $PWD \n \n 1 2 3 \n \n \n  HBase \n \n \n \n    node1:  \n      stop-hbase.sh\n      start-hbase.sh\n \n 1 2 3 \n hbase \n region server \n region server: \n \n : \n        regionServer, master, regionServermasterregion, masterregionServerregion, metaregion, , region, , region, regionServer,  \n regionServer \n \n \t   : HregionServer, regionServerregion(),masterregion, regionServer , regionServer, regionServer, region, regionregionServer (, ) \n master \n \n : \n     master, HBase, hbaseHMaster, , \n      master,      \n HBase \n  \n Meat \nrowkey -StratRow-TimeStamp-EncodedName\n\n4\n\ninforegioninfoEncodedNameRegionNameRegionStartRowRegionStopRow\n\ninfoseqnumDuringOpenRegionsequenceId\n\ninfoserverRegionRegionServer\n\ninfoserverstartcodeRegionServer\n \n 1 2 3 4 5 6 7 8 9 10 11 12 \n HBase \n \n    :   scan '' \n  \n   1)   zookeeper, zookeeper HBase:Meta (metaregionServer) \n          HBase : Meta   hbase, region  hbase,  , region, regionregionServer ......   \n   \n   2)    (hbase meta)regionServer, meta, metaregion, regionregionserver,  \n  \n  \n   3)   ()regionServer, regionServerregion, scan ()), get, region, rowkey \n       region :     memStore  --\x3e blockCache ---\x3eStoreFile(File) --\x3e HFile \n     \n \n 1 2 3 4 5 6 7 8 9 10 11 \n  \n \n HBase \n \n    :   put 'user' ,'rk001','C1:name','' \n  \n   1)   zookeeper, HBase:MetaregionServer \n   2)   regionServer, meta,rowkeyregionregionServer \n   3)   regionServer,  \n   4)   regionServerHLog(WAL), memStore(memStore) \n   5)   HLog  memStore,  , .... \n  \n  -------------------------------------------------------------\n    : \n  \n   6)   , memStore, memStore(128M/1), flush, memStore \"\" HDFS, storeFile(File) \n  \n   7)   flush, HDFS, File, Hfile(3), compact(), Hfile \"\"  HFile \n  \n   8)   compact, Hfile , Hfile(\"\"10GB),  split, Hfile , Hfile, region , region, regionHfile, split, region(: split, ) \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n Hbase \n  HBaseflush \n \n HBaseflush: \n \n flush :    \n        : \n              memStoreHDFS,   storeFile \n        :   \n               :    128M \n               :     1 \n               :   , flush \n\n       flush :   hbase 2.0 flush \n               1)   memStore, memStore,  \n               2)   memStore   (), HBase2.0, ,, memStore, , ,  \n               3)   memStore , memStore, (?), flush,  , , storeFile; \n\n  :    , hbase2.0, 2.0, , HDFS, storeFile, , memStore, memStorestoreFile \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 : \n      , hbase2.0, hbase2.x, , ,  \n : \n \n \n 1:  \n \n : hbase-site.xml , adaptive \n \n \n \n \n 2:  \n \n : ,  \n \n \n \n \n :  \n \n basic():\n \n : , , , ,  \n \n \n eager():\n \n : , , ,  , ,  \n \n \n adaptive():\n \n : ,   eager, ,  \n HBasestoreFile \n \n compact \n \n compact   :   \n           :    storeFile(Hfile) HFile \n           :     3 \n\n\n\n minor :  \n       :    storeFile Hfile \n\n       : \n           1)   storeFile(3), storeFile, storeFile \n           2)   , minor , , ,  \n    \n\n major :  \n\n      :   Hfile Hfile Hfile \n\n       : \n            1)   , (7|)major, HFileHfile, Hfile \n            2)   , , , major,  \n\n      :   major, , ,  , , Hbase \n          ,   major,  \n\n     : \n            HDFS,    , HDFS \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #  Hbasesplit(region) \n \n   region,  \n  : \n     ,   Region ,    1^2 * 128     10GB , , ,  regionHfile 128M  \n     ,   R=2, , regionHfile 512M,  \n    \n    \n     region   9, region 10GB  \n \n 1 2 3 4 5 6 #  HBaseBulk Load  \n :  , , HBase, hbase javaAPI,  HBase, : HLog memStore, memStorestoreFile, storeFileHfile ,  \n , , Hbase, , , ,  \n  ? \n 1)   HFile\n2) HFileHBase, Hbase\n\nHLog , HDFS, HDFS\n \n 1 2 3 4 \n  \n \n HBase \n \n 5.1  \n        HDFS CSV, ,  Hbase, ,  bulk_load HBase \n        \n  \n \n Hbase: \n \n create   'TRANSFER_RECORD' , 'C1' \n \n 1 \n   HDFS \n \n hdfs dfs  -mkdir   -p  /hbase/bulkload/input\n\nrz Linux\n\nhdfs dfs  -put  bank_record.csv /hbase/bulkload/input\n \n 1 2 3 4 5 \n \n \n IDEA: day02_hbase_bulk_load \n \n \n \n \n pom \n \n \n \n        < repositories > \x3c!----\x3e \n           < repository > \n               < id > aliyun </ id > \n               < url > http://maven.aliyun.com/nexus/content/groups/public/ </ url > \n               < releases > < enabled > true </ enabled > </ releases > \n               < snapshots > \n                   < enabled > false </ enabled > \n                   < updatePolicy > never </ updatePolicy > \n               </ snapshots > \n           </ repository > \n       </ repositories > \n  \n       < dependencies > \n           < dependency > \n               < groupId > org.apache.hbase </ groupId > \n               < artifactId > hbase-client </ artifactId > \n               < version > 2.1.0 </ version > \n           </ dependency > \n           < dependency > \n               < groupId > commons-io </ groupId > \n               < artifactId > commons-io </ artifactId > \n               < version > 2.6 </ version > \n           </ dependency > \n  \n           < dependency > \n               < groupId > org.apache.hbase </ groupId > \n               < artifactId > hbase-mapreduce </ artifactId > \n               < version > 2.1.0 </ version > \n           </ dependency > \n  \n           < dependency > \n               < groupId > org.apache.hadoop </ groupId > \n               < artifactId > hadoop-mapreduce-client-jobclient </ artifactId > \n               < version > 2.7.5 </ version > \n           </ dependency > \n  \n           < dependency > \n               < groupId > org.apache.hadoop </ groupId > \n               < artifactId > hadoop-common </ artifactId > \n               < version > 2.7.5 </ version > \n           </ dependency > \n  \n           < dependency > \n               < groupId > org.apache.hadoop </ groupId > \n               < artifactId > hadoop-mapreduce-client-core </ artifactId > \n               < version > 2.7.5 </ version > \n           </ dependency > \n  \n           < dependency > \n               < groupId > org.apache.hadoop </ groupId > \n               < artifactId > hadoop-auth </ artifactId > \n               < version > 2.7.5 </ version > \n           </ dependency > \n  \n           < dependency > \n               < groupId > org.apache.hadoop </ groupId > \n               < artifactId > hadoop-hdfs </ artifactId > \n               < version > 2.7.5 </ version > \n           </ dependency > \n  \n       </ dependencies > \n  \n       < build > \n           < plugins > \n               < plugin > \n                   < groupId > org.apache.maven.plugins </ groupId > \n                   < artifactId > maven-compiler-plugin </ artifactId > \n                   < version > 3.1 </ version > \n                   < configuration > \n                       < target > 1.8 </ target > \n                       < source > 1.8 </ source > \n                   </ configuration > \n               </ plugin > \n           </ plugins > \n       </ build > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 \n \n \n  \n \n \n com.itheima.hbase.bulkLoad \n MR-bulkload \n CSVHFile()==>spark \n \n \n \n  Mapper \n \n \n \n package   com . itheima . hbase . bulkLoad ; \n\n import   org . apache . hadoop . hbase . client . Put ; \n import   org . apache . hadoop . hbase . io . ImmutableBytesWritable ; \n import   org . apache . hadoop . io . LongWritable ; \n import   org . apache . hadoop . io . Text ; \n import   org . apache . hadoop . mapreduce . Mapper ; \n\n import   java . io . IOException ; \n\n public   class   BulkLoadMapper   extends   Mapper < LongWritable , Text , ImmutableBytesWritable ,   Put >   { \n     private   ImmutableBytesWritable  k2  =   new   ImmutableBytesWritable ( ) ; \n     @Override \n     protected   void   map ( LongWritable  key ,   Text  value ,   Context  context )   throws   IOException ,   InterruptedException   { \n\n         //1.  \n         String  line  =  value . toString ( ) ; \n\n         //2.  \n         if ( line  !=   null   &&   ! \"\" . equals ( line . trim ( ) ) ) { \n\n             //3.  \n\n             String [ ]  fields  =  line . split ( \",\" ) ; \n\n             //4.  k2  v2 \n             byte [ ]  rowkey  =  fields [ 0 ] . getBytes ( ) ; \n            k2 . set ( rowkey ) ; \n\n             Put  v2  =   new   Put ( rowkey ) ; \n\n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"code\" . getBytes ( ) , fields [ 1 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"rec_account\" . getBytes ( ) , fields [ 2 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"rec_bank_name\" . getBytes ( ) , fields [ 3 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"rec_name\" . getBytes ( ) , fields [ 4 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"pay_account\" . getBytes ( ) , fields [ 5 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"pay_name\" . getBytes ( ) , fields [ 6 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"pay_comments\" . getBytes ( ) , fields [ 7 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"pay_channel\" . getBytes ( ) , fields [ 8 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"pay_way\" . getBytes ( ) , fields [ 9 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"status\" . getBytes ( ) , fields [ 10 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"timestamp\" . getBytes ( ) , fields [ 11 ] . getBytes ( ) ) ; \n            v2 . addColumn ( \"C1\" . getBytes ( ) , \"money\" . getBytes ( ) , fields [ 12 ] . getBytes ( ) ) ; \n\n\n             //5.  \n            context . write ( k2 , v2 ) ; \n\n         } \n\n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 \n \n \n   \n \n \n \n package   com . itheima . hbase . bulkLoad ; \n\n import   org . apache . hadoop . conf . Configuration ; \n import   org . apache . hadoop . conf . Configured ; \n import   org . apache . hadoop . fs . Path ; \n import   org . apache . hadoop . hbase . HBaseConfiguration ; \n import   org . apache . hadoop . hbase . TableName ; \n import   org . apache . hadoop . hbase . client . Connection ; \n import   org . apache . hadoop . hbase . client . ConnectionFactory ; \n import   org . apache . hadoop . hbase . client . Put ; \n import   org . apache . hadoop . hbase . client . Table ; \n import   org . apache . hadoop . hbase . io . ImmutableBytesWritable ; \n import   org . apache . hadoop . hbase . mapreduce . HFileOutputFormat2 ; \n import   org . apache . hadoop . mapreduce . Job ; \n import   org . apache . hadoop . mapreduce . lib . input . TextInputFormat ; \n import   org . apache . hadoop . util . Tool ; \n import   org . apache . hadoop . util . ToolRunner ; \n\n public   class   BulkLoadDriver   extends   Configured   implements   Tool   { \n     @Override \n     public   int   run ( String [ ]  args )   throws   Exception   { \n\n         //1.  Job \n         Job  job  =   Job . getInstance ( super . getConf ( ) ,   \"BulkLoadDriver\" ) ; \n         // yarn  \n        job . setJarByClass ( BulkLoadDriver . class ) ; \n         //2.   \n         //2.1:    \n        job . setInputFormatClass ( TextInputFormat . class ) ; \n         TextInputFormat . addInputPath ( job , new   Path ( \"hdfs://node1:8020/hbase/bulkload/input\" ) ) ; \n         //2.2:  mapper mapk2v2 \n        job . setMapperClass ( BulkLoadMapper . class ) ; \n        job . setMapOutputKeyClass ( ImmutableBytesWritable . class ) ; \n        job . setMapOutputValueClass ( Put . class ) ; \n         //2.3:  shuffle:      \n\n         //2.7:  reduce   reduce  k3v3 \n        job . setNumReduceTasks ( 0 ) ;   // reduce \n\n        job . setOutputKeyClass ( ImmutableBytesWritable . class ) ; \n        job . setOutputValueClass ( Put . class ) ; \n\n         //2.8: ,   Hfile \n        job . setOutputFormatClass ( HFileOutputFormat2 . class ) ; \n         //    table \n         Connection  hbConn  =   ConnectionFactory . createConnection ( super . getConf ( ) ) ; \n         Table  table  =  hbConn . getTable ( TableName . valueOf ( \"TRANSFER_RECORD\" ) ) ; \n\n         HFileOutputFormat2 . configureIncrementalLoad ( job , table , hbConn . getRegionLocator ( TableName . valueOf ( \"TRANSFER_RECORD\" ) ) ) ; \n\n         HFileOutputFormat2 . setOutputPath ( job ,   new   Path ( \"hdfs://node1:8020/hbase/bulkload/output\" ) ) ; \n\n         //3.  \n         boolean  flag  =  job . waitForCompletion ( true ) ; \n\n         return  flag  ?   0 : 1 ; \n     } \n\n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         Configuration  conf  =   HBaseConfiguration . create ( ) ; \n        conf . set ( \"hbase.zookeeper.quorum\" , \"node1:2181,node2:2181,node3:2181\" ) ; \n         int  i  =   ToolRunner . run ( conf ,   new   BulkLoadDriver ( ) ,  args ) ; \n\n         System . exit ( i ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \n \n \n : \n \n \n \n \n HfileHBase \n  \n hbase org.apache.hadoop.hbase.tool.LoadIncrementalHFiles  MR  HBase\n \n 1 : \n hbase org.apache.hadoop.hbase.tool.LoadIncrementalHFiles  /hbase/bulkload/output  TRANSFER_RECORD\n \n 1 ,  :  10  \n \n : \n \n    Hbase: regionServer \n      regionServer, \n     , regionregionServer, regionServer\n \n 1 2 3 \n \n : \n \n   hbase   hdfs,  hbase, ,  \n \n 1 \n \n : \n \n \n \n  hbase-site.xml  hbase.root.dirhdfs? \n \n \n \n \n \n  hdfs core-site.xml  fs.defaultFShdfs? \n \n \n \n \n \n ,  core-site.xml \n \n \n ,  node2  node3 \n \n \n \n \n  hadoop  hbase , \n Spark bulkLoad \n /**\n  * @Author bigdatalearnshare\n  */ \nobject  App   { \n\n  def  main ( args :   Array [ String ] ) :   Unit   =   { \n     System . setProperty ( \"HADOOP_USER_NAME\" ,   \"root\" ) \n\n    val sparkSession  =   SparkSession \n       . builder ( ) \n       . config ( \"spark.serializer\" ,   \"org.apache.spark.serializer.KryoSerializer\" ) \n       . master ( \"local[*]\" ) \n       . getOrCreate ( ) \n    \n    val rowKeyField  =   \"id\" \n    \n    val df  =  sparkSession . read . format ( \"json\" ) . load ( \"/people.json\" ) \n\n    val fields  =  df . columns . filterNot ( _  ==   \"id\" ) . sorted\n\n    val data  =  df . rdd . map  {  row  = > \n      val rowKey  =   Bytes . toBytes ( row . getAs ( rowKeyField ) . toString ) \n\n      val kvs  =  fields . map  {  field  = > \n         new   KeyValue ( rowKey ,   Bytes . toBytes ( \"hfile-fy\" ) ,   Bytes . toBytes ( field ) ,   Bytes . toBytes ( row . getAs ( field ) . toString ) ) \n       } \n\n       ( new   ImmutableBytesWritable ( rowKey ) ,  kvs ) \n     } . flatMapValues ( x  = >  x ) . sortByKey ( ) \n    \n    val hbaseConf  =   HBaseConfiguration . create ( sparkSession . sessionState . newHadoopConf ( ) ) \n    hbaseConf . set ( \"hbase.zookeeper.quorum\" ,   \"linux-1:2181,linux-2:2181,linux-3:2181\" ) \n    hbaseConf . set ( TableOutputFormat . OUTPUT_TABLE ,   \"hfile\" ) \n    val connection  =   ConnectionFactory . createConnection ( hbaseConf ) \n\n    val tableName  =   TableName . valueOf ( \"hfile\" ) \n\n     //HBase \n     creteHTable ( tableName ,  connection ) \n\n    val table  =  connection . getTable ( tableName ) \n\n     try   { \n      val regionLocator  =  connection . getRegionLocator ( tableName ) \n\n      val job  =   Job . getInstance ( hbaseConf ) \n\n      job . setMapOutputKeyClass ( classOf [ ImmutableBytesWritable ] ) \n      job . setMapOutputValueClass ( classOf [ KeyValue ] ) \n\n       HFileOutputFormat2 . configureIncrementalLoad ( job ,  table ,  regionLocator ) \n\n      val savePath  =   \"hdfs://linux-1:9000/hfile_save\" \n       delHdfsPath ( savePath ,  sparkSession ) \n\n      job . getConfiguration . set ( \"mapred.output.dir\" ,  savePath ) \n\n      data . saveAsNewAPIHadoopDataset ( job . getConfiguration ) \n\n      val bulkLoader  =   new   LoadIncrementalHFiles ( hbaseConf ) \n      bulkLoader . doBulkLoad ( new   Path ( savePath ) ,  connection . getAdmin ,  table ,  regionLocator ) \n\n     }   finally   { \n       //WARN LoadIncrementalHFiles: Skipping non-directory hdfs://linux-1:9000/hfile_save/_SUCCESS ,HBASEHDFS \n      table . close ( ) \n      connection . close ( ) \n     } \n\n    sparkSession . stop ( ) \n   } \n\n  def  creteHTable ( tableName :   TableName ,  connection :   Connection ) :   Unit   =   { \n    val admin  =  connection . getAdmin\n\n     if   ( ! admin . tableExists ( tableName ) )   { \n      val tableDescriptor  =   new   HTableDescriptor ( tableName ) \n      tableDescriptor . addFamily ( new   HColumnDescriptor ( Bytes . toBytes ( \"hfile-fy\" ) ) ) \n      admin . createTable ( tableDescriptor ) \n     } \n   } \n\n  def  delHdfsPath ( path :   String ,  sparkSession :   SparkSession )   { \n    val hdfs  =   FileSystem . get ( sparkSession . sessionState . newHadoopConf ( ) ) \n    val hdfsPath  =   new   Path ( path ) \n\n     if   ( hdfs . exists ( hdfsPath ) )   { \n       //val filePermission = new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.READ) \n      hdfs . delete ( hdfsPath ,   true ) \n     } \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 #  HBaseHive \n hbasehive \n \n hive:   , HADOOP, Datanode, MR, SQL, join   ,  \n hbase: nosql, , hadoop, datanode, SQL, join ,  ,  \n \n : \n      hive  hbasehadoop, hivehbase,hive on hbase hqlphoenix \n 1.2 hbasehive \n : \n \n \n \n hivehbase, Hbaselib \n \n \n \n    cd  /export/server/hive-2.1.0/lib/\n   cp  hive-hbase-handler-2.1.0.jar  /export/server/hbase-2.1.0/lib/\n \n 1 2 \n \n \n    node1  node2hbaselib \n \n \n \n    cd  /export/server/hbase-2.1.0/lib/\n   scp  hive-hbase-handler-2.1.0.jar node1:/export/server/hbase-2.1.0/lib/\n   scp  hive-hbase-handler-2.1.0.jar node2:/export/server/hbase-2.1.0/lib/\n \n 1 2 3 \n \n \n  hive : \n \n \n hive-site.xml \n \n \n \n   cd /export/server/hive-2.1.0/conf/ \n  vim hive-site.xml \n  \n  :\n    < property > \n           < name > hive.zookeeper.quorum </ name > \n           < value > node1,node2,node3 </ value > \n    </ property > \n  \n  \n    < property > \n           < name > hbase.zookeeper.quorum </ name > \n           < value > node1,node2,node3 </ value > \n    </ property > \n  \n    < property > \n           < name > hive.server2.enable.doAs </ name > \n           < value > false </ value > \n    </ property > \n  \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \n hive-env.sh \n \n    cd  /export/server/hive-2.1.0/conf/ \n   vim  hive-env.sh \n  \n  :\n   export   HBASE_HOME = /export/server/hbase-2.1.0\n \n 1 2 3 4 5 \n \n  \n \n   1)    zookeeper , zookeeper \n   2)    hadoop, hadoop \n   3)    hbase, hbase \n   4)    hive  , hive \n  \n    :   3   4   \n \n 1 2 3 4 5 6 \n  : \n \n   : hbaseshell,  \n  hbase ( main ) :007: 0 >  create  'hbase_hive_score' , 'cf' \n  \n  hbase ( main ) :007: 0 >  put  'hbase_hive_score'  , '1' , 'cf:name' , 'zhangsan' \n                                                                               \n  hbase ( main ) :007: 0 >  put  'hbase_hive_score'  , '1' , 'cf:age' , '25' \n                                                                                 \n  hbase ( main ) :008: 0 >  put  'hbase_hive_score'  , '2' , 'cf:name' , 'lisi' \n                                                                                   \n  hbase ( main ) :009: 0 >  put  'hbase_hive_score'  , '2' , 'cf:age' , '30' \n                                                                                \n  hbase ( main ) :010: 0 >  put  'hbase_hive_score'  , '3' , 'cf:name' , 'wangwu' \n                                                                                 \n  hbase ( main ) :011: 0 >  put  'hbase_hive_score'  , '3' , 'cf:age' , '18' \n  \n  hbase ( main ) :012: 0 >  scan  'hbase_hive_score' \n  ROW                        COLUMN+CELL                                                              \n    1                           column = cf:age,  timestamp = 1615427034130 ,  value = 25                          \n    1                           column = cf:name,  timestamp = 1615427024464 ,  value = zhangsan                  \n    2                           column = cf:age,  timestamp = 1615427052348 ,  value = 30                          \n    2                           column = cf:name,  timestamp = 1615427045923 ,  value = lisi                      \n    3                           column = cf:age,  timestamp = 1615427082291 ,  value = 18                          \n    3                           column = cf:name,  timestamp = 1615427073970 ,  value = wangwu      \n  \n  \n  \n  : hivehbase, hbase, hive\n  :\n      create  external table    ( \n         \n         1 ,\n         2 ,\n         3 \n          .. .. \n       )  stored by  'org.apache.hadoop.hive.hbase.HBaseStorageHandler'  with serdeproperties ( 'hbase.columns.mapping' = ':key,1:1...' )  tblproperties ( 'hbase.table.name' = 'hbase' ) ; \n      \n  : \n       :  hbase  ( OK ) \n       :  hbase \n         , , primary key\n  \n  create  external table  day03_hivehbase.hbase_hive_score  ( \n   id  int,\n  name string,\n  age int\n   )  stored by  'org.apache.hadoop.hive.hbase.HBaseStorageHandler'  with serdeproperties ( 'hbase.columns.mapping' = ':key,cf:name,cf:age' )  tblproperties ( 'hbase.table.name' = 'hbase_hive_score' ) ; \n  \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 \n hive: \n  Apache Phoenix \n Apache   Phoenix   SQLCRUDhbase hbase \n  phoenixhbasehiveimpala \n HBase \n 3.1  \n hbase \n \n \n \n observer: \n \n \n \n        observer    , observer(), , ,  \n       \n  ?\n       1)    \n       2)    \n \n 1 2 3 4 5  ![image-20210311153033576](hbase.assets/image-20210311153033576.png)\n \n \n \n \n endpoint: \n \n \n \n          , java,   \n      ,   server(), , ,   \n     \n    :      sum  count  max ... \n \n 1 2 3 4 3.2  \n \n \n :    \n \n \n \n   hbase-site.xml    , hbase  \n \n 1 \n \n : ,  \n \n \n :  \n \n \n :  \n \n \n \n :  \n \n \n \n \n : \n \n \n :  \n \n \n :  \n \n \n \n :  \n \n \n \n \n apache Phoenix \n : \n 1) , hbase,  hbase lib,3 OK\n\n2) , Phoenix, ,  Phoenixbinhbase-site.xml hbaseconfhbase-site.xml\n \n 1 2 3 #  Apache Phoenix \n Grammar | Apache Phoenix \n \n \n \n Phoenix \n \n \n \n    --:  \n   create   table   [ if   not   exists ]    ( \n     rowkey    primary   key , \n      .  1    , \n      .  2    , \n      .  3    , \n      .  4   \n      . . . . . \n   ) ; \n \n 1 2 3 4 5 6 7 8 9 \n :  \n \n    create   table   order_dtl  ( \n     id   varchar   primary   key , \n     c1 . status   varchar   , \n     c1 . money   integer   , \n     c1 . pay_way  integer   , \n     c1 . user_id  varchar , \n     c1 . operation  varchar , \n     c1 . category  varchar  \n   ) ; \n \n 1 2 3 4 5 6 7 8 9 \n , hbasewebui, , region1,  \n \n      : Phoenix:    \n      : ,  ,  \n create table  \"order_dtl_01\" (\n   \"id\"  varchar primary key,\n   \"c1\".\"status\" varchar ,\n   \"c1\".money  integer ,\n   c1.\"pay_way\" integer ,\n   c1.user_id varchar,\n   c1.operation varchar,\n   c1.category varchar \n);\n \n : \n, () \n   \n : , , , ,  \n \n \n \n Phoenix \n \n \n \n   :\n      !table\n \n 1 2 \n \n \n \n  \n \n \n \n    : \n      !desc  \n \n 1 2 \n : Phoenix:    \n : ,  ,  \n \n \n \n  \n \n \n \n   :\n     upsert into (.1,.2,... ) values(1,2 ....)\n     \n  :\n     upsert into ORDER_DTL values('000002','',4070,1,'4944191','2020/04/25 12:09:16','');\n \n 1 2 3 4 5 \n \n \n \n : SQL \n \n \n   join   \n \n \n \n \n : SQL \n \n \n \n \n \n  \n \n \n \n   : \n      select * from   limit n offset(m-1)\n      (mn)\n      \n  :\n  1) \n  UPSERT INTO \"ORDER_DTL\" VALUES('000002','',4070,1,'4944191','2020-04-25 12:09:16',';');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000003','',4350,1,'1625615','2020-04-25 12:09:37',';;;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000004','',6370,3,'3919700','2020-04-25 12:09:39',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000005','',6370,3,'3919700','2020-04-25 12:09:44',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000006','',9380,1,'2993700','2020-04-25 12:09:41',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000007','',9380,1,'2993700','2020-04-25 12:09:46',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000008','',6400,2,'5037058','2020-04-25 12:10:13',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000009','',280,1,'3018827','2020-04-25 12:09:53',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000010','',5600,1,'6489579','2020-04-25 12:08:55',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000011','',5600,1,'6489579','2020-04-25 12:09:00',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000012','',8340,2,'2948003','2020-04-25 12:09:26',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000013','',8340,2,'2948003','2020-04-25 12:09:30',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000014','',7060,2,'2092774','2020-04-25 12:09:38',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000015','',640,3,'7152356','2020-04-25 12:09:49',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000016','',9410,3,'7152356','2020-04-25 12:10:01',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000017','',9390,3,'8237476','2020-04-25 12:10:08',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000018','',7490,2,'7813118','2020-04-25 12:09:05',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000019','',7490,2,'7813118','2020-04-25 12:09:06',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000020','',5360,2,'5301038','2020-04-25 12:08:50',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000021','',5360,2,'5301038','2020-04-25 12:08:53',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000022','',5360,2,'5301038','2020-04-25 12:08:58',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000023','',6490,0,'3141181','2020-04-25 12:09:22',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000024','',3820,1,'9054826','2020-04-25 12:10:04',';;;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000025','',4650,2,'5837271','2020-04-25 12:08:52',';;');\n  UPSERT INTO \"ORDER_DTL\" VALUES('000026','',4650,2,'5837271','2020-04-25 12:08:57',';;');\n  \n  2) :  5   , \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \n \n \n \n : SQL \n Apache Phoenix \n \tPhoenix, , region \n Phoenix: \n \n \n \n : \n \n \n \n   :\n   create   table   [ if   not   exists ]    ( \n     rowkey    primary   key , \n      .  1    , \n      .  2    , \n      .  3    , \n      .  4   \n      . . . . . \n   ) \n  compression = 'GZ'   --  \n  split  on ( region )    --  \n   ; \n  \n  : \n   drop   table  order_dtl ; \n   create   table   order_dtl  ( \n     id   varchar   primary   key , \n     c1 . status   varchar   , \n     c1 . money   integer   , \n     c1 . pay_way  integer   , \n     c1 . user_id  varchar , \n     c1 . operation  varchar , \n     c1 . category  varchar  \n   ) \n  compression = 'GZ' \n  split  on ( '10' , '20' , '30' ) ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n \n \n hash: \n \n \n \n   :\n   create   table   [ if   not   exists ]    ( \n     rowkey    primary   key , \n      .  1    , \n      .  2    , \n      .  3    , \n      .  4   \n      . . . . . \n   ) \n  compression = 'GZ' ,   --  \n  salt_buckets = N   --  (hash + rowkey) \n   ; \n  \n  \n   drop   table  order_dtl ; \n   create   table   order_dtl  ( \n     id   varchar   primary   key , \n     c1 . status   varchar   , \n     c1 . money   integer   , \n     c1 . pay_way  integer   , \n     c1 . user_id  varchar , \n     c1 . operation  varchar , \n     c1 . category  varchar  \n   ) \n  compression = 'GZ'   , \n  salt_buckets = 10 ; \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26       : Phoenix, Phoenix, rowkey, Phoenix,hbase \n apache  Phoenix \n \t   , PhoenixPhoenix, hbasehbase, Phoenix, , Phoenixhbase \n \t   PhoenixhbaseSQL, Phoenix \n ? \n :\n    create    view    \"\" . \"hbase\"   ( \n        key   varchar    primary   key , \n        \"\" . \"\"   , \n        . . . . . \n   \n    )   [ default_colunm_family = '' ] ; \n\n:\n       hbase\n      key  ,  primary   key \n      ,   hbase\n     \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 :  WATER_BILL   \n \n   create    view    \"WATER_BILL\"   ( \n       ID  varchar    primary   key , \n       C1 . NAME  varchar , \n       C1 . ADDRESS  varchar , \n       C1 . LATEST_DATE  varchar , \n       C1 . NUM_CURRENT UNSIGNED_DOUBLE  , \n       C1 . NUM_PREVIOUS UNSIGNED_DOUBLE  , \n       C1 . NUM_USAGE UNSIGNED_DOUBLE  , \n       C1 . PAY_DATE  varchar , \n       C1 . RECORD_DATE  varchar , \n       C1 . SEX  varchar , \n       C1 . TOTAL_MONEY UNSIGNED_DOUBLE \n    ) ; \n   \n   UNSIGNED_DOUBLE:  double \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n , : \n \n   \n select   name , num_usage ,  record_date  from  water_bill  where  record_date  between   '2020-06-01'   and   '2020-06-30' ; \n \n 1 \n Apache Phoenix \n     :   \n \n 2.1 Phoenix \n Phoenix: \n \n \n \n  \n \n \n : \n \n \n \n       ,   ,region,  , , , , ,  \n  \n    :  \n       ,   , , ,  \n       ,   SQL,  \n  \n   ,        \n \n 1 2 3 4 5 6 7 \n :   \n ? \n \n   :\n       create   index      on    (  1 ,  2   . . . . ) \n \n 1 2 \n ? \n \n   :\n      drop   index     on   ; \n \n 1 2 \n \n \n \n  \n \n \n : \n \n \n \n       ,   , , (  ) , ,  \n       ,   Phoenix, ,  \n  \n    : \n       ,   , ,  \n \n 1 2 3 4 5 \n :      \n ? \n \n   :\n       create   local   index      on    (  1 ,  2   . . . . ) \n \n 1 2 \n  \n \n   :\n      drop   index     on   ; \n \n 1 2 \n \n \n  \n \n \n : \n \n \n \n        ,   , , , , , , ,  \n       \n       \n \n 1 2 3 \n :    \n ? \n \n   :\n      create   [ local ]   index      on   (  1 ,   2. . . . )  include (  1 ,  2. . . ) \n \n 1 2 \n \n :  \n \n \n \n  \n \n \n  \n \n \n \n       ,   ,  , ,  \n \n 1 \n : SQL,  \n ? \n \n   :\n      create   [ local ]   index      on   (  1 ,  2 ,  (  . . . ) . . . ) \n \n 1 2 2.2 : + \n \n \n :   id 8237476  \n \n \n SQL: \n \n \n    select   USER_ID , ID , MONEY  from  order_dtl  where  USER_ID = '8237476' ; \n  \n    explain   : \n   explain   select   USER_ID , ID , MONEY  from  order_dtl  where  USER_ID = '8237476' ; \n  \n  :   \n \n 1 2 3 4 5 6 \n \n  \n \n 1 )  :\n       create    index   idx_index_order_dtl  on  order_dtl ( user_id )  include ( ID , MONEY ) ; \n \n 1 2 \n 2 )   SQL ,  \n    explain   select   USER_ID , ID , MONEY  from  order_dtl  where  USER_ID = '8237476' ; \n \n 1 2 \n , , ? \n \n :  \n : ,  \n :   \n explain   select   /*+INDEX(order_dtl idx_index_order_dtl) */    *   from  order_dtl  where  USER_ID = '8237476' ; \n \n 1 \n  \n drop   index  idx_index_order_dtl  on  order_dtl ; \n \n 1 2.3 :   \n \n :     ID        ID  \n \n 1) :\n   create local index  IDX_LOCAL_order_dtl  on  order_dtl(ID,STATUS,MONEY,PAY_WAY,USER_ID);\n \n 1 2 \n : !table, hbase \n 2 )   1 : \n    explain   select   USER_ID , ID , MONEY  from  order_dtl  where  USER_ID = '8237476' ; \n \n 1 2 \n 3 )   2 :  \n    explain   select   USER_ID , ID , MONEY , CATEGORY  from  order_dtl  where  USER_ID = '8237476' ; \n \n 1 2 \n 3 )   3 :  \n     explain   select    *   from  order_dtl  where  USER_ID = '8237476' ;  \n    \n:\n   \n \n 1 2 3 4 5 \n :  , , API  \n \n 2.4 : WATER_BILL \n \n WATER_BILLSQL: \n \n select  name,num_usage, record_date from water_bill where record_date between '2020-06-01' and '2020-06-30';\n\n: 8s\n \n 1 2 3 \n : \n \n 1 )  :   |   +  \n    create   index  IDX_INDEX_WATER_BILL  ON  WATER_BILL ( record_date )  include  ( name , num_usage ) ; \n\n 2 )  : \n    select   name , num_usage ,  record_date  from  water_bill  where  record_date  between   '2020-06-01'   and   '2020-06-30' ; \n  \n:   7 s\n\n \n 1 2 3 4 5 6 7 8  \n 1)    \n 2)   , ,  \n 3)   ,,  \n\n  :    \n\n  :     \n\n  :     \n \n 1 2 3 4 5 6 7 8 9 #  hbase \n hbase() \n \thbase   mysql  hive, hbase() \n : mysqlhive ? \n \n \n \n , \n \n \n \n \n  \n \n \n \n \n  \n \n \n \n , hbase \n hbase, , ,  hbase   default \n \n  hbase:  hbase, , , hbase\n \n :  meta\n \n hbase \n \n \n \n \n  default :   , hbase, , \n \n  hivedefault \n   \n \n \n \n \n HBase? \n: \ncreate_namespace   '' \n # \nlist_namespace\n # \ndescribe_namespace  '' \n # \ndrop_namespace  '' \n   : , \n # \ncreate  ':' , ''   .. .\n   : default, , ,hbasedefault\n \n 1 2 3 4 5 6 7 8 9 10 11 12 #  hbase \n :  , , ,  \n : \n      regionstore, storememStore  storeFile, , , IO,  \n      , region  compact \n HFile \n :  ?  2~5 \n \n , , , ,  \n , ,   \n hbase \n       hbase  region,  regionregionServer \n : \n       , hbase, , , ? \n \t :  regionregionServer,  \n       : region, regionServer, regionServer \n ? \n   ,   region, regionregionServer,  \n \n 1 : HBase \n \n \n :  hbase, region \n \n \n \n : \n \n \n \n  \n \n create  ''  , '1' ,  SPLITS = > [  ]   \n\n: \ncreate  't1_split' , 'f1' ,SPLITS = > [ '10' , '20' , '30' , '40' ] \n \n 1 2 3 4 \n \n \n \n  hash \n \n  \n    create  ''  , '1' ,  { NUMREGIONS = > N,SPLITALGO = > 'HexStringSplit' } \n    \n: \n    create  't2_split' , 'f1' , { NUMREGIONS = > 10 ,SPLITALGO = > 'HexStringSplit' } \n \n 1 2 3 4 5 \n hbase.hregion.max.filesize5-10GBHBasemajor_compactmajor_compactsplit region5GBhbase.hregion.max.filesizeregionserverregions200regionsregionserver FullGC \n hbaseTTL \n  \n  hbase  \n \n : hbase ,  ,\n \n  0 \n \n \n : hbase , , \n \n  1 \n \n \n \n 2.6.2 TTL \n hbase, , , hbase \n 2.6.3 TTL \n \n : \n \n public   class   HBaseTTLTest   { \n     //1. Hbase (   TTL) \n     //2. ,   () \n     //3. :   \n\n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         //1. hbase \n         Configuration  conf  =   HBaseConfiguration . create ( ) ; \n        conf . set ( \"hbase.zookeeper.quorum\" , \"node1:2181,node2:2181,node3:2181\" ) ; \n         Connection  hbConn  =   ConnectionFactory . createConnection ( conf ) ; \n\n         //2.  \n         Admin  admin  =  hbConn . getAdmin ( ) ; \n\n         //3. : \n\n         //3.1:  \n         if (   ! admin . tableExists ( TableName . valueOf ( \"day03_hbaseTTL\" ) ) ) { \n             //    \n\n             TableDescriptorBuilder  descBuilder  =   TableDescriptorBuilder . newBuilder ( TableName . valueOf ( \"day03_hbaseTTL\" ) ) ; \n\n             ColumnFamilyDescriptorBuilder  familyDesc  =   ColumnFamilyDescriptorBuilder . newBuilder ( \"C1\" . getBytes ( ) ) ; \n\n             //      TTL \n            familyDesc . setMinVersions ( 3 ) ;    //   \n            familyDesc . setMaxVersions ( 5 ) ;    //   \n\n            familyDesc . setTimeToLive ( 30 ) ;    //  ttl  30s \n\n             ColumnFamilyDescriptor  family  =  familyDesc . build ( ) ; \n            descBuilder . setColumnFamily ( family ) ; \n\n             TableDescriptor  desc  =  descBuilder . build ( ) ; \n\n             //3.2:  \n            admin . createTable ( desc ) ; \n         } \n\n\n         //3.3:   \n         //3.3.1:  table \n         Table  table  =  hbConn . getTable ( TableName . valueOf ( \"day03_hbaseTTL\" ) ) ; \n\n         //3.3.2:  \n\n         for ( int  i  =   1   ;  i <=   2   ;  i ++ ) { \n             Put  put  =   new   Put ( \"rk001\" . getBytes ( ) ) ; \n\n            put . addColumn ( \"C1\" . getBytes ( ) , \"NAME\" . getBytes ( ) , ( \"zhangsan\" + i ) . getBytes ( ) ) ; \n\n            table . put ( put ) ; \n         } \n\n         //3.4:  \n         Get  get  =   new   Get ( \"rk001\" . getBytes ( ) ) ; \n        get . readAllVersions ( ) ;   //  \n         Result  result  =  table . get ( get ) ; \n\n         //Cell[] rawCells = result.rawCells(); \n         List < Cell >  cells  =  result . listCells ( ) ; \n\n         for   ( Cell  cell  :  rawCells )   { \n\n             System . out . println ( Bytes . toString ( CellUtil . cloneValue ( cell ) ) ) ; \n         } \n\n\n         //4.  \n        table . close ( ) ; \n        admin . close ( ) ; \n        hbConn . close ( ) ; \n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 \n \n : \n        , hbase min_version, .  \n hbaserowkey \n : \n        hbase, region, region \n : \n      , rowkey region,  \n : rowkey, , region, ,  \n rowkey? \n \n rowkey \n \n 1)    / rowkey \n 2)   rowkey  \n    ,   rowkey   , , flush,  \n    rowkey :   64kb \n     :    100 ,  10~20 \n   \n 3)   Long String \n 4)   rowkey \n \n 1 2 3 4 5 6 7 8 ? \n \n \n \n  \n \n \n  : ,  \n \n \n \n \n (rowkey) \n \n \n : ,  \n \n \n \n \n  MurmurHash3:   hash,region \n hbase \n \n ? \n 1) , , , ,  snappy\n2) , , , ,  GZIP|GZ\n \n 1 2 hbase?   HBASE,  \n \n : \n :\n   : create '' , {NAME=>'',COMPRESSION=>'GZ|SNAPPY'}\n   , :  alter '', {NAME=>'',COMPRESSION=>'GZ|SNAPPY'}\n \n 1 2 3 \n : \n        SNAPPY , HBase ,  Hadoop, Chbase  \n        LZO, LZOjar \n HBase \n  \n B+HBaseKuduLSMLSM(Log-Structured Merge-Tree) \n \n \n B+B+ IO  \n \n \n LSM   \n \n \n LSM \n \n \n \n LSM LSM \n \n \n HBaseMemStoreStoreFile \n \n \n BHBaseB+ \n \n \n C0 \n \n \n C1CKkey \n \n \n"},{title:"Docker",frontmatter:{title:"Docker",date:"2023-06-10T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:[""]},regularPath:"/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%84%9A%E6%9C%AC/protobuf3%E8%AF%AD%E6%B3%95.html",relativePath:"/protobuf3.md",key:"v-e463dc58",path:"/2023/06/10/protobuf3%E8%AF%AD%E6%B3%95/",headers:[{level:2,title:"protobuf",slug:"protobuf"},{level:2,title:"ProtoBuf",slug:"protobuf"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"Reserved",slug:"-reserved"},{level:2,title:".proto",slug:"-proto"},{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"Oneof",slug:"oneof"},{level:3,title:"Oneof",slug:"oneof"},{level:3,title:"Oneof ",slug:"oneof-"},{level:3,title:"",slug:""},{level:2,title:"Map",slug:"map-"},{level:3,title:"",slug:"-2"},{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"(Service)",slug:"-service"},{level:2,title:"JSON ",slug:"json-"},{level:2,title:"",slug:""},{level:3,title:"",slug:""}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:' Protobuf \n protobuf \n protobuf? \n (1)java \n (2)XML \n (3)jsonjsonjson \n protobuf \n ProtoBuf 3 \n ProtoBuf \n .proto \n syntax   =   "proto3" ; \n message   SearchRequest   { \n\t string  query  =   1 ; \n\t int32  page_number  =   2 ; \n\t int32  result_per_page  =   3 ; \n } \n \n 1 2 3 4 5 6 \n proto3proto2 \n SearchRequest3 \n  \n  \n \n page_numberresult_per_page \n stringquery \n \n enumerations \n  \n  \n \n \n [1,15] \n [16,2047]2 \n  [1,15] \n  \n \n \n \n 12^29 - 1, or 536,870,911 \n [1900019999] Protobuf \n  \n  \n \n \n singular011 \n \n \n repeated0 \n proto3repeatedpacked \n pakced Protocol Buffer  \n  \n .protoSearchResponse.proto \n message   SearchRequest   { \n\t string  query  =   1 ; \n\t int32  page_number  =   2 ; \n\t int32  result_per_page  =   3 ; \n } \n message   SearchResponse   { \n\t . . . \n } \n \n 1 2 3 4 5 6 7 8 #   \n .protoC/C++/java//  \n message   SearchRequest   { \n\t string  query  =   1 ; \n\t // Which page number do we want? \n\t int32  page_number  =   2 ;  \n\t // Number of results to return per page. \n\t int32  result_per_page  =   3 ;  \n } \n \n 1 2 3 4 5 6 7 #  Reserved \n .protoprotocol buffer \n message   Foo   { \n\t reserved   2 ,   15 ,   9   to   11 ; \n\t reserved   "foo" ,   "bar" ; \n } \n \n 1 2 3 4 \n reserved \n .proto \n protocol buffer.proto.protoJava.javaBuilder \n  \n .proto \n \n \n \n .proto Type \n Notes \n Java Type \n \n \n \n \n double \n  \n double \n \n \n float \n  \n float \n \n \n int32 \n sint64 \n int \n \n \n uint32 \n  \n int \n \n \n uint64 \n  \n long \n \n \n sint32 \n int32 \n int \n \n \n sint64 \n int64 \n long \n \n \n fixed32 \n 4228uint32 \n int \n \n \n fixed64 \n 8256uint64 \n long \n \n \n sfixed32 \n 4 \n int \n \n \n sfixed64 \n 8 \n long \n \n \n bool \n  \n boolean \n \n \n string \n UTF-87-bit ASCII \n String \n \n \n bytes \n  \n ByteString \n \n \n \n \n java3264 \n  \n 6432ilongint \n Integer64string32 \n  \n singular \n \n \n stringsstring \n \n \n bytesbytes \n \n \n boolsfalse \n \n \n 0 \n \n \n 0; \n \n \n message \n \n \n  \n \n \n \n booleanfalsebooleanfalse \n  \n SearchRequest corpuscorpusUNIVERSALWEBIMAGESLOCALNEWSPRODUCTSVIDEO enum \n Corpus Corpus \n message   SearchRequest   { \n\t string  query  =   1 ; \n\t int32  page_number  =   2 ; \n\t int32  result_per_page  =   3 ; \n\t enum   Corpus   { \n\t\tUNIVERSAL  =   0 ; \n\t\tWEB  =   1 ; \n\t\tIMAGES  =   2 ; \n\t\tLOCAL  =   3 ; \n\t\tNEWS  =   4 ; \n\t\tPRODUCTS  =   5 ; \n\t\tVIDEO  =   6 ; \n      } \n      Corpus  corpus  =   4 ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Corpus00 \n \n \n 00 \n \n \n proto2 \n \n \n allow_aliastrue \n \n \n 32enumenum .proto MessageType.EnumType \n \n \n .protoprotocol bufferenumJavaC++EnumDescriptor Pythonsymbolic constants \n \n \n C++GoJava \n  \n SearchResponseResult.protoResultSearchResponseResult \n  \n ResultSearchResponse \n Parent.Type \n message   SomeOtherMessage   { \n\t SearchResponse . Result  result  =   1 ; \n } \n \n 1 2 3 #   \n  \n \n  \n proto2proto2 \n requiredOBSOLETE_.proto \n int32, uint32, int64, uint64,bool C++64int32 32 \n sint32sint64 \n stringbytesbytesUTF-8 \n bytesbytes \n fixed32sfixed32fixed64sfixed64 \n int32uint32int64uint64proto3int \n Oneof \n   oneof. \n Oneof     case()  WhichOneof()  oneof. \n Oneof \n .protoOneof oneof, test_oneof: \n message SampleMessage {\n\toneof test_oneof {\n\t\tstring name = 4;\n\t\tSubMessage sub_message = 9;\n\t}\n}\n \n 1 2 3 4 5 6 oneof oneof . , repeated . \n , oneof getters setters . . \n Oneof  \n \n oneofoneof. . \n \n SampleMessage  message ; \nmessage . set_name ( "name" ) ; \n CHECK ( message . has_name ( ) ) ; \nmessage . mutable_sub_message ( ) ;    // Will clear name field.``CHECK(!message.has_name());` \n \n 1 2 3 4 \n oneof \n oneof repeated . \n APIoneof . \n  \n oneof. oneof None/NOT_SET , oneof oneof \n Tage  \n \n oneof \n oneof \n oneof \n Map \n protocol buffer \n map<key_type, value_type> map_field = N;\n \n 1  key_type Integerstringfloatingbytes value_type  \n project Projecct stringkey \n map<string, Project> projects = 3;\n \n 1 \n Maprepeated \n mapMap \n .protomapkey key \n keykeymapkey \n \n mapAPIproto3 API  \n  \n mapmapprotocol buffer \n message   MapFieldEntry   { \n   key_type  key  =   1 ; \n   value_type  value  =   2 ; \n } \n\n repeated   MapFieldEntry  map_field  =  N ; \n \n 1 2 3 4 5 6 #   \n .protopackage \n packag foo . bar ; \n message   Open   {   . . .   } \n \n 1 2 + \n message   Foo   { \n\t . . . \n\t required   foo . bar . Open  open  =   1 ; \n\t . . . } \n \n 1 2 3 4  \n  \n Protocol bufferC++  foo.bar.Baz . \n ProtocolBuffer.proto  \n (Service) \n RPC().protoRPCprotocol bufferRPC SearchRequestSearchResponse.proto \n service   SearchService   { \n\t rpc   Search   ( SearchRequest )   returns   ( SearchResponse ) ; \n } \n \n 1 2 3 protocol bufferRPC gRPC PRCgRPCprotocl bufferprotocol buffer.protoRPC \n gRPCprotocol bufferRPC proto2 \n PRCProtocol Buffer wiki  \n JSON  \n Proto3 JSON \n JSON null protocol bufferprotocol bufferJSONJSON \n \n \n \n proto3 \n JSON \n JSON \n  \n \n \n \n \n message \n object \n {fBar: v, g: null, } \n JSONlowerCamelCaseJSONnull \n \n \n enum \n string \n FOO_BAR \n proto \n \n \n map \n object \n {k: v, } \n string \n \n \n repeated V \n array \n [v, ] \n null \n \n \n bool \n true, false \n true, false \n \n \n \n string \n string \n Hello World! \n \n \n \n bytes \n base64 string \n YWJjMTIzIT8kKiYoKSctPUB+ \n \n \n \n int32, fixed32, uint32 \n number \n 1, -10, 0 \n JSONstring \n \n \n int64, fixed64, uint64 \n string \n 1, -10 \n JSONstring \n \n \n float, double \n number \n 1.1, -10.0, 0, NaN, Infinity \n JSONNaN,infinity-Infinity \n \n \n Any \n object \n {@type: url, f: v,  } \n AnyJSON {"@type": xxx, "value": yyy} JSON @type  \n \n \n Timestamp \n string \n 1972-01-01T10:00:20.021Z \n RFC 339Z-0369 \n \n \n Duration \n string \n 1.000340012s, 1s \n 0369 \n \n \n Struct \n object \n {  } \n JSONstruct.proto \n \n \n Wrapper types \n various types \n 2, 2, foo, true, true, null, 0,  \n JSONnulllnull \n \n \n FieldMask \n string \n f.fooBar,h \n fieldmask.proto \n \n \n ListValue \n array \n [foo, bar, ] \n \n \n \n Value \n value \n \n JSON \n \n \n NullValue \n null \n \n JSON null \n  \n .protooptionsOptionsgoogle/protobuf/descriptor.proto \n enumenumenum \n  \n 1 java_package  () :java.protojava_package javajava \n option java_package  =   "com.example.foo" ; \n \n 1 2 java_outer_classname  (): Java.protojava_outer_classnameclass.protofoo_bar.protojavaFooBar.java,java \n option  java_outer_classname  =   "Ponycopter" ; \n \n 1 3 optimize_for ():  SPEED, CODE_SIZE,LITE_RUNTIMEC++java \n \n SPEED (default) : protocol buffer \n CODE_SIZE : protocol bufferSPEED APISPEED.proto  \n LITE_RUNTIME : protocol bufferlibprotobuf-lite libprotobuf SPEED MessageLiteMessager \n \n option  optimize_for  =  CODE_SIZE ; \n \n 1 \n deprecated (): true java @Deprecated  \n \n int32 old_field = 6 [deprecated=true];\n \n 1 #   \n ProtocolBuffers  Proto2 Language Guide proto3 \n'},{title:"ElasticSearch",frontmatter:{title:"ElasticSearch",date:"2019-10-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["",""]},regularPath:"/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/ElasticSearch.html",relativePath:"/ElasticSearch.md",key:"v-007b24d3",path:"/2019/10/08/elasticsearch/",headers:[{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"ES",slug:"es"},{level:2,title:"ES",slug:"es"},{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"ES",slug:"es"},{level:2,title:"ES",slug:"es"},{level:3,title:"",slug:""},{level:3,title:"!image-20210621212250395",slug:""},{level:2,title:"Elasticsearch",slug:"elasticsearch"},{level:2,title:"ES()",slug:"es-"},{level:2,title:"ES",slug:"es"},{level:2,title:"9.ESRestFulAPI",slug:"_9-esrestfulapi"},{level:3,title:"",slug:""}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:'  \n  \n \n \n  \n \n elasticSearch :    :    ---  \n logstash:   : --  \n Kibana :      ---  \n Beats :    --  \n --ETL()-- \n \n \n \n \n  \n \n  \n  \n 360 \n UC \n JD- \n OA \n \n \n \n  \n \n \n \n  \n \n \n \n \n  \n ES \n \n \n ES \n \n \n \n \n \n ES \n \n Lucene(Jar)---------Solr(3.0)---------ES() \n ESJava \n ES \n 1. \n 2. \n 3.PB \n \n \n ES\n \n \n \n \n \n \n \n SparkHadoop100\n \n SparkHadoop \n Hadoop3.xSpark3 \n \n \n ESSolr\n \n ES Json    \n ZookeeperES() \n  \n \n \n Lucene \n \n  \n \n \n \n  \n \n 1-\n \n MySQL \n ES-Solr-lucecne \n \n \n 2-\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n  \n \n \n 1- \n \n \n  \n \n \n  \n \n \n     \n \n \n        \n \n \n IK----- \n \n \n \n \n \n \n package   cn . itcast . lucene ; \n\n import   org . wltea . analyzer . core . IKSegmenter ; \n import   org . wltea . analyzer . core . Lexeme ; \n\n import   java . io . IOException ; \n import   java . io . StringReader ; \n\n /**\n * IK  \n * IKesIKik_smartesIKik_max_word\n * demo\n * -\n * \n */ \n public   class   IKSegmenterTest   { \n     static   String  text  =   "" ; \n\n     public   static   void   main ( String [ ]  args )   throws   IOException   { \n         IKSegmenter  segmenter  =   new   IKSegmenter ( new   StringReader ( text ) ,   false ) ; \n         Lexeme  next ; \n         System . out . print ( "" ) ; \n         while   ( ( next  =  segmenter . next ( ) )   !=   null )   { \n             System . out . print ( next . getLexemeText ( )   +   " " ) ; \n         } \n         System . out . println ( ) ; \n         System . out . println ( "-------------------------------------------------" ) ; \n         IKSegmenter  smartSegmenter  =   new   IKSegmenter ( new   StringReader ( text ) ,   true ) ; \n         System . out . print ( "" ) ; \n         while   ( ( next  =  smartSegmenter . next ( ) )   !=   null )   { \n             System . out . print ( next . getLexemeText ( )   +   " " ) ; \n             //       \n         } \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \n \n 2- \n \n \n  \n \n \n  \n \n 1-\n \n 1-StandardIK \n 2- \n \n \n 2-\n \n  \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n  \n \n \n \n \n \n IK \n \n IKPythonJieba(Python) \n JavaIK \n \n \n \n \n \n \n 3- \n \n \n  \n \n \n Lucene \n \n  \n \n \n \n () \n \n \n  \n \n lucene \n  \n  \n \n 1-\n \n 1-StandardIKjava IKPythonJieba(Python) \n 2- \n \n \n 2-\n \n  \n topn \n ES \n \n \n JPS \n \n jpsJDK 1.5javapidlinux/unixjava \n hadoop \n \n \n \n \n  \n \n \n WebUIES \n \n Web-- \n \n \n \n \n \n Web--nodejs \n ES-Head \n \n \n \n \n \n \n \n \n \n \n  \n \n Master \n Worker \n Primary Shard---- \n Repication Shard \n index \n type \n id \n document \n \n field \n ES \n   \n \n \n \n DataNodenode2.itcast.cnnode2.itcast.cn   coordinating node \n \n \n  \n \n \n l ID \n l  shard = hash(routing) % number_of_primary_shards \n l routing   _id \n \n \n coordinating nodeprimary shardDataNodeprimary shardnode1.itcast.cnreplica shardnode2.itcast.cn \n \n \n node1.itcast.cnPrimary Shard         Replica shard \n \n \n Primary ShardReplica Shardclient \n   \n \n \n clientDataNodeDataNodeCoordinating Node \n 2.Coordinating Node       \n \n \n   ID  \n \n \n  \n \n \n  IDget   \n  Elasticsearch \n  \n ESbuffersegment   \n ES1refresh \n translog \n translogrefreshtranslog \n segmenttranslog \n flush \n ES 30  \n segment  \n SegmentESsegmentsegmentIOESdelete,Hbase \n ES() \n \n \n  \n \n \n mapping  \n \n primary shardreplica \n \n \n \n \n Double \n ESString\n \n String \n --keyword \n -----text \n ES \n \n \n  \n \n ESRoot \n \n \n 1----- \n 2-ConfigConf \n 3-ES \n \n \n \n 4-bin \n 5-WebUi \n \n \n \n  \n \n \n \n\n1-\nuseradd itcast \npasswd itcast\n2-sudo\nvisudo\n100\nitcast      ALL=(ALL)       ALL\n3-\nmkdir -p /export/server/es\nchown -R itcast:itcast /export/server/es\n4-root\nnode1.itcast.cnnode2.itcast.cnnode3.itcast.cnesowneritcast\nmkdir -p /export/server/es\nchown -R itcast:itcast /export/server/es\n5-itcast\nElasticsearch\ncd /export/software/ \ntar -zvxf elasticsearch-7.6.1-linux-x86_64.tar.gz -C /export/server/es/\n6-node1\ncd /export/server/es/elasticsearch-7.6.1/config\nmkdir -p /export/server/es/elasticsearch-7.6.1/log\nmkdir -p /export/server/es/elasticsearch-7.6.1/data\nrm -rf elasticsearch.yml\n\nvim elasticsearch.yml\ncluster.name: itcast-es\nnode.name: node1.itcast.cn\npath.data: /export/server/es/elasticsearch-7.6.1/data\npath.logs: /export/server/es/elasticsearch-7.6.1/log\nnetwork.host: node1.itcast.cn\nhttp.port: 9200\ndiscovery.seed_hosts: ["node1.itcast.cn", "node2.itcast.cn", "node3.itcast.cn"]\ncluster.initial_master_nodes: ["node1.itcast.cn", "node2.itcast.cn"]\nbootstrap.system_call_filter: false\nbootstrap.memory_lock: false\nhttp.cors.enabled: true\nhttp.cors.allow-origin: "*"\n\ncd /export/server/es/elasticsearch-7.6.1/config\nvim jvm.options\n-Xms2g\n-Xmx2g\n\n7-\ncd /export/server/es/\nscp -r elasticsearch-7.6.1/ node2.itcast.cn:$PWD\nscp -r elasticsearch-7.6.1/ node3.itcast.cn:$PWD\n\n8-node2node3\nnode.name: node2.itcast.cn\nnetwork.host: node2.itcast.cn\n\nnode.name: node3.itcast.cn\nnetwork.host: node3.itcast.cn\n8-\nsudo vi /etc/security/limits.conf \n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 2048\n* hard nproc 4096\n9-\nCentos7\nsudo vi /etc/security/limits.d/20-nproc.conf\n* soft nproc 4096\n\n10-\n\n \n: ,    ()es\nsudo  sysctl -w vm.max_map_count=262144    \n\n\n\n11-\n/export/server/es/elasticsearch-7.6.1/bin/elasticsearch ---\nnohup /export/server/es/elasticsearch-7.6.1/bin/elasticsearch  & \nnohup /export/server/es/elasticsearch-7.6.1/bin/elasticsearch 2>&1 &\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 \n \n  \n \n \n \n \n \n ES--ES-Head \n \n  \n \n \n \n ESIK \n 9.ESRestFulAPI \n \n \n Rest:URIHTTP(GET, POST, PUT, DELETE) \n \n \n RESTful API  \n \n \n \n \n \n \nPUT /my-index\n{\n    "mapping": {\n        "properties": {\n            "employee-id": {\n                "type": "keyword",\n                "index": false\n            }\n        }\n    }\n}\n\n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n \n GET   / _sql ? format = json\n { \n     "query" :   "SELECT * FROM tbl_waybill limit 1" \n } \n\n \n 1 2 3 4 5 \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n VSCodeES \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \n \n MySQL \n Lucene \n Solr3.0Solr \n ES \n \n \n \n \n ES\n \n  \n  \n  \n \n \n Lucene\n \n IK \n lucene \n  \n \n \n ES \n \n Master \n Worker(DataNode) \n PrimaryShard \n Replica Shard \n Index \n Type \n documnet \n filed \n mapping \n \n \n ES()\n \n \n \n masterworker \n primary shardreplica shard \n \n \n \n \n ES----\n \n elasticserach.yml \n  \n \n \n ES--ES-Head\n \n web \n \n \n ESRestFulAPI\n \n Json \n \n \n VSCodeES\n \n ElasticSearch-Vscode \n  \n  \n'},{frontmatter:{},regularPath:"/%E5%85%B6%E4%BB%96/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95.html",relativePath:"/.md",key:"v-305ac8e0",path:"/1970/01/01/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/",lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:""},{title:"redis",frontmatter:{},regularPath:"/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%84%9A%E6%9C%AC/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86.html",relativePath:"/redis.md",key:"v-4fd15c58",path:"/1970/01/01/redis%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E9%94%A6%E9%9B%86/",headers:[{level:2,title:"redis",slug:"redis"},{level:2,title:"redisString",slug:"redisstring"},{level:2,title:"redishash",slug:"redishash"},{level:2,title:"redislist",slug:"redislist"},{level:2,title:"redisset",slug:"redisset"},{level:2,title:"redissortedsetzset:",slug:"redissortedset-zset-"}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:" redis \n redis \n redis \n \n \n \n  String    ----  \n \n \n :  \n :   \n \n \n \n \n   hash \n \n \n : javahashMap \n : javaBean, ,String \n \n \n \n \n list \n \n \n : javalinkList,  \n : ,\n \n java \n \n \n \n \n \n \n set  \n \n \n : ,  \n : \n \n  \n \n \n \n \n \n \n set: sorted set \n \n \n :,  \n :  \n redisString \n \n :  set key value\n \n keyvaluekeyOK \n \n \n : get key\n \n keyvaluekeyvalueStringredisgetString valuekey(nil) \n \n \n : del key\n \n key \n \n \n :\n \n : incr key\n \n keyvalue1.key0incr1valuehello \n \n \n : decr key\n \n keyvalue1.key0incr-1valuehello \n \n \n : incrby keyincrement\n \n keyvalueincrementkey0incrbyincrementhello \n \n \n : decrby key decrement\n \n keyvaluedecrementkey0decrbydecrementhello \n \n \n \n \n value:  append key  value\n \n keyvaluekeykey|value \n \n \n key:\n \n key\n \n setex key seconds value \n \n \n key\n \n expire key seconds \n \n \n \n \n key: exists key\n \n 1 , 0  \n \n \n key:  ttl  key\n \n : -1  -2  \n redishash \n \n : hset key field value\n \n key, valuemap,fieldvalue \n \n \n :\n \n keyfield:  hgetkey field \n keyfield: hmget key fields \n keyfieldvalue:  hgetall key \n keymapfield: hkeys key \n keymapvalue: hvals key \n \n \n :\n \n hdel key field [field ]  \n del key  \n \n \n :\n \n hincrby key field numberkey \n \n \n keyfiled: hexists key field\n \n  0,  1  \n \n \n keyfield: hlen key \n redislist \n redislistjavalinkedlist,,  \n \n :\n \n : lpushkey values[value1 value2]\n \n keylistvalueskeykey \n \n \n : rpushkey values[value1value2]\n \n list \n \n \n \n \n  : lrangekey start end\n \n startendstartend0end-1-2 \n \n \n ():\n \n :lpop key\n \n keykeynilkey \n \n \n : rpopkey\n \n  \n \n \n \n \n : llen key\n \n key \n \n \n key, key, \n \n :lpushxkeyvalue \n : rpushx key value \n \n \n lremkey count value:\n \n countvaluecount0countvaluecount0count0value \n \n \n lsetkey index value:\n \n index0-1 \n \n \n linsertkey before|after pivot value\n \n pivotvalue \n \n \n rpoplpush resource destination\n \n [] \n rpoplpush  list1 list2 list1list2 \n \n \n :index0 index-1 \n redisset \n \n : sadd key values[value1value2]\n \n setkey \n \n \n : sremkey members[member1member2]\n \n set \n \n \n : smembers key\n \n set \n \n \n : sismember key member\n \n set10key \n \n \n : sdiff key1 key2\n \n key1key2key, key \n \n \n :sinter key1 key2 key3\n \n , key \n \n \n :sunion key1 key2 key3\n \n  \n \n \n set:\n \n scard key \n \n \n set:\n \n srandmember key \n \n \n key1key2destination:\n \n sdiffstore destination key1key2 \n \n \n destination:\n \n sinterstore destination key[key] \n \n \n destination:\n \n sunionstore destination key[key] \n redissortedsetzset: \n \n \n : zadd key score member \n \n sorted-set \n \n \n \n : \n \n zscorekey member:  \n zcardkey:  \n \n \n \n :zrem key member[member] \n \n  \n \n \n \n zrank key member: \n \n  \n \n \n \n zrevrank key member \n \n  \n \n \n \n zincrby key  member: \n \n  ... \n \n \n \n : \n \n zrange key start end [withscores]: start-end[withscores] \n zrevrange key start stop [withscores]: startstop \n \n \n \n zremrangebyrankkey start stop:  \n \n \n zremrangebyscorekey min max:  \n \n \n zrangebyscore key min max [withscores][limit offset count]: \n \n [min,max][withscores][limit offset count]offsetoffsetcounta \n \n \n \n zcount key min max: \n \n [min,max] \n \n \n \n"},{title:"Apache beam",frontmatter:{title:"Apache beam",date:"2023-04-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["","pipeline"]},regularPath:"/%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/Apache-beam.html",relativePath:"/Apache-beam.md",key:"v-36184f82",path:"/2023/04/08/apache-beam/",headers:[{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""}],lastUpdated:"2023-7-19 7:18:59 F10: PM",lastUpdatedTimestamp:1689765539e3,content:'  \n apache beam? \n Apache Beam . \n Apache beam \n https://beam.apache.org \n  \n 1.apache-beam \n < properties > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < project.reporting.outputEncoding > UTF-8 </ project.reporting.outputEncoding > \n         < java.version > 1.8 </ java.version > \n         < apache.beam.version > 2.47.0 </ apache.beam.version > \n         < commons.io.version > 2.8.0 </ commons.io.version > \n         < flink.version > 1.12.7 </ flink.version > \n         < spark.version > 3.1.2 </ spark.version > \n         < jackson.version > 2.14.1 </ jackson.version > \n     </ properties > \n     \x3c!----\x3e \n     < dependencyManagement > \n         < dependencies > \n             < dependency > \n                 < groupId > commons-io </ groupId > \n                 < artifactId > commons-io </ artifactId > \n                 < version > ${commons.io.version} </ version > \n             </ dependency > \n             < dependency > \n                 < groupId > com.fasterxml.jackson.core </ groupId > \n                 < artifactId > jackson-databind </ artifactId > \n                 < version > ${jackson.version} </ version > \n             </ dependency > \n             < dependency > \n                 < groupId > com.fasterxml.jackson.core </ groupId > \n                 < artifactId > jackson-core </ artifactId > \n                 < version > ${jackson.version} </ version > \n             </ dependency > \n         </ dependencies > \n     </ dependencyManagement > \n\n     < dependencies > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-core </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-direct-java </ artifactId > \n             < version > ${apache.beam.version} </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-jdbc </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > mysql </ groupId > \n             < artifactId > mysql-connector-java </ artifactId > \n             < version > 5.1.48 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > com.mchange </ groupId > \n             < artifactId > c3p0 </ artifactId > \n             < version > 0.9.5.4 </ version > \n         </ dependency > \n\n         \x3c!-- https://mvnrepository.com/artifact/org.apache.beam/beam-sdks-java-io-hcatalog --\x3e \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-hcatalog </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n\n         \x3c!-- https://mvnrepository.com/artifact/org.apache.hive.hcatalog/hive-hcatalog-core --\x3e \n         < dependency > \n             < groupId > org.apache.hive.hcatalog </ groupId > \n             < artifactId > hive-hcatalog-core </ artifactId > \n             < version > 2.1.0 </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > org.apache.calcite </ groupId > \n                     < artifactId > calcite-avatica </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-hadoop-format </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-io-kafka </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.kafka </ groupId > \n             < artifactId > kafka-clients </ artifactId > \n             < version > 2.4.1 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-flink-1.12 </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n\n         \x3c!--<dependency>\n            <groupId>org.apache.beam</groupId>\n            <artifactId>beam-examples-java</artifactId>\n            <version>${apache.beam.version}</version>\n        </dependency>--\x3e \n         \x3c!-- https://mvnrepository.com/artifact/commons-cli/commons-cli --\x3e \n         < dependency > \n             < groupId > commons-cli </ groupId > \n             < artifactId > commons-cli </ artifactId > \n             < version > 1.4 </ version > \n         </ dependency > \n\n         \x3c!-- https://mvnrepository.com/artifact/commons-io/commons-io --\x3e \n         < dependency > \n             < groupId > commons-io </ groupId > \n             < artifactId > commons-io </ artifactId > \n         </ dependency > \n\n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-clients_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-runtime_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-java_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-runners-spark-3 </ artifactId > \n             < version > ${apache.beam.version} </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-core_2.12 </ artifactId > \n             < version > ${spark.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.module </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-streaming_2.12 </ artifactId > \n             < version > ${spark.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > * </ artifactId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > com.fasterxml.jackson.core </ groupId > \n             < artifactId > jackson-core </ artifactId > \n         </ dependency > \n\n         < dependency > \n             < groupId > com.fasterxml.jackson.core </ groupId > \n             < artifactId > jackson-databind </ artifactId > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.beam </ groupId > \n             < artifactId > beam-sdks-java-extensions-sql </ artifactId > \n             < version > ${apache.beam.version} </ version > \n             < exclusions > \n                 < exclusion > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                     < artifactId > jackson-databind </ artifactId > \n                 </ exclusion > \n                 < exclusion > \n                     < artifactId > jackson-annotations </ artifactId > \n                     < groupId > com.fasterxml.jackson.core </ groupId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n\n     </ dependencies > \n\n     < build > \n         < plugins > \n\n             \x3c!--  --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.5.1 </ version > \n                 < configuration > \n                     < source > 1.8 </ source > \n                     < target > 1.8 </ target > \n                     \x3c!--<encoding>${project.build.sourceEncoding}</encoding>--\x3e \n                 </ configuration > \n             </ plugin > \n\n             \x3c!-- () --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-shade-plugin </ artifactId > \n                 < version > 2.3 </ version > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < goals > \n                             < goal > shade </ goal > \n                         </ goals > \n                         < configuration > \n                             < filters > \n                                 < filter > \n                                     < artifact > *:* </ artifact > \n                                     < excludes > \n                                         \x3c!--\n                                        zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --\x3e \n                                         < exclude > META-INF/*.SF </ exclude > \n                                         < exclude > META-INF/*.DSA </ exclude > \n                                         < exclude > META-INF/*.RSA </ exclude > \n                                     </ excludes > \n                                 </ filter > \n                             </ filters > \n                             < transformers > \n                                 < transformer   implementation = " org.apache.maven.plugins.shade.resource.ManifestResourceTransformer " > \n                                     \x3c!-- jar() --\x3e \n                                     < mainClass > com.gordon.quickstart.HelloWorld </ mainClass > \n                                 </ transformer > \n                             </ transformers > \n                         </ configuration > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n         </ plugins > \n     </ build > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 2.HelloWorld \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . options . * ; \n import   org . apache . beam . sdk . transforms . Create ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n\n import   java . util . Arrays ; \n\n /***\n * 1.Pipline\n * 2.PCollection\n * 3PCollection\n * 4.\n */ \n public   class   HelloWorld   { \n\n     public   static   void   main ( String [ ]  args )   { \n         //todo 1.Pipline \n         PipelineOptions  options  =   PipelineOptionsFactory . create ( ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n         //todo 2.PCollection \n        pipeline\n                 . apply ( "Create elements" ,   Create . of ( Arrays . asList ( "Hello" ,   "World!" ) ) ) \n                 //todo 3.PCollection \n                 . apply ( "Print elements" , \n                         MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                             System . out . println ( x ) ; \n                             return  x ; \n                         } ) ) ; \n         //todo 4. \n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 java   -cp  apache-beam-1.0-SNAPSHOT.jar com.gordon.quickstart.HelloWorld  --runner = DirectRunner\n \n 1 \n FlinkRunner \n import   org . apache . beam . runners . flink . FlinkPipelineOptions ; \n import   org . apache . beam . runners . spark . SparkPipelineOptions ; \n import   org . apache . beam . sdk . options . Default ; \n import   org . apache . beam . sdk . options . Description ; \n import   org . apache . beam . sdk . options . PipelineOptions ; \n\n public   interface   MyOptions   extends   SparkPipelineOptions , FlinkPipelineOptions , PipelineOptions   { \n     //You can also specify a description, which appears when a user passes as a command-line argument, and a default value.--help \n     //You set the description and default value using annotations \n     @Description ( "Input for the pipeline" ) \n     @Default.String ( "/" ) \n     String   getInput ( ) ; \n     void   setInput ( String  input ) ; \n\n     @Description ( "Output for the pipeline" ) \n     @Default.String ( "/test1" ) \n     String   getOutput ( ) ; \n     void   setOutput ( String  output ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 HelloWorld \n \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . Create ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . slf4j . Logger ; \n import   org . slf4j . LoggerFactory ; \n\n import   java . util . Arrays ; \n\n /***\n * 1.Pipline\n * 2.PCollection\n * 3PCollection\n * 4.\n */ \n public   class   HelloWorld   { \n\n     private   static   final   Logger   LOGGER   =   LoggerFactory . getLogger ( HelloWorld . class ) ; \n\n     public   static   void   main ( String [ ]  args )   { \n\n         //todo 1.Pipline \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         //PipelineOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().create(); \n         //commindline \n         //options.setRunner(FlinkRunner.class); \n         System . out . println ( "options.getRunner() = "   +  options . getRunner ( ) ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n         //todo 2.PCollection \n        pipeline\n                 . apply ( "Create elements" ,   Create . of ( Arrays . asList ( "Hello" ,   "World!" ) ) ) \n                 //todo 3PCollection \n                 . apply ( "Print elements" , \n                         MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                             System . out . println ( x ) ; \n                             try   { \n                                 Thread . sleep ( 60 * 1000 ) ; \n                             }   catch   ( InterruptedException  e )   { \n                                e . printStackTrace ( ) ; \n                             } \n                             return  x ; \n                         } ) ) ; \n\n         //todo 4. \n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 DirectRunner \n \n runner \n //options.setRunner(FlinkRunner.class)\n/export/server/flink/bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 /export/server/flink/examples/batch/apache-beam-1.0-SNAPSHOT.jar --runner=org.apache.beam.runners.flink.FlinkRunner\n \n 1 2 \n FlinkUIHelloWorld \n \n SparkRunner \n bin/spark-submit    --master   yarn    --deploy-mode client   /export/server/spark3/examples/jars/apache-beam-1.0-SNAPSHOT.jar  --runner = org.apache.beam.runners.spark.SparkRunner\n \n 1 \n  \n  \n Beam model, Beam SDKs and Runners, and Beams native I/O connectors. \n Beam model \n  \n \n Pipeline  -   \n PCollection  - A   PCollection  \n PTransform  - A     \n Aggregation  -  1  \n User-defined function (UDF)    \n Schema   -   \n SDK  -    \n Runner -   \n [ Window ](https://beam.apache.org/documentation/basics/#window) - A     \n Watermark  -     \n Trigger  -   \n State and timers  -    \n State and timers   DoFns     \n \n Pipline \n pipline \n  \n \n  \n \n \n \n  \n \n PCollection \n PCollection  piplinePCollectionrunner \n PCollection \n 1.Serializablebeam \n public   class   MyCustomCoder   extends   CustomCoder < KV < String ,   Long > >   { \n     private   final   String  key ; \n\n     public   MyCustomCoder ( String  key )   { \n       this . key  =  key ; \n     } \n\n     @Override \n     public   void   encode ( KV < String ,   Long >  kv ,   OutputStream  out )   throws   IOException   { \n       new   DataOutputStream ( out ) . writeLong ( kv . getValue ( ) ) ; \n     } \n\n     @Override \n     public   KV < String ,   Long >   decode ( InputStream  inStream )   throws   IOException   { \n       return   KV . of ( key ,   new   DataInputStream ( inStream ) . readLong ( ) ) ; \n     } \n\n     @Override \n     public   boolean   equals ( @Nullable   Object  other )   { \n       return  other  instanceof   MyCustomCoder   &&  key . equals ( ( ( MyCustomCoder )  other ) . key ) ; \n     } \n\n     @Override \n     public   int   hashCode ( )   { \n       return  key . hashCode ( ) ; \n     } \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 2.schemaJSON, Protocol Buffer, Avro, and database records \n 3.PtranformPcollection \n 4. \n 5.PCollectionPCollectionfilestreaming() \n 6.PCollection \n        PCollection < LogEntry >  unstampedLogs  =   . . . ; \n       PCollection < LogEntry >  stampedLogs  = \n          unstampedLogs . apply ( ParDo . of ( new   DoFn < LogEntry ,   LogEntry > ( )   { \n             public   void   processElement ( @Element   LogEntry  element ,   OutputReceiver < LogEntry >  out )   { \n               // Extract the timestamp from log entry we\'re currently processing. \n               Instant  logTimeStamp  =   extractTimeStampFromLogEntry ( element ) ; \n               // Use OutputReceiver.outputWithTimestamp (rather than \n               // OutputReceiver.output) to emit the entry with timestamp attached. \n              out . outputWithTimestamp ( element ,  logTimeStamp ) ; \n             } \n           } ) ) ; \n \n 1 2 3 4 5 6 7 8 9 10 11 PTtransform \n transfrom  \n \n Source transforms  such as  TextIO.Read  and  Create . A source transform conceptually has no input. \n Processing and conversion operations  such as  ParDo ,  GroupByKey ,  CoGroupByKey ,  Combine , and  Count . \n Outputting transforms  such as  TextIO.Write . \n User-defined , application-specific composite transforms. \n Beam SDKs \n SDKpipeline \n Runners \n javapythongo \n Beams native I/O connectors. \n IO connector  I/O Connectors (apache.org) \n  \n helloworld \n 1.Pipline \n \n option \n \n // Start by defining the options for the pipeline. \n PipelineOptions  options  =   PipelineOptionsFactory . create ( ) ; \n\n // Then create the pipeline. \n Pipeline  p  =   Pipeline . create ( options ) ; \n \n 1 2 3 4 5 \n option35 \n \n --help \n \n PipelineOptions \n \n option\n \n  \n MyoptionsPipelineOptions \n \n \n \n Myoptions \n import   org . apache . beam . sdk . options . Default ; \n import   org . apache . beam . sdk . options . Description ; \n import   org . apache . beam . sdk . options . PipelineOptions ; \n\n public   interface   MyOptions   extends   PipelineOptions   { \n     //You can also specify a description, which appears when a user passes as a command-line argument, and a default value.--help \n     //You set the description and default value using annotations \n     @Description ( "Input for the pipeline" ) \n     @Default.String ( "gs://my-bucket/input" ) \n     String   getInput ( ) ; \n     void   setInput ( String  input ) ; \n\n     @Description ( "Output for the pipeline" ) \n     @Default.String ( "gs://my-bucket/output" ) \n     String   getOutput ( ) ; \n     void   setOutput ( String  output ) ; \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . options . PipelineOptions ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . Create ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n\n import   java . util . Arrays ; \n\n public   class   CreatePipeline   { \n     public   static   void   main ( String [ ]  args )   { \n\n         // \n         String  command_line = "--jobName=CreatePipeline" ; \n\n         //todo 1.Pipline \n\n         //todo 1.1  \n         /*PipelineOptions options = PipelineOptionsFactory.fromArgs(command_line).withValidation().create();\n        System.out.println("options.getJobName() = " + options.getJobName());*/ \n\n         /**\n         * fromArgsstrictParsing falseisCli truewithValidation validation true\n         * Builder(String[] args, boolean validation, boolean strictParsing, boolean isCli)\n         * public PipelineOptionsFactory.Builder fromArgs(String... args) {\n         *             Preconditions.checkNotNull(args, "Arguments should not be null.");\n         *             return new PipelineOptionsFactory.Builder(args, this.validation, this.strictParsing, true);\n         *         }\n         */ \n\n         //todo 1.2 Myoptions \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( command_line ) \n                 . withValidation ( ) \n                 . as ( MyOptions . class ) ; \n         System . out . println ( "options.getInput() = "   +  options . getInput ( ) ) ; \n         System . out . println ( "options.getOutput() = "   +  options . getOutput ( ) ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n         //todo 2.PCollection \n        pipeline\n                 . apply ( "Create elements" ,   Create . of ( Arrays . asList ( "Hello" ,   "World!" ) ) ) \n                 //todo 3PCollection \n                 . apply ( "Print elements" , \n                         MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                             System . out . println ( x ) ; \n                             return  x ; \n                         } ) ) ; \n         //todo 4. \n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #   2. \n IO connector  I/O Connectors (apache.org) \n \n text file,TextIO connector, \n \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . TextIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . joda . time . Duration ; \n\n import   java . util . Arrays ; \n import   java . util . List ; \n\n import   static   org . apache . beam . sdk . transforms . Watch . Growth . afterTimeSinceNewOutput ; \n\n public   class   AboutText   { \n\n     // Create a Java Collection, in this case a List of Strings. \n     static   final   List < String >   LINES   =   Arrays . asList ( \n             "To be, or not to be: that is the question: " , \n             "Whether \'tis nobler in the mind to suffer " , \n             "The slings and arrows of outrageous fortune, " , \n             "Or to take arms against a sea of troubles, " ) ; \n\n     public   static   void   main ( String [ ]  args )   { \n\n\n         //todo 1.pipline \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         //todo 2. \n         //todo  ListPCollection \n         /*PCollection<String> from_list = pipeline.apply("from List", Create.of(LINES));\n        from_list.apply("Print elements",\n                MapElements.into(TypeDescriptors.strings()).via(x -> {\n                    System.out.println(x);\n                    return x;\n                }));*/ \n\n         //todo  text file \n         /*PCollection<String> from_text = pipeline.apply(TextIO.read().from("input/input.txt"));\n        from_text\n                .apply("Print elements",\n                        MapElements.into(TypeDescriptors.strings()).via(x -> {\n                            System.out.println(x);\n                            return x;\n                        }));\n\n        //\n        from_text.apply("write_text", TextIO.write().to("output/from_text"));*/ \n\n         //streaming \n         PCollection < String >  from_text  =  pipeline . apply ( TextIO . read ( ) \n                 . from ( "input/*.txt" ) \n                 . watchForNewFiles ( \n                         // Check for new files every ten seconds \n                         Duration . standardSeconds ( 10 ) , \n                         // Stop watching the filepattern if no new files appear within an minute \n                         afterTimeSinceNewOutput ( Duration . standardMinutes ( 1 ) ) ) ) ; \n\n        from_text . apply ( "Print elements" , \n                 MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                     System . out . println ( x ) ; \n                     return  x ; \n                 } ) ) ; \n\n         //TextIO.Read.withHintMatchesManyFiles() \n         // todo from_mysql \n\n         // todo hadoop \n\n         // todo from_hive \n\n         // todo kafka \n\n\n\n         //todo 4. \n        pipeline . run ( ) . waitUntilFinish ( ) ; \n\n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 \n mysql JdbcIO \n \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n\n import   org . apache . beam . sdk . io . jdbc . JdbcIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . ProcessFunction ; \n import   org . apache . beam . sdk . values . * ; \n import   org . checkerframework . checker . initialization . qual . Initialized ; \n import   org . checkerframework . checker . nullness . qual . NonNull ; \n import   org . checkerframework . checker . nullness . qual . UnknownKeyFor ; \n\n import   java . sql . PreparedStatement ; \n import   java . sql . ResultSet ; \n import   java . sql . SQLException ; \n\n public   class   AboutMysql   { \n     public   static   void   main ( String [ ]  args )   { \n         // todo Pipeline \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         // todo  \n         /*PCollection<KV<Integer, String>> from_mysql = pipeline.apply(JdbcIO.<KV<Integer, String>>read()\n                .withDataSourceConfiguration(JdbcIO.DataSourceConfiguration.create(\n                        "com.mysql.jdbc.Driver", "jdbc:mysql://node1:3306/shop")\n                        .withUsername("root")\n                        .withPassword("123456"))\n                .withQuery("select pid,pname from shop_product")\n                .withRowMapper(new JdbcIO.RowMapper<KV<Integer, String>>() {\n                    public KV<Integer, String> mapRow(ResultSet resultSet) throws Exception {\n                        System.out.print("pid = " + resultSet.getInt(1)+"\\t");\n                        System.out.println("pname = " + resultSet.getString(2));\n                        return KV.of(resultSet.getInt(1), resultSet.getString(2));\n                    }\n                })\n        );*/ \n         //todo  \n         /*PCollection<KV<Integer, String>> from_mysql = pipeline.apply(JdbcIO.<KV<Integer, String>>read()\n                .withDataSourceConfiguration(JdbcIO.DataSourceConfiguration.create(\n                        "com.mysql.jdbc.Driver", "jdbc:mysql://node1:3306/shop")\n                        .withUsername("root")\n                        .withPassword("123456"))\n                .withQuery("select pid,pname from shop_product where 1 = ?")\n                .withStatementPreparator(new JdbcIO.StatementPreparator() {\n                    @Override\n                    public void setParameters(PreparedStatement preparedStatement) throws Exception {\n                        preparedStatement.setInt(1, 1);\n                    }\n                })\n                .withRowMapper(new JdbcIO.RowMapper<KV<Integer, String>>() {\n                    public KV<Integer, String> mapRow(ResultSet resultSet) throws Exception {\n                        System.out.print("pid = " + resultSet.getInt(1) + "\\t");\n                        System.out.println("pname = " + resultSet.getString(2));\n\n                        return KV.of(resultSet.getInt(1), resultSet.getString(2));\n                    }\n                })\n        );\n        from_mysql.apply(JdbcIO.<KV<Integer, String>>write()\n                .withDataSourceConfiguration(JdbcIO.DataSourceConfiguration.create(\n                        "com.mysql.jdbc.Driver", "jdbc:mysql://node1:3306/shop")\n                        .withUsername("root")\n                        .withPassword("123456"))\n                .withStatement("insert into shop_product values(?, ?,1,1)")\n                .withPreparedStatementSetter(new JdbcIO.PreparedStatementSetter<KV<Integer, String>>() {\n                    public void setParameters(KV<Integer, String> element, PreparedStatement query)\n                            throws SQLException {\n                        query.setInt(1, element.getKey());\n                        query.setString(2, element.getValue());\n                    }\n                })\n        );*/ \n\n         // todo  \n\n         PCollection < ProductEntity >  from_mysql  =  pipeline . apply ( JdbcIO . < ProductEntity > read ( ) \n                 . withDataSourceProviderFn ( new   MyDataSourceProviderFn ( ) ) \n                 . withQuery ( "select pid,pname,pprice,stock from shop_product where 1 = ?" ) \n                 . withStatementPreparator ( new   JdbcIO . StatementPreparator ( )   { \n                     @Override \n                     public   void   setParameters ( PreparedStatement  preparedStatement )   throws   Exception   { \n                        preparedStatement . setInt ( 1 ,   1 ) ; \n                     } \n                 } ) \n                 . withRowMapper ( new   JdbcIO . RowMapper < ProductEntity > ( )   { \n                     //ProductEntity productEntity=new ProductEntity();  \n                     public   ProductEntity   mapRow ( ResultSet  resultSet )   throws   Exception   { \n                         ProductEntity  productEntity = new   ProductEntity ( ) ; \n                         System . out . println ( "resultSet.getInt(0) = "   +  resultSet . getInt ( 1 ) ) ; \n                        productEntity . setPid ( resultSet . getInt ( 1 ) ) ; \n                        productEntity . setPname ( resultSet . getString ( 2 ) ) ; \n                        productEntity . setPrice ( resultSet . getDouble ( 3 ) ) ; \n                        productEntity . setStock ( resultSet . getInt ( 4 ) ) ; \n                         return  productEntity ; \n                     } \n                 } ) \n         ) ; \n\n\n        from_mysql . apply ( JdbcIO . < ProductEntity > write ( ) \n                 . withDataSourceProviderFn ( new   MyDataSourceProviderFn ( ) ) \n                 . withStatement ( "insert into shop.shop_product(pid, pname, pprice, stock) values (?,?,?,?)"   + \n                         " on duplicate key update pname=values(pname),pprice=values(pprice),stock=values(stock)" ) \n                 . withPreparedStatementSetter ( new   JdbcIO . PreparedStatementSetter < ProductEntity > ( )   { \n                     public   void   setParameters ( ProductEntity  productEntity ,   PreparedStatement  query ) \n                             throws   SQLException   { \n\n                        query . setInt ( 1 ,  productEntity . getPid ( ) + 10 ) ; \n                        query . setString ( 2 ,  productEntity . getPname ( ) ) ; \n                        query . setDouble ( 3 ,  productEntity . getPrice ( ) ) ; \n                        query . setInt ( 4 ,  productEntity . getStock ( ) ) ; \n                     } \n                 } ) \n         ) ; \n\n\n         //todo  \n        pipeline . run ( ) . waitUntilFinish ( ) ; \n\n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 MyDataSourceProviderFn \n import   org . apache . beam . sdk . transforms . SerializableFunction ; \n\n import   javax . sql . DataSource ; \n\n public    class   MyDataSourceProviderFn   implements   SerializableFunction < Void ,   DataSource > { \n     private   static   transient   DataSource  dataSource ; \n\n     @Override \n     public   DataSource   apply ( Void  input )   { \n         if   ( dataSource  ==   null )   { \n            dataSource  =   C3P0Utils . getDataSource ( "otherc3p0" ) ; \n         } \n         return  dataSource ; \n     } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ProductEntity \n import   java . io . Serializable ; \n\n public   class   ProductEntity   implements   Serializable   { \n     private   Integer  pid ; \n     private   String  pname ; \n     private   double  price ; \n     private   Integer  stock ; \n\n     public   static    ProductEntity  of  ( Integer  pid ,   String  pname ,   double  price ,   Integer  stock )   { \n         return   new   ProductEntity (  pid ,   pname ,   price ,   stock ) ; \n     } \n\n     public   ProductEntity ( )   { \n     } \n\n     public   void   setPid ( Integer  pid )   { \n         this . pid  =  pid ; \n     } \n\n     public   void   setPname ( String  pname )   { \n         this . pname  =  pname ; \n     } \n\n     public   void   setPrice ( double  price )   { \n         this . price  =  price ; \n     } \n\n     public   void   setStock ( Integer  stock )   { \n         this . stock  =  stock ; \n     } \n\n     public   ProductEntity ( Integer  pid ,   String  pname ,   double  price ,   Integer  stock )   { \n         this . pid  =  pid ; \n         this . pname  =  pname ; \n         this . price  =  price ; \n         this . stock  =  stock ; \n     } \n\n     public   Integer   getPid ( )   { \n         return  pid ; \n     } \n\n     public   String   getPname ( )   { \n         return  pname ; \n     } \n\n     public   double   getPrice ( )   { \n         return  price ; \n     } \n\n     public   Integer   getStock ( )   { \n         return  stock ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "ProductEntity{"   + \n                 "pid="   +  pid  + \n                 ", pname=\'"   +  pname  +   \'\\\'\'   + \n                 ", price="   +  price  + \n                 ", stock="   +  stock  + \n                 \'}\' ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 \n import   com . mchange . v2 . c3p0 . ComboPooledDataSource ; \n\n import   javax . sql . DataSource ; \n import   java . sql . Connection ; \n import   java . sql . ResultSet ; \n import   java . sql . SQLException ; \n import   java . sql . Statement ; \n\n //DBCP: Apache, . \n //3P0: Apache, , Java, . \n\n //C3P0. \n public   class   C3P0Utils   { \n     //. \n     //1. , . \n     private   static    ComboPooledDataSource  cpds = new   ComboPooledDataSource ( ) ; \n\n     //2. . \n     private   C3P0Utils ( )   { \n     } \n\n\n     //3. ,  . \n     //DataSourceJDBC, : . \n     public   static   DataSource   getDataSource ( String  s )   { \n        cpds = new   ComboPooledDataSource ( s ) ; \n         return  cpds ; \n     } \n     /*public static DataSource getDataSource() {\n        cpds=new ComboPooledDataSource();\n        return cpds;\n    }*/ \n\n     //4. ,  ,  . \n     public   static   Connection   getConnection ( )   { \n         try   { \n             return  cpds . getConnection ( ) ; \n         }   catch   ( SQLException  throwables )   { \n            throwables . printStackTrace ( ) ; \n         } \n         return   null ; \n     } \n\n     //5. ,  . \n     public   static   void   release ( Connection  conn ,   Statement  stat ,   ResultSet  rs )   { \n         try   { \n             if   ( rs  !=   null )   { \n                rs . close ( ) ; \n                rs  =   null ;    //GCnull. \n             } \n         }   catch   ( SQLException  throwables )   { \n            throwables . printStackTrace ( ) ; \n         }   finally   { \n             try   { \n                 if   ( stat  !=   null )   { \n                    stat . close ( ) ; \n                    stat  =   null ;    //GCnull. \n                 } \n             }   catch   ( SQLException  throwables )   { \n                throwables . printStackTrace ( ) ; \n             }   finally   { \n                 try   { \n                     if   ( conn  !=   null )   { \n                        conn . close ( ) ; \n                        conn  =   null ;    //GCnull. \n                     } \n                 }   catch   ( SQLException  throwables )   { \n                    throwables . printStackTrace ( ) ; \n                 } \n             } \n         } \n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 hiveHCatalogIO \n import   com . google . common . collect . ImmutableMap ; \n import   com . gordon . create_pipeline . MyOptions ; \n import   com . gordon . read_write . entity . StudentEntity ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . hcatalog . HCatalogIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . ProcessFunction ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . apache . hive . hcatalog . data . HCatRecord ; \n\n import   java . util . Map ; \n\n public   class   AboutHive   { \n     static   final   Map < String ,   String >  configProperties  =   ImmutableMap . of ( "hive.metastore.uris" ,   "thrift://node3:9083" ) ; \n\n\n     public   static   void   main ( String [ ]  args )   { \n         //todo Pipeline \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         //todo hive io sql \n         PCollection < HCatRecord >  from_hive  =  pipeline\n                 . apply ( HCatalogIO . read ( ) \n                         . withConfigProperties ( configProperties ) \n                         . withDatabase ( "default" )   //optional, assumes default if none specified \n                         . withTable ( "student" ) ) ; \n                         //.withFilter(filterString)); //optional, may be specified if the table is partitioned , \n         //from_hive.apply(ParDo.of(new CreateHCatFn())); \n        from_hive . apply ( "from_hive" , MapElements . into ( TypeDescriptors . nulls ( ) ) . via ( new   ProcessFunction < HCatRecord ,   Void > ( )   { \n             @Override \n             public   Void   apply ( HCatRecord  input )   throws   Exception   { \n                 StudentEntity  studentEntity  =   new   StudentEntity ( ( String )  input . get ( 0 ) ,   ( String )  input . get ( 1 ) ,   ( String )  input . get ( 2 ) ,   ( String )  input . get ( 3 ) ) ; \n\n                 System . out . println ( "studentEntity = "   +  studentEntity ) ; \n                 return   null ; \n             } \n         } ) ) ; \n         // \n        from_hive . apply ( HCatalogIO . write ( ) \n                 . withConfigProperties ( configProperties ) \n                 . withDatabase ( "default" )   //optional, assumes default if none specified \n                 . withTable ( "student" ) \n                 //.withPartition(partitionValues) //optional, may be specified if the table is partitioned \n                 . withBatchSize ( 1024L ) ) ;   //optional, assumes a default batch size of 1024 if none specified \n\n         //todo run \n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import   org . apache . beam . sdk . transforms . DoFn ; \n import   org . apache . hive . hcatalog . data . HCatRecord ; \n\n public   class   CreateHCatFn   extends   DoFn < HCatRecord , Void >   { \n\n     @ProcessElement \n     public   void   processElement ( ProcessContext  c )   { \n         System . out . println ( "c.element().get(0) = "   +  c . element ( ) . get ( 0 ) ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 import   java . io . Serializable ; \n\n public   class   StudentEntity   implements   Serializable   { \n     private   String  s_id ; \n     private   String  s_name ; \n     private   String  s_birth ; \n     private   String  s_sex ; \n\n     public   StudentEntity ( String  s_id ,   String  s_name ,   String  s_birth ,   String  s_sex )   { \n         this . s_id  =  s_id ; \n         this . s_name  =  s_name ; \n         this . s_birth  =  s_birth ; \n         this . s_sex  =  s_sex ; \n     } \n\n     public   StudentEntity ( )   { \n     } \n\n     public   String   getS_id ( )   { \n         return  s_id ; \n     } \n\n     public   void   setS_id ( String  s_id )   { \n         this . s_id  =  s_id ; \n     } \n\n     public   String   getS_name ( )   { \n         return  s_name ; \n     } \n\n     public   void   setS_name ( String  s_name )   { \n         this . s_name  =  s_name ; \n     } \n\n     public   String   getS_birth ( )   { \n         return  s_birth ; \n     } \n\n     public   void   setS_birth ( String  s_birth )   { \n         this . s_birth  =  s_birth ; \n     } \n\n     public   String   getS_sex ( )   { \n         return  s_sex ; \n     } \n\n     public   void   setS_sex ( String  s_sex )   { \n         this . s_sex  =  s_sex ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "StudentEntity{"   + \n                 "s_id=\'"   +  s_id  +   \'\\\'\'   + \n                 ", s_name=\'"   +  s_name  +   \'\\\'\'   + \n                 ", s_birth=\'"   +  s_birth  +   \'\\\'\'   + \n                 ", s_sex=\'"   +  s_sex  +   \'\\\'\'   + \n                 \'}\' ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 hadoopHadoopFormatIO \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . hadoop . format . HDFSSynchronization ; \n import   org . apache . beam . sdk . io . hadoop . format . HadoopFormatIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . values . KV ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptor ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . apache . hadoop . conf . Configuration ; \n import   org . apache . hadoop . io . LongWritable ; \n import   org . apache . hadoop . io . Text ; \n import   org . apache . hadoop . mapreduce . InputFormat ; \n import   org . apache . hadoop . mapreduce . OutputFormat ; \n import   org . apache . hadoop . mapreduce . lib . input . TextInputFormat ; \n import   org . apache . hadoop . mapreduce . lib . output . TextOutputFormat ; \n\n import   java . io . IOException ; \n\n public   class   AboutHadoop   { \n     public   static   void   main ( String [ ]  args )   throws   IOException   { \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         // todo HadoopFormatIO \n\n\n         Configuration  conf  =   new   Configuration ( ) ; \n // Set Hadoop InputFormat, key and value class in configuration \n         //mapreduce map input keyvalue \n        conf . set ( "fs.defaultFS" , "hdfs://node1:8020" ) ; \n        conf . setClass ( "key.class" ,   LongWritable . class ,   Object . class ) ; \n        conf . setClass ( "value.class" ,   Text . class ,   Object . class ) ; \n        conf . setClass ( "mapreduce.job.inputformat.class" ,   TextInputFormat . class ,   InputFormat . class ) ; \n        conf . set ( "mapreduce.input.fileinputformat.inputdir" , "/data/words.txt" ) ; \n\n         PCollection < KV < LongWritable ,   Text > >  from_hdfs  =  pipeline . apply ( "from_hdfs" ,   HadoopFormatIO . < LongWritable ,   Text > read ( ) . withConfiguration ( conf ) ) ; \n         PCollection < KV < Text ,   Text > >  res  =  from_hdfs\n                 . apply ( "from_hadoop" ,   MapElements . into ( TypeDescriptors . kvs ( new   TypeDescriptor < Text > ( )   { \n                 } ,   new   TypeDescriptor < Text > ( )   { \n                 } ) ) . via ( x  ->   { \n                     System . out . println ( "line = "   +  x . getValue ( ) ) ; \n                     return   KV . of ( new   Text ( String . valueOf ( System . currentTimeMillis ( ) ) ) , x . getValue ( ) ) ; \n                 } ) ) ; \n        conf . set ( HadoopFormatIO . JOB_ID , "AboutHadoop" ) ; \n        conf . setClass ( HadoopFormatIO . OUTPUT_FORMAT_CLASS_ATTR ,   TextOutputFormat . class ,   OutputFormat . class ) ; \n        conf . setClass ( HadoopFormatIO . OUTPUT_KEY_CLASS , Text . class , Object . class ) ; \n        conf . setClass ( HadoopFormatIO . OUTPUT_VALUE_CLASS , Text . class , Object . class ) ; \n        conf . setInt ( HadoopFormatIO . NUM_REDUCES , 2 ) ; \n        conf . set ( HadoopFormatIO . OUTPUT_DIR , "hdfs://node1:8020/data/words3.txt" ) ; \n\n\n\n        res . apply ( \n                 "writeBatch" , \n                 HadoopFormatIO . < Text ,   Text > write ( ) \n                         . withConfiguration ( conf ) \n                         . withPartitioning ( ) \n                         . withExternalSynchronization ( new   HDFSSynchronization ( conf . get ( HadoopFormatIO . OUTPUT_DIR ) ) ) ) ; \n\n         /*unboundedWordsCount.apply(\n                "writeStream",\n                HadoopFormatIO.<Text, LongWritable>write()\n                        .withConfigurationTransform(configTransform)\n                        .withExternalSynchronization(new HDFSSynchronization(locksDirPath)));*/ \n\n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 KafkaKafkaIO \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . runners . flink . FlinkRunner ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . kafka . KafkaIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . Values ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n import   org . apache . beam . vendor . guava . v26_0_jre . com . google . common . collect . ImmutableMap ; \n import   org . apache . kafka . common . serialization . StringDeserializer ; \n import   org . apache . kafka . common . serialization . StringSerializer ; \n\n public   class   AboutKafka   { \n\n     public   static   void   main ( String [ ]  args )   { \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n        options . setRunner ( FlinkRunner . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         //todo KafkaIO \n         PCollection < String >  from_kafka  =  pipeline\n                 . apply ( KafkaIO . < String ,   String > read ( ) \n                         . withBootstrapServers ( "node1:9092,node2:9092,node3:9092" ) \n                         . withTopic ( "KafkaWordCount" )    // use withTopics(List<String>) to read from multiple topics. \n                         . withKeyDeserializer ( StringDeserializer . class ) \n                         . withValueDeserializer ( StringDeserializer . class ) \n                         // Rest of the settings are optional : \n\n                         // you can further customize KafkaConsumer used to read the records by adding more \n                         // settings for ConsumerConfig. e.g : \n                         . withConsumerConfigUpdates ( ImmutableMap . of ( \n                                 "group.id" ,   "flink_wordcount" , \n                                 "auto.offset.reset" ,   "latest" , \n                                 "enable.auto.commit" ,   "true" \n                         ) ) \n                         . withoutMetadata ( ) \n\n                 ) . apply ( Values . create ( ) ) \n                 . apply ( "from_kafka" ,   MapElements . into ( TypeDescriptors . strings ( ) ) . via ( x  ->   { \n                     System . out . println ( x ) ; \n                     return  x ; \n                 } ) ) ; \n\n\n        from_kafka . apply ( KafkaIO . < Void ,   String > write ( ) \n                         . withBootstrapServers ( "node1:9092,node2:9092,node3:9092" ) \n                         . withTopic ( "test01" )    // use withTopics(List<String>) to read from multiple topics. \n                         . withValueSerializer ( StringSerializer . class )   // just need serializer for value \n                         . values ( ) ) ; \n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 FlinkRunner on Yarn \n /export/server/flink/bin/flink run  -m  yarn-cluster  -yjm   1024   -ytm   1024   -c  com.gordon.source_sink.AboutKafka /export/server/flink/examples/batch/apache-beam-1.0-SNAPSHOT.jar  --runner = org.apache.beam.runners.flink.FlinkRunner\n \n 1 \n SparkRunner \n bin/spark-submit  --master   yarn  --deploy-mode client  --class = com.gordon.source_sink.AboutKafka /export/server/spark3/examples/jars/apache-beam-1.0-SNAPSHOT.jar  --runner = org.apache.beam.runners.spark.SparkRunner\n \n 1 \n 3. \n transformworkerrdd \n transformapply \n 1.ParDocombin \n 2.transform \n  \n ParDoMap \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . TextIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . DoFn ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . ParDo ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n\n\n public   class   ParDoDemo   { \n     public   static   void   main ( String [ ]  args )   { \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n\n         PCollection < String >  words  =  pipeline . apply ( TextIO . read ( ) . from ( "/input/input.txt" ) ) ; \n         //DoFn \n        words . apply ( "ParDo" , ParDo . of ( new   DoFn < String ,   Integer > ( )   { \n             @ProcessElement \n             public   void   processElement ( ProcessContext  c ) { \n                 System . out . println ( "ParDo.line.length() = "   +  c . element ( ) . length ( ) ) ; \n               c . output ( c . element ( ) . length ( ) ) ; \n             } \n         } ) ) ; \n\n         // MapElements \n        words . apply ( "MapElements" ,   MapElements . into ( TypeDescriptors . integers ( ) ) . via ( ( String  line ) -> { \n             System . out . println ( "MapElements.line.length() = "   +  line . length ( ) ) ; \n             return  line . length ( ) ; \n         } ) ) ; \n\n\n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 GroupByKey/CoGroupByKey/Combine \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . TextIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . * ; \n import   org . apache . beam . sdk . transforms . join . CoGbkResult ; \n import   org . apache . beam . sdk . transforms . join . CoGroupByKey ; \n import   org . apache . beam . sdk . transforms . join . KeyedPCollectionTuple ; \n import   org . apache . beam . sdk . values . * ; \n\n import   java . util . Arrays ; \n import   java . util . List ; \n\n\n public   class   GroupByKey_Join_CombineDemo   { \n     public   static   void   main ( String [ ]  args )   { \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n\n         PCollection < KV < String ,   Integer > >  wordNum  =  pipeline . apply ( TextIO . read ( ) . from ( "/input/input.txt" ) ) \n                 . apply ( ParDo . of ( new   DoFn < String ,  KV < String ,   Integer > > ( )   { \n                     @ProcessElement \n                     public   void   processElement ( ProcessContext  c )   { \n                         String [ ]  split  =  c . element ( ) . split ( "\\\\s+" ) ; \n                         for   ( String  s  :  split )   { \n                            c . output ( KV . of ( s ,   1 ) ) ; \n                         } \n                     } \n                 } ) ) \n                 // Combine(map side join) \n                 . apply ( Sum . < String > integersPerKey ( ) ) ; \n\n        wordNum\n                 //GroupByKey \n                 . apply ( GroupByKey . create ( ) ) \n                 . apply ( MapElements . into ( TypeDescriptors . nulls ( ) ) . via ( x  ->   { \n                     System . out . println ( String . format ( "%s, %s" ,  x . getKey ( ) ,  x . getValue ( ) ) ) ; \n                     return   null ; \n                 } ) ) ; \n\n         //CoGroupByKey join \n         PCollection < KV < String ,   Integer > >  wordNum2  =  pipeline . apply ( TextIO . read ( ) . from ( "/input/input2.txt" ) ) \n                 . apply ( ParDo . of ( new   DoFn < String ,  KV < String ,   Integer > > ( )   { \n                     @ProcessElement \n                     public   void   processElement ( ProcessContext  c )   { \n                         String [ ]  split  =  c . element ( ) . split ( "\\\\s+" ) ; \n                         for   ( String  s  :  split )   { \n                            c . output ( KV . of ( s ,   1 ) ) ; \n                         } \n                     } \n                 } ) ) \n                 . apply ( Sum . < String > integersPerKey ( ) ) ; \n         final   TupleTag < Integer >  num1Tag  =   new   TupleTag < > ( ) ; \n         final   TupleTag < Integer >  num2Tag  =   new   TupleTag < > ( ) ; \n         PCollection < KV < String ,   CoGbkResult > >  join  =   KeyedPCollectionTuple . of ( num1Tag ,  wordNum ) . and ( num2Tag ,  wordNum2 ) \n                 . apply ( CoGroupByKey . create ( ) ) ; \n\n         PCollection < KV < String ,   Integer > >  res  =  join . apply ( ParDo . of ( new   DoFn < KV < String ,   CoGbkResult > ,  KV < String ,   Integer > > ( )   { \n             @ProcessElement \n             public   void   processElement ( ProcessContext  c )   { \n                 KV < String ,   CoGbkResult >  element  =  c . element ( ) ; \n                 Integer  first  =  element . getValue ( ) . getAll ( num1Tag ) . iterator ( ) . next ( ) ; \n                 Integer  second  =  element . getValue ( ) . getOnly ( num2Tag ) ; \n                 //sum \n                 //System.out.println(String.format("%s,%s", element.getKey(), first + second)); \n                c . output ( KV . of ( element . getKey ( ) ,  first  +  second ) ) ; \n             } \n         } ) ) ; \n         //,sideinputtask \n         PCollectionView < List < String > >  black_list  =  pipeline\n                 . apply ( Create . of ( Arrays . asList ( "c++" ,   "go" ,   "python" ,   "scala" ) ) ) \n                 . apply ( View . asList ( ) ) ; \n        res\n                 . apply ( ParDo \n                         . of ( new   DoFn < KV < String ,   Integer > ,  KV < String ,   Integer > > ( )   { \n                             @ProcessElement \n                             public   void   processElement ( ProcessContext  c )   { \n                                 KV < String ,   Integer >  element  =  c . element ( ) ; \n                                 List < String >  list  =  c . sideInput ( black_list ) ; \n                                 if ( ! list . contains ( element . getKey ( ) ) ) { \n                                     System . out . println ( String . format ( "%s,%s" ,  element . getKey ( ) ,  element . getValue ( ) ) ) ; \n                                    c . output ( element ) ; \n                                 } \n                             } \n                         } ) . withSideInputs ( black_list ) \n                 ) \n                 . apply ( Combine . globally ( new   CustomerCombinFn ( ) ) ) \n                 . apply ( MapElements . into ( TypeDescriptors . nulls ( ) ) . via ( x  ->   { \n                     System . out . println ( String . format ( "%s, %s" ,  x . getKey ( ) ,  x . getValue ( ) ) ) ; \n                     return   null ; \n                 } ) ) ; \n\n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n\n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 Combine \n import   org . apache . beam . sdk . transforms . Combine ; \n import   org . apache . beam . sdk . values . KV ; \n\n import   java . io . Serializable ; \n //CombinFn \n class   CustomerCombinFn   extends   Combine . CombineFn < KV < String , Integer > ,   CustomerCombinFn . Max ,  KV < String , Integer > >    {   // CombineFn \n   public   static   class   Max   implements   Serializable   { // Max \n     String  key ; \n     int  max_num  ; \n\n   } \n   //todo 1. \n   @Override \n   public   Max   createAccumulator ( )   {   return   new   Max ( ) ;   } \n\n   //todo  \n   @Override \n   public   Max   addInput ( Max  max ,   KV < String , Integer >  input )   { \n     if ( input . getValue ( ) > max . max_num ) { \n      max . key = input . getKey ( ) ; \n      max . max_num = input . getValue ( ) ; \n     } \n      return  max ; \n   } \n\n   //todo work \n   @Override \n   public   Max   mergeAccumulators ( Iterable < Max >  maxs )   { \n     Max  merged  =   createAccumulator ( ) ; \n     for   ( Max  max  :  maxs )   { \n      merged = max . max_num > merged . max_num ? max : merged ; \n     } \n     return  merged ; \n   } \n   //todo  \n   @Override \n   public   KV < String , Integer >   extractOutput ( Max  merged )   { \n     return   KV . of ( merged . key , merged . max_num ) ; \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 FlatMapElements/Partition \n import   com . gordon . create_pipeline . MyOptions ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . io . TextIO ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . transforms . FlatMapElements ; \n import   org . apache . beam . sdk . transforms . InferableFunction ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . transforms . Partition ; \n import   org . apache . beam . sdk . values . PCollection ; \n import   org . apache . beam . sdk . values . PCollectionList ; \n\n import   java . util . Arrays ; \n import   java . util . List ; \n\n public   class   FlatMapDemo   { \n     public   static   void   main ( String [ ]  args )   { \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         PCollection < String >  lines  =  pipeline . apply ( TextIO . read ( ) . from ( "/input/input.txt" ) ) ; \n         /*PCollection<List<String>> map_list = lines.apply(MapElements.via(\n                new InferableFunction<String, List<String>>() {\n                    public List<String> apply(String line) throws Exception {\n\n                        return Arrays.asList(line.split("\\\\s+"));\n                    }\n                }));\n        map_list.apply(Flatten.iterables())\n                .apply(MapElements.via(new InferableFunction<String, Void>() {\n                    @Override\n                    public Void apply(String input) throws Exception {\n                        System.out.println(" map Flatten word = " + input);\n                        return null;\n                    }\n                }));*/ \n\n         PCollection < String >  words  =  lines . apply ( FlatMapElements . via ( \n                 new   InferableFunction < String ,   List < String > > ( )   { \n                     public   List < String >   apply ( String  line )   throws   Exception   { \n                         return   Arrays . asList ( line . split ( "\\\\s+" ) ) ; \n                     } \n                 } ) ) ; \n         //flatmap mapflatten,transform \n        words . apply ( MapElements . via ( new   InferableFunction < String ,   Void > ( )   { \n             @Override \n             public   Void   apply ( String  input )   throws   Exception   { \n                 System . out . println ( "flatmap word = "   +  input ) ; \n                 return   null ; \n             } \n         } ) ) ; \n\n\n         PCollectionList < String >  partition  =  words\n                 . apply ( Partition . of ( 5 ,   new   Partition . PartitionFn < String > ( )   { \n                     int  count  =   0 ; \n\n                     @Override \n                     public   int   partitionFor ( String  elem ,   int  numPartitions )   { \n                         int  i  =  count  /  numPartitions ; \n                        count ++ ; \n                         return  i ; \n                     } \n                 } ) ) ; \n\n        partition . get ( 1 ) . apply ( MapElements . via ( new   InferableFunction < String ,   Void > ( )   { \n             @Override \n             public   Void   apply ( String  input )   throws   Exception   { \n                 System . out . println ( "patition = "   +  input ) ; \n                 return   null ; \n             } \n         } ) ) ; \n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 4.sql \n \n \n sqlDemo \n \n \n import   com . gordon . create_pipeline . MyOptions ; \n import   com . gordon . source_sink . entity . ProductEntity ; \n import   org . apache . beam . sdk . Pipeline ; \n import   org . apache . beam . sdk . coders . RowCoder ; \n import   org . apache . beam . sdk . extensions . sql . SqlTransform ; \n import   org . apache . beam . sdk . options . PipelineOptionsFactory ; \n import   org . apache . beam . sdk . schemas . Schema ; \n import   org . apache . beam . sdk . transforms . Create ; \n import   org . apache . beam . sdk . transforms . MapElements ; \n import   org . apache . beam . sdk . values . PBegin ; \n import   org . apache . beam . sdk . values . Row ; \n import   org . apache . beam . sdk . values . TypeDescriptors ; \n\n public   class   SqlDemo   { \n     public   static   void   main ( String [ ]  args )   { \n         //pipline \n         PipelineOptionsFactory . register ( MyOptions . class ) ; \n         MyOptions  options  =   PipelineOptionsFactory . fromArgs ( args ) . withValidation ( ) . as ( MyOptions . class ) ; \n         Pipeline  pipeline  =   Pipeline . create ( options ) ; \n\n         //SchemaRow \n         //todo Schema \n         Schema  appSchema  =   Schema . builder ( ) \n                 . addInt32Field ( "pid" ) \n                 . addStringField ( "pname" ) \n                 . addDoubleField ( "pprice" ) \n                 . addInt32Field ( "stock" ) \n                 . build ( ) ; \n         //todo RowPCollection \n         Row  row  =   Row . withSchema ( appSchema ) \n                 . addValues ( 25 ,   "" ,   21999d ,   123 ) \n                 . build ( ) ; \n\n         PBegin . in ( pipeline ) . apply ( Create . of ( row ) . withCoder ( RowCoder . of ( appSchema ) ) ) \n                 . apply ( SqlTransform . query ( "select * from PCOLLECTION" ) ) \n         . apply ( MapElements . into ( TypeDescriptors . nulls ( ) ) . via ( ( Row  r ) -> { \n             System . out . println ( "r.getValues() = "   +  r . getValues ( ) ) ; \n             return   null ; \n         } ) ) ; \n\n         //todo pojo ,pojo@DefaultSchema(JavaBeanSchema.class) \n        pipeline\n                 . apply ( Create . of ( \n                         ProductEntity . of ( 25 ,   "" ,   21999d ,   123 ) , \n                         ProductEntity . of ( 26 ,   "" ,   21999d ,   123 ) ) \n                 ) . apply ( SqlTransform . query ( "select * from PCOLLECTION" ) ) //PCOLLECTION \n                 . apply ( MapElements . into ( TypeDescriptors . nulls ( ) ) . via ( ( Row  r ) -> { \n                     System . out . println ( "r.getValues() = "   +  r . getValues ( ) ) ; \n                     return   null ; \n                 } ) ) ; \n\n\n\n\n\n        pipeline . run ( ) . waitUntilFinish ( ) ; \n\n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \n \n @DefaultSchema ( JavaBeanSchema . class ) \n public   class   ProductEntity   implements   Serializable   { \n     private   Integer  pid ; \n     private   String  pname ; \n     private   double  price ; \n     private   Integer  stock ; \n\n     public   static    ProductEntity  of  ( Integer  pid ,   String  pname ,   double  price ,   Integer  stock )   { \n         return   new   ProductEntity (  pid ,   pname ,   price ,   stock ) ; \n     } \n\n     public   ProductEntity ( )   { \n     } \n\n     public   void   setPid ( Integer  pid )   { \n         this . pid  =  pid ; \n     } \n\n     public   void   setPname ( String  pname )   { \n         this . pname  =  pname ; \n     } \n\n     public   void   setPrice ( double  price )   { \n         this . price  =  price ; \n     } \n\n     public   void   setStock ( Integer  stock )   { \n         this . stock  =  stock ; \n     } \n\n     public   ProductEntity ( Integer  pid ,   String  pname ,   double  price ,   Integer  stock )   { \n         this . pid  =  pid ; \n         this . pname  =  pname ; \n         this . price  =  price ; \n         this . stock  =  stock ; \n     } \n\n     public   Integer   getPid ( )   { \n         return  pid ; \n     } \n\n     public   String   getPname ( )   { \n         return  pname ; \n     } \n\n     public   double   getPrice ( )   { \n         return  price ; \n     } \n\n     public   Integer   getStock ( )   { \n         return  stock ; \n     } \n\n     @Override \n     public   String   toString ( )   { \n         return   "ProductEntity{"   + \n                 "pid="   +  pid  + \n                 ", pname=\'"   +  pname  +   \'\\\'\'   + \n                 ", price="   +  price  + \n                 ", stock="   +  stock  + \n                 \'}\' ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 \n \n SqlOnHive \n \n \n / export / server / spark3 / bin / spark - submit  -- master yarn  -- deploy - mode client  -- class = com . gordon . sql . SqlOnHive   / export / server / spark3 / examples / jars / apache - beam - 1.0 - SNAPSHOT . jar  -- runner = org . apache . beam . runners . spark . SparkRunner \n \n 1 \n \n \n \n \n Flink 1.2 \n Apache beam  \n --beam code runnerpipeline \n'},{title:"flink",frontmatter:{title:"flink",date:"2019-08-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["",""]},regularPath:"/%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/flink.html",relativePath:"/flink.md",key:"v-49f14a1b",path:"/2019/08/08/flink/",headers:[{level:2,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"Flink",slug:"flink"},{level:3,title:"local-",slug:"local-"},{level:3,title:"Standalone-",slug:"standalone-"},{level:3,title:"Standalone-HA-",slug:"standalone-ha-"},{level:3,title:"Flink-On-Yarn-",slug:"flink-on-yarn-"},{level:2,title:"Flink",slug:"flink"},{level:2,title:"Flink",slug:"flink"},{level:3,title:"",slug:""},{level:3,title:"DataFlowOperatorPartitionParallelismSubTask",slug:"dataflowoperatorpartitionparallelismsubtask"},{level:3,title:"OperatorChainTask",slug:"operatorchaintask"},{level:3,title:"TaskSlotTaskSlotSharing",slug:"taskslottaskslotsharing"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"Operator",slug:"operator"},{level:3,title:"Source",slug:"source"},{level:3,title:"Transformation",slug:"transformation"},{level:3,title:"Sink",slug:"sink"},{level:2,title:"Flink",slug:"flink"},{level:3,title:"Window",slug:"window"},{level:3,title:"Time/Watermarker",slug:"time-watermarker"},{level:3,title:"State",slug:"state"},{level:3,title:"Checkpoint",slug:"checkpoint"},{level:2,title:"BroadcastState-",slug:"broadcaststate--"},{level:2,title:"Flink-Join",slug:"flink-join"},{level:2,title:"Flink-End-to-End Exactly-Once",slug:"flink-end-to-end-exactly-once"},{level:2,title:"Flink-IO-",slug:"flink-io-"},{level:2,title:"Flink-Streaming Flie SinkFile sink",slug:"flink-streaming-flie-sink--file-sink"},{level:2,title:"Flink",slug:"flink"},{level:2,title:"Flink",slug:"flink"},{level:2,title:"Flink",slug:"flink"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:2,title:"Flink Table&SQL",slug:"flink-table-sql"},{level:3,title:"Table planners",slug:"table-planners"},{level:3,title:"",slug:""},{level:3,title:"Flink SQLidle state retention time",slug:"flink-sql-idle-state-retention-time"},{level:3,title:"Flink---FlinkSQLHive",slug:"flink---flinksqlhive"},{level:3,title:"kafkahive",slug:"kafkahive"},{level:3,title:"Flink---",slug:"flink---"},{level:3,title:"Flink---",slug:"flink---"}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:'  \n  \n  \n  \n  \n  \n Flink \n local- \n  \n \n  \n 1. \n https://archive.apache.org/dist/flink/ \n 2.flink-1.12.0-bin-scala_2.12.tgznode1 \n 3. \n tar -zxvf flink-1.12.0-bin-scala_2.12.tgz \n 4. \n chown -R root:root /export/server/flink-1.12.0 \n 5. \n mv flink-1.12.0 flink \n ln -s /export/server/flink-1.12.0 /export/server/flink \n  \n 1./root/words.txt \n vim /root/words.txt \n hello me you her\nhello me you\nhello me\nhello\n \n 1 2 3 4 2.Flink \n /export/server/flink/bin/start-cluster.sh \n 3.jps \n - TaskManagerRunner \n - StandaloneSessionClusterEntrypoint \n 4.FlinkWeb UI \n http://node1:8081/#/overview \n \n slotFlinkFlinkslot \n 5. \n /export/server/flink/bin/flink run /export/server/flink/examples/batch/WordCount.jar  --input  /root/words.txt  --output  /root/out\n \n 1 6.Flink \n /export/server/flink/bin/stop-cluster.sh \n shell(Scala 2.12 Scala Shell) \n /export/server/flink/bin/start-scala-shell.sh local \n  \n benv.readTextFile("/root/words.txt").flatMap(_.split(" ")).map((_,1)).groupBy(0).sum(1).print()\n \n 1 shell \n :quit \n Standalone- \n  \n \n  \n 1.: \n - : node1(Master + Slave): JobManager + TaskManager \n - : node2(Slave): TaskManager \n - : node3(Slave): TaskManager \n 2.flink-conf.yaml \n vim /export/server/flink/conf/flink-conf.yaml \n jobmanager.rpc.address: node1\ntaskmanager.numberOfTaskSlots: 2\nweb.submit.enable: true\n\n#\njobmanager.archive.fs.dir: hdfs://node1:8020/flink/completed-jobs/\nhistoryserver.web.address: node1\nhistoryserver.web.port: 8082\nhistoryserver.archive.fs.dir: hdfs://node1:8020/flink/completed-jobs/\n \n 1 2 3 4 5 6 7 8 9 2.masters \n vim /export/server/flink/conf/masters \n node1:8081\n \n 1 3.slaves \n vim /export/server/flink/conf/workers \n node1\nnode2\nnode3\n \n 1 2 3 4.HADOOP_CONF_DIR \n vim /etc/profile \n export HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop\n \n 1 5. \n scp -r /export/server/flink node2:/export/server/flink \n scp -r /export/server/flink node3:/export/server/flink \n scp  /etc/profile node2:/etc/profile \n scp  /etc/profile node3:/etc/profile \n  \n  for i in {2..3}; do scp -r flink node$i:$PWD; done\n \n 1 6.source \n source /etc/profile \n  \n 1.node1 \n /export/server/flink/bin/start-cluster.sh \n  \n /export/server/flink/bin/jobmanager.sh ((start|start-foreground) cluster)|stop|stop-all \n /export/server/flink/bin/taskmanager.sh start|start-foreground|stop|stop-all \n 2. \n \t/export/server/flink/bin/historyserver.sh start \n 3.Flink UIjps \n http://node1:8081/#/overview \n http://node1:8082/#/overview \n 4. \n /export/server/flink/bin/flink run /export/server/flink/examples/batch/WordCount.jar\n \n 1 6.Flink \n /export/server/flink/bin/stop-cluster.sh \n Standalone-HA- \n  \n \n  \n 1. \n - : node1(Master + Slave): JobManager + TaskManager \n - : node2(Master + Slave): JobManager + TaskManager \n - : node3(Slave): TaskManager \n 2.ZooKeeper \n zkServer.sh status \n zkServer.sh stop \n zkServer.sh start \n 3.HDFS \n /export/serves/hadoop/sbin/start-dfs.sh \n 4.Flink \n /export/server/flink/bin/stop-cluster.sh \n 5.flink-conf.yaml \n vim /export/server/flink/conf/flink-conf.yaml \n G \n state.backend: filesystem\nstate.backend.fs.checkpointdir: hdfs://node1:8020/flink-checkpoints\nhigh-availability: zookeeper\nhigh-availability.storageDir: hdfs://node1:8020/flink/ha/\nhigh-availability.zookeeper.quorum: node1:2181,node2:2181,node3:2181\n \n 1 2 3 4 5 6.masters \n vim /export/server/flink/conf/masters \n 7. \n scp -r /export/server/flink/conf/flink-conf.yaml node2:/export/server/flink/conf/\nscp -r /export/server/flink/conf/flink-conf.yaml node3:/export/server/flink/conf/\nscp -r /export/server/flink/conf/masters node2:/export/server/flink/conf/\nscp -r /export/server/flink/conf/masters node3:/export/server/flink/conf/\n \n 1 2 3 4 8.node2flink-conf.yaml \n vim /export/server/flink/conf/flink-conf.yaml \n jobmanager.rpc.address: node2\n \n 1 9.Flink,node1 \n /export/server/flink/bin/stop-cluster.sh \n /export/server/flink/bin/start-cluster.sh \n \n 10.jps \n Flink \n 11. \n cat /export/server/flink/log/flink-root-standalonesession-0-node1.log \n  \n \n Flink1.8,FlinkHDFSjar \n 12.jarFlinklibjarFlinkHadoop \n  \n https://flink.apache.org/downloads.html \n 13.lib \n cd /export/server/flink/lib \n \n 14. \n for i in {2..3}; do scp -r flink-shaded-hadoop-2-uber-2.7.5-10.0.jar node$i:$PWD; done \n 15.Flink,node1 \n /export/server/flink/bin/stop-cluster.sh \n /export/server/flink/bin/start-cluster.sh \n 16.jps,ok \n  \n 1.WebUI \n http://node1:8081/#/job-manager/config \n http://node2:8081/#/job-manager/config \n 2.wc \n /export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar \n 3.killmaster \n 4.wc, \n /export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar \n 3. \n /export/server/flink/bin/stop-cluster.sh \n Flink-On-Yarn- \n  \n  \n Session \n \n Job \n \n  \n 1.yarn \n vim /export/server/hadoop/etc/hadoop/yarn-site.xml \n  \x3c!-- yarn --\x3e\n    <property>\n        <name>yarn.nodemanager.pmem-check-enabled</name>\n        <value>false</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.vmem-check-enabled</name>\n        <value>false</value>\n    </property>\n \n 1 2 3 4 5 6 7 8 9 2. \n scp -r /export/server/hadoop/etc/hadoop/yarn-site.xml node2:/export/server/hadoop/etc/hadoop/yarn-site.xml\nscp -r /export/server/hadoop/etc/hadoop/yarn-site.xml node3:/export/server/hadoop/etc/hadoop/yarn-site.xml\n \n 1 2 3.yarn \n /export/server/hadoop/sbin/stop-yarn.sh \n /export/server/hadoop/sbin/start-yarn.sh \n  \n Session \n YarnFlink,,,,---- \n 1.yarnFlink/node1 \n \n /export/server/flink/bin/yarn-session.sh -n 2 -tm 800 -s 1 -d \n \n : \n 2CPU1600M \n-n 2taskmanager \n-tm TaskManager \n-s TaskManagerslots \n-d  \n : \n  \n WARN  org.apache.hadoop.hdfs.DFSClient  - Caught exception \n java.lang.InterruptedException \n 2.UI \n http://node1:8088/cluster \n \n 3.flink run \n /export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar \n  \n /export/server/flink/bin/flink run  /export/server/flink/examples/batch/WordCount.jar \n 4.ApplicationMasterFlink \n \n \n ==5.yarn-session== \n yarn application -kill application_1609508087977_0005 \n \n Job-- \n FlinkYarnFlink,,---- \n 1.job \n /export/server/flink/bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 /export/server/flink/examples/batch/WordCount.jar \n-m  jobmanager \n-yjm 1024 jobmanager \n-ytm 1024 taskmanager \n 2.UI \n http://node1:8088/cluster \n \n \n  \n /export/server/flink/bin/flink run  -m  yarn-cluster  -yjm   1024   -ytm   1024  /export/server/flink/examples/batch/WordCount.jar/export/server/flink/bin/flink  --help \nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding  in   [ jar:file:/export/server/flink/lib/log4j-slf4j-impl-2.12.1.jar ! /org/slf4j/impl/StaticLoggerBinder.class ] \nSLF4J: Found binding  in   [ jar:file:/export/server/hadoop-2.7.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar ! /org/slf4j/impl/StaticLoggerBinder.class ] \nSLF4J: See http://www.slf4j.org/codes.html #multiple_bindings for an explanation. \nSLF4J: Actual binding is of  type   [ org.apache.logging.slf4j.Log4jLoggerFactory ] \n./flink  < ACTION >   [ OPTIONS ]   [ ARGUMENTS ] \n\nThe following actions are available:\n\nAction  "run"  compiles and runs a program.\n\n  Syntax: run  [ OPTIONS ]   < jar-file >   < arguments > \n   "run"  action options:\n     -c,--class  < classname >                Class with the program entry point\n                                           ( "main()"  method ) . Only needed  if  the\n                                          JAR  file  does not specify the class  in \n                                          its manifest.\n     -C,--classpath  < url >                  Adds a URL to each user code\n                                          classloader  on all nodes  in  the\n                                          cluster. The paths must specify a\n                                          protocol  ( e.g. file:// )  and be\n                                          accessible on all nodes  ( e.g. by means\n                                          of a NFS share ) . You can use this\n                                          option multiple  times   for  specifying\n                                           more  than one URL. The protocol must\n                                          be supported by the  { @link\n                                          java.net.URLClassLoader } .\n     -d,--detached                        If present, runs the job  in  detached\n                                          mode\n     -n,--allowNonRestoredState           Allow to skip savepoint state that\n                                          cannot be restored. You need to allow\n                                          this  if  you removed an operator from\n                                          your program that was part of the\n                                          program when the savepoint was\n                                          triggered.\n     -p,--parallelism  < parallelism >        The parallelism with  which  to run the\n                                          program. Optional flag to override the\n                                          default value specified  in  the\n                                          configuration.\n     -py,--python  < pythonFile >             Python script with the program entry\n                                          point. The dependent resources can be\n                                          configured with the  ` --pyFiles ` \n                                          option.\n     -pyarch,--pyArchives  < arg >            Add python archive files  for  job. The\n                                          archive files will be extracted to the\n                                          working directory of python UDF\n                                          worker. Currently only zip-format is\n                                          supported. For each archive file, a\n                                          target directory be specified. If the\n                                          target directory name is specified,\n                                          the archive  file  will be extracted to\n                                          a name can directory with the\n                                          specified name. Otherwise, the archive\n                                           file  will be extracted to a directory\n                                          with the same name of the archive\n                                          file. The files uploaded via this\n                                          option are accessible via relative\n                                          path.  \'#\'  could be used as the\n                                          separator of the archive  file  path and\n                                          the target directory name. Comma  ( \',\' ) \n                                          could be used as the separator to\n                                          specify multiple archive files. This\n                                          option can be used to upload the\n                                          virtual environment, the data files\n                                          used  in  Python UDF  ( e.g.:  --pyArchives \n                                          file:///tmp/py37.zip,file:///tmp/data.\n                                           zip #data --pyExecutable \n                                          py37.zip/py37/bin/python ) . The data\n                                          files could be accessed  in  Python UDF,\n                                          e.g.: f  =  open ( \'data/data.txt\' ,  \'r\' ) .\n     -pyexec,--pyExecutable  < arg >          Specify the path of the python\n                                          interpreter used to execute the python\n                                          UDF worker  ( e.g.:  --pyExecutable \n                                          /usr/local/bin/python3 ) . The python\n                                          UDF worker depends on Python  3.5 +,\n                                          Apache Beam  ( version  ==   2.23 .0 ) , Pip\n                                           ( version  >=   7.1 .0 )  and SetupTools\n                                           ( version  >=   37.0 .0 ) . Please ensure\n                                          that the specified environment meets\n                                          the above requirements.\n     -pyfs,--pyFiles  < pythonFiles >         Attach custom python files  for  job.\n                                          These files will be added to the\n                                          PYTHONPATH of both the  local  client\n                                          and the remote python UDF worker. The\n                                          standard python resource  file  suffixes\n                                          such as .py/.egg/.zip or directory are\n                                          all supported. Comma  ( \',\' )  could be\n                                          used as the separator to specify\n                                          multiple files  ( e.g.:  --pyFiles \n                                          file:///tmp/myresource.zip,hdfs:/// $na \n                                          menode_address/myresource2.zip ) .\n     -pym,--pyModule  < pythonModule >        Python module with the program entry\n                                          point. This option must be used  in \n                                          conjunction with  ` --pyFiles ` . \n     -pyreq,--pyRequirements  < arg >         Specify a requirements.txt  file   which \n                                          defines the third-party dependencies.\n                                          These dependencies will be installed\n                                          and added to the PYTHONPATH of the\n                                          python UDF worker. A directory  which \n                                          contains the installation packages of\n                                          these dependencies could be specified\n                                          optionally. Use  \'#\'  as the separator\n                                           if  the optional parameter exists\n                                           ( e.g.:  --pyRequirements \n                                          file:///tmp/requirements.txt #file:///t \n                                          mp/cached_dir ) .\n     -s,--fromSavepoint  < savepointPath >    Path to a savepoint to restore the job\n                                          from  ( for example\n                                          hdfs:///flink/savepoint-1537 ) .\n     -sae,--shutdownOnAttachedExit        If the job is submitted  in  attached\n                                          mode, perform a best-effort cluster\n                                           shutdown  when the CLI is terminated\n                                          abruptly, e.g.,  in  response to a user\n                                          interrupt, such as typing Ctrl + C.\n  Options  for  Generic CLI mode:\n      -D   < property = value >    Allows specifying multiple generic configuration\n                           options. The available options can be found at\n                           https://ci.apache.org/projects/flink/flink-docs-stabl\n                           e/ops/config.html\n     -e,--executor  < arg >    DEPRECATED: Please use the  -t  option instead  which  is\n                           also available with the  "Application Mode" . \n                           The name of the executor to be used  for  executing the\n                           given job,  which  is equivalent to the\n                            "execution.target"  config option. The currently\n                           available executors are:  "remote" ,  "local" ,\n                            "kubernetes-session" ,  "yarn-per-job" ,  "yarn-session" . \n     -t,--target  < arg >      The deployment target  for  the given application,\n                            which  is equivalent to the  "execution.target"  config\n                           option. For the  "run"  action the currently available\n                           targets are:  "remote" ,  "local" ,  "kubernetes-session" ,\n                            "yarn-per-job" ,  "yarn-session" .  For the\n                            "run-application"  action the currently available\n                           targets are:  "kubernetes-application" ,\n                            "yarn-application" . \n\n  Options  for  yarn-cluster mode:\n     -d,--detached                        If present, runs the job  in  detached\n                                          mode\n     -m,--jobmanager  < arg >                 Set to yarn-cluster to use YARN\n                                          execution mode.\n     -yat,--yarnapplicationType  < arg >      Set a custom application  type   for  the\n                                          application on YARN\n      -yD   < property = value >                  use value  for  given property\n     -yd,--yarndetached                   If present, runs the job  in  detached\n                                          mode  ( deprecated ;  use non-YARN\n                                          specific option instead ) \n     -yh,--yarnhelp                       Help  for  the Yarn session CLI.\n     -yid,--yarnapplicationId  < arg >        Attach to running YARN session\n     -yj,--yarnjar  < arg >                   Path to Flink jar  file \n     -yjm,--yarnjobManagerMemory  < arg >     Memory  for  JobManager Container with\n                                          optional unit  ( default: MB ) \n     -ynl,--yarnnodeLabel  < arg >            Specify YARN  node  label  for  the YARN\n                                          application\n     -ynm,--yarnname  < arg >                 Set a custom name  for  the application\n                                          on YARN\n     -yq,--yarnquery                      Display available YARN resources\n                                           ( memory, cores ) \n     -yqu,--yarnqueue  < arg >                Specify YARN queue.\n     -ys,--yarnslots  < arg >                 Number of slots per TaskManager\n     -yt,--yarnship  < arg >                  Ship files  in  the specified directory\n                                           ( t  for  transfer ) \n     -ytm,--yarntaskManagerMemory  < arg >    Memory per TaskManager Container with\n                                          optional unit  ( default: MB ) \n     -yz,--yarnzookeeperNamespace  < arg >    Namespace to create the Zookeeper\n                                          sub-paths  for  high availability mode\n     -z,--zookeeperNamespace  < arg >         Namespace to create the Zookeeper\n                                          sub-paths  for  high availability mode\n\n  Options  for  default mode:\n      -D   < property = value >              Allows specifying multiple generic\n                                     configuration options. The available\n                                     options can be found at\n                                     https://ci.apache.org/projects/flink/flink-\n                                     docs-stable/ops/config.html\n     -m,--jobmanager  < arg >            Address of the JobManager to  which  to\n                                     connect. Use this flag to connect to a\n                                     different JobManager than the one specified\n                                      in  the configuration. Attention: This\n                                     option is respected only  if  the\n                                     high-availability configuration is NONE.\n     -z,--zookeeperNamespace  < arg >    Namespace to create the Zookeeper sub-paths\n                                      for  high availability mode\n\n\n\nAction  "run-application"  runs an application  in  Application Mode.\n\n  Syntax: run-application  [ OPTIONS ]   < jar-file >   < arguments > \n  Options  for  Generic CLI mode:\n      -D   < property = value >    Allows specifying multiple generic configuration\n                           options. The available options can be found at\n                           https://ci.apache.org/projects/flink/flink-docs-stabl\n                           e/ops/config.html\n     -e,--executor  < arg >    DEPRECATED: Please use the  -t  option instead  which  is\n                           also available with the  "Application Mode" . \n                           The name of the executor to be used  for  executing the\n                           given job,  which  is equivalent to the\n                            "execution.target"  config option. The currently\n                           available executors are:  "remote" ,  "local" ,\n                            "kubernetes-session" ,  "yarn-per-job" ,  "yarn-session" . \n     -t,--target  < arg >      The deployment target  for  the given application,\n                            which  is equivalent to the  "execution.target"  config\n                           option. For the  "run"  action the currently available\n                           targets are:  "remote" ,  "local" ,  "kubernetes-session" ,\n                            "yarn-per-job" ,  "yarn-session" .  For the\n                            "run-application"  action the currently available\n                           targets are:  "kubernetes-application" ,\n                            "yarn-application" . \n\n\n\nAction  "info"  shows the optimized execution plan of the program  ( JSON ) .\n\n  Syntax: info  [ OPTIONS ]   < jar-file >   < arguments > \n   "info"  action options:\n     -c,--class  < classname >            Class with the program entry point\n                                       ( "main()"  method ) . Only needed  if  the JAR\n                                       file  does not specify the class  in  its\n                                      manifest.\n     -p,--parallelism  < parallelism >    The parallelism with  which  to run the\n                                      program. Optional flag to override the\n                                      default value specified  in  the\n                                      configuration.\n\n\nAction  "list"  lists running and scheduled programs.\n\n  Syntax: list  [ OPTIONS ] \n   "list"  action options:\n     -a,--all         Show all programs and their JobIDs\n     -r,--running     Show only running programs and their JobIDs\n     -s,--scheduled   Show only scheduled programs and their JobIDs\n  Options  for  Generic CLI mode:\n      -D   < property = value >    Allows specifying multiple generic configuration\n                           options. The available options can be found at\n                           https://ci.apache.org/projects/flink/flink-docs-stabl\n                           e/ops/config.html\n     -e,--executor  < arg >    DEPRECATED: Please use the  -t  option instead  which  is\n                           also available with the  "Application Mode" . \n                           The name of the executor to be used  for  executing the\n                           given job,  which  is equivalent to the\n                            "execution.target"  config option. The currently\n                           available executors are:  "remote" ,  "local" ,\n                            "kubernetes-session" ,  "yarn-per-job" ,  "yarn-session" . \n     -t,--target  < arg >      The deployment target  for  the given application,\n                            which  is equivalent to the  "execution.target"  config\n                           option. For the  "run"  action the currently available\n                           targets are:  "remote" ,  "local" ,  "kubernetes-session" ,\n                            "yarn-per-job" ,  "yarn-session" .  For the\n                            "run-application"  action the currently available\n                           targets are:  "kubernetes-application" ,\n                            "yarn-application" . \n\n  Options  for  yarn-cluster mode:\n     -m,--jobmanager  < arg >             Set to yarn-cluster to use YARN execution\n                                      mode.\n     -yid,--yarnapplicationId  < arg >    Attach to running YARN session\n     -z,--zookeeperNamespace  < arg >     Namespace to create the Zookeeper\n                                      sub-paths  for  high availability mode\n\n  Options  for  default mode:\n      -D   < property = value >              Allows specifying multiple generic\n                                     configuration options. The available\n                                     options can be found at\n                                     https://ci.apache.org/projects/flink/flink-\n                                     docs-stable/ops/config.html\n     -m,--jobmanager  < arg >            Address of the JobManager to  which  to\n                                     connect. Use this flag to connect to a\n                                     different JobManager than the one specified\n                                      in  the configuration. Attention: This\n                                     option is respected only  if  the\n                                     high-availability configuration is NONE.\n     -z,--zookeeperNamespace  < arg >    Namespace to create the Zookeeper sub-paths\n                                      for  high availability mode\n\n\n\nAction  "stop"  stops a running program with a savepoint  ( streaming  jobs  only ) .\n\n  Syntax: stop  [ OPTIONS ]   < Job ID > \n   "stop"  action options:\n     -d,--drain                           Send MAX_WATERMARK before taking the\n                                          savepoint and stopping the pipelne.\n     -p,--savepointPath  < savepointPath >    Path to the savepoint  ( for example\n                                          hdfs:///flink/savepoint-1537 ) . If no\n                                          directory is specified, the configured\n                                          default will be used\n                                           ( "state.savepoints.dir" ) .\n  Options  for  Generic CLI mode:\n      -D   < property = value >    Allows specifying multiple generic configuration\n                           options. The available options can be found at\n                           https://ci.apache.org/projects/flink/flink-docs-stabl\n                           e/ops/config.html\n     -e,--executor  < arg >    DEPRECATED: Please use the  -t  option instead  which  is\n                           also available with the  "Application Mode" . \n                           The name of the executor to be used  for  executing the\n                           given job,  which  is equivalent to the\n                            "execution.target"  config option. The currently\n                           available executors are:  "remote" ,  "local" ,\n                            "kubernetes-session" ,  "yarn-per-job" ,  "yarn-session" . \n     -t,--target  < arg >      The deployment target  for  the given application,\n                            which  is equivalent to the  "execution.target"  config\n                           option. For the  "run"  action the currently available\n                           targets are:  "remote" ,  "local" ,  "kubernetes-session" ,\n                            "yarn-per-job" ,  "yarn-session" .  For the\n                            "run-application"  action the currently available\n                           targets are:  "kubernetes-application" ,\n                            "yarn-application" . \n\n  Options  for  yarn-cluster mode:\n     -m,--jobmanager  < arg >             Set to yarn-cluster to use YARN execution\n                                      mode.\n     -yid,--yarnapplicationId  < arg >    Attach to running YARN session\n     -z,--zookeeperNamespace  < arg >     Namespace to create the Zookeeper\n                                      sub-paths  for  high availability mode\n\n  Options  for  default mode:\n      -D   < property = value >              Allows specifying multiple generic\n                                     configuration options. The available\n                                     options can be found at\n                                     https://ci.apache.org/projects/flink/flink-\n                                     docs-stable/ops/config.html\n     -m,--jobmanager  < arg >            Address of the JobManager to  which  to\n                                     connect. Use this flag to connect to a\n                                     different JobManager than the one specified\n                                      in  the configuration. Attention: This\n                                     option is respected only  if  the\n                                     high-availability configuration is NONE.\n     -z,--zookeeperNamespace  < arg >    Namespace to create the Zookeeper sub-paths\n                                      for  high availability mode\n\n\n\nAction  "cancel"  cancels a running program.\n\n  Syntax: cancel  [ OPTIONS ]   < Job ID > \n   "cancel"  action options:\n     -s,--withSavepoint  < targetDirectory >    **DEPRECATION WARNING**: Cancelling\n                                            a job with savepoint is deprecated.\n                                            Use  "stop"  instead.\n                                            Trigger savepoint and cancel job.\n                                            The target directory is optional. If\n                                            no directory is specified, the\n                                            configured default directory\n                                             ( state.savepoints.dir )  is used.\n  Options  for  Generic CLI mode:\n      -D   < property = value >    Allows specifying multiple generic configuration\n                           options. The available options can be found at\n                           https://ci.apache.org/projects/flink/flink-docs-stabl\n                           e/ops/config.html\n     -e,--executor  < arg >    DEPRECATED: Please use the  -t  option instead  which  is\n                           also available with the  "Application Mode" . \n                           The name of the executor to be used  for  executing the\n                           given job,  which  is equivalent to the\n                            "execution.target"  config option. The currently\n                           available executors are:  "remote" ,  "local" ,\n                            "kubernetes-session" ,  "yarn-per-job" ,  "yarn-session" . \n     -t,--target  < arg >      The deployment target  for  the given application,\n                            which  is equivalent to the  "execution.target"  config\n                           option. For the  "run"  action the currently available\n                           targets are:  "remote" ,  "local" ,  "kubernetes-session" ,\n                            "yarn-per-job" ,  "yarn-session" .  For the\n                            "run-application"  action the currently available\n                           targets are:  "kubernetes-application" ,\n                            "yarn-application" . \n\n  Options  for  yarn-cluster mode:\n     -m,--jobmanager  < arg >             Set to yarn-cluster to use YARN execution\n                                      mode.\n     -yid,--yarnapplicationId  < arg >    Attach to running YARN session\n     -z,--zookeeperNamespace  < arg >     Namespace to create the Zookeeper\n                                      sub-paths  for  high availability mode\n\n  Options  for  default mode:\n      -D   < property = value >              Allows specifying multiple generic\n                                     configuration options. The available\n                                     options can be found at\n                                     https://ci.apache.org/projects/flink/flink-\n                                     docs-stable/ops/config.html\n     -m,--jobmanager  < arg >            Address of the JobManager to  which  to\n                                     connect. Use this flag to connect to a\n                                     different JobManager than the one specified\n                                      in  the configuration. Attention: This\n                                     option is respected only  if  the\n                                     high-availability configuration is NONE.\n     -z,--zookeeperNamespace  < arg >    Namespace to create the Zookeeper sub-paths\n                                      for  high availability mode\n\n\n\nAction  "savepoint"  triggers savepoints  for  a running job or disposes existing ones.\n\n  Syntax: savepoint  [ OPTIONS ]   < Job ID >   [ < target directory > ] \n   "savepoint"  action options:\n     -d,--dispose  < arg >        Path of savepoint to dispose.\n     -j,--jarfile  < jarfile >    Flink program JAR file.\n  Options  for  Generic CLI mode:\n      -D   < property = value >    Allows specifying multiple generic configuration\n                           options. The available options can be found at\n                           https://ci.apache.org/projects/flink/flink-docs-stabl\n                           e/ops/config.html\n     -e,--executor  < arg >    DEPRECATED: Please use the  -t  option instead  which  is\n                           also available with the  "Application Mode" . \n                           The name of the executor to be used  for  executing the\n                           given job,  which  is equivalent to the\n                            "execution.target"  config option. The currently\n                           available executors are:  "remote" ,  "local" ,\n                            "kubernetes-session" ,  "yarn-per-job" ,  "yarn-session" . \n     -t,--target  < arg >      The deployment target  for  the given application,\n                            which  is equivalent to the  "execution.target"  config\n                           option. For the  "run"  action the currently available\n                           targets are:  "remote" ,  "local" ,  "kubernetes-session" ,\n                            "yarn-per-job" ,  "yarn-session" .  For the\n                            "run-application"  action the currently available\n                           targets are:  "kubernetes-application" ,\n                            "yarn-application" . \n\n  Options  for  yarn-cluster mode:\n     -m,--jobmanager  < arg >             Set to yarn-cluster to use YARN execution\n                                      mode.\n     -yid,--yarnapplicationId  < arg >    Attach to running YARN session\n     -z,--zookeeperNamespace  < arg >     Namespace to create the Zookeeper\n                                      sub-paths  for  high availability mode\n\n  Options  for  default mode:\n      -D   < property = value >              Allows specifying multiple generic\n                                     configuration options. The available\n                                     options can be found at\n                                     https://ci.apache.org/projects/flink/flink-\n                                     docs-stable/ops/config.html\n     -m,--jobmanager  < arg >            Address of the JobManager to  which  to\n                                     connect. Use this flag to connect to a\n                                     different JobManager than the one specified\n                                      in  the configuration. Attention: This\n                                     option is respected only  if  the\n                                     high-availability configuration is NONE.\n     -z,--zookeeperNamespace  < arg >    Namespace to create the Zookeeper sub-paths\n                                      for  high availability mode\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 #  Flink \n  \n \n :DataSet,DataStream \n \n \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/batch/ \n \n  \n \n <?xml version="1.0" encoding="UTF-8"?> \n < project   xmlns = " http://maven.apache.org/POM/4.0.0 " \n          xmlns: xsi = " http://www.w3.org/2001/XMLSchema-instance " \n          xsi: schemaLocation = " http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd " > \n     < modelVersion > 4.0.0 </ modelVersion > \n\n     < groupId > cn.itcast </ groupId > \n     < artifactId > flink_study_47 </ artifactId > \n     < version > 1.0-SNAPSHOT </ version > \n     \x3c!-- aliyunapachecloudera --\x3e \n     < repositories > \n         < repository > \n             < id > aliyun </ id > \n             < url > http://maven.aliyun.com/nexus/content/groups/public/ </ url > \n         </ repository > \n         < repository > \n             < id > apache </ id > \n             < url > https://repository.apache.org/content/repositories/snapshots/ </ url > \n         </ repository > \n         < repository > \n             < id > cloudera </ id > \n             < url > https://repository.cloudera.com/artifactory/cloudera-repos/ </ url > \n         </ repository > \n     </ repositories > \n\n     < properties > \n         < encoding > UTF-8 </ encoding > \n         < project.build.sourceEncoding > UTF-8 </ project.build.sourceEncoding > \n         < maven.compiler.source > 1.8 </ maven.compiler.source > \n         < maven.compiler.target > 1.8 </ maven.compiler.target > \n         < java.version > 1.8 </ java.version > \n         < scala.version > 2.12 </ scala.version > \n         < flink.version > 1.12.0 </ flink.version > \n     </ properties > \n     < dependencies > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-clients_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-scala_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-java </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-scala_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-streaming-java_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-table-api-scala-bridge_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-table-api-java-bridge_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         \x3c!-- flink,1.9--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-table-planner_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         \x3c!-- blink,1.11+--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-table-planner-blink_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-table-common </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n\n         \x3c!--<dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-cep_2.12</artifactId>\n            <version>${flink.version}</version>\n        </dependency>--\x3e \n\n         \x3c!-- flink--\x3e \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-connector-kafka_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-sql-connector-kafka_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-connector-jdbc_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-csv </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-json </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n\n         \x3c!-- <dependency>\n           <groupId>org.apache.flink</groupId>\n           <artifactId>flink-connector-filesystem_2.12</artifactId>\n           <version>${flink.version}</version>\n       </dependency>--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-jdbc_2.12</artifactId>\n            <version>${flink.version}</version>\n        </dependency>--\x3e \n         \x3c!--<dependency>\n              <groupId>org.apache.flink</groupId>\n              <artifactId>flink-parquet_2.12</artifactId>\n              <version>${flink.version}</version>\n         </dependency>--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.avro</groupId>\n            <artifactId>avro</artifactId>\n            <version>1.9.2</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.parquet</groupId>\n            <artifactId>parquet-avro</artifactId>\n            <version>1.10.0</version>\n        </dependency>--\x3e \n\n\n         < dependency > \n             < groupId > org.apache.bahir </ groupId > \n             < artifactId > flink-connector-redis_2.11 </ artifactId > \n             < version > 1.0 </ version > \n             < exclusions > \n                 < exclusion > \n                     < artifactId > flink-streaming-java_2.11 </ artifactId > \n                     < groupId > org.apache.flink </ groupId > \n                 </ exclusion > \n                 < exclusion > \n                     < artifactId > flink-runtime_2.11 </ artifactId > \n                     < groupId > org.apache.flink </ groupId > \n                 </ exclusion > \n                 < exclusion > \n                     < artifactId > flink-core </ artifactId > \n                     < groupId > org.apache.flink </ groupId > \n                 </ exclusion > \n                 < exclusion > \n                     < artifactId > flink-java </ artifactId > \n                     < groupId > org.apache.flink </ groupId > \n                 </ exclusion > \n             </ exclusions > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-connector-hive_2.12 </ artifactId > \n             < version > ${flink.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.hive </ groupId > \n             < artifactId > hive-metastore </ artifactId > \n             < version > 2.1.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.hive </ groupId > \n             < artifactId > hive-exec </ artifactId > \n             < version > 2.1.0 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.flink </ groupId > \n             < artifactId > flink-shaded-hadoop-2-uber </ artifactId > \n             < version > 2.7.5-10.0 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.apache.hbase </ groupId > \n             < artifactId > hbase-client </ artifactId > \n             < version > 2.1.0 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > mysql </ groupId > \n             < artifactId > mysql-connector-java </ artifactId > \n             < version > 5.1.38 </ version > \n             \x3c!--<version>8.0.20</version>--\x3e \n         </ dependency > \n\n         \x3c!-- Vertx--\x3e \n         < dependency > \n             < groupId > io.vertx </ groupId > \n             < artifactId > vertx-core </ artifactId > \n             < version > 3.9.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > io.vertx </ groupId > \n             < artifactId > vertx-jdbc-client </ artifactId > \n             < version > 3.9.0 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > io.vertx </ groupId > \n             < artifactId > vertx-redis-client </ artifactId > \n             < version > 3.9.0 </ version > \n         </ dependency > \n\n         \x3c!--  --\x3e \n         < dependency > \n             < groupId > org.slf4j </ groupId > \n             < artifactId > slf4j-log4j12 </ artifactId > \n             < version > 1.7.7 </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > log4j </ groupId > \n             < artifactId > log4j </ artifactId > \n             < version > 1.2.17 </ version > \n             < scope > runtime </ scope > \n         </ dependency > \n\n         < dependency > \n             < groupId > com.alibaba </ groupId > \n             < artifactId > fastjson </ artifactId > \n             < version > 1.2.44 </ version > \n         </ dependency > \n\n         < dependency > \n             < groupId > org.projectlombok </ groupId > \n             < artifactId > lombok </ artifactId > \n             < version > 1.18.2 </ version > \n             < scope > provided </ scope > \n         </ dependency > \n\n         \x3c!-- https://blog.csdn.net/f641385712/article/details/84109098--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.commons</groupId>\n            <artifactId>commons-collections4</artifactId>\n            <version>4.4</version>\n        </dependency>--\x3e \n         \x3c!--<dependency>\n            <groupId>org.apache.thrift</groupId>\n            <artifactId>libfb303</artifactId>\n            <version>0.9.3</version>\n            <type>pom</type>\n            <scope>provided</scope>\n         </dependency>--\x3e \n         \x3c!--<dependency>\n           <groupId>com.google.guava</groupId>\n           <artifactId>guava</artifactId>\n           <version>28.2-jre</version>\n       </dependency>--\x3e \n\n     </ dependencies > \n\n     < build > \n         < sourceDirectory > src/main/java </ sourceDirectory > \n         < plugins > \n             \x3c!--  --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.5.1 </ version > \n                 < configuration > \n                     < source > 1.8 </ source > \n                     < target > 1.8 </ target > \n                     \x3c!--<encoding>${project.build.sourceEncoding}</encoding>--\x3e \n                 </ configuration > \n             </ plugin > \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-surefire-plugin </ artifactId > \n                 < version > 2.18.1 </ version > \n                 < configuration > \n                     < useFile > false </ useFile > \n                     < disableXmlReport > true </ disableXmlReport > \n                     < includes > \n                         < include > **/*Test.* </ include > \n                         < include > **/*Suite.* </ include > \n                     </ includes > \n                 </ configuration > \n             </ plugin > \n             \x3c!-- () --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-shade-plugin </ artifactId > \n                 < version > 2.3 </ version > \n                 < executions > \n                     < execution > \n                         < phase > package </ phase > \n                         < goals > \n                             < goal > shade </ goal > \n                         </ goals > \n                         < configuration > \n                             < filters > \n                                 < filter > \n                                     < artifact > *:* </ artifact > \n                                     < excludes > \n                                         \x3c!--\n                                        zip -d learn_spark.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF --\x3e \n                                         < exclude > META-INF/*.SF </ exclude > \n                                         < exclude > META-INF/*.DSA </ exclude > \n                                         < exclude > META-INF/*.RSA </ exclude > \n                                     </ excludes > \n                                 </ filter > \n                             </ filters > \n                             < transformers > \n                                 < transformer   implementation = " org.apache.maven.plugins.shade.resource.ManifestResourceTransformer " > \n                                     \x3c!-- jar() --\x3e \n                                     < mainClass > </ mainClass > \n                                 </ transformer > \n                             </ transformers > \n                         </ configuration > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n         </ plugins > \n     </ build > \n </ project > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 \n \n -DataSet- \n \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . java . DataSet ; \n import   org . apache . flink . api . java . ExecutionEnvironment ; \n import   org . apache . flink . api . java . operators . AggregateOperator ; \n import   org . apache . flink . api . java . operators . UnsortedGrouping ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc Flink-DataSet-API-WordCount\n */ \n public   class   WordCount   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         ExecutionEnvironment  env  =   ExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //TODO 1.source \n         DataSet < String >  lines  =  env . fromElements ( "itcast hadoop spark" ,   "itcast hadoop spark" ,   "itcast hadoop" ,   "itcast" ) ; \n\n         //TODO 2.transformation \n         // \n         /*\n        @FunctionalInterface\n        public interface FlatMapFunction<T, O> extends Function, Serializable {\n            void flatMap(T value, Collector<O> out) throws Exception;\n        }\n         */ \n         DataSet < String >  words  =  lines . flatMap ( new   FlatMapFunction < String ,   String > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < String >  out )   throws   Exception   { \n                 //value \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( word ) ; \n                 } \n             } \n         } ) ; \n\n         //1 \n         /*\n        @FunctionalInterface\n        public interface MapFunction<T, O> extends Function, Serializable {\n            O map(T value) throws Exception;\n        }\n         */ \n         DataSet < Tuple2 < String ,   Integer > >  wordAndOne  =  words . map ( new   MapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   Tuple2 < String ,   Integer >   map ( String  value )   throws   Exception   { \n                 //value \n                 return   Tuple2 . of ( value ,   1 ) ; \n             } \n         } ) ; \n\n         // \n         UnsortedGrouping < Tuple2 < String ,   Integer > >  grouped  =  wordAndOne . groupBy ( 0 ) ; \n\n         // \n         AggregateOperator < Tuple2 < String ,   Integer > >  result  =  grouped . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 -DataStream-- \n \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author\n * Desc Flink-DataStream-API-WordCount\n * :Flink1.12DataStream,?\n */ \n public   class   WordCount2   {  \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         //ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //env.setRuntimeMode(RuntimeExecutionMode.BATCH);//:DataStream \n         //env.setRuntimeMode(RuntimeExecutionMode.STREAMING);//:DataStream \n         //env.setRuntimeMode(RuntimeExecutionMode.AUTOMATIC);//:DataStream \n\n         //TODO 1.source \n         //DataSet<String> lines = env.fromElements("itcast hadoop spark", "itcast hadoop spark", "itcast hadoop", "itcast"); \n         DataStream < String >  lines  =  env . fromElements ( "itcast hadoop spark" ,   "itcast hadoop spark" ,   "itcast hadoop" ,   "itcast" ) ; \n\n         //TODO 2.transformation \n         // \n         /*\n        @FunctionalInterface\n        public interface FlatMapFunction<T, O> extends Function, Serializable {\n            void flatMap(T value, Collector<O> out) throws Exception;\n        }\n         */ \n         DataStream < String >  words  =  lines . flatMap ( new   FlatMapFunction < String ,   String > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < String >  out )   throws   Exception   { \n                 //value \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( word ) ; \n                 } \n             } \n         } ) ; \n\n         //1 \n         /*\n        @FunctionalInterface\n        public interface MapFunction<T, O> extends Function, Serializable {\n            O map(T value) throws Exception;\n        }\n         */ \n         DataStream < Tuple2 < String ,   Integer > >  wordAndOne  =  words . map ( new   MapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   Tuple2 < String ,   Integer >   map ( String  value )   throws   Exception   { \n                 //value \n                 return   Tuple2 . of ( value ,   1 ) ; \n             } \n         } ) ; \n\n         //:DataSetgroupBy,DataStreamkeyBy \n         //wordAndOne.keyBy(0); \n         /*\n        @FunctionalInterface\n        public interface KeySelector<IN, KEY> extends Function, Serializable {\n            KEY getKey(IN value) throws Exception;\n        }\n         */ \n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  grouped  =  wordAndOne . keyBy ( t  ->  t . f0 ) ; \n\n         // \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  grouped . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute/ \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 -DataStream-- \n package   cn . itcast . hello ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * \n * Desc Flink-DataStream-API-WordCount\n * :Flink1.12DataStream,?\n */ \n public   class   WordCount3   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         //ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //env.setRuntimeMode(RuntimeExecutionMode.BATCH);//:DataStream \n         //env.setRuntimeMode(RuntimeExecutionMode.STREAMING);//:DataStream \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; //:DataStream \n\n         //TODO 1.source \n         //DataSet<String> lines = env.fromElements("itcast hadoop spark", "itcast hadoop spark", "itcast hadoop", "itcast"); \n         //DataStream<String> lines = env.fromElements("itcast hadoop spark", "itcast hadoop spark", "itcast hadoop", "itcast"); \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n         //TODO 2.transformation \n         // \n         /*\n        @FunctionalInterface\n        public interface FlatMapFunction<T, O> extends Function, Serializable {\n            void flatMap(T value, Collector<O> out) throws Exception;\n        }\n         */ \n         DataStream < String >  words  =  lines . flatMap ( new   FlatMapFunction < String ,   String > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < String >  out )   throws   Exception   { \n                 //value \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( word ) ; \n                 } \n             } \n         } ) ; \n\n         //1 \n         /*\n        @FunctionalInterface\n        public interface MapFunction<T, O> extends Function, Serializable {\n            O map(T value) throws Exception;\n        }\n         */ \n         DataStream < Tuple2 < String ,   Integer > >  wordAndOne  =  words . map ( new   MapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   Tuple2 < String ,   Integer >   map ( String  value )   throws   Exception   { \n                 //value \n                 return   Tuple2 . of ( value ,   1 ) ; \n             } \n         } ) ; \n\n         //:DataSetgroupBy,DataStreamkeyBy \n         //wordAndOne.keyBy(0); \n         /*\n        @FunctionalInterface\n        public interface KeySelector<IN, KEY> extends Function, Serializable {\n            KEY getKey(IN value) throws Exception;\n        }\n         */ \n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  grouped  =  wordAndOne . keyBy ( t  ->  t . f0 ) ; \n\n         // \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  grouped . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute/ \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 -DataStream-Lambda \n package   cn . itcast . hello ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . typeinfo . Types ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . Arrays ; \n\n /**\n * Author itcast\n * Desc Flink-DataStream-API-WordCount\n * :Flink1.12DataStream,?\n */ \n public   class   WordCount4   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         //ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //env.setRuntimeMode(RuntimeExecutionMode.BATCH);//:DataStream \n         //env.setRuntimeMode(RuntimeExecutionMode.STREAMING);//:DataStream \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; //:DataStream \n\n         //TODO 1.source \n         //DataSet<String> lines = env.fromElements("itcast hadoop spark", "itcast hadoop spark", "itcast hadoop", "itcast"); \n         DataStream < String >  lines  =  env . fromElements ( "itcast hadoop spark" ,   "itcast hadoop spark" ,   "itcast hadoop" ,   "itcast" ) ; \n\n         //TODO 2.transformation \n         // \n         /*\n        @FunctionalInterface\n        public interface FlatMapFunction<T, O> extends Function, Serializable {\n            void flatMap(T value, Collector<O> out) throws Exception;\n        }\n         */ \n         /*DataStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public void flatMap(String value, Collector<String> out) throws Exception {\n                //value\n                String[] arr = value.split(" ");\n                for (String word : arr) {\n                    out.collect(word);\n                }\n            }\n        });*/ \n         SingleOutputStreamOperator < String >  words  =  lines . flatMap ( \n                 ( String  value ,   Collector < String >  out )   ->   Arrays . stream ( value . split ( " " ) ) . forEach ( out :: collect ) \n         ) . returns ( Types . STRING ) ; \n\n\n         //1 \n         /*\n        @FunctionalInterface\n        public interface MapFunction<T, O> extends Function, Serializable {\n            O map(T value) throws Exception;\n        }\n         */ \n         /*DataStream<Tuple2<String, Integer>> wordAndOne = words.map(new MapFunction<String, Tuple2<String, Integer>>() {\n            @Override\n            public Tuple2<String, Integer> map(String value) throws Exception {\n                //value\n                return Tuple2.of(value, 1);\n            }\n        });*/ \n         DataStream < Tuple2 < String ,   Integer > >  wordAndOne  =  words . map ( \n                 ( String  value )   ->   Tuple2 . of ( value ,   1 ) \n         ) . returns ( Types . TUPLE ( Types . STRING , Types . INT ) ) ; \n\n         //:DataSetgroupBy,DataStreamkeyBy \n         //wordAndOne.keyBy(0); \n         /*\n        @FunctionalInterface\n        public interface KeySelector<IN, KEY> extends Function, Serializable {\n            KEY getKey(IN value) throws Exception;\n        }\n         */ \n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  grouped  =  wordAndOne . keyBy ( t  ->  t . f0 ) ; \n\n         // \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  grouped . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute/ \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 -On-Yarn- \n import   org . apache . flink . api . common . typeinfo . Types ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . api . java . utils . ParameterTool ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . Arrays ; \n\n /**\n * \n * Desc Flink-DataStream-API-WordCount\n * :Flink1.12DataStream,?\n */ \n public   class   WordCount5_Yarn   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         ParameterTool  parameterTool  =   ParameterTool . fromArgs ( args ) ; \n         String  output  =   "" ; \n         if   ( parameterTool . has ( "output" ) )   { \n            output  =  parameterTool . get ( "output" ) ; \n             System . out . println ( ":"   +  output ) ; \n         }   else   { \n            output  =   "hdfs://node1:8020/wordcount/output47_" ; \n             System . out . println ( " --output ,:"   +  output ) ; \n         } \n\n         //TODO 0.env \n         //ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //env.setRuntimeMode(RuntimeExecutionMode.BATCH);//:DataStream \n         //env.setRuntimeMode(RuntimeExecutionMode.STREAMING);//:DataStream \n         //env.setRuntimeMode(RuntimeExecutionMode.AUTOMATIC);//:DataStream \n\n         //TODO 1.source \n         //DataSet<String> lines = env.fromElements("itcast hadoop spark", "itcast hadoop spark", "itcast hadoop", "itcast"); \n         DataStream < String >  lines  =  env . fromElements ( "itcast hadoop spark" ,   "itcast hadoop spark" ,   "itcast hadoop" ,   "itcast" ) ; \n\n         //TODO 2.transformation \n         // \n         /*\n        @FunctionalInterface\n        public interface FlatMapFunction<T, O> extends Function, Serializable {\n            void flatMap(T value, Collector<O> out) throws Exception;\n        }\n         */ \n         /*DataStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public void flatMap(String value, Collector<String> out) throws Exception {\n                //value\n                String[] arr = value.split(" ");\n                for (String word : arr) {\n                    out.collect(word);\n                }\n            }\n        });*/ \n         SingleOutputStreamOperator < String >  words  =  lines . flatMap ( \n                 ( String  value ,   Collector < String >  out )   ->   Arrays . stream ( value . split ( " " ) ) . forEach ( out :: collect ) \n         ) . returns ( Types . STRING ) ; \n\n\n         //1 \n         /*\n        @FunctionalInterface\n        public interface MapFunction<T, O> extends Function, Serializable {\n            O map(T value) throws Exception;\n        }\n         */ \n         /*DataStream<Tuple2<String, Integer>> wordAndOne = words.map(new MapFunction<String, Tuple2<String, Integer>>() {\n            @Override\n            public Tuple2<String, Integer> map(String value) throws Exception {\n                //value\n                return Tuple2.of(value, 1);\n            }\n        });*/ \n         DataStream < Tuple2 < String ,   Integer > >  wordAndOne  =  words . map ( \n                 ( String  value )   ->   Tuple2 . of ( value ,   1 ) \n         ) . returns ( Types . TUPLE ( Types . STRING ,   Types . INT ) ) ; \n\n         //:DataSetgroupBy,DataStreamkeyBy \n         //wordAndOne.keyBy(0); \n         /*\n        @FunctionalInterface\n        public interface KeySelector<IN, KEY> extends Function, Serializable {\n            KEY getKey(IN value) throws Exception;\n        }\n         */ \n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  grouped  =  wordAndOne . keyBy ( t  ->  t . f0 ) ; \n\n         // \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  grouped . sum ( 1 ) ; \n\n         //TODO 3.sink \n         //hdfs, hadoop fs -chmod -R 777  / \n         System . setProperty ( "HADOOP_USER_NAME" ,   "root" ) ; // \n         //result.print(); \n         //result.writeAsText("hdfs://node1:8020/wordcount/output47_"+System.currentTimeMillis()).setParallelism(1); \n        result . writeAsText ( output  +   System . currentTimeMillis ( ) ) . setParallelism ( 1 ) ; \n\n         //TODO 4.execute/ \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105  \n \n  \n /export/server/flink/bin/flink run -Dexecution.runtime-mode = BATCH  -m  yarn-cluster  -yjm   1024   -ytm   1024   -c  cn.itcast.hello.WordCount5_Yarn /root/wc.jar  --output  hdfs://node1:8020/wordcount/output_xx\n \n 1  \n RuntimeExecutionMode.BATCH//DataStream\nRuntimeExecutionMode.STREAMING//DataStream\nRuntimeExecutionMode.AUTOMATIC//DataStream\n//,\n \n 1 2 3 4 Flink,AUTOMATIC \n Flink1.12StreamingFileSinkhdfstruncate file fail \n  \n 1.cdhhadoop \n 2.spark \n Flink \n  \n \n  \n \n DataFlow \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/glossary.html \n DataFlowOperatorPartitionParallelismSubTask \n OperatorChainTask \n TaskSlotTaskSlotSharing \n  \n  \n unbounded stream: \n Operator \n Source \n  \n \n package   cn . itcast . source ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n import   java . util . Arrays ; \n\n /**\n * Author itcast\n * Desc DataStream-Source-\n */ \n public   class   SourceDemo01_Collection   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  ds1  =  env . fromElements ( "hadoop spark flink" ,   "hadoop spark flink" ) ; \n         DataStream < String >  ds2  =  env . fromCollection ( Arrays . asList ( "hadoop spark flink" ,   "hadoop spark flink" ) ) ; \n         DataStream < Long >  ds3  =  env . generateSequence ( 1 ,   100 ) ; \n         DataStream < Long >  ds4  =  env . fromSequence ( 1 ,   100 ) ; \n\n         //TODO 2.transformation \n\n         //TODO 3.sink \n        ds1 . print ( ) ; \n        ds2 . print ( ) ; \n        ds3 . print ( ) ; \n        ds4 . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #   \n \n package   cn . itcast . source ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n /**\n * Author itcast\n * Desc DataStream-Source-/HDFS//\n */ \n public   class   SourceDemo02_File   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  ds1  =  env . readTextFile ( "data/input/words.txt" ) ; \n         DataStream < String >  ds2  =  env . readTextFile ( "data/input/dir" ) ; \n         DataStream < String >  ds3  =  env . readTextFile ( "data/input/wordcount.txt.gz" ) ; \n\n\n         //TODO 2.transformation \n\n         //TODO 3.sink \n        ds1 . print ( ) ; \n        ds2 . print ( ) ; \n        ds3 . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #  Socket \n \n package   cn . itcast . source ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc DataStream-Source-Socket\n */ \n public   class   SourceDemo03_Socket   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n\n         //TODO 2.transformation \n         /*SingleOutputStreamOperator<String> words = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public void flatMap(String value, Collector<String> out) throws Exception {\n                String[] arr = value.split(" ");\n                for (String word : arr) {\n                    out.collect(word);\n                }\n            }\n        });\n\n        words.map(new MapFunction<String, Tuple2<String,Integer>>() {\n            @Override\n            public Tuple2<String, Integer> map(String value) throws Exception {\n                return Tuple2.of(value,1);\n            }\n        });*/ \n\n         //:21,1 \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  wordAndOne  =  lines . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n\n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  wordAndOne . keyBy ( t  ->  t . f0 ) . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 #  Source- \n : lombok \n \n \n \n package   cn . itcast . source ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichParallelSourceFunction ; \n\n import   java . util . Random ; \n import   java . util . UUID ; \n\n /**\n * Author itcast\n * Desc DataStream-Source-\n * :\n */ \n public   class   SourceDemo04_Customer   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < Order >  orderDS  =  env . addSource ( new   MyOrderSource ( ) ) . setParallelism ( 2 ) ; \n\n         //TODO 2.transformation \n\n         //TODO 3.sink \n        orderDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   Order { \n         private   String  id ; \n         private   Integer  userId ; \n         private   Integer  money ; \n         private   Long  createTime ; \n     } \n     public   static   class   MyOrderSource   extends   RichParallelSourceFunction < Order > { \n\n         private   Boolean  flag  =   true ; \n         // \n         @Override \n         public   void   run ( SourceContext < Order >  ctx )   throws   Exception   { \n             Random  random  =   new   Random ( ) ; \n             while   ( flag )   { \n                 String  oid  =   UUID . randomUUID ( ) . toString ( ) ; \n                 int  userId  =  random . nextInt ( 3 ) ; \n                 int  money  =  random . nextInt ( 101 ) ; \n                 long  createTime  =   System . currentTimeMillis ( ) ; \n                ctx . collect ( new   Order ( oid , userId , money , createTime ) ) ; \n                 Thread . sleep ( 1000 ) ; \n             } \n         } \n\n         //cancel \n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 #  Source-MySQL \n \n package   cn . itcast . source ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichParallelSourceFunction ; \n\n import   java . sql . Connection ; \n import   java . sql . DriverManager ; \n import   java . sql . PreparedStatement ; \n import   java . sql . ResultSet ; \n\n /**\n * Author itcast\n * Desc DataStream-Source--MySQL\n * :\n */ \n public   class   SourceDemo05_Customer_MySQL   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < Student >  studentDS  =  env . addSource ( new   MySQLSource ( ) ) . setParallelism ( 1 ) ; \n\n         //TODO 2.transformation \n\n         //TODO 3.sink \n        studentDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n\n    /*\n   CREATE TABLE `t_student` (\n    `id` int(11) NOT NULL AUTO_INCREMENT,\n    `name` varchar(255) DEFAULT NULL,\n    `age` int(11) DEFAULT NULL,\n    PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8;\n\nINSERT INTO `t_student` VALUES (\'1\', \'jack\', \'18\');\nINSERT INTO `t_student` VALUES (\'2\', \'tom\', \'19\');\nINSERT INTO `t_student` VALUES (\'3\', \'rose\', \'20\');\nINSERT INTO `t_student` VALUES (\'4\', \'tom\', \'19\');\nINSERT INTO `t_student` VALUES (\'5\', \'jack\', \'18\');\nINSERT INTO `t_student` VALUES (\'6\', \'rose\', \'20\');\n    */ \n\n     @Data \n     @NoArgsConstructor \n     @AllArgsConstructor \n     public   static   class   Student   { \n         private   Integer  id ; \n         private   String  name ; \n         private   Integer  age ; \n     } \n\n     public   static   class   MySQLSource   extends   RichParallelSourceFunction < Student >   { \n         private   boolean  flag  =   true ; \n         private   Connection  conn  =   null ; \n         private   PreparedStatement  ps  = null ; \n         private   ResultSet  rs   =   null ; \n         //open, \n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            conn  =   DriverManager . getConnection ( "jdbc:mysql://localhost:3306/bigdata" ,   "root" ,   "root" ) ; \n             String  sql  =   "select id,name,age from t_student" ; \n            ps  =  conn . prepareStatement ( sql ) ; \n         } \n\n         @Override \n         public   void   run ( SourceContext < Student >  ctx )   throws   Exception   { \n             while   ( flag )   { \n                rs  =  ps . executeQuery ( ) ; \n                 while   ( rs . next ( ) )   { \n                     int  id  =  rs . getInt ( "id" ) ; \n                     String  name  =  rs . getString ( "name" ) ; \n                     int  age   =  rs . getInt ( "age" ) ; \n                    ctx . collect ( new   Student ( id , name , age ) ) ; \n                 } \n                 Thread . sleep ( 5000 ) ; \n             } \n         } \n\n         //cancel \n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n\n         //close \n         @Override \n         public   void   close ( )   throws   Exception   { \n             if ( conn  !=   null )  conn . close ( ) ; \n             if ( ps  !=   null )  ps . close ( ) ; \n             if ( rs  !=   null )  rs . close ( ) ; \n\n         } \n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 #  Transformation \n  \n map/flatMap/filter/keyBy/sum/reduce... \n Scala/Spark \n  \n TMD(Theater Missile Defense ) \n package   cn . itcast . transformation ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FilterFunction ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . common . functions . ReduceFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc DataStream-Transformation-\n */ \n public   class   TransformationDemo01   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n\n         //TODO 2.transformation \n         DataStream < String >  words  =  lines . flatMap ( new   FlatMapFunction < String ,   String > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < String >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( word ) ; \n                 } \n             } \n         } ) ; \n\n         DataStream < String >  filted  =  words . filter ( new   FilterFunction < String > ( )   { \n             @Override \n             public   boolean   filter ( String  value )   throws   Exception   { \n                 return   ! value . equals ( "TMD" ) ; //TMDfalse \n             } \n         } ) ; \n\n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  wordAndOne  =  filted . map ( new   MapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   Tuple2 < String ,   Integer >   map ( String  value )   throws   Exception   { \n                 return   Tuple2 . of ( value ,   1 ) ; \n             } \n         } ) ; \n\n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  grouped  =  wordAndOne . keyBy ( t  ->  t . f0 ) ; \n\n         //SingleOutputStreamOperator<Tuple2<String, Integer>> result = grouped.sum(1); \n\n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  grouped . reduce ( new   ReduceFunction < Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   Tuple2 < String ,   Integer >   reduce ( Tuple2 < String ,   Integer >  value1 ,   Tuple2 < String ,   Integer >  value2 )   throws   Exception   { \n                 //Tuple2<String, Integer> value1 :(,) \n                 //Tuple2<String, Integer> value2 :(,1) \n                 //(,) \n                 return   Tuple2 . of ( value1 . f0 ,  value1 . f1  +  value2 . f1 ) ;   //_+_ \n             } \n         } ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 #   \n \n package   cn . itcast . transformation ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . streaming . api . datastream . ConnectedStreams ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . co . CoMapFunction ; \n\n /**\n * Author itcast\n * Desc DataStream-Transformation-\n */ \n public   class   TransformationDemo02   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  ds1  =  env . fromElements ( "hadoop" ,   "spark" ,   "flink" ) ; \n         DataStream < String >  ds2  =  env . fromElements ( "hadoop" ,   "spark" ,   "flink" ) ; \n         DataStream < Long >  ds3  =  env . fromElements ( 1L ,   2L ,   3L ) ; \n\n         //TODO 2.transformation \n         DataStream < String >  result1  =  ds1 . union ( ds2 ) ; //union \n         //ds1.union(ds3);//union \n         ConnectedStreams < String ,   String >  result2  =  ds1 . connect ( ds2 ) ; //:connect \n         ConnectedStreams < String ,   Long >  result3  =  ds1 . connect ( ds3 ) ; //conncet \n\n         /*\n        public interface CoMapFunction<IN1, IN2, OUT> extends Function, Serializable {\n            OUT map1(IN1 value) throws Exception;\n            OUT map2(IN2 value) throws Exception;\n        }\n         */ \n         SingleOutputStreamOperator < String >  result  =  result3 . map ( new   CoMapFunction < String ,   Long ,   String > ( )   { \n             @Override \n             public   String   map1 ( String  value )   throws   Exception   { \n                 return   "String:"   +  value ; \n             } \n\n             @Override \n             public   String   map2 ( Long  value )   throws   Exception   { \n                 return   "Long:"   +  value ; \n             } \n         } ) ; \n\n\n         //TODO 3.sink \n        result1 . print ( ) ; \n         //result2.print();//:connect, \n         //result3.print();//:connect, \n        result . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 #   \n \n package   cn . itcast . transformation ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . typeinfo . TypeInformation ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . ProcessFunction ; \n import   org . apache . flink . util . Collector ; \n import   org . apache . flink . util . OutputTag ; \n\n /**\n * Author itcast\n * Desc DataStream-Transformation-(split)(select)\n * splitselectflink1.12\n * outPutTagprocess\n * :\n */ \n public   class   TransformationDemo03   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStreamSource < Integer >  ds  =  env . fromElements ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ,   9 ,   10 ) ; \n\n         //TODO 2.transformation \n         //: \n         OutputTag < Integer >  oddTag  =   new   OutputTag < > ( "" ,   TypeInformation . of ( Integer . class ) ) ; \n         OutputTag < Integer >  evenTag  =   new   OutputTag < > ( "" , TypeInformation . of ( Integer . class ) ) ; \n\n         /*\n        public abstract class ProcessFunction<I, O> extends AbstractRichFunction {\n            public abstract void processElement(I value, ProcessFunction.Context ctx, Collector<O> out) throws Exception;\n        }\n         */ \n         SingleOutputStreamOperator < Integer >  result  =  ds . process ( new   ProcessFunction < Integer ,   Integer > ( )   { \n             @Override \n             public   void   processElement ( Integer  value ,   Context  ctx ,   Collector < Integer >  out )   throws   Exception   { \n                 //out,ctxOutputTag \n                 if   ( value  %   2   ==   0 )   { \n                    ctx . output ( evenTag ,  value ) ; \n                 }   else   { \n                    ctx . output ( oddTag ,  value ) ; \n                 } \n             } \n         } ) ; \n\n         DataStream < Integer >  oddResult  =  result . getSideOutput ( oddTag ) ; \n         DataStream < Integer >  evenResult  =  result . getSideOutput ( evenTag ) ; \n\n         //TODO 3.sink \n         System . out . println ( oddTag ) ; //OutputTag(Integer, ) \n         System . out . println ( evenTag ) ; //OutputTag(Integer, ) \n        oddResult . print ( ":" ) ; \n        evenResult . print ( ":" ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 #  rebalance \n  \n \n \n package   cn . itcast . transformation ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FilterFunction ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n /**\n * Author itcast\n * Desc DataStream-Transformation-rebalance-\n */ \n public   class   TransformationDemo04   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < Long >  longDS  =  env . fromSequence ( 0 ,   100 ) ; \n         //, \n         DataStream < Long >  filterDS  =  longDS . filter ( new   FilterFunction < Long > ( )   { \n             @Override \n             public   boolean   filter ( Long  num )   throws   Exception   { \n                 return  num  >   10 ; \n             } \n         } ) ; \n\n         //TODO 2.transformation \n         //rebalance \n         SingleOutputStreamOperator < Tuple2 < Integer ,   Integer > >  result1  =  filterDS\n                 . map ( new   RichMapFunction < Long ,   Tuple2 < Integer ,   Integer > > ( )   { \n                     @Override \n                     public   Tuple2 < Integer ,   Integer >   map ( Long  value )   throws   Exception   { \n                         int  subTaskId  =   getRuntimeContext ( ) . getIndexOfThisSubtask ( ) ; //id/ \n                         return   Tuple2 . of ( subTaskId ,   1 ) ; \n                     } \n                     //id/,/ \n                 } ) . keyBy ( t  ->  t . f0 ) . sum ( 1 ) ; \n\n         //rebalance \n         SingleOutputStreamOperator < Tuple2 < Integer ,   Integer > >  result2  =  filterDS . rebalance ( ) \n                 . map ( new   RichMapFunction < Long ,   Tuple2 < Integer ,   Integer > > ( )   { \n                     @Override \n                     public   Tuple2 < Integer ,   Integer >   map ( Long  value )   throws   Exception   { \n                         int  subTaskId  =   getRuntimeContext ( ) . getIndexOfThisSubtask ( ) ; //id/ \n                         return   Tuple2 . of ( subTaskId ,   1 ) ; \n                     } \n                     //id/,/ \n                 } ) . keyBy ( t  ->  t . f0 ) . sum ( 1 ) ; \n\n\n         //TODO 3.sink \n        result1 . print ( "result1" ) ; \n        result2 . print ( "result2" ) ; \n\n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  \n \n package   cn . itcast . transformation ; \n\n         import   org . apache . flink . api . common . RuntimeExecutionMode ; \n         import   org . apache . flink . api . common . functions . FlatMapFunction ; \n         import   org . apache . flink . api . common . functions . Partitioner ; \n         import   org . apache . flink . api . java . tuple . Tuple2 ; \n         import   org . apache . flink . streaming . api . datastream . DataStream ; \n         import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n         import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n         import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc DataStream-Transformation-\n */ \n public   class   TransformationDemo05   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  linesDS  =  env . readTextFile ( "data/input/words.txt" ) ; \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  tupleDS  =  linesDS . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  words  =  value . split ( " " ) ; \n                 for   ( String  word  :  words )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n\n         //TODO 2.transformation \n         DataStream < Tuple2 < String ,   Integer > >  result1  =  tupleDS . global ( ) ; \n         DataStream < Tuple2 < String ,   Integer > >  result2  =  tupleDS . broadcast ( ) ; \n         DataStream < Tuple2 < String ,   Integer > >  result3  =  tupleDS . forward ( ) ; \n         DataStream < Tuple2 < String ,   Integer > >  result4  =  tupleDS . shuffle ( ) ; \n         DataStream < Tuple2 < String ,   Integer > >  result5  =  tupleDS . rebalance ( ) ; \n         DataStream < Tuple2 < String ,   Integer > >  result6  =  tupleDS . rescale ( ) ; \n         DataStream < Tuple2 < String ,   Integer > >  result7  =  tupleDS . partitionCustom ( new   MyPartitioner ( ) ,  t  ->  t . f0 ) ; \n\n\n         //TODO 3.sink \n        result1 . print ( "result1" ) ; \n        result2 . print ( "result2" ) ; \n        result3 . print ( "result3" ) ; \n        result4 . print ( "result4" ) ; \n        result5 . print ( "result5" ) ; \n        result6 . print ( "result6" ) ; \n        result7 . print ( "result7" ) ; \n\n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     public   static   class   MyPartitioner   implements   Partitioner < String > { \n         @Override \n         public   int   partition ( String  key ,   int  numPartitions )   { \n             //if(key.equals("")) return 0;   \n             return   0 ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 \n Sink \n  \n \n package   cn . itcast . sink ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n /**\n * Author itcast\n * Desc DataStream-Sink-\n */ \n public   class   SinkDemo01   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  ds  =  env . readTextFile ( "data/input/words.txt" ) ; \n\n         //TODO 2.transformation \n         //TODO 3.sink \n        ds . print ( ) ; \n        ds . print ( "" ) ; \n        ds . printToErr ( ) ; // \n        ds . printToErr ( "" ) ; // \n        ds . writeAsText ( "data/output/result1" ) . setParallelism ( 1 ) ; \n        ds . writeAsText ( "data/output/result2" ) . setParallelism ( 2 ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #  Sink \n \n package   cn . itcast . sink ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . sink . RichSinkFunction ; \n\n import   java . sql . Connection ; \n import   java . sql . DriverManager ; \n import   java . sql . PreparedStatement ; \n\n /**\n * Author itcast\n * Desc DataStream-Sink-Sink\n */ \n public   class   SinkDemo02   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < Student >  studentDS  =  env . fromElements ( new   Student ( null ,   "tony" ,   18 ) ) ; \n         //TODO 2.transformation \n         //TODO 3.sink \n        studentDS . addSink ( new   MySQLSink ( ) ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @NoArgsConstructor \n     @AllArgsConstructor \n     public   static   class   Student   { \n         private   Integer  id ; \n         private   String  name ; \n         private   Integer  age ; \n     } \n\n     public   static   class   MySQLSink   extends   RichSinkFunction < Student >   { \n         private   Connection  conn  =   null ; \n         private   PreparedStatement  ps  = null ; \n\n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            conn  =   DriverManager . getConnection ( "jdbc:mysql://localhost:3306/bigdata" ,   "root" ,   "root" ) ; \n             String  sql  =   "INSERT INTO `t_student` (`id`, `name`, `age`) VALUES (null, ?, ?);" ; \n            ps  =  conn . prepareStatement ( sql ) ; \n         } \n\n         @Override \n         public   void   invoke ( Student  value ,   Context  context )   throws   Exception   { \n             //? \n            ps . setString ( 1 , value . getName ( ) ) ; \n            ps . setInt ( 2 , value . getAge ( ) ) ; \n             //sql \n            ps . executeUpdate ( ) ; \n         } \n         @Override \n         public   void   close ( )   throws   Exception   { \n             if ( conn  !=   null )  conn . close ( ) ; \n             if ( ps  !=   null )  ps . close ( ) ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Connectors \n JDBC \n package   cn . itcast . connectors ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . connector . jdbc . JdbcConnectionOptions ; \n import   org . apache . flink . connector . jdbc . JdbcSink ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n /**\n * Author itcast\n * Desc FlinkJdbcSink\n */ \n public   class   JDBCDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < Student >  studentDS  =  env . fromElements ( new   Student ( null ,   "tony2" ,   18 ) ) ; \n         //TODO 2.transformation \n         //TODO 3.sink \n        studentDS . addSink ( JdbcSink . sink ( \n                 "INSERT INTO `t_student` (`id`, `name`, `age`) VALUES (null, ?, ?)" , \n                 ( ps ,  value )   ->   { \n                    ps . setString ( 1 ,  value . getName ( ) ) ; \n                    ps . setInt ( 2 ,  value . getAge ( ) ) ; \n                 } ,   new   JdbcConnectionOptions . JdbcConnectionOptionsBuilder ( ) \n                         . withUrl ( "jdbc:mysql://localhost:3306/bigdata" ) \n                         . withUsername ( "root" ) \n                         . withPassword ( "root" ) \n                         . withDriverName ( "com.mysql.jdbc.Driver" ) \n                         . build ( ) ) ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n\n     @Data \n     @NoArgsConstructor \n     @AllArgsConstructor \n     public   static   class   Student   { \n         private   Integer  id ; \n         private   String  name ; \n         private   Integer  age ; \n     } \n\n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 Kafka Consumer/Source \n  \n env.addSource(new Kafka Consumer/Source()) \n \n package   cn . itcast . connectors ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaConsumer ; \n\n import   java . util . Properties ; \n\n /**\n * Author itcast\n * Desc Flink-Connectors-KafkaComsumer/Source\n */ \n public   class   KafkaComsumerDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         //kafka \n         Properties  props   =   new   Properties ( ) ; \n        props . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; // \n        props . setProperty ( "group.id" ,   "flink" ) ; //id \n        props . setProperty ( "auto.offset.reset" , "latest" ) ; //latestoffset,/ /earliestoffset,/ \n        props . setProperty ( "flink.partition-discovery.interval-millis" , "5000" ) ; //5sKafka,,null \n  //      prop.put("flink.partition-discovery.interval-millis", 60000); \n        props . setProperty ( "enable.auto.commit" ,   "true" ) ; //(,CheckpointCheckpointCheckpoint) \n        props . setProperty ( "auto.commit.interval.ms" ,   "2000" ) ; // \n         //FlinkKafkaConsumer/kafkaSource \n         FlinkKafkaConsumer < String >  kafkaSource  =   new   FlinkKafkaConsumer < String > ( "flink_kafka" ,   new   SimpleStringSchema ( ) ,  props ) ; \n         //kafkaSource \n         DataStream < String >  kafkaDS  =  env . addSource ( kafkaSource ) ; \n\n         //TODO 2.transformation \n\n         //TODO 3.sink \n        kafkaDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n // /export/server/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181 --replication-factor 2 --partitions 3 --topic flink_kafka \n // /export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092 --topic flink_kafka \n //FlinkKafkaConsumer \n // \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #  Kafka Producer/Sink \n  ---\x3e flink_kafka --\x3e Flink --\x3eetl ---\x3e flink_kafka2---\x3e \n package   cn . itcast . connectors ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FilterFunction ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaConsumer ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaProducer ; \n\n import   java . util . Properties ; \n\n /**\n * Author itcast\n * Desc Flink-Connectors-KafkaComsumer/Source + KafkaProducer/Sink\n */ \n public   class   KafkaSinkDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         //kafka \n         Properties  props   =   new   Properties ( ) ; \n        props . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; // \n        props . setProperty ( "group.id" ,   "flink" ) ; //id \n        props . setProperty ( "auto.offset.reset" , "latest" ) ; //latestoffset,/ /earliestoffset,/ \n        props . setProperty ( "flink.partition-discovery.interval-millis" , "5000" ) ; //5sKafka, \n        props . setProperty ( "enable.auto.commit" ,   "true" ) ; //(,CheckpointCheckpointCheckpoint) \n        props . setProperty ( "auto.commit.interval.ms" ,   "2000" ) ; // \n         //FlinkKafkaConsumer/kafkaSource \n         FlinkKafkaConsumer < String >  kafkaSource  =   new   FlinkKafkaConsumer < String > ( "flink_kafka" ,   new   SimpleStringSchema ( ) ,  props ) ; \n         //kafkaSource \n         DataStream < String >  kafkaDS  =  env . addSource ( kafkaSource ) ; \n\n         //TODO 2.transformation \n         SingleOutputStreamOperator < String >  etlDS  =  kafkaDS . filter ( new   FilterFunction < String > ( )   { \n             @Override \n             public   boolean   filter ( String  value )   throws   Exception   { \n                 return  value . contains ( "success" ) ; \n             } \n         } ) ; \n\n         //TODO 3.sink \n        etlDS . print ( ) ; \n\n         Properties  props2  =   new   Properties ( ) ; \n        props2 . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; \n         FlinkKafkaProducer < String >  kafkaSink  =   new   FlinkKafkaProducer < > ( "flink_kafka2" ,   new   SimpleStringSchema ( ) ,  props2 ) ; \n        etlDS . addSink ( kafkaSink ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n // ---\x3e flink_kafka --\x3e Flink --\x3eetl ---\x3e flink_kafka2---\x3e \n // /export/server/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181 --replication-factor 2 --partitions 3 --topic flink_kafka \n // /export/server/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181 --replication-factor 2 --partitions 3 --topic flink_kafka2 \n // /export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092 --topic flink_kafka \n //log:2020-10-10 success xxx \n //log:2020-10-10 success xxx \n //log:2020-10-10 success xxx \n //log:2020-10-10 fail xxx \n // /export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic flink_kafka2 --from-beginning \n //FlinkKafkaConsumer \n // \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 #  Redis \n https://bahir.apache.org/docs/flink/current/flink-streaming-redis/ \n \n : \n Socket,WordCount,Redis \n : \n : (key-String, value-String) \n wcresult: : (key-String, value-Hash) \n : RedisKeyString, value:String/Hash/List/Set/Set \n package   cn . itcast . connectors ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . connectors . redis . RedisSink ; \n import   org . apache . flink . streaming . connectors . redis . common . config . FlinkJedisPoolConfig ; \n import   org . apache . flink . streaming . connectors . redis . common . mapper . RedisCommand ; \n import   org . apache . flink . streaming . connectors . redis . common . mapper . RedisCommandDescription ; \n import   org . apache . flink . streaming . connectors . redis . common . mapper . RedisMapper ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc Flink-Connectors-RedisSink\n */ \n public   class   RedisDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n         //TODO 2.transformation \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  lines . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) . keyBy ( t  ->  t . f0 ) . sum ( 1 ) ; \n\n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         FlinkJedisPoolConfig  conf  =   new   FlinkJedisPoolConfig . Builder ( ) . setHost ( "127.0.0.1" ) . build ( ) ; \n         RedisSink < Tuple2 < String ,   Integer > >  redisSink  =   new   RedisSink < Tuple2 < String ,   Integer > > ( conf , new   MyRedisMapper ( ) ) ; \n        result . addSink ( redisSink ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     public   static   class   MyRedisMapper   implements   RedisMapper < Tuple2 < String ,   Integer > > { \n         @Override \n         public   RedisCommandDescription   getCommandDescription ( )   { \n             // key:String("wcresult"),value:Hash(,),HSET \n             return   new   RedisCommandDescription ( RedisCommand . HSET , "wcresult" ) ; \n         } \n\n         @Override \n         public   String   getKeyFromData ( Tuple2 < String ,   Integer >  t )   { \n             return   t . f0 ; \n         } \n\n         @Override \n         public   String   getValueFromData ( Tuple2 < String ,   Integer >  t )   { \n             return  t . f1 . toString ( ) ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 #  Flink \n Window \n  \n \n \n () \n () \n () \n () \n \n API \n - \n \n package   cn . itcast . window ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . windowing . assigners . SlidingProcessingTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . assigners . TumblingProcessingTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n\n /**\n * Author itcast\n * Desc \n */ \n public   class   WindowDemo_1_2   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n         //TODO 2.transformation \n         SingleOutputStreamOperator < CartInfo >  carDS  =  lines . map ( new   MapFunction < String ,   CartInfo > ( )   { \n             @Override \n             public   CartInfo   map ( String  value )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( "," ) ; \n                 return   new   CartInfo ( arr [ 0 ] ,   Integer . parseInt ( arr [ 1 ] ) ) ; \n             } \n         } ) ; \n\n         //: /, \n         //carDS.keyBy(car->car.getSensorId()) \n         KeyedStream < CartInfo ,   String >  keyedDS  =  carDS . keyBy ( CartInfo :: getSensorId ) ; \n         // * 1:55-- \n         //keyedDS.timeWindow(Time.seconds(5)) \n         SingleOutputStreamOperator < CartInfo >  result1  =  keyedDS\n                 . window ( TumblingProcessingTimeWindows . of ( Time . seconds ( 5 ) ) ) \n                 . sum ( "count" ) ; \n         // * 2:510-- \n         SingleOutputStreamOperator < CartInfo >  result2  =  keyedDS\n                 //of(Time size, Time slide) \n                 . window ( SlidingProcessingTimeWindows . of ( Time . seconds ( 10 ) , Time . seconds ( 5 ) ) ) \n                 . sum ( "count" ) ; \n\n         //TODO 3.sink \n         //result1.print(); \n        result2 . print ( ) ; \n /*\n1,5\n2,5\n3,5\n4,5\n*/ \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   CartInfo   { \n         private   String  sensorId ; //id \n         private   Integer  count ; // \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 #   \n \n package   cn . itcast . window ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n /**\n * Author itcast\n * Desc \n */ \n public   class   WindowDemo_3_4   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n         //TODO 2.transformation \n         SingleOutputStreamOperator < CartInfo >  carDS  =  lines . map ( new   MapFunction < String ,   CartInfo > ( )   { \n             @Override \n             public   CartInfo   map ( String  value )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( "," ) ; \n                 return   new   CartInfo ( arr [ 0 ] ,   Integer . parseInt ( arr [ 1 ] ) ) ; \n             } \n         } ) ; \n\n         //: /, \n         //carDS.keyBy(car->car.getSensorId()) \n         KeyedStream < CartInfo ,   String >  keyedDS  =  carDS . keyBy ( CartInfo :: getSensorId ) ; \n\n         // * 1:5,,key5-- \n         SingleOutputStreamOperator < CartInfo >  result1  =  keyedDS\n                 . countWindow ( 5 ) \n                 . sum ( "count" ) ; \n         // * 2:5,,key3-- \n         SingleOutputStreamOperator < CartInfo >  result2  =  keyedDS\n                 . countWindow ( 5 , 3 ) \n                 . sum ( "count" ) ; \n\n         //TODO 3.sink \n         //result1.print(); \n         /*\n1,1\n1,1\n1,1\n1,1\n2,1\n1,1\n         */ \n        result2 . print ( ) ; \n         /*\n1,1\n1,1\n2,1\n1,1\n2,1\n3,1\n4,1\n         */ \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   CartInfo   { \n         private   String  sensorId ; //id \n         private   Integer  count ; // \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 #  Session \n () \n \n package   cn . itcast . window ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . windowing . assigners . ProcessingTimeSessionWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n\n /**\n * Author itcast\n * Desc \n */ \n public   class   WindowDemo_5   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n         //TODO 2.transformation \n         SingleOutputStreamOperator < CartInfo >  carDS  =  lines . map ( new   MapFunction < String ,   CartInfo > ( )   { \n             @Override \n             public   CartInfo   map ( String  value )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( "," ) ; \n                 return   new   CartInfo ( arr [ 0 ] ,   Integer . parseInt ( arr [ 1 ] ) ) ; \n             } \n         } ) ; \n\n         //: /, \n         //carDS.keyBy(car->car.getSensorId()) \n         KeyedStream < CartInfo ,   String >  keyedDS  =  carDS . keyBy ( CartInfo :: getSensorId ) ; \n\n         //:10s,10s,(!) \n         SingleOutputStreamOperator < CartInfo >  result  =  keyedDS . window ( ProcessingTimeSessionWindows . withGap ( Time . seconds ( 10 ) ) ) \n                 . sum ( "count" ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n         /*\n1,1\n1,1\n2,1\n2,1\n         */ \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   CartInfo   { \n         private   String  sensorId ; //id \n         private   Integer  count ; // \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 session window()sessionsessionsession \n Time/Watermarker \n  \n \n EventTimeWatermarker \n watermark \n 1.--\x3e1.11idle \n structed streaming flinktriger \n 2.--\x3e1.5 \n trigger \n import   groovy . lang . Tuple ; \n import   org . apache . commons . lang3 . time . FastDateFormat ; \n import   org . apache . flink . api . common . eventtime . WatermarkStrategy ; \n import   org . apache . flink . api . common . functions . RichFlatMapFunction ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . api . java . tuple . Tuple3 ; \n import   org . apache . flink . api . java . utils . ParameterTool ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . SourceFunction ; \n import   org . apache . flink . streaming . api . windowing . assigners . SlidingEventTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . assigners . TumblingEventTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n import   org . apache . flink . streaming . api . windowing . triggers . * ; \n import   org . apache . flink . streaming . api . windowing . windows . TimeWindow ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaConsumer ; \n import   org . apache . flink . util . Collector ; \n\n import   java . sql . Timestamp ; \n import   java . time . Duration ; \n import   java . util . Properties ; \n import   java . util . Random ; \n import   java . util . UUID ; \n\n /**\n * 2019-10-10 12:00:03,dog\n * 2019-10-10 12:00:04,cat\n * 2019-10-10 12:00:04,dog\n * 2019-10-10 12:00:20,dog\n * 2019-10-10 12:00:30,dog\n * 2019-10-10 12:00:40,dog\n * 2019-10-10 12:00:03,dog\n * 2019-10-10 12:00:04,cat\n * 2019-10-10 12:00:20,dog\n * 2019-10-10 12:00:30,dog\n */ \n public   class   WaterMaker   { \n\n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //env.setStreamTimeCharacteristic(); \n        env . setParallelism ( 3 ) ; \n         /*FastDateFormat df = FastDateFormat.getInstance("HH:mm:ss");\n        DataStreamSource<Order> orderDS = env.addSource(new SourceFunction<Order>() {\n            boolean flag = true;\n\n            @Override\n            public void run(SourceContext<Order> ctx) throws Exception {\n\n                while (flag) {\n                    Random random = new Random();\n                    String orderId = UUID.randomUUID().toString();\n                    Integer userId = random.nextInt(3);\n                    Integer money = random.nextInt(100);\n                    Long evenTime = System.currentTimeMillis() - random.nextInt(6) * 1000;\n                    System.out.println("1: " + userId + "  evenTime: " + df.format(evenTime));\n                    ctx.collect(new Order(orderId, userId, money, evenTime));\n                    Thread.sleep(2000);\n                }\n            }\n\n            @Override\n            public void cancel() {\n                flag = false;\n            }\n        });*/ \n         //DataStreamSource<String> source = env.socketTextStream("node1", 9999); \n         ParameterTool  param  =   ParameterTool . fromArgs ( args ) ; \n         String  topic  =  param . get ( "topic" ,   "KafkaWordCount" ) ; \n         String  group_id  =  param . get ( "group_id" ,   "flink_wordcount" ) ; \n         boolean  isWriteKafka  =  param . getBoolean ( "isWriteKafka" ,   false ) ; \n         boolean  isWriteHdfs  =  param . getBoolean ( "isWriteHdfs" ,   false ) ; \n         boolean  isWriteMysql  =  param . getBoolean ( "isWriteMysql" ,   false ) ; \n\n         Properties  prop  =   new   Properties ( ) ; \n        prop . setProperty ( "bootstrap.servers" ,   "node1:9092,node2:9092,node3:9092" ) ; \n        prop . setProperty ( "group.id" ,  group_id ) ; \n        prop . setProperty ( "auto.offset.reset" ,   "latest" ) ; \n        prop . setProperty ( "enable.auto.commit" ,   "true" ) ; \n        prop . setProperty ( "key.deserializer" ,   "StringDeserializer" ) ; \n        prop . setProperty ( "value.deserializer" ,   "StringDeserializer" ) ; \n         FlinkKafkaConsumer < String >  kafka  =   new   FlinkKafkaConsumer < > ( topic ,   new   SimpleStringSchema ( ) ,  prop ) ; \n         DataStreamSource < String >  source  =  env . addSource ( kafka ) ; \n\n         SingleOutputStreamOperator < Order >  result  =  source . rebalance ( ) . map ( new   RichMapFunction < String ,   Order > ( )   { \n             @Override \n             public   Order   map ( String  value )   throws   Exception   { \n                 String [ ]  timeAndWord  =  value . split ( "," ) ; \n                 Timestamp  timestamp  =   Timestamp . valueOf ( timeAndWord [ 0 ] ) ; \n                 Order  order  =   new   Order ( ) ; \n                order . setOrderId ( timeAndWord [ 1 ] ) ; \n                order . setMoney ( 1 ) ; \n                order . setEvenTime ( timestamp . getTime ( ) ) ; \n                 return  order ; \n             } \n         } ) \n                 . assignTimestampsAndWatermarks ( WatermarkStrategy \n                         . < Order > forBoundedOutOfOrderness ( Duration . ofSeconds ( 10 ) ) \n                         . withTimestampAssigner ( ( element ,  recordTimestamp )   ->  element . getEvenTime ( ) ) ) \n                 . keyBy ( t  ->  t . getOrderId ( ) ) \n                 // \n                 . window ( TumblingEventTimeWindows . of ( Time . seconds ( 10 ) ) ) \n                 //trigger \n                 /**\n                 * :> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:03}\n                 * :> Order{orderId=\'cat\', userId=null, money=1, evenTime=12:00:04}\n                 * :> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:20}\n                 *\n                 */ \n                 //ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(10)EventTimeTrigger,key \n                 //.trigger(ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(10))) \n                 // spark \n                 . trigger ( ProcessingTimeoutTrigger . of ( EventTimeTrigger . create ( ) ,   Duration . ofSeconds ( 5 ) , false , true ) ) \n                 /**\n                 * \n                 * :> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:03}\n                 * :> Order{orderId=\'cat\', userId=null, money=1, evenTime=12:00:04}\n                 * :> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:20}\n                 * --\n                 * :> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:30}\n                 * :> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:40}\n                 * :> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:30}\n                 * --\n                 */ \n                 //ProcessingTimeTrigger sparktrigger0 \n                 //.trigger(ProcessingTimeTrigger.create()) \n                 // \n                 //.trigger(ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(5),false,true)) \n                 //.trigger(EventAndProcessingTimeOutTrigger.of(Duration.ofSeconds(5),false)) \n\n         /**\n         * 11.1 idle\n         *       eventtime trigger\n         * 2\n         *       1.5eventtime trigger\n *       * eventtime trigger1.2.3.\n         */ \n\n\n                 . sum ( "money" ) ; \n\n        result . print ( ":" ) ; \n\n        env . execute ( ) ; \n\n\n     } \n     public   static   class   Order { \n         private   String  orderId ; \n         private   Integer  userId ; \n         private   Integer  money ; \n         private   Long  evenTime ; \n\n         public   Order ( String  orderId ,   Integer  userId ,   Integer  money ,   Long  evenTime )   { \n             this . orderId  =  orderId ; \n             this . userId  =  userId ; \n             this . money  =  money ; \n             this . evenTime  =  evenTime ; \n         } \n\n         public   Order ( )   { \n         } \n\n         public   String   getOrderId ( )   { \n             return  orderId ; \n         } \n\n         public   void   setOrderId ( String  orderId )   { \n             this . orderId  =  orderId ; \n         } \n\n         public   Integer   getUserId ( )   { \n             return  userId ; \n         } \n\n         public   void   setUserId ( Integer  userId )   { \n             this . userId  =  userId ; \n         } \n\n         public   Integer   getMoney ( )   { \n             return  money ; \n         } \n\n         public   void   setMoney ( Integer  money )   { \n             this . money  =  money ; \n         } \n\n         public   Long   getEvenTime ( )   { \n             return  evenTime ; \n         } \n\n         public   void   setEvenTime ( Long  evenTime )   { \n             this . evenTime  =  evenTime ; \n         } \n\n         @Override \n         public   String   toString ( )   { \n             FastDateFormat  df  =   FastDateFormat . getInstance ( "HH:mm:ss" ) ; \n             return   "Order{"   + \n                     "orderId=\'"   +  orderId  +   \'\\\'\'   + \n                     ", userId="   +  userId  + \n                     ", money="   +  money  + \n                     ", evenTime="   +  df . format ( evenTime )   + \n                     \'}\' ; \n         } \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 trigger \n import   org . apache . flink . api . common . state . ValueState ; \n import   org . apache . flink . api . common . state . ValueStateDescriptor ; \n import   org . apache . flink . api . common . typeutils . base . LongSerializer ; \n import   org . apache . flink . runtime . operators . TaskContext ; \n import   org . apache . flink . streaming . api . windowing . triggers . Trigger ; \n import   org . apache . flink . streaming . api . windowing . triggers . TriggerResult ; \n import   org . apache . flink . streaming . api . windowing . windows . Window ; \n import   org . slf4j . Logger ; \n import   org . slf4j . LoggerFactory ; \n\n\n import   java . sql . Timestamp ; \n import   java . time . Duration ; \n\n\n\n public   class   EventAndProcessingTimeOutTrigger < T ,   W   extends   Window >   extends   Trigger < T ,   W >   { \n     private   static   final   Logger   LOG =   LoggerFactory . getLogger ( EventAndProcessingTimeOutTrigger . class ) ; \n     private   static   final   long  serialVersionUID  =   1L ; \n     private   final   long  interval ; \n     private   final   boolean  resetTimerOnNewRecord ; \n\n     private   final   ValueStateDescriptor < Long >  timeoutStateDesc  =   new   ValueStateDescriptor < > ( "timeout" ,   LongSerializer . INSTANCE ) ; \n\n\n     private   EventAndProcessingTimeOutTrigger ( long  interval , boolean  resetTimerOnNewRecord )   { \n         this . interval = interval ; \n         this . resetTimerOnNewRecord = resetTimerOnNewRecord ; \n     } \n\n     @Override \n     public   TriggerResult   onElement ( T  element ,   long  timestamp ,   W  window ,   TriggerContext  ctx )   throws   Exception   { \n         if   ( window . maxTimestamp ( )   <=  ctx . getCurrentWatermark ( ) )   { \n             // if the watermark is already past the window fire immediately \n             return   TriggerResult . FIRE ; \n         }   else   { \n             ValueState < Long >  timeoutState  =  ctx . getPartitionedState ( this . timeoutStateDesc ) ; \n             long  nextFireTimestamp  =  ctx . getCurrentProcessingTime ( )   +   this . interval ; \n             Long  timeoutTimestamp  =  timeoutState . value ( ) ; \n             if   ( timeoutTimestamp  !=   null   &&  resetTimerOnNewRecord )   { \n                ctx . deleteProcessingTimeTimer ( timeoutTimestamp ) ; \n                timeoutState . clear ( ) ; \n                timeoutTimestamp  =   null ; \n             } \n\n             if   ( timeoutTimestamp  ==   null )   { \n                timeoutState . update ( nextFireTimestamp ) ; \n                ctx . registerProcessingTimeTimer ( nextFireTimestamp ) ; \n             } \n\n            ctx . registerEventTimeTimer ( window . maxTimestamp ( ) ) ; \n             return   TriggerResult . CONTINUE ; \n         } \n\n     } \n\n     @Override \n     public   TriggerResult   onProcessingTime ( long  time ,   W  window ,   TriggerContext  ctx )   throws   Exception   { \n         //   \n         long  maxTimestamp  =  window . maxTimestamp ( ) ; \n         this . clear ( window ,  ctx ) ; \n         System . out . println ( "maxTimestamp = "   +   new   Timestamp ( maxTimestamp ) + " CurrentWatermark = " + new   Timestamp ( ctx . getCurrentWatermark ( ) ) ) ; \n         LOG . warn ( "LOG: maxTimestamp = "   +   new   Timestamp ( maxTimestamp ) + " CurrentWatermark = " + new   Timestamp ( ctx . getCurrentWatermark ( ) ) ) ; \n        ctx . registerEventTimeTimer ( maxTimestamp  + this . interval ) ; \n         return   TriggerResult . FIRE ; \n         /**\n         * \n         * :> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:03}\n         * :> Order{orderId=\'cat\', userId=null, money=1, evenTime=12:00:04}\n         * :> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:20}\n         * maxTimestamp = 2019-10-10 12:00:39.999 CurrentWatermark = 2019-10-10 12:00:29.999\n         * :> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:30}\n         * maxTimestamp = 2019-10-10 12:00:49.999 CurrentWatermark = 2019-10-10 12:00:29.999\n         * :> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:40}\n         * maxTimestamp = 2019-10-10 12:00:39.999 CurrentWatermark = 2019-10-10 12:00:29.999\n         * :> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:30}\n         */ \n     } \n\n     @Override \n     public   TriggerResult   onEventTime ( long  time ,   W  window ,   TriggerContext  ctx )   throws   Exception   { \n         return  time  ==  window . maxTimestamp ( )   ? \n                 TriggerResult . FIRE   : \n                 TriggerResult . CONTINUE ; \n     } \n\n     @Override \n     public   boolean   canMerge ( )   { \n         return   true ; \n     } \n\n     @Override \n     public   void   onMerge ( W  window ,   OnMergeContext  ctx )   throws   Exception   { \n         // only register a timer if the watermark is not yet past the end of the merged window \n         // this is in line with the logic in onElement(). If the watermark is past the end of \n         // the window onElement() will fire and setting a timer here would fire the window twice. \n         long  windowMaxTimestamp  =  window . maxTimestamp ( ) ; \n         if   ( windowMaxTimestamp  >  ctx . getCurrentWatermark ( ) )   { \n            ctx . registerEventTimeTimer ( windowMaxTimestamp ) ; \n         } \n     } \n\n     @Override \n     public   void   clear ( W  window ,   TriggerContext  ctx )   throws   Exception   { \n         ValueState < Long >  timeoutTimestampState  =  ctx . getPartitionedState ( this . timeoutStateDesc ) ; \n         Long  timeoutTimestamp  =  timeoutTimestampState . value ( ) ; \n         if   ( timeoutTimestamp  !=   null )   { \n            ctx . deleteProcessingTimeTimer ( timeoutTimestamp ) ; \n            timeoutTimestampState . clear ( ) ; \n         } \n        ctx . deleteEventTimeTimer ( window . maxTimestamp ( ) ) ; \n     } \n\n     public   static   < T ,   W   extends   Window >   EventAndProcessingTimeOutTrigger < T ,   W >   of ( Duration  timeout )   { \n         return   new   EventAndProcessingTimeOutTrigger < > ( timeout . toMillis ( ) , true ) ; \n     } \n     public   static   < T ,   W   extends   Window >   EventAndProcessingTimeOutTrigger < T ,   W >   of ( Duration  timeout , boolean  resetTimerOnNewRecord )   { \n         return   new   EventAndProcessingTimeOutTrigger < > ( timeout . toMillis ( ) , resetTimerOnNewRecord ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 #  Watermarker \n ==:== \n 1.Watermarker \n 2.Watermarker =  - ==== \n 3.Watermarker  \n ==4.Watermarker  >=  == \n 5. - >=  \n 6. >=  + \n 7. [window_start_time,window_end_time)  \n \n \n \n \n \n D10:00:00-10:10:00 \n -- \n package   cn . itcast . watermaker ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . commons . lang3 . time . FastDateFormat ; \n import   org . apache . flink . api . common . eventtime . * ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . SourceFunction ; \n import   org . apache . flink . streaming . api . functions . windowing . WindowFunction ; \n import   org . apache . flink . streaming . api . windowing . assigners . TumblingEventTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n import   org . apache . flink . streaming . api . windowing . windows . TimeWindow ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . ArrayList ; \n import   java . util . List ; \n import   java . util . Random ; \n import   java . util . UUID ; \n\n /**\n * Author itcast\n * Desc\n * ,: (IDID/)\n * 5s,5()\n * Watermaker\n */ \n public   class   WatermakerDemo02_Check   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         FastDateFormat  df  =   FastDateFormat . getInstance ( "HH:mm:ss" ) ; \n\n         //TODO 1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n\n         //TODO 2.Source \n         //() \n         DataStreamSource < Order >  orderDS  =  env . addSource ( new   SourceFunction < Order > ( )   { \n             private   boolean  flag  =   true ; \n             @Override \n             public   void   run ( SourceContext < Order >  ctx )   throws   Exception   { \n                 Random  random  =   new   Random ( ) ; \n                 while   ( flag )   { \n                     String  orderId  =   UUID . randomUUID ( ) . toString ( ) ; \n                     int  userId  =  random . nextInt ( 3 ) ; \n                     int  money  =  random . nextInt ( 100 ) ; \n                     //! \n                     long  eventTime  =   System . currentTimeMillis ( )   -  random . nextInt ( 5 )   *   1000 ; \n                     System . out . println ( ": " + userId  +   " : "   +  df . format ( eventTime ) ) ; \n                    ctx . collect ( new   Order ( orderId ,  userId ,  money ,  eventTime ) ) ; \n                     //TimeUnit.SECONDS.sleep(1); \n                     Thread . sleep ( 1000 ) ; \n                 } \n             } \n\n             @Override \n             public   void   cancel ( )   { \n                flag  =   false ; \n             } \n         } ) ; \n\n         //TODO 3.Transformation \n         /*DataStream<Order> watermakerDS = orderDS\n                .assignTimestampsAndWatermarks(\n                        WatermarkStrategy.<Order>forBoundedOutOfOrderness(Duration.ofSeconds(3))\n                                .withTimestampAssigner((event, timestamp) -> event.getEventTime())\n                );*/ \n\n         // \n         // \n         DataStream < Order >  watermakerDS  =  orderDS\n                 . assignTimestampsAndWatermarks ( \n                         new   WatermarkStrategy < Order > ( )   { \n                             @Override \n                             public   WatermarkGenerator < Order >   createWatermarkGenerator ( WatermarkGeneratorSupplier . Context  context )   { \n                                 return   new   WatermarkGenerator < Order > ( )   { \n                                     private   int  userId  =   0 ; \n                                     private   long  eventTime  =   0L ; \n                                     private   final   long  outOfOrdernessMillis  =   3000 ; \n                                     private   long  maxTimestamp  =   Long . MIN_VALUE   +  outOfOrdernessMillis  +   1 ; \n\n                                     @Override \n                                     public   void   onEvent ( Order  event ,   long  eventTimestamp ,   WatermarkOutput  output )   { \n                                        userId  =  event . userId ; \n                                        eventTime  =  event . eventTime ; \n                                        maxTimestamp  =   Math . max ( maxTimestamp ,  eventTimestamp ) ; \n                                     } \n\n                                     @Override \n                                     public   void   onPeriodicEmit ( WatermarkOutput  output )   { \n                                         //Watermaker =  -  \n                                         Watermark  watermark  =   new   Watermark ( maxTimestamp  -  outOfOrdernessMillis  -   1 ) ; \n                                         System . out . println ( "key:"   +  userId  +   ",:"   +  df . format ( System . currentTimeMillis ( ) )   +   ",:"   +  df . format ( eventTime )   +   ",:"   +  df . format ( watermark . getTimestamp ( ) ) ) ; \n                                        output . emitWatermark ( watermark ) ; \n                                     } \n                                 } ; \n                             } \n                         } . withTimestampAssigner ( ( order ,  timestamp )   ->  order . getEventTime ( ) ) \n                 ) ; \n\n\n         //,Watermaker! \n         //5s,5() \n        /* DataStream<Order> result = watermakerDS\n                 .keyBy(Order::getUserId)\n                //.timeWindow(Time.seconds(5), Time.seconds(5))\n                .window(TumblingEventTimeWindows.of(Time.seconds(5)))\n                .sum("money");*/ \n\n         // \n         //,,Watermaker \n         SingleOutputStreamOperator < String >  result  =  watermakerDS\n                 . keyBy ( Order :: getUserId ) \n                 . window ( TumblingEventTimeWindows . of ( Time . seconds ( 5 ) ) ) \n                 //apply \n                 //WindowFunction<IN, OUT, KEY, W extends Window> \n                 . apply ( new   WindowFunction < Order ,   String ,   Integer ,   TimeWindow > ( )   { \n                     @Override \n                     public   void   apply ( Integer  key ,   TimeWindow  window ,   Iterable < Order >  orders ,   Collector < String >  out )   throws   Exception   { \n                         // \n                         List < String >  list  =   new   ArrayList < > ( ) ; \n                         for   ( Order  order  :  orders )   { \n                             Long  eventTime  =  order . eventTime ; \n                             String  formatEventTime  =  df . format ( eventTime ) ; \n                            list . add ( formatEventTime ) ; \n                         } \n                         String  start  =  df . format ( window . getStart ( ) ) ; \n                         String  end  =  df . format ( window . getEnd ( ) ) ; \n                         //,, \n                         String  outStr  =   String . format ( "key:%s,:[%s~%s),:%s" ,  key . toString ( ) ,  start ,  end ,  list . toString ( ) ) ; \n                        out . collect ( outStr ) ; \n                     } \n                 } ) ; \n\n         //4.Sink \n        result . print ( ) ; \n\n         //5.execute \n        env . execute ( ) ; \n     } \n\n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   Order   { \n         private   String  orderId ; \n         private   Integer  userId ; \n         private   Integer  money ; \n         private   Long  eventTime ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 \n - \n \n public   class   WaterMaker   { \n\n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n\n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //env.setStreamTimeCharacteristic(); \n        env . setParallelism ( 1 ) ; \n         /*FastDateFormat df = FastDateFormat.getInstance("HH:mm:ss");\n        DataStreamSource<Order> orderDS = env.addSource(new SourceFunction<Order>() {\n            boolean flag = true;\n\n            @Override\n            public void run(SourceContext<Order> ctx) throws Exception {\n\n                while (flag) {\n                    Random random = new Random();\n                    String orderId = UUID.randomUUID().toString();\n                    Integer userId = random.nextInt(3);\n                    Integer money = random.nextInt(100);\n                    Long evenTime = System.currentTimeMillis() - random.nextInt(6) * 1000;\n                    System.out.println("1: " + userId + "  evenTime: " + df.format(evenTime));\n                    ctx.collect(new Order(orderId, userId, money, evenTime));\n                    Thread.sleep(2000);\n                }\n            }\n\n            @Override\n            public void cancel() {\n                flag = false;\n            }\n        });*/ \n         DataStreamSource < String >  source  =  env . socketTextStream ( "node1" ,   9999 ) ; \n         /*ParameterTool param = ParameterTool.fromArgs(args);\n        String topic = param.get("topic", "KafkaWordCount");\n        String group_id = param.get("group_id", "flink_wordcount");\n        boolean isWriteKafka = param.getBoolean("isWriteKafka", false);\n        boolean isWriteHdfs = param.getBoolean("isWriteHdfs", false);\n        boolean isWriteMysql = param.getBoolean("isWriteMysql", false);\n\n        Properties prop = new Properties();\n        prop.setProperty("bootstrap.servers", "node1:9092,node2:9092,node3:9092");\n        prop.setProperty("group.id", group_id);\n        prop.setProperty("auto.offset.reset", "latest");\n        prop.setProperty("enable.auto.commit", "true");\n        prop.setProperty("key.deserializer", "StringDeserializer");\n        prop.setProperty("value.deserializer", "StringDeserializer");\n        FlinkKafkaConsumer<String> kafka = new FlinkKafkaConsumer<>(topic, new SimpleStringSchema(), prop);\n        DataStreamSource<String> source = env.addSource(kafka);*/ \n\n         OutputTag < Order >  seriousLateOutputTag  =   new   OutputTag < > ( "seriousLateOutputTag" ,   TypeInformation . of ( Order . class ) ) ; \n         SingleOutputStreamOperator < Order >  result  =  source . rebalance ( ) . map ( new   RichMapFunction < String ,   Order > ( )   { \n             @Override \n             public   Order   map ( String  value )   throws   Exception   { \n                 String [ ]  timeAndWord  =  value . split ( "," ) ; \n                 Timestamp  timestamp  =   Timestamp . valueOf ( timeAndWord [ 0 ] ) ; \n                 Order  order  =   new   Order ( ) ; \n                order . setOrderId ( timeAndWord [ 1 ] ) ; \n                order . setMoney ( 1 ) ; \n                order . setEvenTime ( timestamp . getTime ( ) ) ; \n                 return  order ; \n             } \n         } ) \n                 . assignTimestampsAndWatermarks ( WatermarkStrategy \n                         . < Order > forBoundedOutOfOrderness ( Duration . ofSeconds ( 10 ) ) \n                         . withTimestampAssigner ( ( element ,  recordTimestamp )   ->  element . getEvenTime ( ) ) ) \n                 . keyBy ( t  ->  t . getOrderId ( ) ) \n                 // \n                 . window ( TumblingEventTimeWindows . of ( Time . seconds ( 10 ) ) ) \n                 //trigger \n                 /**\n                 * :> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:03}\n                 * :> Order{orderId=\'cat\', userId=null, money=1, evenTime=12:00:04}\n                 * :> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:20}\n                 *\n                 */ \n                 //ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(10)EventTimeTrigger,key \n                 //.trigger(ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(10))) \n                 // spark \n                 //.trigger(ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(5),false,true)) \n                 /**\n                 * \n                 * :> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:03}\n                 * :> Order{orderId=\'cat\', userId=null, money=1, evenTime=12:00:04}\n                 * :> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:20}\n                 * --\n                 * :> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:30}\n                 * :> Order{orderId=\'dog\', userId=null, money=1, evenTime=12:00:40}\n                 * :> Order{orderId=\'dog\', userId=null, money=2, evenTime=12:00:30}\n                 * --\n                 */ \n                 //ProcessingTimeTrigger sparktrigger0 \n                 //.trigger(ProcessingTimeTrigger.create()) \n                 // \n                 //.trigger(ProcessingTimeoutTrigger.of(EventTimeTrigger.create(), Duration.ofSeconds(5),false,true)) \n                 . trigger ( EventAndProcessingTimeOutTrigger . of ( Duration . ofSeconds ( 5 ) , false ) ) \n\n         /**\n         * 11.1 idle\n         *       eventtime trigger\n         * 2\n         *       1.5eventtime trigger\n *       * eventtime trigger1.2.3.\n         *\n         */ \n                 . allowedLateness ( Time . seconds ( 15 ) ) \n                 . sideOutputLateData ( seriousLateOutputTag ) \n                 . sum ( "money" ) ; \n\n        result . print ( ":" ) ; \n        result . getSideOutput ( seriousLateOutputTag ) . print ( ":" ) ; \n\n        env . execute ( ) ; \n\n\n     } \n     public   static   class   Order { \n         private   String  orderId ; \n         private   Integer  userId ; \n         private   Integer  money ; \n         private   Long  evenTime ; \n\n         public   Order ( String  orderId ,   Integer  userId ,   Integer  money ,   Long  evenTime )   { \n             this . orderId  =  orderId ; \n             this . userId  =  userId ; \n             this . money  =  money ; \n             this . evenTime  =  evenTime ; \n         } \n\n         public   Order ( )   { \n         } \n\n         public   String   getOrderId ( )   { \n             return  orderId ; \n         } \n\n         public   void   setOrderId ( String  orderId )   { \n             this . orderId  =  orderId ; \n         } \n\n         public   Integer   getUserId ( )   { \n             return  userId ; \n         } \n\n         public   void   setUserId ( Integer  userId )   { \n             this . userId  =  userId ; \n         } \n\n         public   Integer   getMoney ( )   { \n             return  money ; \n         } \n\n         public   void   setMoney ( Integer  money )   { \n             this . money  =  money ; \n         } \n\n         public   Long   getEvenTime ( )   { \n             return  evenTime ; \n         } \n\n         public   void   setEvenTime ( Long  evenTime )   { \n             this . evenTime  =  evenTime ; \n         } \n\n         @Override \n         public   String   toString ( )   { \n             FastDateFormat  df  =   FastDateFormat . getInstance ( "HH:mm:ss" ) ; \n             return   "Order{"   + \n                     "orderId=\'"   +  orderId  +   \'\\\'\'   + \n                     ", userId="   +  userId  + \n                     ", money="   +  money  + \n                     ", evenTime="   +  df . format ( evenTime )   + \n                     \'}\' ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 #  State \n Flink \n Flink, \n hello ,(hello,1) \n hello ,(hello,2) \n Flink/, \n , \n !---! \n state! \n package   cn . itcast . source ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc DataStream-Source-Socket\n */ \n public   class   SourceDemo03_Socket   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n\n         //TODO 2.transformation \n         /*SingleOutputStreamOperator<String> words = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public void flatMap(String value, Collector<String> out) throws Exception {\n                String[] arr = value.split(" ");\n                for (String word : arr) {\n                    out.collect(word);\n                }\n            }\n        });\n\n        words.map(new MapFunction<String, Tuple2<String,Integer>>() {\n            @Override\n            public Tuple2<String, Integer> map(String value) throws Exception {\n                return Tuple2.of(value,1);\n            }\n        });*/ \n\n         //:21,1 \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  wordAndOne  =  lines . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n\n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  wordAndOne . keyBy ( t  ->  t . f0 ) . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 #   \n \n ,, map \n \n hello --\x3e (hello,1) \n hello --\x3e (hello,1) \n \n \n ,,:sum \n \n hello , (hello,1) \n hello , (hello,2) \n  \n \n State\n \n ManagerState-- : Fink/,\n \n KeyState--keyedStream, \n- OperatorState--Source,ListState \n \n \n RawState--,byte[],Operator\n \n OperatorState \n \n \n \n \n \n : \n ManagerState-keyState \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/state.html \n package   cn . itcast . state ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . common . state . ValueState ; \n import   org . apache . flink . api . common . state . ValueStateDescriptor ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . api . java . tuple . Tuple3 ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n\n /**\n * Author itcast\n * Desc KeyStateValueState/maxBy\n */ \n public   class   StateDemo01_KeyState   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n //RuntimeExecutionMode.AUTOMATIC,Demo,keyBy \n         /*\n        (,2,2)\n        (,8,8)\n        (,4,8)\n        (,1,1)\n        (,6,6)\n        (,3,6)\n         */ \n         //,,, \n         /*\n        (,1,1)\n        (,2,2)\n        (,6,6)\n        (,8,8)\n        (,3,6)\n        (,4,8)\n         */ \n         //TODO 1.source \n         DataStream < Tuple2 < String ,   Long > >  tupleDS  =  env . fromElements ( \n                 Tuple2 . of ( "" ,   1L ) , \n                 Tuple2 . of ( "" ,   2L ) , \n                 Tuple2 . of ( "" ,   6L ) , \n                 Tuple2 . of ( "" ,   8L ) , \n                 Tuple2 . of ( "" ,   3L ) , \n                 Tuple2 . of ( "" ,   4L ) \n         ) ; \n\n         //TODO 2.transformation \n         //:value \n         //maxBy \n         DataStream < Tuple2 < String ,   Long > >  result1  =  tupleDS . keyBy ( t  ->  t . f0 ) . maxBy ( 1 ) ; \n\n         //KeyStateValueStatemaxBy \n         DataStream < Tuple3 < String ,   Long ,   Long > >  result2  =  tupleDS . keyBy ( t  ->  t . f0 ) . map ( new   RichMapFunction < Tuple2 < String ,   Long > ,   Tuple3 < String ,   Long ,   Long > > ( )   { \n             //-1. \n             private   ValueState < Long >  maxValueState ; \n\n             //-2. \n               /*\n                    open,,,,,.\n                     */ \n             @Override \n             public   void   open ( Configuration  parameters )   throws   Exception   { \n                 // \n                 ValueStateDescriptor  stateDescriptor  =   new   ValueStateDescriptor ( "maxValueState" ,   Long . class ) ; \n                 /// \n                maxValueState  =   getRuntimeContext ( ) . getState ( stateDescriptor ) ; \n             } \n\n             //-3. \n             @Override \n             public   Tuple3 < String ,   Long ,   Long >   map ( Tuple2 < String ,   Long >  value )   throws   Exception   { \n                 Long  currentValue  =  value . f1 ; \n                 // \n                 Long  historyValue  =  maxValueState . value ( ) ; \n                 // \n                 if   ( historyValue  ==   null   ||  currentValue  >  historyValue )   { \n                    historyValue  =  currentValue ; \n                     // \n                    maxValueState . update ( historyValue ) ; \n                     return   Tuple3 . of ( value . f0 ,  currentValue ,  historyValue ) ; \n                 }   else   { \n                     return   Tuple3 . of ( value . f0 ,  currentValue ,  historyValue ) ; \n                 } \n             } \n         } ) ; \n\n\n         //TODO 3.sink \n         //result1.print(); \n         //4> (,6) \n         //1> (,8) \n        result2 . print ( ) ; \n         //1> (,xxx,8) \n         //4> (,xxx,6) \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 #  ManagerState-OperatorState \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/state.html \n package   cn . itcast . state ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . restartstrategy . RestartStrategies ; \n import   org . apache . flink . api . common . state . ListState ; \n import   org . apache . flink . api . common . state . ListStateDescriptor ; \n import   org . apache . flink . runtime . state . FunctionInitializationContext ; \n import   org . apache . flink . runtime . state . FunctionSnapshotContext ; \n import   org . apache . flink . runtime . state . filesystem . FsStateBackend ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . checkpoint . CheckpointedFunction ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . environment . CheckpointConfig ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichParallelSourceFunction ; \n\n import   java . util . Iterator ; \n\n /**\n * Author itcast\n * Desc OperatorStateListStateKafkaSourceoffset\n */ \n public   class   StateDemo02_OperatorState   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n        env . setParallelism ( 1 ) ; //1 \n         //Checkpoint, \n        env . enableCheckpointing ( 1000 ) ; //1sCheckpoint \n        env . setStateBackend ( new   FsStateBackend ( "file:///D:/ckp" ) ) ; \n        env . getCheckpointConfig ( ) . enableExternalizedCheckpoints ( CheckpointConfig . ExternalizedCheckpointCleanup . RETAIN_ON_CANCELLATION ) ; \n        env . getCheckpointConfig ( ) . setCheckpointingMode ( CheckpointingMode . EXACTLY_ONCE ) ; \n         //: 232 \n        env . setRestartStrategy ( RestartStrategies . fixedDelayRestart ( 2 ,   3000 ) ) ; \n\n         //TODO 1.source \n         DataStreamSource < String >  ds  =  env . addSource ( new   MyKafkaSource ( ) ) . setParallelism ( 1 ) ; \n\n         //TODO 2.transformation \n\n         //TODO 3.sink \n        ds . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     //OperatorStateListStateKafkaSourceoffset \n     public   static   class   MyKafkaSource   extends   RichParallelSourceFunction < String >   implements   CheckpointedFunction   { \n         private   boolean  flag  =   true ; \n         //-1.ListState \n         private   ListState < Long >  offsetState  =   null ;   //offset \n         private   Long  offset  =   0L ; //offset \n\n         //-2./ListState \n         @Override \n         public   void   initializeState ( FunctionInitializationContext  context )   throws   Exception   { \n             ListStateDescriptor < Long >  stateDescriptor  =   new   ListStateDescriptor < > ( "offsetState" ,   Long . class ) ; \n            offsetState  =  context . getOperatorStateStore ( ) . getListState ( stateDescriptor ) ; \n         } \n         //-3.state \n         @Override \n         public   void   run ( SourceContext < String >  ctx )   throws   Exception   { \n          \n                 Iterator < Long >  iterator  =  offsetState . get ( ) . iterator ( ) ; \n                 if ( iterator . hasNext ( ) ) { \n                    offset  =  iterator . next ( ) ; \n                 } \n               while   ( flag ) { \n                offset  +=   1 ; \n                 int  subTaskId  =   getRuntimeContext ( ) . getIndexOfThisSubtask ( ) ; \n                ctx . collect ( "subTaskId:" +  subTaskId  +   ",offset:" + offset ) ; \n                 Thread . sleep ( 1000 ) ; \n\n                 // \n                 if ( offset  %   5   ==   0 ) { \n                     throw   new   Exception ( "bug....." ) ; \n                 } \n             } \n         } \n         //-4.state \n         //stateCheckpoint \n         @Override \n         public   void   snapshotState ( FunctionSnapshotContext  context )   throws   Exception   { \n            offsetState . clear ( ) ; //Checkpoint \n            offsetState . add ( offset ) ; \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 #  state \n ctx.timerService().registerProcessingTimeTimer(long time onTimer \n State TTL redis \n  flink  \n  GROUP BY  JOIN  \n  OOMRocksDB  Flink 1.6 State TTL   Keyed Table API  SQL Idle State Retention Time \n  Flink  State TTL \n import   org . apache . flink . api . common . state . StateTtlConfig ; \n import   org . apache . flink . api . common . state . ValueStateDescriptor ; \n import   org . apache . flink . api . common . time . Time ; \n\n StateTtlConfig  ttlConfig  =   StateTtlConfig \n    . newBuilder ( Time . seconds ( 1 ) ) \n    . setUpdateType ( StateTtlConfig . UpdateType . OnCreateAndWrite ) \n    . setStateVisibility ( StateTtlConfig . StateVisibility . NeverReturnExpired ) \n    . build ( ) ; \n   \n ValueStateDescriptor < String >  stateDescriptor  =   new   ValueStateDescriptor < > ( "text state" ,   String . class ) ; \nstateDescriptor . enableTimeToLive ( ttlConfig ) ; \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13  State TTL   StateTtlConfig   \n 1.Builder Pattern Time  TTL ; \n 2.Update Type \n 3.State Visibility \n 4.State Descriptor State TTL  \n StateTtl \n Config  \n l  TTL  org.apache.flink.api.common.time.Time  TTL + TTL org.apache.flink.runtime.state.ttl.TtlUtils  expired  \n l  UpdateType  Enum  Disabled OnCreateAndWrite OnReadAndWrite \n l  StateVisibility  Enum  ReturnExpiredIfNotCleanedUp  NeverReturnExpired \n TimeCharacteristic    TtlTimeCharacteristic  State TTL  Enum  Deprecated TtlTimeCharacteristic  Flink 1.8 ProcessingTime  EventTime  State TTL  \n CleanupStrategies  \n 1.keyrocksdb \n 2. \n  Enum  FULL_STATE_SCAN_SNAPSHOT  EmptyCleanupStrategy Snapshot / Checkpoint \n Flink  Heap StateBackend  INCREMENTAL_CLEANUP IncrementalCleanupStrategy  RocksDB StateBackend  ROCKSDB_COMPACTION_FILTER RocksdbCompactFilterCleanupStrategy  \n Flink  RocksDB  JNI  C++  FlinkCompactionFilter  RocksDB  Compaction  \n StateTtlConfignewBuilder \n TTL OnCreateAndWrite \n \n \n \n  \n  \n \n \n \n \n StateTtlConfig.UpdateType.Disabled \n TTL \n \n \n StateTtlConfig.UpdateType.OnCreateAndWrite \n State \n \n \n StateTtlConfig.UpdateType.OnReadAndWrite \n State \n \n \n \n NeverReturnExpired \n \n \n \n  \n  \n \n \n \n \n StateTtlConfig.StateVisibility.NeverReturnExpired \n  \n \n \n StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp \n  \n Checkpoint \n CheckpointState \n Checkpoint \n \n 0.FlinkJobManagerCheckpointCoordinator \n 1.CoordinatorSourceOperatorBarrier(Checkpoint) \n 2.SourceOperatorBarrier,(,),State, (HDFS),  okCoordinatorBarrierOperator \n 3.TransformationOperatorBarrier,2,BarrierSink \n 4.SinkBarrier2 \n 5.CoordinatorOperatorok, \n Chandy-Lamport algorithm  \n FlinkCheckpointChandy-Lamport algorithm    ! \n https://zhuanlan.zhihu.com/p/53482103 \n Chandy-Lamport algorithmZKPaxos  \n https://www.cnblogs.com/shenguanpu/p/4048660.html \n FlinkChandy-Lamport algorithm,SparkStructuredStreaming \n / \n \n \n \n \n   < dependency > \n        < groupId > org.apache.flink </ groupId > \n        < artifactId > flink-statebackend-rocksdb_2.12 </ artifactId > \n        < version > 1.12.0 </ version > \n     </ dependency > \n \n 1 2 3 4 5 Checkpoint \n \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/checkpointing.html \n package   cn . itcast . checkpoint ; \n\n import   org . apache . commons . lang3 . SystemUtils ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . runtime . state . filesystem . FsStateBackend ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . CheckpointConfig ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaProducer ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . Properties ; \n\n /**\n * Author itcast\n * Desc Flink-Checkpoint\n */ \n public   class   CheckpointDemo01   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO ===========Checkpoint==== \n         //===========1:============= \n         //Checkpoint1000msCheckpoint/1000msBarrier! \n        env . enableCheckpointing ( 1000 ) ; \n         //State/ \n         //Memory:State,Checkpoint--! \n         //Fs:State,CheckpointFS(/HDFS)-- \n         //RocksDB:StateRocksDB(+),CheckpointFS(/HDFS)--, \n         /*if(args.length > 0){\n            env.setStateBackend(new FsStateBackend(args[0]));\n        }else {\n            env.setStateBackend(new FsStateBackend("file:///D:\\\\data\\\\ckp"));\n        }*/ \n         if   ( SystemUtils . IS_OS_WINDOWS )   { \n            env . setStateBackend ( new   FsStateBackend ( "file:///D:/ckp" ) ) ; \n         }   else   { \n            env . setStateBackend ( new   FsStateBackend ( "hdfs://node1:8020/flink-checkpoint/checkpoint" ) ) ; \n         } \n         //===========2:=========== \n         //Checkpoint ,Checkpoint 500ms(1000msCheckpoint,) \n         //:,1s,500m \n        env . getCheckpointConfig ( ) . setMinPauseBetweenCheckpoints ( 500 ) ; //0 \n         //Checkpointtrue  false \n         //env.getCheckpointConfig().setFailOnCheckpointingErrors(false);//true \n        env . getCheckpointConfig ( ) . setTolerableCheckpointFailureNumber ( 10 ) ; //0 \n         //, Cancel  Checkpoint CheckpointCancel \n         //ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATIONtrue,checkpoint() \n         //ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATIONfalse,checkpoint \n        env . getCheckpointConfig ( ) . enableExternalizedCheckpoints ( CheckpointConfig . ExternalizedCheckpointCleanup . RETAIN_ON_CANCELLATION ) ; \n\n         //===========3:=============== \n         //checkpointEXACTLY_ONCE() \n        env . getCheckpointConfig ( ) . setCheckpointingMode ( CheckpointingMode . EXACTLY_ONCE ) ; \n         //checkpoint, Checkpoint 60sCheckpoint, \n        env . getCheckpointConfig ( ) . setCheckpointTimeout ( 60000 ) ; //10 \n         //checkpoint \n         //env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);//1 \n\n         //2.Source \n         DataStream < String >  linesDS  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n         //3.Transformation \n         //3.11 \n         DataStream < Tuple2 < String ,   Integer > >  wordAndOneDS  =  linesDS . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 //value \n                 String [ ]  words  =  value . split ( " " ) ; \n                 for   ( String  word  :  words )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n         //3.2 \n         //:groupBy,keyBy \n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  groupedDS  =  wordAndOneDS . keyBy ( t  ->  t . f0 ) ; \n         //3.3 \n         DataStream < Tuple2 < String ,   Integer > >  aggResult  =  groupedDS . sum ( 1 ) ; \n\n         DataStream < String >  result  =   ( SingleOutputStreamOperator < String > )  aggResult . map ( new   RichMapFunction < Tuple2 < String ,   Integer > ,   String > ( )   { \n             @Override \n             public   String   map ( Tuple2 < String ,   Integer >  value )   throws   Exception   { \n                 return  value . f0  +   ":::"   +  value . f1 ; \n             } \n         } ) ; \n\n         //4.sink \n        result . print ( ) ; \n\n         Properties  props  =   new   Properties ( ) ; \n        props . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; \n         FlinkKafkaProducer < String >  kafkaSink  =   new   FlinkKafkaProducer < > ( "flink_kafka" ,   new   SimpleStringSchema ( ) ,  props ) ; \n        result . addSink ( kafkaSink ) ; \n\n         //5.execute \n        env . execute ( ) ; \n\n         // /export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic flink_kafka \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 #  -- \n \n package   cn . itcast . checkpoint ; \n\n import   org . apache . commons . lang3 . SystemUtils ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . common . restartstrategy . RestartStrategies ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . api . common . time . Time ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . runtime . state . filesystem . FsStateBackend ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . KeyedStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . CheckpointConfig ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaProducer ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . Properties ; \n import   java . util . concurrent . TimeUnit ; \n\n /**\n * Author itcast\n * Desc Flink-Checkpoint+\n */ \n public   class   CheckpointDemo02_Restart   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO ===========Checkpoint==== \n         //===========1:============= \n         //Checkpoint1000msCheckpoint/1000msBarrier! \n        env . enableCheckpointing ( 1000 ) ; \n         //State/ \n         //Memory:State,Checkpoint--! \n         //Fs:State,CheckpointFS(/HDFS)-- \n         //RocksDB:StateRocksDB(+),CheckpointFS(/HDFS)--, \n         /*if(args.length > 0){\n            env.setStateBackend(new FsStateBackend(args[0]));\n        }else {\n            env.setStateBackend(new FsStateBackend("file:///D:\\\\data\\\\ckp"));\n        }*/ \n         if   ( SystemUtils . IS_OS_WINDOWS )   { \n            env . setStateBackend ( new   FsStateBackend ( "file:///D:/ckp" ) ) ; \n         }   else   { \n            env . setStateBackend ( new   FsStateBackend ( "hdfs://node1:8020/flink-checkpoint/checkpoint" ) ) ; \n         } \n         //===========2:=========== \n         //Checkpoint ,Checkpoint 500ms(1000msCheckpoint,) \n         //:,1s,500m \n        env . getCheckpointConfig ( ) . setMinPauseBetweenCheckpoints ( 500 ) ; //0 \n         //Checkpointtrue  false \n         //env.getCheckpointConfig().setFailOnCheckpointingErrors(false);//true \n        env . getCheckpointConfig ( ) . setTolerableCheckpointFailureNumber ( 10 ) ; //0 \n         //, Cancel  Checkpoint CheckpointCancel \n         //ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATIONtrue,checkpoint() \n         //ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATIONfalse,checkpoint \n        env . getCheckpointConfig ( ) . enableExternalizedCheckpoints ( CheckpointConfig . ExternalizedCheckpointCleanup . RETAIN_ON_CANCELLATION ) ; \n\n         //===========3:=============== \n         //checkpointEXACTLY_ONCE() \n        env . getCheckpointConfig ( ) . setCheckpointingMode ( CheckpointingMode . EXACTLY_ONCE ) ; \n         //checkpoint, Checkpoint 60sCheckpoint, \n        env . getCheckpointConfig ( ) . setCheckpointTimeout ( 60000 ) ; //10 \n         //checkpoint \n        env . getCheckpointConfig ( ) . setMaxConcurrentCheckpoints ( 1 ) ; //1 \n\n         //TODO ===: \n         //1.Checkpoint:,,bug \n         //2. \n         //env.setRestartStrategy(RestartStrategies.noRestart()); \n         //3.-- \n        env . setRestartStrategy ( RestartStrategies . fixedDelayRestart ( \n                 3 ,   // 3 \n                 Time . of ( 5 ,   TimeUnit . SECONDS )   //  \n         ) ) ; \n         //:job,3, 5s \n         //4.-- \n         /*env.setRestartStrategy(RestartStrategies.failureRateRestart(\n                3, // \n                Time.of(1, TimeUnit.MINUTES), //\n                Time.of(3, TimeUnit.SECONDS) // \n        ));*/ \n         //:1job,,3s (13,) \n\n\n         //2.Source \n         DataStream < String >  linesDS  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n         //3.Transformation \n         //3.11 \n         DataStream < Tuple2 < String ,   Integer > >  wordAndOneDS  =  linesDS . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 //value \n                 String [ ]  words  =  value . split ( " " ) ; \n                 for   ( String  word  :  words )   { \n                     if   ( word . equals ( "bug" ) )   { \n                         System . out . println ( "bug....." ) ; \n                         throw   new   Exception ( "bug....." ) ; \n                     } \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n         //3.2 \n         //:groupBy,keyBy \n         KeyedStream < Tuple2 < String ,   Integer > ,   String >  groupedDS  =  wordAndOneDS . keyBy ( t  ->  t . f0 ) ; \n         //3.3 \n         DataStream < Tuple2 < String ,   Integer > >  aggResult  =  groupedDS . sum ( 1 ) ; \n\n         DataStream < String >  result  =   ( SingleOutputStreamOperator < String > )  aggResult . map ( new   RichMapFunction < Tuple2 < String ,   Integer > ,   String > ( )   { \n             @Override \n             public   String   map ( Tuple2 < String ,   Integer >  value )   throws   Exception   { \n                 return  value . f0  +   ":::"   +  value . f1 ; \n             } \n         } ) ; \n\n         //4.sink \n        result . print ( ) ; \n\n         Properties  props  =   new   Properties ( ) ; \n        props . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; \n         FlinkKafkaProducer < String >  kafkaSink  =   new   FlinkKafkaProducer < > ( "flink_kafka" ,   new   SimpleStringSchema ( ) ,  props ) ; \n        result . addSink ( kafkaSink ) ; \n\n         //5.execute \n        env . execute ( ) ; \n\n         // /export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic flink_kafka \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 -- \n 1.-kafka \n 2.Flink \n 3.jar \n http://node1:8081/#/submit \n \n 4.hdfs \n 5. \n \n 6.ckp \n hdfs://node1:8020/flink-checkpoint/checkpoint/acb9071752276e86552a30fda41e021c/chk-100 \n \n 7. \n Savepoint- \n \n \n  \nyarn session\n/export/server/flink/bin/yarn-session.sh -n 2 -tm 800 -s 2 -d\njob-Checkpoint\n/export/server/flink/bin/flink run --class cn.itcast.checkpoint.CheckpointDemo01 /root/ckp.jar\nsavepoint--Checkpoint\n/export/server/flink/bin/flink savepoint 0e921a10eb31bb0983b637929ec87a8a hdfs://node1:8020/flink-checkpoint/savepoint/\njob\n/export/server/flink/bin/flink cancel 0e921a10eb31bb0983b637929ec87a8a\njob,savepoint\n/export/server/flink/bin/flink run -s hdfs://node1:8020/flink-checkpoint/savepoint/savepoint-0e921a-1cac737bff7a --class cn.itcast.checkpoint.CheckpointDemo01 /root/ckp.jar \nyarn session\nyarn application -kill application_1607782486484_0014\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #  BroadcastState- \n flink \n 1. \n // \n        env . registerCachedFile ( "D:/wxgz-local/resources_ceshi/too.properties" ,   "too" ) \n         //open \n            val file :   File   =  getRuntimeContext . getDistributedCache . getFile ( "too" ) \n        val prop  =   new   Properties \n\n        prop . load ( new   FileInputStream ( file ) ) \n\n        val value  =  prop . getProperty ( "cycle" ) \n                \n //==>Guava Cache +reload/lookup \n //open \n : \n 1.  , worker\n 2.  , copy , \n\n Flink hadooptaskmanagertask\n ( hdfss3 )  ExecutionEnvironment \n Flink taskmanagertaskmanager\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 2. \n //checkpoint2.3.join \n // \n 1. \n    checkpointsavepoint\n 2. joinjoin\n     SQL  temporary join join\n \n 1 2 3 4 5 6 runningcheckpointmysqlckmysqlkafkacdc changelog \n \n  \n \n \n l  \n \n \n Broadcast State Map K-V  \n \n \n Broadcast State , BroadcastProcessFunction KeyedBroadcastProcessFunction processBroadcastElement  BroadcastProcessFunction KeyedBroadcastProcessFunction processElement  \n \n \n Broadcast State Task  \n \n \n Broadcast State Checkpoint Task Checkpoint  \n \n \n Broadcast State   RocksDB State Backend  \n \n \n \n --,id, \n /---- \n , , (id,, ) \n /--  (/--) \n  \n 1.broadcast state \n  \n /**\n     * --\n     * id,,,id\n     * <userID, eventTime, eventType, productID>\n     */ \n     public   static   class   MySource   implements   SourceFunction < Tuple4 < String ,   String ,   String ,   Integer > >   { \n         private   boolean  isRunning  =   true ; \n         @Override \n         public   void   run ( SourceContext < Tuple4 < String ,   String ,   String ,   Integer > >  ctx )   throws   Exception   { \n             Random  random  =   new   Random ( ) ; \n             SimpleDateFormat  df  =   new   SimpleDateFormat ( "yyyy-MM-dd HH:mm:ss" ) ; \n             while   ( isRunning ) { \n                 int  id  =  random . nextInt ( 4 )   +   1 ; \n                 String  user_id  =   "user_"   +  id ; \n                 String  eventTime  =  df . format ( new   Date ( ) ) ; \n                 String  eventType  =   "type_"   +  random . nextInt ( 3 ) ; \n                 int  productId  =  random . nextInt ( 4 ) ; \n                ctx . collect ( Tuple4 . of ( user_id , eventTime , eventType , productId ) ) ; \n                 Thread . sleep ( 500 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            isRunning  =   false ; \n         } \n     } \n     /**\n     * //--\n     * <id,<,>>\n     */ \n     /*\nCREATE TABLE `user_info` (\n  `userID` varchar(20) NOT NULL,\n  `userName` varchar(10) DEFAULT NULL,\n  `userAge` int(11) DEFAULT NULL,\n  PRIMARY KEY (`userID`) USING BTREE\n) ENGINE=MyISAM DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;\n\nINSERT INTO `user_info` VALUES (\'user_1\', \'\', 10);\nINSERT INTO `user_info` VALUES (\'user_2\', \'\', 20);\nINSERT INTO `user_info` VALUES (\'user_3\', \'\', 30);\nINSERT INTO `user_info` VALUES (\'user_4\', \'\', 40);\n     */ \n     public   static   class   MySQLSource   extends   RichSourceFunction < Map < String ,   Tuple2 < String ,   Integer > > >   { \n         private   boolean  flag  =   true ; \n         private   Connection  conn  =   null ; \n         private   PreparedStatement  ps  =   null ; \n         private   ResultSet  rs  =   null ; \n\n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            conn  =   DriverManager . getConnection ( "jdbc:mysql://localhost:3306/bigdata" ,   "root" ,   "root" ) ; \n             String  sql  =   "select `userID`, `userName`, `userAge` from `user_info`" ; \n            ps  =  conn . prepareStatement ( sql ) ; \n         } \n         @Override \n         public   void   run ( SourceContext < Map < String ,   Tuple2 < String ,   Integer > > >  ctx )   throws   Exception   { \n             while   ( flag ) { \n                 Map < String ,   Tuple2 < String ,   Integer > >  map  =   new   HashMap < > ( ) ; \n                 ResultSet  rs  =  ps . executeQuery ( ) ; \n                 while   ( rs . next ( ) ) { \n                     String  userID  =  rs . getString ( "userID" ) ; \n                     String  userName  =  rs . getString ( "userName" ) ; \n                     int  userAge  =  rs . getInt ( "userAge" ) ; \n                     //Map<String, Tuple2<String, Integer>> \n                    map . put ( userID ,   Tuple2 . of ( userName , userAge ) ) ; \n                 } \n                ctx . collect ( map ) ; \n                 Thread . sleep ( 5000 ) ; //5s! \n             } \n         } \n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n         @Override \n         public   void   close ( )   throws   Exception   { \n             if   ( conn  !=   null )  conn . close ( ) ; \n             if   ( ps  !=   null )  ps . close ( ) ; \n             if   ( rs  !=   null )  rs . close ( ) ; \n         } \n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83  \n 1.env\n2.source\n-1.-\n<userID, eventTime, eventType, productID>\n-2.-MySQL\n<id,<,>>\n3.transformation\n-1.\nMapStateDescriptor<Void, Map<String, Tuple2<String, Integer>>> descriptor =\nnew MapStateDescriptor<>("config",Types.VOID, Types.MAP(Types.STRING, Types.TUPLE(Types.STRING, Types.INT)));\n\n-2.\nBroadcastStream<Map<String, Tuple2<String, Integer>>> broadcastDS = configDS.broadcast(descriptor);\n-3.\nBroadcastConnectedStream<Tuple4<String, String, String, Integer>, Map<String, Tuple2<String, Integer>>> connectDS =eventDS.connect(broadcastDS);\n-4.-\n\n4.sink\n5.execute\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  \n package   cn . itcast . feature ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . state . BroadcastState ; \n import   org . apache . flink . api . common . state . MapStateDescriptor ; \n import   org . apache . flink . api . common . state . ReadOnlyBroadcastState ; \n import   org . apache . flink . api . common . typeinfo . Types ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . api . java . tuple . Tuple4 ; \n import   org . apache . flink . api . java . tuple . Tuple6 ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . BroadcastConnectedStream ; \n import   org . apache . flink . streaming . api . datastream . BroadcastStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . co . BroadcastProcessFunction ; \n import   org . apache . flink . streaming . api . functions . source . RichSourceFunction ; \n import   org . apache . flink . streaming . api . functions . source . SourceFunction ; \n import   org . apache . flink . util . Collector ; \n\n import   java . sql . Connection ; \n import   java . sql . DriverManager ; \n import   java . sql . PreparedStatement ; \n import   java . sql . ResultSet ; \n import   java . text . SimpleDateFormat ; \n import   java . util . Date ; \n import   java . util . HashMap ; \n import   java . util . Map ; \n import   java . util . Random ; \n\n /**\n * Author itcast\n * Desc\n */ \n public   class   BroadcastStateDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n        env . setParallelism ( 1 ) ; \n\n         //TODO 2.source \n         //-1.-- \n         //<userID, eventTime, eventType, productID> \n         DataStreamSource < Tuple4 < String ,   String ,   String ,   Integer > >  eventDS  =  env . addSource ( new   MySource ( ) ) ; \n\n         //-2.//---MySQL \n         //<id,<,>> \n         DataStreamSource < Map < String ,   Tuple2 < String ,   Integer > > >  userDS  =  env . addSource ( new   MySQLSource ( ) ) ; \n\n         //TODO 3.transformation \n         //-1. \n         //keynull \n         MapStateDescriptor < Void ,   Map < String ,   Tuple2 < String ,   Integer > > >  descriptor  = \n                 new   MapStateDescriptor < > ( "info" ,   Types . VOID ,   Types . MAP ( Types . STRING ,   Types . TUPLE ( Types . STRING ,   Types . INT ) ) ) ; \n\n         //-2. \n         BroadcastStream < Map < String ,   Tuple2 < String ,   Integer > > >  broadcastDS  =  userDS . broadcast ( descriptor ) ; \n\n         //-3. \n         BroadcastConnectedStream < Tuple4 < String ,   String ,   String ,   Integer > ,   Map < String ,   Tuple2 < String ,   Integer > > >  connectDS  =  eventDS . connect ( broadcastDS ) ; \n\n         //-4.- \n         //BroadcastProcessFunction<IN1, IN2, OUT> \n         SingleOutputStreamOperator < Tuple6 < String ,   String ,   String ,   Integer ,   String ,   Integer > >  result  = \n                connectDS . process ( new   BroadcastProcessFunction < \n                         //<userID, eventTime, eventType, productID> // \n                         Tuple4 < String ,   String ,   String ,   Integer > , \n                         //<id,<,>> // \n                         Map < String ,   Tuple2 < String ,   Integer > > , \n                         //<ideventTimeeventTypeproductID> //  \n                         Tuple6 < String ,   String ,   String ,   Integer ,   String ,   Integer > \n                         > ( )   { \n                     // \n                     @Override \n                     public   void   processElement ( Tuple4 < String ,   String ,   String ,   Integer >  value ,   ReadOnlyContext  ctx ,   Collector < Tuple6 < String ,   String ,   String ,   Integer ,   String ,   Integer > >  out )   throws   Exception   { \n                         //value \n                         //<userID, eventTime, eventType, productID> //-- \n                         //Tuple4<String, String, String, Integer>, \n                         //value, \n                         //<id,<,>> //-- \n                         //Map<String, Tuple2<String, Integer>> \n                         //<ideventTimeeventTypeproductID> //  \n                         // Tuple6<String, String, String, Integer, String, Integer> \n\n                         // \n                         ReadOnlyBroadcastState < Void ,   Map < String ,   Tuple2 < String ,   Integer > > >  broadcastState  =  ctx . getBroadcastState ( descriptor ) ; \n                         //id,<,> \n                         Map < String ,   Tuple2 < String ,   Integer > >  map  =  broadcastState . get ( null ) ; // \n                         if   ( map  !=   null )   { \n                             //valueidmap \n                             String  userId  =  value . f0 ; \n                             Tuple2 < String ,   Integer >  tuple2  =  map . get ( userId ) ; \n                             String  username  =  tuple2 . f0 ; \n                             Integer  age  =  tuple2 . f1 ; \n\n                             // \n                            out . collect ( Tuple6 . of ( userId ,  value . f1 ,  value . f2 ,  value . f3 ,  username ,  age ) ) ; \n                         } \n                     } \n\n                     // \n                     @Override \n                     public   void   processBroadcastElement ( Map < String ,   Tuple2 < String ,   Integer > >  value ,   Context  ctx ,   Collector < Tuple6 < String ,   String ,   String ,   Integer ,   String ,   Integer > >  out )   throws   Exception   { \n                         //valueMySQL5! \n                         //state \n                         BroadcastState < Void ,   Map < String ,   Tuple2 < String ,   Integer > > >  broadcastState  =  ctx . getBroadcastState ( descriptor ) ; \n                        broadcastState . clear ( ) ; // \n                        broadcastState . put ( null ,  value ) ; // \n                     } \n                 } ) ; \n\n         //TODO 4.sink \n        result . print ( ) ; \n\n         //TODO 5.execute \n        env . execute ( ) ; \n     } \n\n\n     /**\n     * --\n     * id,,,id\n     * <userID, eventTime, eventType, productID>\n     */ \n     public   static   class   MySource   implements   SourceFunction < Tuple4 < String ,   String ,   String ,   Integer > >   { \n         private   boolean  isRunning  =   true ; \n\n         @Override \n         public   void   run ( SourceContext < Tuple4 < String ,   String ,   String ,   Integer > >  ctx )   throws   Exception   { \n             Random  random  =   new   Random ( ) ; \n             SimpleDateFormat  df  =   new   SimpleDateFormat ( "yyyy-MM-dd HH:mm:ss" ) ; \n             while   ( isRunning )   { \n                 int  id  =  random . nextInt ( 4 )   +   1 ; \n                 String  user_id  =   "user_"   +  id ; \n                 String  eventTime  =  df . format ( new   Date ( ) ) ; \n                 String  eventType  =   "type_"   +  random . nextInt ( 3 ) ; \n                 int  productId  =  random . nextInt ( 4 ) ; \n                ctx . collect ( Tuple4 . of ( user_id ,  eventTime ,  eventType ,  productId ) ) ; \n                 Thread . sleep ( 500 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            isRunning  =   false ; \n         } \n     } \n\n     /**\n     * //--\n     * <id,<,>>\n     */ \n     /*\nCREATE TABLE `user_info` (\n  `userID` varchar(20) NOT NULL,\n  `userName` varchar(10) DEFAULT NULL,\n  `userAge` int(11) DEFAULT NULL,\n  PRIMARY KEY (`userID`) USING BTREE\n) ENGINE=MyISAM DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;\n\nINSERT INTO `user_info` VALUES (\'user_1\', \'\', 10);\nINSERT INTO `user_info` VALUES (\'user_2\', \'\', 20);\nINSERT INTO `user_info` VALUES (\'user_3\', \'\', 30);\nINSERT INTO `user_info` VALUES (\'user_4\', \'\', 40);\n     */ \n     public   static   class   MySQLSource   extends   RichSourceFunction < Map < String ,   Tuple2 < String ,   Integer > > >   { \n         private   boolean  flag  =   true ; \n         private   Connection  conn  =   null ; \n         private   PreparedStatement  ps  =   null ; \n         private   ResultSet  rs  =   null ; \n\n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            conn  =   DriverManager . getConnection ( "jdbc:mysql://localhost:3306/bigdata" ,   "root" ,   "root" ) ; \n             String  sql  =   "select `userID`, `userName`, `userAge` from `user_info`" ; \n            ps  =  conn . prepareStatement ( sql ) ; \n         } \n\n         @Override \n         public   void   run ( SourceContext < Map < String ,   Tuple2 < String ,   Integer > > >  ctx )   throws   Exception   { \n             while   ( flag )   { \n                 Map < String ,   Tuple2 < String ,   Integer > >  map  =   new   HashMap < > ( ) ; \n                 ResultSet  rs  =  ps . executeQuery ( ) ; \n                 while   ( rs . next ( ) )   { \n                     String  userID  =  rs . getString ( "userID" ) ; \n                     String  userName  =  rs . getString ( "userName" ) ; \n                     int  userAge  =  rs . getInt ( "userAge" ) ; \n                     //Map<String, Tuple2<String, Integer>> \n                    map . put ( userID ,   Tuple2 . of ( userName ,  userAge ) ) ; \n                 } \n                ctx . collect ( map ) ; \n                 Thread . sleep ( 5000 ) ; //5s! \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n\n         @Override \n         public   void   close ( )   throws   Exception   { \n             if   ( conn  !=   null )  conn . close ( ) ; \n             if   ( ps  !=   null )  ps . close ( ) ; \n             if   ( rs  !=   null )  rs . close ( ) ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211  \n 1.broadcast+ \n reloadreloadtaskio \n 2.joininterval join(ioGuava Cache) \n 3.flink sql temporary joinstate \n idletrigger \n Flink-Join \n join \n \n join() "Window join"// inner join \n coGroup() \n inner join  left/right outer join  coGroup()  \n  join()  CoGroupFunction  JoinFunction / \n dataStream . coGroup ( otherStream ) \n     . where ( 0 ) . equalTo ( 1 ) \n     . window ( TumblingEventTimeWindows . of ( Time . seconds ( 3 ) ) ) \n     . apply  ( new   CoGroupFunction   ( )   { . . . } ) ; \n \n 1 2 3 4 \n \n \n join()  coGroup()  join  \n  Flink "Interval join"right.timestamp  [left.timestamp + lowerBound; left.timestamp + upperBound] \n interval join  inner join  assignTimestampsAndWatermarks()  \n //between \n if   ( timeBehaviour  !=   TimeBehaviour . EventTime )   { \n\t\t\t\t throw   new   UnsupportedTimeCharacteristicException ( "Time-bounded stream joins are only supported in event time" ) ; \n\t\t\t } \n \n 1 2 3 4 window joininterval joininterval state \n joinstatejoinwindow state  stateinterval keystatejoinjoin \n \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/joining.html \n -WindowJoin \n \n package   cn . itcast . feature ; \n\n import   com . alibaba . fastjson . JSON ; \n import   lombok . Data ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . eventtime . * ; \n import   org . apache . flink . api . common . functions . JoinFunction ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichSourceFunction ; \n import   org . apache . flink . streaming . api . windowing . assigners . TumblingEventTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n\n import   java . math . BigDecimal ; \n import   java . util . ArrayList ; \n import   java . util . List ; \n import   java . util . Random ; \n import   java . util . UUID ; \n import   java . util . concurrent . TimeUnit ; \n\n /**\n * Author itcast\n * Desc FlinkJoin-windowJoin\n */ \n public   class   JoinDemo01_WindowJoin   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         // \n         DataStreamSource < Goods >  goodsDS  =  env . addSource ( new   GoodsSource ( ) ) ; \n         // \n         DataStreamSource < OrderItem >   OrderItemDS   =  env . addSource ( new   OrderItemSource ( ) ) ; \n         //() \n         /*\n         SingleOutputStreamOperator<Order> orderDSWithWatermark = orderDS.assignTimestampsAndWatermarks(\n                WatermarkStrategy.<Order>forBoundedOutOfOrderness(Duration.ofSeconds(3))//maxOutOfOrderness//\n                        .withTimestampAssigner((order, timestamp) -> order.getEventTime())//\n        );\n         */ \n         SingleOutputStreamOperator < Goods >  goodsDSWithWatermark  =  goodsDS . assignTimestampsAndWatermarks ( new   GoodsWatermark ( ) ) ; \n         SingleOutputStreamOperator < OrderItem >   OrderItemDSWithWatermark   =   OrderItemDS . assignTimestampsAndWatermarks ( new   OrderItemWatermark ( ) ) ; \n\n\n         //TODO 2.transformation--- \n         //(id,,) \n         //(id,id,) \n         //(id,,,*) \n         DataStream < FactOrderItem >  resultDS  =  goodsDSWithWatermark . join ( OrderItemDSWithWatermark ) \n                 . where ( Goods :: getGoodsId ) \n                 . equalTo ( OrderItem :: getGoodsId ) \n                 . window ( TumblingEventTimeWindows . of ( Time . seconds ( 5 ) ) ) \n                 //<IN1, IN2, OUT> \n                 . apply ( new   JoinFunction < Goods ,   OrderItem ,   FactOrderItem > ( )   { \n                     @Override \n                     public   FactOrderItem   join ( Goods  first ,   OrderItem  second )   throws   Exception   { \n                         FactOrderItem  result  =   new   FactOrderItem ( ) ; \n                        result . setGoodsId ( first . getGoodsId ( ) ) ; \n                        result . setGoodsName ( first . getGoodsName ( ) ) ; \n                        result . setCount ( new   BigDecimal ( second . getCount ( ) ) ) ; \n                        result . setTotalMoney ( new   BigDecimal ( second . getCount ( ) ) . multiply ( first . getGoodsPrice ( ) ) ) ; \n                         return  result ; \n                     } \n                 } ) ; \n\n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     //(id,,) \n     @Data \n     public   static   class   Goods   { \n         private   String  goodsId ; \n         private   String  goodsName ; \n         private   BigDecimal  goodsPrice ; \n         public   static   List < Goods >   GOODS_LIST ; \n         public   static   Random  r ; \n\n         static    { \n            r  =   new   Random ( ) ; \n             GOODS_LIST   =   new   ArrayList < > ( ) ; \n             GOODS_LIST . add ( new   Goods ( "1" ,   "12" ,   new   BigDecimal ( 4890 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "2" ,   "iphone12" ,   new   BigDecimal ( 12000 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "3" ,   "MacBookPro" ,   new   BigDecimal ( 15000 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "4" ,   "Thinkpad X1" ,   new   BigDecimal ( 9800 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "5" ,   "MeiZu One" ,   new   BigDecimal ( 3200 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "6" ,   "Mate 40" ,   new   BigDecimal ( 6500 ) ) ) ; \n         } \n         public   static   Goods   randomGoods ( )   { \n             int  rIndex  =  r . nextInt ( GOODS_LIST . size ( ) ) ; \n             return   GOODS_LIST . get ( rIndex ) ; \n         } \n         public   Goods ( )   { \n         } \n         public   Goods ( String  goodsId ,   String  goodsName ,   BigDecimal  goodsPrice )   { \n             this . goodsId  =  goodsId ; \n             this . goodsName  =  goodsName ; \n             this . goodsPrice  =  goodsPrice ; \n         } \n         @Override \n         public   String   toString ( )   { \n             return   JSON . toJSONString ( this ) ; \n         } \n     } \n\n     //(id,id,) \n     @Data \n     public   static   class   OrderItem   { \n         private   String  itemId ; \n         private   String  goodsId ; \n         private   Integer  count ; \n         @Override \n         public   String   toString ( )   { \n             return   JSON . toJSONString ( this ) ; \n         } \n     } \n\n     //(id,,) \n     //(id,id,) \n     //(id,,,*) \n     @Data \n     public   static   class   FactOrderItem   { \n         private   String  goodsId ; \n         private   String  goodsName ; \n         private   BigDecimal  count ; \n         private   BigDecimal  totalMoney ; \n         @Override \n         public   String   toString ( )   { \n             return   JSON . toJSONString ( this ) ; \n         } \n     } \n\n     // \n     //Stream \n     public   static   class   GoodsSource   extends   RichSourceFunction < Goods >   { \n         private   Boolean  isCancel ; \n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            isCancel  =   false ; \n         } \n         @Override \n         public   void   run ( SourceContext  sourceContext )   throws   Exception   { \n             while ( ! isCancel )   { \n                 Goods . GOODS_LIST . stream ( ) . forEach ( goods  ->  sourceContext . collect ( goods ) ) ; \n                 TimeUnit . SECONDS . sleep ( 1 ) ; \n             } \n         } \n         @Override \n         public   void   cancel ( )   { \n            isCancel  =   true ; \n         } \n     } \n     // \n     //Stream \n     public   static   class   OrderItemSource   extends   RichSourceFunction < OrderItem >   { \n         private   Boolean  isCancel ; \n         private   Random  r ; \n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            isCancel  =   false ; \n            r  =   new   Random ( ) ; \n         } \n         @Override \n         public   void   run ( SourceContext  sourceContext )   throws   Exception   { \n             while ( ! isCancel )   { \n                 Goods  goods  =   Goods . randomGoods ( ) ; \n                 OrderItem  orderItem  =   new   OrderItem ( ) ; \n                orderItem . setGoodsId ( goods . getGoodsId ( ) ) ; \n                orderItem . setCount ( r . nextInt ( 10 )   +   1 ) ; \n                orderItem . setItemId ( UUID . randomUUID ( ) . toString ( ) ) ; \n                sourceContext . collect ( orderItem ) ; \n                orderItem . setGoodsId ( "111" ) ; \n                sourceContext . collect ( orderItem ) ; \n                 TimeUnit . SECONDS . sleep ( 1 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            isCancel  =   true ; \n         } \n     } \n     // \n     public   static   class   GoodsWatermark   implements   WatermarkStrategy < Goods >   { \n         @Override \n         public   TimestampAssigner < Goods >   createTimestampAssigner ( TimestampAssignerSupplier . Context  context )   { \n             return   ( element ,  recordTimestamp )   ->   System . currentTimeMillis ( ) ; \n         } \n         @Override \n         public   WatermarkGenerator < Goods >   createWatermarkGenerator ( WatermarkGeneratorSupplier . Context  context )   { \n             return   new   WatermarkGenerator < Goods > ( )   { \n                 @Override \n                 public   void   onEvent ( Goods  event ,   long  eventTimestamp ,   WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n\n                 @Override \n                 public   void   onPeriodicEmit ( WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n             } ; \n         } \n     } \n     // \n     public   static   class   OrderItemWatermark   implements   WatermarkStrategy < OrderItem >   { \n         @Override \n         public   TimestampAssigner < OrderItem >   createTimestampAssigner ( TimestampAssignerSupplier . Context  context )   { \n             return   ( element ,  recordTimestamp )   ->   System . currentTimeMillis ( ) ; \n         } \n         @Override \n         public   WatermarkGenerator < OrderItem >   createWatermarkGenerator ( WatermarkGeneratorSupplier . Context  context )   { \n             return   new   WatermarkGenerator < OrderItem > ( )   { \n                 @Override \n                 public   void   onEvent ( OrderItem  event ,   long  eventTimestamp ,   WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n                 @Override \n                 public   void   onPeriodicEmit ( WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n             } ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 #  -IntervalJoin \n \n package   cn . itcast . feature ; \n\n import   com . alibaba . fastjson . JSON ; \n import   lombok . Data ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . eventtime . * ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . co . ProcessJoinFunction ; \n import   org . apache . flink . streaming . api . functions . source . RichSourceFunction ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n import   org . apache . flink . util . Collector ; \n\n import   java . math . BigDecimal ; \n import   java . util . ArrayList ; \n import   java . util . List ; \n import   java . util . Random ; \n import   java . util . UUID ; \n import   java . util . concurrent . TimeUnit ; \n\n /**\n * Author itcast\n * Desc FlinkJoin-IntervalJoin\n */ \n public   class   JoinDemo02_IntervalJoin   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         // \n         DataStreamSource < Goods >  goodsDS  =  env . addSource ( new   GoodsSource ( ) ) ; \n         // \n         DataStreamSource < OrderItem >   OrderItemDS   =  env . addSource ( new   OrderItemSource ( ) ) ; \n         //() \n         /*\n         SingleOutputStreamOperator<Order> orderDSWithWatermark = orderDS.assignTimestampsAndWatermarks(\n                WatermarkStrategy.<Order>forBoundedOutOfOrderness(Duration.ofSeconds(3))//maxOutOfOrderness//\n                        .withTimestampAssigner((order, timestamp) -> order.getEventTime())//\n        );\n         */ \n         SingleOutputStreamOperator < Goods >  goodsDSWithWatermark  =  goodsDS . assignTimestampsAndWatermarks ( new   GoodsWatermark ( ) ) ; \n         SingleOutputStreamOperator < OrderItem >   OrderItemDSWithWatermark   =   OrderItemDS . assignTimestampsAndWatermarks ( new   OrderItemWatermark ( ) ) ; \n\n\n         //TODO 2.transformation--- \n         //(id,,) \n         //(id,id,) \n         //(id,,,*) \n         SingleOutputStreamOperator < FactOrderItem >  resultDS  =  goodsDSWithWatermark . keyBy ( Goods :: getGoodsId ) \n                 . intervalJoin ( OrderItemDSWithWatermark . keyBy ( OrderItem :: getGoodsId ) ) \n                 //join: \n                 // 1.id \n                 // 2. OrderItem - 2 <=Goods <= OrderItem + 1 \n                 . between ( Time . seconds ( - 2 ) ,   Time . seconds ( 1 ) ) \n                 //ProcessJoinFunction<IN1, IN2, OUT> \n                 . process ( new   ProcessJoinFunction < Goods ,   OrderItem ,   FactOrderItem > ( )   { \n                     @Override \n                     public   void   processElement ( Goods  left ,   OrderItem  right ,   Context  ctx ,   Collector < FactOrderItem >  out )   throws   Exception   { \n                         FactOrderItem  result  =   new   FactOrderItem ( ) ; \n                        result . setGoodsId ( left . getGoodsId ( ) ) ; \n                        result . setGoodsName ( left . getGoodsName ( ) ) ; \n                        result . setCount ( new   BigDecimal ( right . getCount ( ) ) ) ; \n                        result . setTotalMoney ( new   BigDecimal ( right . getCount ( ) ) . multiply ( left . getGoodsPrice ( ) ) ) ; \n                        out . collect ( result ) ; \n                     } \n                 } ) ; \n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     //(id,,) \n     @Data \n     public   static   class   Goods   { \n         private   String  goodsId ; \n         private   String  goodsName ; \n         private   BigDecimal  goodsPrice ; \n         public   static   List < Goods >   GOODS_LIST ; \n         public   static   Random  r ; \n\n         static    { \n            r  =   new   Random ( ) ; \n             GOODS_LIST   =   new   ArrayList < > ( ) ; \n             GOODS_LIST . add ( new   Goods ( "1" ,   "12" ,   new   BigDecimal ( 4890 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "2" ,   "iphone12" ,   new   BigDecimal ( 12000 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "3" ,   "MacBookPro" ,   new   BigDecimal ( 15000 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "4" ,   "Thinkpad X1" ,   new   BigDecimal ( 9800 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "5" ,   "MeiZu One" ,   new   BigDecimal ( 3200 ) ) ) ; \n             GOODS_LIST . add ( new   Goods ( "6" ,   "Mate 40" ,   new   BigDecimal ( 6500 ) ) ) ; \n         } \n         public   static   Goods   randomGoods ( )   { \n             int  rIndex  =  r . nextInt ( GOODS_LIST . size ( ) ) ; \n             return   GOODS_LIST . get ( rIndex ) ; \n         } \n         public   Goods ( )   { \n         } \n         public   Goods ( String  goodsId ,   String  goodsName ,   BigDecimal  goodsPrice )   { \n             this . goodsId  =  goodsId ; \n             this . goodsName  =  goodsName ; \n             this . goodsPrice  =  goodsPrice ; \n         } \n         @Override \n         public   String   toString ( )   { \n             return   JSON . toJSONString ( this ) ; \n         } \n     } \n\n     //(id,id,) \n     @Data \n     public   static   class   OrderItem   { \n         private   String  itemId ; \n         private   String  goodsId ; \n         private   Integer  count ; \n         @Override \n         public   String   toString ( )   { \n             return   JSON . toJSONString ( this ) ; \n         } \n     } \n\n     //(id,,) \n     //(id,id,) \n     //(id,,,*) \n     @Data \n     public   static   class   FactOrderItem   { \n         private   String  goodsId ; \n         private   String  goodsName ; \n         private   BigDecimal  count ; \n         private   BigDecimal  totalMoney ; \n         @Override \n         public   String   toString ( )   { \n             return   JSON . toJSONString ( this ) ; \n         } \n     } \n\n     // \n     //Stream \n     public   static   class   GoodsSource   extends   RichSourceFunction < Goods >   { \n         private   Boolean  isCancel ; \n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            isCancel  =   false ; \n         } \n         @Override \n         public   void   run ( SourceContext  sourceContext )   throws   Exception   { \n             while ( ! isCancel )   { \n                 Goods . GOODS_LIST . stream ( ) . forEach ( goods  ->  sourceContext . collect ( goods ) ) ; \n                 TimeUnit . SECONDS . sleep ( 1 ) ; \n             } \n         } \n         @Override \n         public   void   cancel ( )   { \n            isCancel  =   true ; \n         } \n     } \n     // \n     //Stream \n     public   static   class   OrderItemSource   extends   RichSourceFunction < OrderItem >   { \n         private   Boolean  isCancel ; \n         private   Random  r ; \n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n            isCancel  =   false ; \n            r  =   new   Random ( ) ; \n         } \n         @Override \n         public   void   run ( SourceContext  sourceContext )   throws   Exception   { \n             while ( ! isCancel )   { \n                 Goods  goods  =   Goods . randomGoods ( ) ; \n                 OrderItem  orderItem  =   new   OrderItem ( ) ; \n                orderItem . setGoodsId ( goods . getGoodsId ( ) ) ; \n                orderItem . setCount ( r . nextInt ( 10 )   +   1 ) ; \n                orderItem . setItemId ( UUID . randomUUID ( ) . toString ( ) ) ; \n                sourceContext . collect ( orderItem ) ; \n                orderItem . setGoodsId ( "111" ) ; \n                sourceContext . collect ( orderItem ) ; \n                 TimeUnit . SECONDS . sleep ( 1 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            isCancel  =   true ; \n         } \n     } \n     // \n     public   static   class   GoodsWatermark   implements   WatermarkStrategy < Goods >   { \n         @Override \n         public   TimestampAssigner < Goods >   createTimestampAssigner ( TimestampAssignerSupplier . Context  context )   { \n             return   ( element ,  recordTimestamp )   ->   System . currentTimeMillis ( ) ; \n         } \n         @Override \n         public   WatermarkGenerator < Goods >   createWatermarkGenerator ( WatermarkGeneratorSupplier . Context  context )   { \n             return   new   WatermarkGenerator < Goods > ( )   { \n                 @Override \n                 public   void   onEvent ( Goods  event ,   long  eventTimestamp ,   WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n\n                 @Override \n                 public   void   onPeriodicEmit ( WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n             } ; \n         } \n     } \n     // \n     public   static   class   OrderItemWatermark   implements   WatermarkStrategy < OrderItem >   { \n         @Override \n         public   TimestampAssigner < OrderItem >   createTimestampAssigner ( TimestampAssignerSupplier . Context  context )   { \n             return   ( element ,  recordTimestamp )   ->   System . currentTimeMillis ( ) ; \n         } \n         @Override \n         public   WatermarkGenerator < OrderItem >   createWatermarkGenerator ( WatermarkGeneratorSupplier . Context  context )   { \n             return   new   WatermarkGenerator < OrderItem > ( )   { \n                 @Override \n                 public   void   onEvent ( OrderItem  event ,   long  eventTimestamp ,   WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n                 @Override \n                 public   void   onPeriodicEmit ( WatermarkOutput  output )   { \n                    output . emitWatermark ( new   Watermark ( System . currentTimeMillis ( ) ) ) ; \n                 } \n             } ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 #  Flink-End-to-End Exactly-Once \n  \n  \n \n \n \n : \n Exactly-Once  : \n ! \n ,,,! \n \n \n End-To-End Exactly-Once \n Source  Transformation  Sink Exactly-Once ! \n Exactly-Once \n : \n 1. \n \n 2. \n INSERT   INTO  t_student  ( id , `name` , age ) VALUES ( 9 , \'Gordon\' , 18 ) \n >   1062   -   Duplicate  entry  \'9\'   for  key \' PRIMARY \'\n >   :   0.001 s\n \n 1 2 3 \n 3./Checkpoint---Flink \n End-To-End Exactly-Once \n \n Source: Kafkaoffset replay// \n Transformation: Checkpoint \n Sink: Checkpoint +  \n \n  \n \n \n \n \n \n \n SourceOperaterKafka/offset \n TransformationOperaterCheckpoint \n SinkOperatorKafka \n \n :sink: \n 1. \n 2.OperatorbarrierCheckpoint,  \n 3.Operator \n 4.Checkpoint \n  \n kafkaflink-kafka1 ---\x3e \n Flink Source --\x3e \n Flink-TransformationWordCount--\x3e \n kafka-flink-kafka2 \n //1.  \n / export / server / kafka / bin / kafka - topics . sh  -- zookeeper node1 : 2181   -- create  -- replication - factor  2   -- partitions  3   -- topic flink_kafka1\n / export / server / kafka / bin / kafka - topics . sh  -- zookeeper node1 : 2181   -- create  -- replication - factor  2   -- partitions  3   -- topic flink_kafka2\n //2.  \n / export / server / kafka / bin / kafka - console - producer . sh  -- broker - list node1 : 9092   -- topic flink_kafka1\n //3.  \n / export / server / kafka / bin / kafka - console - consumer . sh  -- bootstrap - server node1 : 9092   -- topic flink_kafka2\n \n 1 2 3 4 5 6 7 package   cn . itcast . feature ; \n\n import   org . apache . commons . lang3 . SystemUtils ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . common . restartstrategy . RestartStrategies ; \n import   org . apache . flink . api . common . serialization . SimpleStringSchema ; \n import   org . apache . flink . api . common . time . Time ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . runtime . state . filesystem . FsStateBackend ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . CheckpointConfig ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaConsumer ; \n import   org . apache . flink . streaming . connectors . kafka . FlinkKafkaProducer ; \n import   org . apache . flink . streaming . connectors . kafka . internals . KeyedSerializationSchemaWrapper ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . Properties ; \n import   java . util . Random ; \n import   java . util . concurrent . TimeUnit ; \n\n /**\n * Author itcast\n * Desc FlinkEndToEnd_Exactly_Once\n * :\n * kafkaflink-kafka1 ---\x3eFlink Source --\x3eFlink-TransformationWordCount--\x3ekafka-flink-kafka2\n */ \n public   class   Flink_Kafka_EndToEnd_Exactly_Once   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //Checkpoint \n         //===========1:============= \n         //Checkpoint1000msCheckpoint/1000msBarrier! \n        env . enableCheckpointing ( 1000 ) ; \n         if   ( SystemUtils . IS_OS_WINDOWS )   { \n            env . setStateBackend ( new   FsStateBackend ( "file:///D:/ckp" ) ) ; \n         }   else   { \n            env . setStateBackend ( new   FsStateBackend ( "hdfs://node1:8020/flink-checkpoint/checkpoint" ) ) ; \n         } \n         //===========2:=========== \n         //Checkpoint ,Checkpoint 500ms(1000msCheckpoint,) \n         //:,1s,500m \n        env . getCheckpointConfig ( ) . setMinPauseBetweenCheckpoints ( 500 ) ; //0 \n         //Checkpointtrue  false \n         //env.getCheckpointConfig().setFailOnCheckpointingErrors(false);//true \n        env . getCheckpointConfig ( ) . setTolerableCheckpointFailureNumber ( 10 ) ; //0 \n         //, Cancel  Checkpoint CheckpointCancel \n         //ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATIONtrue,checkpoint() \n         //ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATIONfalse,checkpoint \n        env . getCheckpointConfig ( ) . enableExternalizedCheckpoints ( CheckpointConfig . ExternalizedCheckpointCleanup . RETAIN_ON_CANCELLATION ) ; \n\n         //===========3:=============== \n         //checkpointEXACTLY_ONCE() \n        env . getCheckpointConfig ( ) . setCheckpointingMode ( CheckpointingMode . EXACTLY_ONCE ) ; \n         //checkpoint, Checkpoint 60sCheckpoint, \n        env . getCheckpointConfig ( ) . setCheckpointTimeout ( 60000 ) ; //10 \n         //checkpoint \n        env . getCheckpointConfig ( ) . setMaxConcurrentCheckpoints ( 1 ) ; //1 \n\n         //TODO ===: \n         //1.Checkpoint:,,bug \n         //2. \n         //env.setRestartStrategy(RestartStrategies.noRestart()); \n         //3.-- \n        env . setRestartStrategy ( RestartStrategies . fixedDelayRestart ( \n                 3 ,   // 3 \n                 Time . of ( 5 ,   TimeUnit . SECONDS )   //  \n         ) ) ; \n         //:job,3, 5s \n         //4.-- \n         /*env.setRestartStrategy(RestartStrategies.failureRateRestart(\n                3, // \n                Time.of(1, TimeUnit.MINUTES), //\n                Time.of(3, TimeUnit.SECONDS) // \n        ));*/ \n         //:1job,,3s (13,) \n\n         //TODO 1.source-:flink-kafka1 \n         //kafka \n         Properties  props1  =   new   Properties ( ) ; \n        props1 . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; // \n        props1 . setProperty ( "group.id" ,   "flink" ) ; //id \n        props1 . setProperty ( "auto.offset.reset" ,   "latest" ) ; //latestoffset,/ /earliestoffset,/ \n        props1 . setProperty ( "flink.partition-discovery.interval-millis" ,   "5000" ) ; //5sKafka, \n         //props1.setProperty("enable.auto.commit", "true");//(,CheckpointCheckpointCheckpoint) \n         //props1.setProperty("auto.commit.interval.ms", "2000");// \n         //FlinkKafkaConsumer/kafkaSource \n         //FlinkKafkaConsumeroffsetCheckpoint! \n         FlinkKafkaConsumer < String >  kafkaSource  =   new   FlinkKafkaConsumer < String > ( "flink_kafka1" ,   new   SimpleStringSchema ( ) ,  props1 ) ; \n        kafkaSource . setCommitOffsetsOnCheckpoints ( true ) ; //true//CheckpointoffsetCheckpoint()() \n\n         //kafkaSource \n         DataStream < String >  kafkaDS  =  env . addSource ( kafkaSource ) ; \n\n         //TODO 2.transformation-WordCount \n         SingleOutputStreamOperator < String >  result  =  kafkaDS . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n            private   Random  ran  =   new   Random ( ) ; \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                     int  num  =  ran . nextInt ( 5 ) ; \n                     if ( num  >   3 ) { \n                         System . out . println ( "" ) ; \n                         throw   new   Exception ( "" ) ; \n                     } \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) . keyBy ( t  ->  t . f0 ) \n           . sum ( 1 ) \n           . map ( new   MapFunction < Tuple2 < String ,   Integer > ,   String > ( )   { \n                     @Override \n                     public   String   map ( Tuple2 < String ,   Integer >  value )   throws   Exception   { \n                         return  value . f0  +   ":"   +  value . f1 ; \n                     } \n            } ) ; \n\n         //TODO 3.sink-:flink-kafka2 \n         Properties  props2  =   new   Properties ( ) ; \n        props2 . setProperty ( "bootstrap.servers" ,   "node1:9092" ) ; \n        props2 . setProperty ( "transaction.timeout.ms" ,   "5000" ) ; \n\n         FlinkKafkaProducer < String >  kafkaSink  =   new   FlinkKafkaProducer < > ( \n                 "flink_kafka2" ,                    // target topic \n                 new   KeyedSerializationSchemaWrapper ( new   SimpleStringSchema ( ) ) ,      // serialization schema \n                props2 ,                    // producer config \n                 FlinkKafkaProducer . Semantic . EXACTLY_ONCE ) ;   // fault-tolerance \n\n        result . addSink ( kafkaSink ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n\n\n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 #  Flink-IO- \n  \n \n API \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/asyncio.html \n \n : IO, Client: \n 1.Client,vertx \n 2. Client \n  \n DROP TABLE IF EXISTS `t_category`;\nCREATE TABLE `t_category` (\n  `id` int(11) NOT NULL,\n  `name` varchar(255) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n-- ----------------------------\n-- Records of t_category\n-- ----------------------------\nINSERT INTO `t_category` VALUES (\'1\', \'\');\nINSERT INTO `t_category` VALUES (\'2\', \'\');\nINSERT INTO `t_category` VALUES (\'3\', \'\');\nINSERT INTO `t_category` VALUES (\'4\', \'\');\nINSERT INTO `t_category` VALUES (\'5\', \'\');\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package   cn . itcast . feature ; \n\n import   io . vertx . core . AsyncResult ; \n import   io . vertx . core . Handler ; \n import   io . vertx . core . Vertx ; \n import   io . vertx . core . VertxOptions ; \n import   io . vertx . core . json . JsonObject ; \n import   io . vertx . ext . jdbc . JDBCClient ; \n import   io . vertx . ext . sql . SQLClient ; \n import   io . vertx . ext . sql . SQLConnection ; \n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . AsyncDataStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . async . ResultFuture ; \n import   org . apache . flink . streaming . api . functions . async . RichAsyncFunction ; \n import   org . apache . flink . streaming . api . functions . source . RichSourceFunction ; \n\n import   java . sql . * ; \n import   java . util . Collections ; \n import   java . util . List ; \n import   java . util . concurrent . ExecutorService ; \n import   java . util . concurrent . LinkedBlockingQueue ; \n import   java . util . concurrent . ThreadPoolExecutor ; \n import   java . util . concurrent . TimeUnit ; \n\n /**\n * io\n * 1.(key/value)client\n * 2.\n */ \n public   class   ASyncIODemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         //2.Source \n         //id \n         //DataStreamSource[1,2,3,4,5] \n         DataStreamSource < CategoryInfo >  categoryDS  =  env . addSource ( new   RichSourceFunction < CategoryInfo > ( )   { \n             private   Boolean  flag  =   true ; \n             @Override \n             public   void   run ( SourceContext < CategoryInfo >  ctx )   throws   Exception   { \n                 Integer [ ]  ids  =   { 1 ,   2 ,   3 ,   4 ,   5 } ; \n                 for   ( Integer  id  :  ids )   { \n                    ctx . collect ( new   CategoryInfo ( id ,   null ) ) ; \n                 } \n             } \n             @Override \n             public   void   cancel ( )   { \n                 this . flag  =   false ; \n             } \n         } ) ; \n         //3.Transformation \n\n\n         //Java-vertxclientIO \n         //unorderedWait \n         SingleOutputStreamOperator < CategoryInfo >  result1  =   AsyncDataStream \n                 . unorderedWait ( categoryDS ,   new   ASyncIOFunction1 ( ) ,   1000 ,   TimeUnit . SECONDS ,   10 ) ; \n\n         //MySQLclient+IO \n         //unorderedWait \n         SingleOutputStreamOperator < CategoryInfo >  result2  =   AsyncDataStream \n                 . unorderedWait ( categoryDS ,   new   ASyncIOFunction2 ( ) ,   1000 ,   TimeUnit . SECONDS ,   10 ) ; \n\n         //4.Sink \n        result1 . print ( "Java-vertxclientIO \\n" ) ; \n        result2 . print ( "MySQLclient+IO \\n" ) ; \n\n         //5.execute \n        env . execute ( ) ; \n     } \n } \n\n @Data \n @NoArgsConstructor \n @AllArgsConstructor \n class   CategoryInfo   { \n     private   Integer  id ; \n     private   String  name ; \n } \n\n //MySQL-:vertx \n class   MysqlSyncClient   { \n     private   static   transient   Connection  connection ; \n     private   static   final   String   JDBC_DRIVER   =   "com.mysql.jdbc.Driver" ; \n     private   static   final   String   URL   =   "jdbc:mysql://localhost:3306/bigdata" ; \n     private   static   final   String   USER   =   "root" ; \n     private   static   final   String   PASSWORD   =   "root" ; \n\n     static   { \n         init ( ) ; \n     } \n\n     private   static   void   init ( )   { \n         try   { \n             Class . forName ( JDBC_DRIVER ) ; \n         }   catch   ( ClassNotFoundException  e )   { \n             System . out . println ( "Driver not found!"   +  e . getMessage ( ) ) ; \n         } \n         try   { \n            connection  =   DriverManager . getConnection ( URL ,   USER ,   PASSWORD ) ; \n         }   catch   ( SQLException  e )   { \n             System . out . println ( "init connection failed!"   +  e . getMessage ( ) ) ; \n         } \n     } \n\n     public   void   close ( )   { \n         try   { \n             if   ( connection  !=   null )   { \n                connection . close ( ) ; \n             } \n         }   catch   ( SQLException  e )   { \n             System . out . println ( "close connection failed!"   +  e . getMessage ( ) ) ; \n         } \n     } \n\n     public   CategoryInfo   query ( CategoryInfo  category )   { \n         try   { \n             String  sql  =   "select id,name from t_category where id = " +  category . getId ( ) ; \n             Statement  statement  =  connection . createStatement ( ) ; \n             ResultSet  rs  =  statement . executeQuery ( sql ) ; \n             if   ( rs  !=   null   &&  rs . next ( ) )   { \n                category . setName ( rs . getString ( "name" ) ) ; \n             } \n         }   catch   ( SQLException  e )   { \n             System . out . println ( "query failed!"   +  e . getMessage ( ) ) ; \n         } \n         return  category ; \n     } \n } \n\n /**\n * Java-vertxclientIO\n */ \n class   ASyncIOFunction1   extends   RichAsyncFunction < CategoryInfo ,   CategoryInfo >   { \n     private   transient   SQLClient  mySQLClient ; \n\n     @Override \n     public   void   open ( Configuration  parameters )   throws   Exception   { \n         JsonObject  mySQLClientConfig  =   new   JsonObject ( ) ; \n        mySQLClientConfig\n                 . put ( "driver_class" ,   "com.mysql.jdbc.Driver" ) \n                 . put ( "url" ,   "jdbc:mysql://localhost:3306/bigdata" ) \n                 . put ( "user" ,   "root" ) \n                 . put ( "password" ,   "root" ) \n                 . put ( "max_pool_size" ,   20 ) ; \n\n         VertxOptions  options  =   new   VertxOptions ( ) ; \n        options . setEventLoopPoolSize ( 10 ) ; \n        options . setWorkerPoolSize ( 20 ) ; \n         Vertx  vertx  =   Vertx . vertx ( options ) ; \n         // \n        mySQLClient  =   JDBCClient . createNonShared ( vertx ,  mySQLClientConfig ) ; \n     } \n\n     // \n     @Override \n     public   void   asyncInvoke ( CategoryInfo  input ,   ResultFuture < CategoryInfo >  resultFuture )   throws   Exception   { \n        mySQLClient . getConnection ( new   Handler < AsyncResult < SQLConnection > > ( )   { \n             @Override \n             public   void   handle ( AsyncResult < SQLConnection >  sqlConnectionAsyncResult )   { \n                 if   ( sqlConnectionAsyncResult . failed ( ) )   { \n                     return ; \n                 } \n                 SQLConnection  connection  =  sqlConnectionAsyncResult . result ( ) ; \n                connection . query ( "select id,name from t_category where id = "   + input . getId ( ) ,   new   Handler < AsyncResult < io . vertx . ext . sql . ResultSet > > ( )   { \n                     @Override \n                     public   void   handle ( AsyncResult < io . vertx . ext . sql . ResultSet >  resultSetAsyncResult )   { \n                         if   ( resultSetAsyncResult . succeeded ( ) )   { \n                             List < JsonObject >  rows  =  resultSetAsyncResult . result ( ) . getRows ( ) ; \n                             for   ( JsonObject  jsonObject  :  rows )   { \n                                 CategoryInfo  categoryInfo  =   new   CategoryInfo ( jsonObject . getInteger ( "id" ) ,  jsonObject . getString ( "name" ) ) ; \n                                resultFuture . complete ( Collections . singletonList ( categoryInfo ) ) ; \n                             } \n                         } \n                     } \n                 } ) ; \n             } \n         } ) ; \n     } \n     @Override \n     public   void   close ( )   throws   Exception   { \n        mySQLClient . close ( ) ; \n     } \n\n     @Override \n     public   void   timeout ( CategoryInfo  input ,   ResultFuture < CategoryInfo >  resultFuture )   throws   Exception   { \n         System . out . println ( "async call time out!" ) ; \n        input . setName ( "" ) ; \n        resultFuture . complete ( Collections . singleton ( input ) ) ; \n     } \n } \n\n /**\n * +IO\n */ \n class   ASyncIOFunction2   extends   RichAsyncFunction < CategoryInfo ,   CategoryInfo >   { \n     private   transient   MysqlSyncClient  client ; \n     private   ExecutorService  executorService ; // \n\n     @Override \n     public   void   open ( Configuration  parameters )   throws   Exception   { \n         super . open ( parameters ) ; \n        client  =   new   MysqlSyncClient ( ) ; \n        executorService  =   new   ThreadPoolExecutor ( 10 ,   10 ,   0L ,   TimeUnit . MILLISECONDS ,   new   LinkedBlockingQueue < Runnable > ( ) ) ; \n     } \n\n     // \n     @Override \n     public   void   asyncInvoke ( CategoryInfo  input ,   ResultFuture < CategoryInfo >  resultFuture )   throws   Exception   { \n        executorService . execute ( new   Runnable ( )   { \n             @Override \n             public   void   run ( )   { \n                resultFuture . complete ( Collections . singletonList ( ( CategoryInfo )  client . query ( input ) ) ) ; \n             } \n         } ) ; \n     } \n\n\n     @Override \n     public   void   close ( )   throws   Exception   { \n     } \n\n     @Override \n     public   void   timeout ( CategoryInfo  input ,   ResultFuture < CategoryInfo >  resultFuture )   throws   Exception   { \n         System . out . println ( "async call time out!" ) ; \n        input . setName ( "" ) ; \n        resultFuture . complete ( Collections . singleton ( input ) ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 #  Flink-Streaming Flie SinkFile sink \n  \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/streamfile_sink.html \n https://blog.csdn.net/u013220482/article/details/100901471 \n  \n package   cn . itcast . feature ; \n\n\n import   org . apache . commons . lang3 . SystemUtils ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . common . serialization . SimpleStringEncoder ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . core . fs . Path ; \n import   org . apache . flink . runtime . state . filesystem . FsStateBackend ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . CheckpointConfig ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . sink . filesystem . OutputFileConfig ; \n import   org . apache . flink . streaming . api . functions . sink . filesystem . StreamingFileSink ; \n import   org . apache . flink . streaming . api . functions . sink . filesystem . rollingpolicies . DefaultRollingPolicy ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . concurrent . TimeUnit ; \n\n /**\n * Author itcast\n * Desc Flink StreamingFileSinkHDFS Checkpoint + \n */ \n public   class   StreamingFileSinkDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //Checkpoint \n         //===========1:============= \n         //Checkpoint1000msCheckpoint/1000msBarrier! \n        env . enableCheckpointing ( 1000 ) ; \n         if   ( SystemUtils . IS_OS_WINDOWS )   { \n            env . setStateBackend ( new   FsStateBackend ( "file:///D:/ckp" ) ) ; \n         }   else   { \n            env . setStateBackend ( new   FsStateBackend ( "hdfs://node1:8020/flink-checkpoint/checkpoint" ) ) ; \n         } \n         //===========2:=========== \n         //Checkpoint ,Checkpoint 500ms(1000msCheckpoint,) \n         //:,1s,500m \n        env . getCheckpointConfig ( ) . setMinPauseBetweenCheckpoints ( 500 ) ; //0 \n         //Checkpointtrue  false \n         //env.getCheckpointConfig().setFailOnCheckpointingErrors(false);//true \n        env . getCheckpointConfig ( ) . setTolerableCheckpointFailureNumber ( 10 ) ; //0 \n         //, Cancel  Checkpoint CheckpointCancel \n         //ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATIONtrue,checkpoint() \n         //ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATIONfalse,checkpoint \n        env . getCheckpointConfig ( ) . enableExternalizedCheckpoints ( CheckpointConfig . ExternalizedCheckpointCleanup . RETAIN_ON_CANCELLATION ) ; \n\n         //===========3:=============== \n         //checkpointEXACTLY_ONCE() \n        env . getCheckpointConfig ( ) . setCheckpointingMode ( CheckpointingMode . EXACTLY_ONCE ) ; \n         //checkpoint, Checkpoint 60sCheckpoint, \n        env . getCheckpointConfig ( ) . setCheckpointTimeout ( 60000 ) ; //10 \n         //checkpoint \n        env . getCheckpointConfig ( ) . setMaxConcurrentCheckpoints ( 1 ) ; //1 \n\n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n\n         //TODO 2.transformation \n         //:21,1 \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  wordAndOne  =  lines . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n\n         SingleOutputStreamOperator < String >  result  =  wordAndOne . keyBy ( t  ->  t . f0 ) . sum ( 1 ) \n                 . map ( new   MapFunction < Tuple2 < String ,   Integer > ,   String > ( )   { \n                     @Override \n                     public   String   map ( Tuple2 < String ,   Integer >  value )   throws   Exception   { \n                         return  value . f0  +   ":"   +  value . f1 ; \n                     } \n                 } ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //StreamingFileSinksinkHDFS \n         OutputFileConfig  config  =   OutputFileConfig \n                 . builder ( ) \n                 . withPartPrefix ( "prefix" ) // \n                 . withPartSuffix ( ".txt" ) // \n                 . build ( ) ; \n\n         StreamingFileSink < String >  streamingFileSink  =   StreamingFileSink . \n                 forRowFormat ( new   Path ( "hdfs://node1:8020/FlinkStreamFileSink/parquet" ) ,   new   SimpleStringEncoder < String > ( "UTF-8" ) ) \n                 . withRollingPolicy ( \n                         DefaultRollingPolicy . builder ( ) \n                                 . withRolloverInterval ( TimeUnit . MINUTES . toMillis ( 15 ) ) //15 \n                                 . withInactivityInterval ( TimeUnit . MINUTES . toMillis ( 5 ) ) //5, \n                                 . withMaxPartSize ( 1024   *   1024   *   1024 ) \n                                 . build ( ) ) \n                 . withOutputFileConfig ( config ) \n                 . build ( ) ; \n\n        result . addSink ( streamingFileSink ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 #  Flink--Flie Sink \n \n package   cn . itcast . feature ; \n\n\n import   org . apache . commons . lang3 . SystemUtils ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . MapFunction ; \n import   org . apache . flink . api . common . serialization . SimpleStringEncoder ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . connector . file . sink . FileSink ; \n import   org . apache . flink . core . fs . Path ; \n import   org . apache . flink . runtime . state . filesystem . FsStateBackend ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . CheckpointConfig ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . sink . filesystem . OutputFileConfig ; \n import   org . apache . flink . streaming . api . functions . sink . filesystem . bucketassigners . DateTimeBucketAssigner ; \n import   org . apache . flink . streaming . api . functions . sink . filesystem . rollingpolicies . DefaultRollingPolicy ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . concurrent . TimeUnit ; \n\n /**\n * Author itcast\n * Desc Flink FileSink/HDFS Checkpoint + \n */ \n public   class   FileSinkDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //Checkpoint \n         //===========1:============= \n         //Checkpoint1000msCheckpoint/1000msBarrier! \n        env . enableCheckpointing ( 1000 ) ; \n         if   ( SystemUtils . IS_OS_WINDOWS )   { \n            env . setStateBackend ( new   FsStateBackend ( "file:///D:/ckp" ) ) ; \n         }   else   { \n            env . setStateBackend ( new   FsStateBackend ( "hdfs://node1:8020/flink-checkpoint/checkpoint" ) ) ; \n         } \n         //===========2:=========== \n         //Checkpoint ,Checkpoint 500ms(1000msCheckpoint,) \n         //:,1s,500m \n        env . getCheckpointConfig ( ) . setMinPauseBetweenCheckpoints ( 500 ) ; //0 \n         //Checkpointtrue  false \n         //env.getCheckpointConfig().setFailOnCheckpointingErrors(false);//true \n        env . getCheckpointConfig ( ) . setTolerableCheckpointFailureNumber ( 10 ) ; //0 \n         //, Cancel  Checkpoint CheckpointCancel \n         //ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATIONtrue,checkpoint() \n         //ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATIONfalse,checkpoint \n        env . getCheckpointConfig ( ) . enableExternalizedCheckpoints ( CheckpointConfig . ExternalizedCheckpointCleanup . RETAIN_ON_CANCELLATION ) ; \n\n         //===========3:=============== \n         //checkpointEXACTLY_ONCE() \n        env . getCheckpointConfig ( ) . setCheckpointingMode ( CheckpointingMode . EXACTLY_ONCE ) ; \n         //checkpoint, Checkpoint 60sCheckpoint, \n        env . getCheckpointConfig ( ) . setCheckpointTimeout ( 60000 ) ; //10 \n         //checkpoint \n        env . getCheckpointConfig ( ) . setMaxConcurrentCheckpoints ( 1 ) ; //1 \n\n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n\n         //TODO 2.transformation \n         //:21,1 \n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  wordAndOne  =  lines . flatMap ( new   FlatMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < Tuple2 < String ,   Integer > >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( Tuple2 . of ( word ,   1 ) ) ; \n                 } \n             } \n         } ) ; \n\n         SingleOutputStreamOperator < String >  result  =  wordAndOne . keyBy ( t  ->  t . f0 ) . sum ( 1 ) \n                 . map ( new   MapFunction < Tuple2 < String ,   Integer > ,   String > ( )   { \n                     @Override \n                     public   String   map ( Tuple2 < String ,   Integer >  value )   throws   Exception   { \n                         return  value . f0  +   ":"   +  value . f1 ; \n                     } \n                 } ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //FileSinksinkHDFS \n         OutputFileConfig  config  =   OutputFileConfig \n                 . builder ( ) \n                 . withPartPrefix ( "prefix" ) \n                 . withPartSuffix ( ".txt" ) \n                 . build ( ) ; \n\n         FileSink < String >  sink  =   FileSink \n                 . forRowFormat ( new   Path ( "hdfs://node1:8020/FlinkFileSink/parquet" ) ,   new   SimpleStringEncoder < String > ( "UTF-8" ) ) \n                 . withRollingPolicy ( \n                         DefaultRollingPolicy . builder ( ) \n                                 . withRolloverInterval ( TimeUnit . MINUTES . toMillis ( 15 ) ) \n                                 . withInactivityInterval ( TimeUnit . MINUTES . toMillis ( 5 ) ) \n                                 . withMaxPartSize ( 1024   *   1024   *   1024 ) \n                                 . build ( ) ) \n                 . withOutputFileConfig ( config ) \n                 . withBucketAssigner ( new   DateTimeBucketAssigner ( "yyyy-MM-dd--HH" ) ) \n                 . build ( ) ; \n\n        result . sinkTo ( sink ) ; \n\n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 #  Flink \n https://blog.lovedata.net/8156c1e1.html \n Metrics \n Metrics \n  \n package   cn . itcast . metrics ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . FlatMapFunction ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . metrics . Counter ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . util . Collector ; \n\n /**\n * Author itcast\n * Desc Flink-Metrics\n * MapCounter,map,WebUI\n */ \n public   class   MetricsDemo   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n\n         //TODO 1.source \n         DataStream < String >  lines  =  env . socketTextStream ( "node1" ,   9999 ) ; \n\n\n         //TODO 2.transformation \n         SingleOutputStreamOperator < String >  words  =  lines . flatMap ( new   FlatMapFunction < String ,   String > ( )   { \n             @Override \n             public   void   flatMap ( String  value ,   Collector < String >  out )   throws   Exception   { \n                 String [ ]  arr  =  value . split ( " " ) ; \n                 for   ( String  word  :  arr )   { \n                    out . collect ( word ) ; \n                 } \n             } \n         } ) ; \n\n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  wordAndOne  =  words\n                 . map ( new   RichMapFunction < String ,   Tuple2 < String ,   Integer > > ( )   { \n                     Counter  myCounter ; //map \n\n                     //Counter \n                     @Override \n                     public   void   open ( Configuration  parameters )   throws   Exception   { \n                        myCounter  =   getRuntimeContext ( ) . getMetricGroup ( ) . addGroup ( "myGroup" ) . counter ( "myCounter" ) ; \n                     } \n                     //,(,1) \n                     @Override \n                     public   Tuple2 < String ,   Integer >   map ( String  value )   throws   Exception   { \n                        myCounter . inc ( ) ; //+1 \n                         return   Tuple2 . of ( value ,   1 ) ; \n                     } \n                 } ) ; \n\n         SingleOutputStreamOperator < Tuple2 < String ,   Integer > >  result  =  wordAndOne . keyBy ( t  ->  t . f0 ) . sum ( 1 ) ; \n\n         //TODO 3.sink \n        result . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n } \n // /export/server/flink/bin/yarn-session.sh -n 2 -tm 800 -s 1 -d \n // /export/server/flink/bin/flink run --class cn.itcast.metrics.MetricsDemo /root/metrics.jar \n // WebUI \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 UI \n 1. \n 2.Yarn \n 3. \n \n 4.f12url \n \n 5. \n package   cn . itcast . metrics ; \n\n import   java . io . BufferedReader ; \n import   java . io . InputStreamReader ; \n import   java . net . URL ; \n import   java . net . URLConnection ; \n\n public   class   MetricsTest   { \n     public   static   void   main ( String [ ]  args )   { \n         //String result = sendGet("http://node1:8088/proxy/application_1609508087977_0010/jobs/558a5a3016661f1d732228330ebfaad5/vertices/cbc357ccb763df2852fee8c4fc7d55f2/metrics?get=0.Map.myGroup.myCounter"); \n         String  result  =   sendGet ( "http://node1:8088/proxy/application_1609508087977_0010/jobs/558a5a3016661f1d732228330ebfaad5" ) ; \n\n         System . out . println ( result ) ; \n     } \n\n     public   static   String   sendGet ( String  url )   { \n         String  result  =   "" ; \n         BufferedReader  in  =   null ; \n         try   { \n             String  urlNameString  =  url ; \n             URL  realUrl  =   new   URL ( urlNameString ) ; \n             URLConnection  connection  =  realUrl . openConnection ( ) ; \n             //  \n            connection . setRequestProperty ( "accept" ,   "*/*" ) ; \n            connection . setRequestProperty ( "connection" ,   "Keep-Alive" ) ; \n            connection . setRequestProperty ( "user-agent" ,   "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)" ) ; \n             //  \n            connection . connect ( ) ; \n            in  =   new   BufferedReader ( new   InputStreamReader ( connection . getInputStream ( ) ) ) ; \n             String  line ; \n             while   ( ( line  =  in . readLine ( ) )   !=   null )   { \n                result  +=  line ; \n             } \n         }   catch   ( Exception  e )   { \n             System . out . println ( "GET"   +  e ) ; \n            e . printStackTrace ( ) ; \n         } \n         // finally \n         finally   { \n             try   { \n                 if   ( in  !=   null )   { \n                    in . close ( ) ; \n                 } \n             }   catch   ( Exception  e2 )   { \n                e2 . printStackTrace ( ) ; \n             } \n         } \n         return  result ; \n     } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 6.flink \n https://blog.lovedata.net/8156c1e1.html \n Flink \n \n #-- \n/export/server/flink/bin/flink run  -m  yarn-cluster  -yjm   1024   -ytm   1024   -p   6   -ys   2  streaming.jar  -c  dictionary_utils.OneDictForExecutor\n \n 1 2 \n Flink \n  \n      \n  GC  \n  \n  \n 1. \n 2. \n 3.gc \n 4. \n 5. \n  \n  \n  \n  \n \n \n \n \n \n  \n 1.credit-base \n  \n  \n  \n --cache \n 2. \n rebalance \n  \n  \n 3. \n  \n 1. \n stream\n     . apply ( new   WindowFunction < WikipediaEditEvent ,   Tuple2 < String ,   Long > ,   String ,   TimeWindow > ( )   { \n         @Override \n         public   void   apply ( String  userName ,   TimeWindow  timeWindow ,   Iterable < WikipediaEditEvent >  iterable ,   Collector < Tuple2 < String ,   Long > >  collector )   throws   Exception   { \n             long  changesCount  =   . . . \n             // A new Tuple instance is created on every execution \n            collector . collect ( new   Tuple2 < > ( userName ,  changesCount ) ) ; \n         } \n     } \n \n 1 2 3 4 5 6 7 8 9 : \n Tuple2 \n stream\n         . apply ( new   WindowFunction < WikipediaEditEvent ,   Tuple2 < String ,   Long > ,   String ,   TimeWindow > ( )   { \n     // Create an instance that we will reuse on every call \n     private   Tuple2 < String ,   Long >  result  =   new   Tuple < > ( ) ; \n     @Override \n     public   void   apply ( String  userName ,   TimeWindow  timeWindow ,   Iterable < WikipediaEditEvent >  iterable ,   Collector < Tuple2 < String ,   Long > >  collector )   throws   Exception   { \n         long  changesCount  =   . . . \n         // Set fields on an existing object instead of creating a new one \n        result . f0  =  userName ; \n         // Auto-boxing!! A new Long value may be created \n        result . f1  =  changesCount ; \n         // Reuse the same Tuple2 object \n        collector . collect ( result ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 2. \n \n rebalance \n  \n key+ \n 3.IO \n 4. \n  \n  \n HDFS \n 5. \n 6. \n 7. \n  1.ds.writeAsText("data/output/result1").setParallelism(1);\n 2.env.setParallelism(1);\n 3.webUI  flink run  -p 10\n 4.flink-conf.yaml parallelism.default: 1\n \n 1 2 3 4 \n Flink Table&SQL \n Table&SQL \n \n  \n Table planners \n 1.planner useOldPlanner \n 2.Blink planner useBlinkPlanner() \n \n  \n \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/ \n \n \n \n  \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html \n \n \n \n  \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html \n \n \n \n \n \n  \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html \n \n \n \n \n  \n \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html \n \n \n \n \n DataStream \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html \n  \n \n \n \n / \n \n \n \n /State \n Flink SQLidle state retention time \n Flink SQL   \n  \n()keyTop-N()Flink SQLidle state retention timekey \n tbEnv . getConfig ( ) . setIdleStateRetention ( Duration . ofDays ( 1 ) ) ; \n \n 1 setIdleStateRetentionTime()minRetentionTimemaxRetentionTime()5minRetentionTimemaxRetentionTimeTimerValueStateTimer \n 1 \n DataStreamTableViewsql \n package   cn . itcast . sql ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . Table ; \n import   org . apache . flink . table . api . bridge . java . StreamTableEnvironment ; \n\n import   java . util . Arrays ; \n\n import   static   org . apache . flink . table . api . Expressions . $ ; \n\n /**\n * Author itcast\n * Desc Flink Table&SQL - DataStreamTableViewsql\n */ \n public   class   Demo01   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         EnvironmentSettings  settings  =   EnvironmentSettings . newInstance ( ) . useBlinkPlanner ( ) . inStreamingMode ( ) . build ( ) ; \n         StreamTableEnvironment  tenv  =   StreamTableEnvironment . create ( env ,  settings ) ; \n\n         //TODO 1.source \n         DataStream < Order >  orderA  =  env . fromCollection ( Arrays . asList ( \n                 new   Order ( 1L ,   "beer" ,   3 ) , \n                 new   Order ( 1L ,   "diaper" ,   4 ) , \n                 new   Order ( 3L ,   "rubber" ,   2 ) ) ) ; \n\n         DataStream < Order >  orderB  =  env . fromCollection ( Arrays . asList ( \n                 new   Order ( 2L ,   "pen" ,   3 ) , \n                 new   Order ( 2L ,   "rubber" ,   3 ) , \n                 new   Order ( 4L ,   "beer" ,   1 ) ) ) ; \n\n         //TODO 2.transformation \n         // DataStreamTableView, \n         Table  tableA  =  tenv . fromDataStream ( orderA ,  $ ( "user" ) ,  $ ( "product" ) ,  $ ( "amount" ) ) ; \n        tableA . printSchema ( ) ; \n         System . out . println ( tableA ) ; \n\n        tenv . createTemporaryView ( "tableB" ,  orderB ,  $ ( "user" ) ,  $ ( "product" ) ,  $ ( "amount" ) ) ; \n\n         //:tableAamount>2tableBamount>1 \n         /*\nselect * from tableA where amount > 2\nunion\n select * from tableB where amount > 1\n         */ \n         String  sql  =   "select * from " + tableA + " where amount > 2 \\n"   + \n                 "union \\n"   + \n                 " select * from tableB where amount > 1" ; \n\n         Table  resultTable  =  tenv . sqlQuery ( sql ) ; \n        resultTable . printSchema ( ) ; \n         System . out . println ( resultTable ) ; //UnnamedTable$1 \n\n\n         //TableDataStream \n         //DataStream<Order> resultDS = tenv.toAppendStream(resultTable, Order.class);//union alltoAppendStream \n         DataStream < Tuple2 < Boolean ,   Order > >  resultDS  =  tenv . toRetractStream ( resultTable ,   Order . class ) ; //uniontoRetractStream \n         //toAppendStream  appendDataStream \n         //toRetractStream   DataStreamtruefalse \n         //StructuredStreamingappend/update/complete \n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @NoArgsConstructor \n     @AllArgsConstructor \n     public   static   class   Order   { \n         public   Long  user ; \n         public   String  product ; \n         public   int  amount ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 2 \n Table/DSLSQLWordCount \n package   cn . itcast . sql ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . Table ; \n import   org . apache . flink . table . api . bridge . java . StreamTableEnvironment ; \n\n import   static   org . apache . flink . table . api . Expressions . $ ; \n\n /**\n * Author itcast\n * Desc Flink Table&SQL - SQLTableWordCount\n */ \n public   class   Demo02   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         EnvironmentSettings  settings  =   EnvironmentSettings . newInstance ( ) . useBlinkPlanner ( ) . inStreamingMode ( ) . build ( ) ; \n         StreamTableEnvironment  tenv  =   StreamTableEnvironment . create ( env ,  settings ) ; \n\n         //TODO 1.source \n         DataStream < WC >  wordsDS  =  env . fromElements ( \n                 new   WC ( "Hello" ,   1 ) , \n                 new   WC ( "World" ,   1 ) , \n                 new   WC ( "Hello" ,   1 ) \n         ) ; \n\n         //TODO 2.transformation \n         //DataStreamViewTable \n        tenv . createTemporaryView ( "t_words" ,  wordsDS , $ ( "word" ) ,  $ ( "frequency" ) ) ; \n /*\nselect word,sum(frequency) as frequency\nfrom t_words\ngroup by word\n */ \n         String  sql  =   "select word,sum(frequency) as frequency\\n "   + \n                 "from t_words\\n "   + \n                 "group by word" ; \n\n         //sql \n         Table  resultTable  =  tenv . sqlQuery ( sql ) ; \n\n         //DataStream \n         DataStream < Tuple2 < Boolean ,  WC > >  resultDS  =  tenv . toRetractStream ( resultTable ,   WC . class ) ; \n         //toAppendStream  appendDataStream \n         //toRetractStream   DataStreamtruefalse \n         //StructuredStreamingappend/update/complete \n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n         //new WC("Hello", 1), \n         //new WC("World", 1), \n         //new WC("Hello", 1) \n         // \n         //(true,Demo02.WC(word=Hello, frequency=1)) \n         //(true,Demo02.WC(word=World, frequency=1)) \n         //(false,Demo02.WC(word=Hello, frequency=1)) \n         //(true,Demo02.WC(word=Hello, frequency=2)) \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @NoArgsConstructor \n     @AllArgsConstructor \n     public   static   class   WC   { \n         public   String  word ; \n         public   long  frequency ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 package   cn . itcast . sql ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . Table ; \n import   org . apache . flink . table . api . bridge . java . StreamTableEnvironment ; \n\n import   static   org . apache . flink . table . api . Expressions . $ ; \n\n /**\n * Author itcast\n * Desc Flink Table&SQL - SQLTableWordCount\n */ \n public   class   Demo02_2   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         EnvironmentSettings  settings  =   EnvironmentSettings . newInstance ( ) . useBlinkPlanner ( ) . inStreamingMode ( ) . build ( ) ; \n         StreamTableEnvironment  tenv  =   StreamTableEnvironment . create ( env ,  settings ) ; \n\n         //TODO 1.source \n         DataStream < WC >  wordsDS  =  env . fromElements ( \n                 new   WC ( "Hello" ,   1 ) , \n                 new   WC ( "World" ,   1 ) , \n                 new   WC ( "Hello" ,   1 ) \n         ) ; \n\n         //TODO 2.transformation \n         //DataStreamViewTable \n         Table  table  =  tenv . fromDataStream ( wordsDS ) ; \n\n         //table/DSL \n         Table  resultTable  =  table\n                 . groupBy ( $ ( "word" ) ) \n                 . select ( $ ( "word" ) ,  $ ( "frequency" ) . sum ( ) . as ( "frequency" ) ) \n                 . filter ( $ ( "frequency" ) . isEqual ( 2 ) ) ; \n\n         //DataStream \n         DataStream < Tuple2 < Boolean ,  WC > >  resultDS  =  tenv . toRetractStream ( resultTable ,   WC . class ) ; \n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @NoArgsConstructor \n     @AllArgsConstructor \n     public   static   class   WC   { \n         public   String  word ; \n         public   long  frequency ; \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 3 \n \n package   cn . itcast . sql ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . eventtime . WatermarkStrategy ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichSourceFunction ; \n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . Table ; \n import   org . apache . flink . table . api . bridge . java . StreamTableEnvironment ; \n import   org . apache . flink . types . Row ; \n\n import   java . time . Duration ; \n import   java . util . Random ; \n import   java . util . UUID ; \n import   java . util . concurrent . TimeUnit ; \n\n import   static   org . apache . flink . table . api . Expressions . $ ; \n\n /**\n * Author itcast\n * Desc Flink Table&SQL - +Watermaker+window\n */ \n public   class   Demo03   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         EnvironmentSettings  settings  =   EnvironmentSettings . newInstance ( ) . useBlinkPlanner ( ) . inStreamingMode ( ) . build ( ) ; \n         StreamTableEnvironment  tenv  =   StreamTableEnvironment . create ( env ,  settings ) ; \n\n         //TODO 1.source \n         DataStreamSource < Order >  orderDS   =  env . addSource ( new   RichSourceFunction < Order > ( )   { \n             private   Boolean  isRunning  =   true ; \n             @Override \n             public   void   run ( SourceContext < Order >  ctx )   throws   Exception   { \n                 Random  random  =   new   Random ( ) ; \n                 while   ( isRunning )   { \n                     Order  order  =   new   Order ( UUID . randomUUID ( ) . toString ( ) ,  random . nextInt ( 3 ) ,  random . nextInt ( 101 ) ,   System . currentTimeMillis ( ) ) ; \n                     TimeUnit . SECONDS . sleep ( 1 ) ; \n                    ctx . collect ( order ) ; \n                 } \n             } \n\n             @Override \n             public   void   cancel ( )   { \n                isRunning  =   false ; \n             } \n         } ) ; \n\n         //TODO 2.transformation \n         //:+Watermarker+FlinkSQLTablewindow \n         DataStream < Order >  orderDSWithWatermark  =  orderDS . assignTimestampsAndWatermarks ( WatermarkStrategy . < Order > forBoundedOutOfOrderness ( Duration . ofSeconds ( 5 ) ) \n                 . withTimestampAssigner ( ( order ,  recordTimestamp )   ->  order . getCreateTime ( ) ) \n         ) ; \n\n         //DataStream--\x3eView/Table,: \n        tenv . createTemporaryView ( "t_order" , orderDSWithWatermark , $ ( "orderId" ) ,  $ ( "userId" ) ,  $ ( "money" ) ,  $ ( "createTime" ) . rowtime ( ) ) ; \n /*\nselect  userId, count(orderId) as orderCount, max(money) as maxMoney,min(money) as minMoney\nfrom t_order\ngroup by userId,\ntumble(createTime, INTERVAL \'5\' SECOND)\n */ \n         String  sql  =   "select userId, count(orderId) as orderCount, max(money) as maxMoney,min(money) as minMoney\\n "   + \n                 "from t_order\\n "   + \n                 "group by userId,\\n "   + \n                 "tumble(createTime, INTERVAL \'5\' SECOND)" ; \n\n         //sql \n         Table  resultTable  =  tenv . sqlQuery ( sql ) ; \n\n         DataStream < Tuple2 < Boolean ,   Row > >  resultDS  =  tenv . toRetractStream ( resultTable ,   Row . class ) ; \n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   Order   { \n         private   String  orderId ; \n         private   Integer  userId ; \n         private   Integer  money ; \n         private   Long  createTime ; // \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 package   cn . itcast . sql ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . flink . api . common . eventtime . WatermarkStrategy ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . datastream . DataStreamSource ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . RichSourceFunction ; \n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . Table ; \n import   org . apache . flink . table . api . Tumble ; \n import   org . apache . flink . table . api . bridge . java . StreamTableEnvironment ; \n import   org . apache . flink . types . Row ; \n\n import   java . time . Duration ; \n import   java . util . Random ; \n import   java . util . UUID ; \n import   java . util . concurrent . TimeUnit ; \n\n import   static   org . apache . flink . table . api . Expressions . $ ; \n import   static   org . apache . flink . table . api . Expressions . lit ; \n\n /**\n * Author itcast\n * Desc Flink Table&SQL - +Watermaker+window-Table\n */ \n public   class   Demo03_2   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 0.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n         StreamTableEnvironment  tblEnv  =   StreamTableEnvironment . create ( env ) ; \n\n         //TODO 1.source \n         DataStreamSource < Order >  orderDS   =  env . addSource ( new   RichSourceFunction < Order > ( )   { \n             private   Boolean  isRunning  =   true ; \n             @Override \n             public   void   run ( SourceContext < Order >  ctx )   throws   Exception   { \n                 Random  random  =   new   Random ( ) ; \n                 while   ( isRunning )   { \n                     Order  order  =   new   Order ( UUID . randomUUID ( ) . toString ( ) ,  random . nextInt ( 3 ) ,  random . nextInt ( 101 ) ,   System . currentTimeMillis ( ) ) ; \n                     TimeUnit . SECONDS . sleep ( 1 ) ; \n                    ctx . collect ( order ) ; \n                 } \n             } \n\n             @Override \n             public   void   cancel ( )   { \n                isRunning  =   false ; \n             } \n         } ) ; \n\n         //TODO 2.transformation \n         //:+Watermarker+FlinkSQLTablewindow \n         DataStream < Order >  orderDSWithWatermark  =  orderDS . assignTimestampsAndWatermarks ( WatermarkStrategy . < Order > forBoundedOutOfOrderness ( Duration . ofSeconds ( 5 ) ) \n                 . withTimestampAssigner ( ( order ,  recordTimestamp )   ->  order . getCreateTime ( ) ) \n         ) ; \n\n         //DataStream--\x3eView/Table,: \n        tenv . createTemporaryView ( "t_order" , orderDSWithWatermark , $ ( "orderId" ) ,  $ ( "userId" ) ,  $ ( "money" ) ,  $ ( "createTime" ) . rowtime ( ) ) ; \n         //Table table = tenv.fromDataStream(orderDSWithWatermark, $("orderId"), $("userId"), $("money"), $("createTime").rowtime()); \n         //table.groupBy().select(); \n /*\nselect  userId, count(orderId) as orderCount, max(money) as maxMoney,min(money) as minMoney\nfrom t_order\ngroup by userId,\ntumble(createTime, INTERVAL \'5\' SECOND)\n */ \n         Table  resultTable  =  tenv . from ( "t_order" ) \n                 . window ( Tumble . over ( lit ( 5 ) . second ( ) ) \n                         . on ( $ ( "createTime" ) ) \n                         . as ( "\n                            " ) ) \n                 . groupBy ( $ ( "tumbleWindow" ) ,  $ ( "userId" ) ) \n                 . select ( \n                        $ ( "userId" ) , \n                        $ ( "orderId" ) . count ( ) . as ( "orderCount" ) , \n                        $ ( "money" ) . max ( ) . as ( "maxMoney" ) , \n                        $ ( "money" ) . min ( ) . as ( "minMoney" ) \n                 ) ; \n\n         DataStream < Tuple2 < Boolean ,   Row > >  resultDS  =  tenv . toRetractStream ( resultTable ,   Row . class ) ; \n\n         //TODO 3.sink \n        resultDS . print ( ) ; \n\n         //TODO 4.execute \n        env . execute ( ) ; \n     } \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   Order   { \n         private   String  orderId ; \n         private   Integer  userId ; \n         private   Integer  money ; \n         private   Long  createTime ; // \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 #  Flink---FlinkSQLHive \n 1. \n \n  \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/ \n \n jar \n < dependency > \n             < groupId > org . apache . flink < / groupId > \n             < artifactId > flink - connector - hive_2 . 12 < / artifactId > \n             < version > $ { flink . version } < / version > \n         < / dependency > \n         < dependency > \n             < groupId > org . apache . hive < / groupId > \n             < artifactId > hive - metastore < / artifactId > \n             < version > 2.1 .0 < / version > \n             < exclusions > \n                 < exclusion > \n                     < artifactId > hadoop - hdfs < / artifactId > \n                     < groupId > org . apache . hadoop < / groupId > \n                 < / exclusion > \n             < / exclusions > \n         < / dependency > \n         < dependency > \n             < groupId > org . apache . hive < / groupId > \n             < artifactId > hive - exec < / artifactId > \n             < version > 2.1 .0 < / version > \n         < / dependency > \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 hivejarflink/lib \n \n \n FlinkSQLHive-CLI \n 1.hive-site.xml \n <property>\n       <name>hive.metastore.uris</name>\n       <value>thrift://node3:9083</value>\n</property>\n \n 1 2 3 4 <?xml-stylesheet type="text/xsl" href="configuration.xsl"?> \n < configuration > \n     < property > \n         < name > javax.jdo.option.ConnectionUserName </ name > \n         < value > root </ value > \n     </ property > \n     < property > \n         < name > javax.jdo.option.ConnectionPassword </ name > \n         < value > 123456 </ value > \n     </ property > \n     < property > \n         < name > javax.jdo.option.ConnectionURL </ name > \n         < value > jdbc:mysql://node3:3306/hive?createDatabaseIfNotExist=true &amp; useSSL=false </ value > \n     </ property > \n     < property > \n         < name > javax.jdo.option.ConnectionDriverName </ name > \n         < value > com.mysql.jdbc.Driver </ value > \n     </ property > \n     < property > \n         < name > hive.metastore.schema.verification </ name > \n         < value > false </ value > \n     </ property > \n     < property > \n         < name > datanucleus.schema.autoCreateAll </ name > \n         < value > true </ value > \n     </ property > \n     < property > \n         < name > hive.server2.thrift.bind.host </ name > \n         < value > node3 </ value > \n     </ property > \n     < property > \n         < name > hive.metastore.uris </ name > \n         < value > thrift://node3:9083 </ value > \n     </ property > \n </ configuration > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 2. \n nohup /export/server/hive/bin/hive --service metastore & \n 3.flink/conf/sql-client-defaults.yaml \n catalogs:\n   - name: myhive\n     type: hive\n     hive-conf-dir: /export/server/hive/conf\n     default-database: default\n \n 1 2 3 4 5 4. \n 5.flink \n /export/server/flink/bin/start-cluster.sh \n 6.flink-sql-hive \n /export/server/flink/bin/sql-client.sh embedded \n 7.sql: \n show catalogs; \n use catalog myhive; \n show tables; \n select * from person; \n https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hive/ \n package   cn . itcast . feature ; \n\n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . TableEnvironment ; \n import   org . apache . flink . table . api . TableResult ; \n import   org . apache . flink . table . catalog . hive . HiveCatalog ; \n\n /**\n * Author itcast\n * Desc\n */ \n public   class   HiveDemo   { \n     public   static   void   main ( String [ ]  args ) { \n         //TODO 0.env \n         EnvironmentSettings  settings  =   EnvironmentSettings . newInstance ( ) . useBlinkPlanner ( ) . build ( ) ; \n         TableEnvironment  tableEnv  =   TableEnvironment . create ( settings ) ; \n\n         //TODO hive \n         String  name             =   "myhive" ; \n         String  defaultDatabase  =   "default" ; \n         String  hiveConfDir  =   "./conf" ; \n\n         //TODO hiveCatalog \n         HiveCatalog  hive  =   new   HiveCatalog ( name ,  defaultDatabase ,  hiveConfDir ) ; \n         //catalog \n        tableEnv . registerCatalog ( "myhive" ,  hive ) ; \n         //catalog \n        tableEnv . useCatalog ( "myhive" ) ; \n\n         //Hive \n         String  insertSQL  =   "insert into person select * from person" ; \n         TableResult  result  =  tableEnv . executeSql ( insertSQL ) ; \n\n         System . out . println ( result . getJobClient ( ) . get ( ) . getJobStatus ( ) ) ; \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #  kafkahive \n 4datastreamsqlsqldatastream \n \n import   org . apache . commons . lang3 . time . FastDateFormat ; \n import   org . apache . flink . api . common . functions . RichMapFunction ; \n import   org . apache . flink . api . common . typeinfo . TypeInformation ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . configuration . RestOptions ; \n import   org . apache . flink . streaming . api . CheckpointingMode ; \n import   org . apache . flink . streaming . api . datastream . SingleOutputStreamOperator ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . table . api . EnvironmentSettings ; \n import   org . apache . flink . table . api . Table ; \n import   org . apache . flink . table . api . TableResult ; \n import   org . apache . flink . table . api . bridge . java . StreamTableEnvironment ; \n import   org . apache . flink . table . catalog . hive . HiveCatalog ; \n import   org . apache . flink . types . Row ; \n\n import   java . io . Serializable ; \n import   java . sql . Timestamp ; \n\n public   class   KafkaToHive   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         Configuration  configuration  =   new   Configuration ( ) ; \n        configuration . setString ( RestOptions . BIND_PORT , "8081-8089" ) ;   // Flink Web UI 9091 \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( configuration ) ; \n         EnvironmentSettings  settings  =   EnvironmentSettings . newInstance ( ) . inStreamingMode ( ) . useBlinkPlanner ( ) . build ( ) ; \n         StreamTableEnvironment  tbEnv  =   StreamTableEnvironment . create ( env , settings ) ; \n         //hivecheckpointck \n        env . enableCheckpointing ( 10 * 60 * 1000 ,   CheckpointingMode . EXACTLY_ONCE ) ; \n        env . setParallelism ( 3 ) ; \n         //socket->producer \n         SingleOutputStreamOperator < Order >  socket  =  env . socketTextStream ( "node1" ,   9999 ) . rebalance ( ) . map ( new   RichMapFunction < String ,   Order > ( )   { \n             @Override \n             public   Order   map ( String  value )   throws   Exception   { \n                 String [ ]  timeAndWord  =  value . split ( "," ) ; \n                 Timestamp  timestamp  =   Timestamp . valueOf ( timeAndWord [ 0 ] ) ; \n                 Order  order  =   new   Order ( ) ; \n                order . setOrderId ( timeAndWord [ 1 ] ) ; \n                order . setMoney ( 1L ) ; \n                order . setEvenTime ( timestamp . getTime ( ) ) ; \n                 return  order ; \n             } \n         } ) ; \n        tbEnv . createTemporaryView ( "socket" , socket ) ; \n        tbEnv . executeSql ( "DROP TABLE IF EXISTS ODS" ) ; \n         TableResult  odsResult  =  tbEnv . executeSql ( "CREATE TABLE ODS (\\n"   + \n                 "  `orderId` STRING,\\n"   + \n                 "  `userId` INT,\\n"   + \n                 "  `money` BIGINT,\\n"   + \n                 "  `evenTime` BIGINT\\n"   + \n                 ") WITH (\\n"   + \n                 "  \'connector\' = \'kafka\',\\n"   + \n                 "  \'topic\' = \'KafkaWordCount\',\\n"   + \n                 "  \'properties.bootstrap.servers\' = \'node1:9092\',\\n"   + \n                 "  \'format\' = \'json\',\\n"   + \n                 "  \'sink.partitioner\' = \'round-robin\'\\n"   + \n                 ") " ) ; \n         String  sql = "insert into ODS select orderId,userId,money,evenTime from socket" ; \n         TableResult  result  =  tbEnv . executeSql ( sql ) ; \n\n         //kafka->hive \n        tbEnv . executeSql ( "DROP TABLE IF EXISTS DW" ) ; \n         //is_generic\'=\'false\'  \n         /**\n         * FlinkHive CatalogFlink SQL \n         * HiveDESCRIBE FORMATTED\n         * is_generic=trueFlinkFlinkHive\n         * FlinkHive\n         */ \n         TableResult  dwResult  =  tbEnv . executeSql ( "CREATE TABLE DW (\\n"   + \n                 "  `orderId` STRING,\\n"   + \n                 "  `userId` INT,\\n"   + \n                 "  `money` BIGINT,\\n"   + \n                 "  `evenTime` BIGINT\\n"   + \n                 ") WITH (\\n"   + \n                 "  \'connector\' = \'kafka\',\\n"   + \n                 "  \'topic\' = \'KafkaWordCount\',\\n"   + \n                 "  \'properties.bootstrap.servers\' = \'node1:9092\',\\n"   + \n                 "  \'properties.group.id\' = \'sink_to_hive\',\\n"   + \n                 "  \'scan.startup.mode\' = \'group-offsets\',\\n"   + \n                 "  \'format\' = \'json\'\\n"   + \n                 ")" ) ; \n\n         //hive \n         //TODO hive \n         String  name             =   "myhive" ; \n         String  defaultDatabase  =   "default" ; \n         String  hiveConfDir  =   "E:\\\\BigData\\\\workspace\\\\Flink\\\\Flink_study\\\\src\\\\main\\\\resources" ; \n\n         //TODO hiveCatalog,hivebeeline thiftserver \n         HiveCatalog  hive  =   new   HiveCatalog ( name ,  defaultDatabase ,  hiveConfDir ) ; \n         //catalog \n        tbEnv . registerCatalog ( "myhive" ,  hive ) ; \n         //catalog \n        tbEnv . useCatalog ( "myhive" ) ; \n         //hive sql \n        tbEnv . executeSql ( "DROP TABLE IF EXISTS OrderTb" ) ; \n         TableResult   OrderTb   =  tbEnv . executeSql ( "create table if not EXISTS OrderTb("   + \n                 "  `orderId` STRING,\\n"   + \n                 "  `userId` INT,\\n"   + \n                 "  `money` BIGINT,\\n"   + \n                 "  `evenTime` BIGINT\\n"   + \n                 ")WITH (\\n"   + \n                 "  \'is_generic\'=\'false\' )" ) ; \n         //Insert overwrite is not supported for streaming write. \n        tbEnv . executeSql ( "insert into OrderTb select orderId,userId,money,evenTime from default_catalog.default_database.DW" ) ; \n\n         Table  table  =  tbEnv . sqlQuery ( "select * from default_catalog.default_database.DW " ) ; \n        tbEnv . toRetractStream ( table ,   TypeInformation . of ( Row . class ) ) . print ( ) ; \n        env . execute ( ) ; \n\n     } \n     public   static   class   Order   implements   Serializable   { \n         private   String  orderId ; \n         private   Integer  userId ; \n         private   Long  money ; \n         private   Long  evenTime ; \n\n         public   Order ( String  orderId ,   Integer  userId ,   Long  money ,   Long  evenTime )   { \n             this . orderId  =  orderId ; \n             this . userId  =  userId ; \n             this . money  =  money ; \n             this . evenTime  =  evenTime ; \n         } \n\n         public   Order ( )   { \n         } \n\n         public   String   getOrderId ( )   { \n             return  orderId ; \n         } \n\n         public   void   setOrderId ( String  orderId )   { \n             this . orderId  =  orderId ; \n         } \n\n         public   Integer   getUserId ( )   { \n             return  userId ; \n         } \n\n         public   void   setUserId ( Integer  userId )   { \n             this . userId  =  userId ; \n         } \n\n         public   Long   getMoney ( )   { \n             return  money ; \n         } \n\n         public   void   setMoney ( Long  money )   { \n             this . money  =  money ; \n         } \n\n         public   Long   getEvenTime ( )   { \n             return  evenTime ; \n         } \n\n         public   void   setEvenTime ( Long  evenTime )   { \n             this . evenTime  =  evenTime ; \n         } \n\n         @Override \n         public   String   toString ( )   { \n             FastDateFormat  df  =   FastDateFormat . getInstance ( "HH:mm:ss" ) ; \n             return   "Order{"   + \n                     "orderId=\'"   +  orderId  +   \'\\\'\'   + \n                     ", userId="   +  userId  + \n                     ", money="   +  money  + \n                     ", evenTime="   +  df . format ( evenTime )   + \n                     \'}\' ; \n         } \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 #  Flink--- \n   \n \n  \n /**\n     * Tuple2<, >\n     */ \n     public   static   class   MySource   implements   SourceFunction < Tuple2 < String ,   Double > >   { \n         private   boolean  flag  =   true ; \n         private   String [ ]  categorys  =   { "" ,   "" ,   "" ,   "" ,   "" ,   "" ,   "" ,   "" ,   "" ,   "" ,   "" ,   "" } ; \n         private   Random  random  =   new   Random ( ) ; \n\n         @Override \n         public   void   run ( SourceContext < Tuple2 < String ,   Double > >  ctx )   throws   Exception   { \n             while   ( flag )   { \n                 // \n                 int  index  =  random . nextInt ( categorys . length ) ; //[0~length) ==> [0~length-1] \n                 String  category  =  categorys [ index ] ; // \n                 double  price  =  random . nextDouble ( )   *   100 ; //nextDouble[0~1),*100[0~100) \n                ctx . collect ( Tuple2 . of ( category ,  price ) ) ; \n                 Thread . sleep ( 20 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  \n 1.env\n2.source\n3.transformation--\n3.1,UTC+08:00UTC\nkeyBy(t->t.f0)\nwindow(TumblingProcessingTimeWindows.of(Time.days(1), Time.hours(-8))\n3.21s\n.trigger(ContinuousProcessingTimeTrigger.of(Time.seconds(1)))\n3.3.aggregate(new PriceAggregate(), new WindowResult());\n3.4\nCategoryPojo(category=, totalPrice=17225.26, dateTime=2020-10-20 08:04:12)\n4.sink-,:\ntempAggResult.keyBy(CategoryPojo::getDateTime)\n//\n                .window(TumblingProcessingTimeWindows.of(Time.seconds(1)))   \n//ProcessWindowFunction\n             \t.process(new WindowResultProcess());\n4.1.\n4.2.top3\n4.3.\n5.execute\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  \n \n package   cn . itcast . action ; \n\n import   lombok . AllArgsConstructor ; \n import   lombok . Data ; \n import   lombok . NoArgsConstructor ; \n import   org . apache . commons . lang3 . StringUtils ; \n import   org . apache . commons . lang3 . time . FastDateFormat ; \n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . functions . AggregateFunction ; \n import   org . apache . flink . api . java . tuple . Tuple2 ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . source . SourceFunction ; \n import   org . apache . flink . streaming . api . functions . windowing . ProcessWindowFunction ; \n import   org . apache . flink . streaming . api . functions . windowing . WindowFunction ; \n import   org . apache . flink . streaming . api . windowing . assigners . TumblingProcessingTimeWindows ; \n import   org . apache . flink . streaming . api . windowing . time . Time ; \n import   org . apache . flink . streaming . api . windowing . triggers . ContinuousProcessingTimeTrigger ; \n import   org . apache . flink . streaming . api . windowing . windows . TimeWindow ; \n import   org . apache . flink . util . Collector ; \n\n import   java . math . BigDecimal ; \n import   java . math . RoundingMode ; \n import   java . util . List ; \n import   java . util . PriorityQueue ; \n import   java . util . Queue ; \n import   java . util . Random ; \n import   java . util . stream . Collectors ; \n\n /**\n * Author itcast\n * Desc\n * 1. 1111 00:00:00 ~ 23:59:59\n * 2.top3\n * 3.\n */ \n public   class   DoubleElevenBigScreem   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n        env . setParallelism ( 1 ) ; // \n         //TODO 2.source \n         DataStream < Tuple2 < String ,   Double > >  orderDS  =  env . addSource ( new   MySource ( ) ) ; \n\n         //TODO 3.transformation--:1s \n         DataStream < CategoryPojo >  tempAggResult  =  orderDS\n                 // \n                 . keyBy ( t  ->  t . f0 ) \n                 //: \n                 //1 \n                 //.window(TumblingProcessingTimeWindows.of(Time.days(1))); \n                 //1s,1111 00:01:00: 1110 00:01:00~1111 00:01:00 ---! \n                 //.window(SlidingProcessingTimeWindows.of(Time.days(1),Time.seconds(1))); \n                 //*UTC+08:00 \n                 //*00:00:00{@code of(.(1),.hours(-8))}. \n                 //00:00:00,/ \n                 //3.1,UTC+08:00UTC \n                 . window ( TumblingProcessingTimeWindows . of ( Time . days ( 1 ) ,   Time . hours ( - 8 ) ) ) \n                 //3.2/ \n                 . trigger ( ContinuousProcessingTimeTrigger . of ( Time . seconds ( 1 ) ) ) \n                 //.sum()// \n                 //3.3 \n                 //aggregate(AggregateFunction<T, ACC, V> aggFunction,WindowFunction<V, R, K, W> windowFunction) \n                 . aggregate ( new   PriceAggregate ( ) ,   new   WindowResult ( ) ) ; //aggregate \n         //3.4 \n        tempAggResult . print ( "" ) ; \n         //> DoubleElevenBigScreem.CategoryPojo(category=, totalPrice=563.8662504982619, dateTime=2021-01-19 10:31:40) \n         //> DoubleElevenBigScreem.CategoryPojo(category=, totalPrice=876.5216500403918, dateTime=2021-01-19 10:31:40) \n\n         //TODO 4.sink-(1s),: \n        tempAggResult . keyBy ( CategoryPojo :: getDateTime ) \n                 . window ( TumblingProcessingTimeWindows . of ( Time . seconds ( 1 ) ) ) //1s \n                 //.sum// \n                 . process ( new   FinalResultWindowProcess ( ) ) ; //ProcessWindowFunction \n\n         //TODO 5.execute \n        env . execute ( ) ; \n     } \n\n     /**\n     * Tuple2<, >\n     */ \n     public   static   class   MySource   implements   SourceFunction < Tuple2 < String ,   Double > >   { \n         private   boolean  flag  =   true ; \n         private   String [ ]  categorys  =   { "" ,   "" ,   "" ,   "" ,   "" ,   "" ,   "" ,   "" ,   "" ,   "" ,   "" ,   "" } ; \n         private   Random  random  =   new   Random ( ) ; \n\n         @Override \n         public   void   run ( SourceContext < Tuple2 < String ,   Double > >  ctx )   throws   Exception   { \n             while   ( flag )   { \n                 // \n                 int  index  =  random . nextInt ( categorys . length ) ; //[0~length) ==> [0~length-1] \n                 String  category  =  categorys [ index ] ; // \n                 double  price  =  random . nextDouble ( )   *   100 ; //nextDouble[0~1),*100[0~100) \n                ctx . collect ( Tuple2 . of ( category ,  price ) ) ; \n                 Thread . sleep ( 20 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n     } \n\n     /**\n     * ,\n     * AggregateFunction<IN, ACC, OUT>\n     */ \n     private   static   class   PriceAggregate   implements   AggregateFunction < Tuple2 < String ,   Double > ,   Double ,   Double >   { \n         // \n         @Override \n         public   Double   createAccumulator ( )   { \n             return   0D ; //Ddouble,LLong \n         } \n\n         // \n         @Override \n         public   Double   add ( Tuple2 < String ,   Double >  value ,   Double  accumulator )   { \n             return  value . f1  +  accumulator ; \n         } \n\n         // \n         @Override \n         public   Double   getResult ( Double  accumulator )   { \n             return  accumulator ; \n         } \n\n         //subtask \n         @Override \n         public   Double   merge ( Double  a ,   Double  b )   { \n             return  a  +  b ; \n         } \n     } \n\n     /**\n     * ,\n     * WindowFunction<IN, OUT, KEY, W extends Window>\n     */ \n     private   static   class   WindowResult   implements   WindowFunction < Double ,   CategoryPojo ,   String ,   TimeWindow >   { \n         private   FastDateFormat  df  =   FastDateFormat . getInstance ( "yyyy-MM-dd HH:mm:ss" ) ; \n         @Override \n         //void apply(KEY key, W window, Iterable<IN> input, Collector<OUT> out) \n         public   void   apply ( String  category ,   TimeWindow  window ,   Iterable < Double >  input ,   Collector < CategoryPojo >  out )   throws   Exception   { \n             long  currentTimeMillis  =   System . currentTimeMillis ( ) ; \n             String  dateTime  =  df . format ( currentTimeMillis ) ; \n             Double  totalPrice  =  input . iterator ( ) . next ( ) ; \n            out . collect ( new   CategoryPojo ( category , totalPrice , dateTime ) ) ; // \n         } \n     } \n\n     /**\n     * \n     */ \n     @Data \n     @AllArgsConstructor \n     @NoArgsConstructor \n     public   static   class   CategoryPojo   { \n         private   String  category ; // \n         private   double  totalPrice ; // \n         private   String  dateTime ; // ,EventTime, \n     } \n\n     /**\n     * top3\n     * abstract class ProcessWindowFunction<IN, OUT, KEY, W extends Window>\n     */ \n     private   static   class   FinalResultWindowProcess   extends   ProcessWindowFunction < CategoryPojo ,   Object ,   String ,   TimeWindow >   { \n         //: \n         //key/dateTime1s \n         //elements:1s \n         @Override \n         public   void   process ( String  dateTime ,   Context  context ,   Iterable < CategoryPojo >  elements ,   Collector < Object >  out )   throws   Exception   { \n             //1. 1111 00:00:00 ~ 23:59:59 \n             double  total  =   0D ; // \n             //2.top3:: "": 10000 "": 9000 "":8000 \n             //:top3,3,!,! \n             //top3: \n             //70 \n             //80 \n             //90 \n             //, \n             //,85,,85,, \n             //80 \n             //85 \n             //90 \n             // \n             Queue < CategoryPojo >  queue  =   new   PriorityQueue < > ( 3 , // \n                     //,,,c1>c21,, \n                     ( c1 ,  c2 )   ->  c1 . getTotalPrice ( )   >=  c2 . getTotalPrice ( )   ?   1   :   - 1 ) ; \n             for   ( CategoryPojo  element  :  elements )   { \n                 double  price  =  element . getTotalPrice ( ) ; \n                total  +=  price ; \n                 if ( queue . size ( ) <   3 ) { \n                    queue . add ( element ) ; //offer \n                 } else { \n                     if ( price  >=  queue . peek ( ) . getTotalPrice ( ) ) { //peek \n                         //queue.remove(queue.peek()); \n                        queue . poll ( ) ; // \n                        queue . add ( element ) ; //offer \n                     } \n                 } \n             } \n             //queuetop3,. \n             List < String >  top3List  =  queue . stream ( ) \n                     . sorted ( ( c1 ,  c2 )   ->  c1 . getTotalPrice ( )   >=  c2 . getTotalPrice ( )   ?   - 1   :   1 ) \n                     . map ( c  ->   ":"   +  c . getCategory ( )   +   " :"   +  c . getTotalPrice ( ) ) \n                     . collect ( Collectors . toList ( ) ) ; \n\n             //3.- \n             double  roundResult  =   new   BigDecimal ( total ) . setScale ( 2 ,   RoundingMode . HALF_UP ) . doubleValue ( ) ; //2 \n             System . out . println ( ": " + dateTime  + "  :"   +  roundResult ) ; \n\n             System . out . println ( "top3: \\n"   +   StringUtils . join ( top3List , "\\n" ) ) ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 #  Flink--- \n  \n \n  \n  \n  \n  \n /key \n  \n 1. \n state \n 2.valuestate mapstate \n valuestate \n 3.15/ \n 4. \n /**\n     * sourceTuple3<id,id, >\n     */ \n     public   static   class   MySource   implements   SourceFunction < Tuple3 < String ,   String ,   Long > >   { \n         private   boolean  flag  =   true ; \n         @Override \n         public   void   run ( SourceContext < Tuple3 < String ,   String ,   Long > >  ctx )   throws   Exception   { \n             Random  random  =   new   Random ( ) ; \n             while   ( flag )   { \n                 String  userId  =  random . nextInt ( 5 )   +   "" ; \n                 String  orderId  =   UUID . randomUUID ( ) . toString ( ) ; \n                 long  currentTimeMillis  =   System . currentTimeMillis ( ) ; \n                ctx . collect ( Tuple3 . of ( userId ,  orderId ,  currentTimeMillis ) ) ; \n                 Thread . sleep ( 500 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  \n 1.env\n2.source\n3.transformation\ninterval.5s\nlong interval = 5000L;\nKeyedProcessFunction\ndataStream.keyBy(0).process(new TimerProcessFuntion(interval));\n3.1MapStatekeyvalue\n3.2MapState\nMapStateDescriptor<String, Long> mapStateDesc =\n            new MapStateDescriptor<>("mapStateDesc", String.class, Long.class);\n            mapState = getRuntimeContext().getMapState(mapStateDesc);\n3.3\nmapState.put(value.f0, value.f1);\nctx.timerService().registerProcessingTimeTimer(value.f1 + interval);\n3.4\n4.sink\n5.execute\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  \n package   cn . itcast . action ; \n\n import   org . apache . flink . api . common . RuntimeExecutionMode ; \n import   org . apache . flink . api . common . state . MapState ; \n import   org . apache . flink . api . common . state . MapStateDescriptor ; \n import   org . apache . flink . api . java . tuple . Tuple3 ; \n import   org . apache . flink . configuration . Configuration ; \n import   org . apache . flink . streaming . api . datastream . DataStream ; \n import   org . apache . flink . streaming . api . environment . StreamExecutionEnvironment ; \n import   org . apache . flink . streaming . api . functions . KeyedProcessFunction ; \n import   org . apache . flink . streaming . api . functions . source . SourceFunction ; \n import   org . apache . flink . util . Collector ; \n\n import   java . util . Iterator ; \n import   java . util . Map ; \n import   java . util . Random ; \n import   java . util . UUID ; \n\n /**\n * Author itcast\n * Desc\n */ \n public   class   OrderAutomaticFavorableComments   { \n     public   static   void   main ( String [ ]  args )   throws   Exception   { \n         //TODO 1.env \n         StreamExecutionEnvironment  env  =   StreamExecutionEnvironment . getExecutionEnvironment ( ) ; \n        env . setRuntimeMode ( RuntimeExecutionMode . AUTOMATIC ) ; \n        env . setParallelism ( 1 ) ; \n\n         //TODO 2.source \n         //Tuple3<id,id,> \n         DataStream < Tuple3 < String ,   String ,   Long > >  orderDS  =  env . addSource ( new   MySource ( ) ) ; \n\n         //TODO 3.transformation \n         //interval.5s \n         long  interval  =   5000L ; //5s \n         //KeyedProcessFunction \n        orderDS . keyBy ( t  ->  t . f0 ) \n                 . process ( new   TimerProcessFunction ( interval ) ) ; \n\n         //TODO 4.sink \n\n         //TODO 5.execute \n        env . execute ( ) ; \n     } \n\n     /**\n     * sourceTuple3<id,id, >\n     */ \n     public   static   class   MySource   implements   SourceFunction < Tuple3 < String ,   String ,   Long > >   { \n         private   boolean  flag  =   true ; \n\n         @Override \n         public   void   run ( SourceContext < Tuple3 < String ,   String ,   Long > >  ctx )   throws   Exception   { \n             Random  random  =   new   Random ( ) ; \n             while   ( flag )   { \n                 String  userId  =  random . nextInt ( 5 )   +   "" ; \n                 String  orderId  =   UUID . randomUUID ( ) . toString ( ) ; \n                 long  currentTimeMillis  =   System . currentTimeMillis ( ) ; \n                ctx . collect ( Tuple3 . of ( userId ,  orderId ,  currentTimeMillis ) ) ; \n                 Thread . sleep ( 500 ) ; \n             } \n         } \n\n         @Override \n         public   void   cancel ( )   { \n            flag  =   false ; \n         } \n     } \n\n     /**\n     * ProcessFunction\n     * interval\n     * abstract class KeyedProcessFunction<K, I, O>\n     */ \n     private   static   class   TimerProcessFunction   extends   KeyedProcessFunction < String ,   Tuple3 < String ,   String ,   Long > ,   Object >   { \n         private   long  interval ; // 5000ms/5s \n\n         public   TimerProcessFunction ( long  interval )   { \n             this . interval  =  interval ; \n         } \n\n         //-0.Stateid \n         private   MapState < String ,   Long >  mapState  =   null ; \n\n         //-1. \n         @Override \n         public   void   open ( Configuration  parameters )   throws   Exception   { \n             MapStateDescriptor < String ,   Long >  mapStateDescriptor  =   new   MapStateDescriptor < > ( "mapState" ,   String . class ,   Long . class ) ; \n            mapState  =   getRuntimeContext ( ) . getMapState ( mapStateDescriptor ) ; \n         } \n\n         //-2. \n         @Override \n         public   void   processElement ( Tuple3 < String ,   String ,   Long >  value ,   Context  ctx ,   Collector < Object >  out )   throws   Exception   { \n             //Tuple3<id,id, > value \n             // \n            mapState . put ( value . f1 ,  value . f2 ) ; //xxx,2020-11-11 00:00:00 ||xx,2020-11-11 00:00:01 \n             //value.f2 + interval/, \n             //value.f2 + interval \n            ctx . timerService ( ) . registerProcessingTimeTimer ( value . f2  +  interval ) ; //2020-11-11 00:00:05  || 2020-11-11 00:00:06 \n         } \n\n         //-3. \n         @Override \n         public   void   onTimer ( long  timestamp ,   OnTimerContext  ctx ,   Collector < Object >  out )   throws   Exception   { \n             //() \n             // \n             Iterator < Map . Entry < String ,   Long > >  iterator  =  mapState . iterator ( ) ; \n             while   ( iterator . hasNext ( ) )   { \n                 Map . Entry < String ,   Long >  map  =  iterator . next ( ) ; \n                 String  orderId  =  map . getKey ( ) ; \n                 Long  orderTime  =  map . getValue ( ) ; \n                 //--, \n                 if   ( ! isFavorable ( orderId ) )   { // \n                     //--, \n                     if   ( System . currentTimeMillis ( )   -  orderTime  >=  interval )   { \n                         System . out . println ( "orderId:"   +  orderId  +   ",!...." ) ; \n                         //, \n                        iterator . remove ( ) ; \n                        mapState . remove ( orderId ) ; \n                     } \n                 }   else   { \n                     System . out . println ( "orderId:"   +  orderId  +   "...." ) ; \n                     //, \n                    iterator . remove ( ) ; \n                    mapState . remove ( orderId ) ; \n                 } \n             } \n         } \n\n         // \n         public   boolean   isFavorable ( String  orderId )   { \n             return  orderId . hashCode ( )   %   2   ==   0 ; \n         } \n     } \n } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 '},{title:"spark",frontmatter:{title:"spark",date:"2019-09-08T00:00:00.000Z",author:"Gordon",sidebar:"auto",categories:[""],tags:["","sparksql"]},regularPath:"/%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/spark.html",relativePath:"/spark.md",key:"v-9f20b3be",path:"/2019/09/08/spark/",headers:[{level:2,title:"Spark",slug:"spark"},{level:2,title:"Spark",slug:"spark"},{level:3,title:"local",slug:"local"},{level:3,title:"Standalone",slug:"standalone"},{level:3,title:"StandaloneHA",slug:"standaloneha"},{level:3,title:"SparkOnYarn",slug:"sparkonyarn"},{level:2,title:"SparkCore--RDD",slug:"sparkcore-rdd"},{level:3,title:"RDD",slug:"rdd"},{level:3,title:"RDD",slug:"rdd"},{level:3,title:"RDD",slug:"rdd"},{level:3,title:"",slug:""},{level:3,title:"RDD",slug:"rdd"},{level:3,title:"RDD",slug:"rdd"},{level:3,title:"RDDCheckPoint",slug:"rddcheckpoint"},{level:3,title:"RDDDAG",slug:"rdddag"},{level:3,title:"",slug:""},{level:3,title:"Kyro",slug:"kyro"},{level:3,title:"",slug:""},{level:3,title:"Sparkshuffle",slug:"sparkshuffle"},{level:2,title:"SparkSQL",slug:"sparksql"},{level:3,title:"sparksql",slug:"sparksql"},{level:3,title:"SparkSQLHive()",slug:"sparksqlhive-"},{level:3,title:"SparkSQL",slug:"sparksql"},{level:3,title:"()",slug:"-"},{level:3,title:"Spark SQLHive",slug:"spark-sqlhive"},{level:3,title:"SparkSQLUDF()",slug:"sparksqludf-"},{level:3,title:"SparkSQL",slug:"sparksql"},{level:3,title:"SparkSQLRDD",slug:"sparksqlrdd"},{level:2,title:"SparkStreaming--RDD--DStream",slug:"sparkstreaming-rdd-dstream"},{level:3,title:"",slug:""},{level:3,title:"SparkStreaming",slug:"sparkstreaming"},{level:3,title:"DStream",slug:"dstream"},{level:3,title:"SparkStreaming-",slug:"sparkstreaming-"},{level:3,title:"",slug:""},{level:3,title:"SparkStreamingSparkSQL",slug:"sparkstreamingsparksql"},{level:3,title:"SparkStreamingKafka",slug:"sparkstreamingkafka"},{level:2,title:"StructuredStreamig",slug:"structuredstreamig"},{level:3,title:"StructuredStreamig",slug:"structuredstreamig"},{level:3,title:"StructuredStreamingsparkstreaming",slug:"structuredstreamingsparkstreaming"},{level:3,title:"",slug:""},{level:3,title:"",slug:""},{level:3,title:"StructuredStreamingKakfa",slug:"structuredstreamingkakfa"},{level:3,title:"--",slug:"-"},{level:3,title:"StructuredStreamingContinue",slug:"structuredstreamingcontinue"}],lastUpdated:"2023-6-24 2:06:37 F10: AM",lastUpdatedTimestamp:1687543597e3,content:' Spark \n \n \n  \n \n MR \n HiveMR,Spark,Tez DAG() \n SparkImpala(DAG) \n FLink(DAG) \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n HadoopSpark \n \n \n  \n \n \n  \n l CPU \n l ,  \n l  \n l  \n l  \n Spark \n local \n \n \n \n \n \n  \n \n local \n \n \n \n \n spark-shell \n \n bin/spark-shell --master local[3] \n \n \n \n wordcount \n \n wordcount \n \n hdfswordcount \n \n Spark \n \n Shuffle \n \n  \n \n \n \n \n  \nSpark-local\n    bin/spark-submit \\\n    --master local[3] \\\n    --class org.apache.spark.examples.SparkPi \\\n    /export/server/spark/examples/jars/spark-examples_2.11-2.4.5.jar \\\n    10\n \n 1 2 3 4 5 6 #  Standalone \n \n \n 2-Standalone \n \n \n \n \n \n  \n \n  \n \n \n \n 1-MasterWorker \n 2-Master7077MasterWebUi8080 \n 3-WOrkerWorkerWebUi \n 4-Spark\n \n Spark-Shell4040UI \n SparkHDFS \n \n \n \n \n  \n \n \n \n \n WebUi \n \n \n \n \n spark-shell \n \n bin/spark-shell --master spark://node1:7077 \n \n \n \n \n Spark \n \n \n \n \n \n     bin/spark-submit  \\ \n     --master  spark://node1:7077  \\ \n     --class  org.apache.spark.examples.SparkPi  \\ \n    /export/server/spark/examples/jars/spark-examples_2.11-2.4.5.jar  \\ \n     10 \n \n 1 2 3 4 5 \n StandaloneHA \n \n \n 3-standaloneHA \n \n \n 1- \n \n \n \n \n \n 2- \n \n 1-spark-env.shmaster(zk) \n 2-zk \n 3- \n \n \n \n \n \n \n 3-spark-shell() \n \n \n bin/spark-shell --master spark://node1:7077,node2:7077 \n \n \n  \n \n \n \n \n \n node2master \n \n \n \n \n \n  \n \n \n \n \n \n rdd \n \n \n \n \n \n 4-() \n \n \n \n \n     bin/spark-submit \\\n    --master spark://node1:7077,node2:7077 \\\n    --class org.apache.spark.examples.SparkPi \\\n    /export/server/spark/examples/jars/spark-examples_2.11-2.4.5.jar \\\n    10\n \n 1 2 3 4 5 \n \n \n \n \n  \n SparkOnYarn \n \n \n Yarn \n \n \n Driver-----AppMaster----ResourceManager ---- -NodeManager----Container----Task \n \n \n 1Yarn \n \n 1-spark-env.shHADOOP_CONFYarn_CONF \n 2- \n \n \n \n 3-YarnSpark\n \n \n \n YarnJobsparkexecutor \n \n \n \n \n \n \n \n \n \n \n \n \n \n 2Yarn \n \n \n   bin/spark-submit \\\n  --master yarn \\\n  --class org.apache.spark.examples.SparkPi \\\n  /export/server/spark/examples/jars/spark-examples_2.11-2.4.5.jar \\\n  10\n \n 1 2 3 4 5 \n \n \n \n \n 3---808819888 \n \n \n \n \n \n \n \n \n  \n \n Spark \n SparkOnYarndeploymode \n ScalaWOrdcountIDEA \n Jar \n  \n SparkMode() \n \n \n standaloneHAYarn \n \n \n ClusterClientDriver \n \n \n \n \n \n Clientdriver \n \n \n \n \n \n   bin/spark-submit \\\n  --master spark://node1:7077 \\\n  --deploy-mode client \\\n  --class org.apache.spark.examples.SparkPi \\\n  /export/server/spark-2.4.5-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.5.jar \\\n  10\n \n 1 2 3 4 5 6 \n \n \n \n \n ClusterdriverMasterworker \nSpark-standalone-cluster \n  bin/spark-submit  \\ \n   --master  spark://node1:7077  \\ \n  --deploy-mode cluster  \\ \n   --class  org.apache.spark.examples.SparkPi  \\ \n  /export/server/spark-2.4.5-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.5.jar  \\ \n   10 \n \n 1 2 3 4 5 6 7 \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n driverclientworker \n SparkOnYarn \n Client \n \n \n \n \n 1-DriverClient \n \n \n 2-ResourceManagerAppMaster \n \n \n 3-AppMasterResourceManager \n \n \n 4-ResourceManager(AppMaster)NodeManagerExecutor \n \n \n 5-ExecutorDriver \n \n \n 6-Driver-----DAGSchedulerTaskScheduler \n \n \n ****ActionJobStageStageTaskSetTaskExecutor**** \n \n \n \n \n \n  \n \n \n   bin/spark-submit  \\ \n   --master   yarn   \\ \n  --deploy-mode client  \\ \n   --class  org.apache.spark.examples.SparkPi  \\ \n  /export/server/spark/examples/jars/spark-examples_2.11-2.4.5.jar  \\ \n   10 \n \n 1 2 3 4 5 6 \n \n \n \n Cluster \n \n \n \n \n \n \n \n \n \n \n \n \n   bin/spark-submit  \\ \n   --master   yarn   \\ \n  --deploy-mode cluster  \\ \n   --class  org.apache.spark.examples.SparkPi  \\ \n  /export/server/spark/examples/jars/spark-examples_2.11-2.4.5.jar  \\ \n   10 \n \n 1 2 3 4 5 6 \n \n conf log4j.properties.tmpletelog4j.properties,stdout \n \n \n \n \n \n \n \n \n \n \n \n ContainerExecutor \n \n \n ExecutorContiner \n \n \n ExecutorTaskTask \n SparkIDEA \n  \n \n \n  \n \n \n 1- \n \n bigdata-spark_2.11 \n spark-chapter01_2.11 \n \n \n \n \n \n \n 2-pommaven \n \n \n    < properties > \n         < encoding > UTF-8 </ encoding > \n         < scala.version > 2.11.12 </ scala.version > \n         < scala.binary.version > 2.11 </ scala.binary.version > \n         < spark.version > 2.4.5 </ spark.version > \n         < hadoop.version > 2.7.5 </ hadoop.version > \n     </ properties > \n  \n     < dependencies > \n         < dependency > \n             < groupId > org.scala-lang </ groupId > \n             < artifactId > scala-library </ artifactId > \n             < version > ${scala.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-core_${scala.binary.version} </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.hadoop </ groupId > \n             < artifactId > hadoop-client </ artifactId > \n             < version > ${hadoop.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > junit </ groupId > \n             < artifactId > junit </ artifactId > \n             < version > 4.10 </ version > \n             < scope > provided </ scope > \n         </ dependency > \n         < dependency > \n             < groupId > mysql </ groupId > \n             < artifactId > mysql-connector-java </ artifactId > \n             < version > 5.1.38 </ version > \n         </ dependency > \n         < dependency > \n             < groupId > com.alibaba </ groupId > \n             < artifactId > fastjson </ artifactId > \n             < version > 1.2.47 </ version > \n         </ dependency > \n         \x3c!--  --\x3e \n         \x3c!--SparkSQL+ Hive--\x3e \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-hive_2.11 </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-hive-thriftserver_2.11 </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n         \x3c!-- spark-streaming--\x3e \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-streaming_2.11 </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n         \x3c!-- SparkMlLib,ALS--\x3e \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-mllib_2.11 </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n         \x3c!--spark-streaming+Kafka--\x3e \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-streaming-kafka-0-10_2.11 </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n         \x3c!--StructuredStreaming+Kafka--\x3e \n         < dependency > \n             < groupId > org.apache.spark </ groupId > \n             < artifactId > spark-sql-kafka-0-10_2.11 </ artifactId > \n             < version > ${spark.version} </ version > \n         </ dependency > \n  \n         < dependency > \n             < groupId > com.hankcs </ groupId > \n             < artifactId > hanlp </ artifactId > \n             < version > portable-1.7.7 </ version > \n         </ dependency > \n         \x3c!-- Redis--\x3e \n         < dependency > \n             < groupId > redis.clients </ groupId > \n             < artifactId > jedis </ artifactId > \n             < version > 2.9.0 </ version > \n         </ dependency > \n     </ dependencies > \n  \n     < build > \n         < outputDirectory > target/classes </ outputDirectory > \n         < testOutputDirectory > target/test-classes </ testOutputDirectory > \n         < resources > \n             < resource > \n                 < directory > ${project.basedir}/src/main/resources </ directory > \n             </ resource > \n         </ resources > \n         \x3c!-- Maven  --\x3e \n         < plugins > \n             \x3c!-- java --\x3e \n             < plugin > \n                 < groupId > org.apache.maven.plugins </ groupId > \n                 < artifactId > maven-compiler-plugin </ artifactId > \n                 < version > 3.0 </ version > \n                 < configuration > \n                     < source > 1.8 </ source > \n                     < target > 1.8 </ target > \n                     < encoding > UTF-8 </ encoding > \n                 </ configuration > \n             </ plugin > \n             \x3c!-- scala --\x3e \n             < plugin > \n                 < groupId > net.alchim31.maven </ groupId > \n                 < artifactId > scala-maven-plugin </ artifactId > \n                 < version > 3.2.0 </ version > \n                 < executions > \n                     < execution > \n                         < goals > \n                             < goal > compile </ goal > \n                             < goal > testCompile </ goal > \n                         </ goals > \n                     </ execution > \n                 </ executions > \n             </ plugin > \n         </ plugins > \n     </ build > \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 #  Scala[] \n \n \n 3-ScalaWordCount \n \n \n IDEAwordcount \n \n \n  \n \n 1-SparkContent \n 2- \n 3-flatmap \n 4-map(word,1) \n 5-reduceByKey() \n 6- \n \n \n \n  \n \n \n    package   cn . itcast . sparkbase \n  \n   import   org . apache . spark . rdd . RDD\n   import   org . apache . spark . { SparkConf ,  SparkContext } \n  \n   /**\n   * DESC:\n   * 1-SparkContent\n   * 2-\n   * 3-flatmap\n   * 4-map(word,1)\n   * 5-reduceByKey()\n   * 6-\n   */ \n   object  _01SparkWordCount  { \n     def  main ( args :  Array [ String ] ) :   Unit   =   { \n       //* 1-SparkContext \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( "_01SparkWordCount" ) . setMaster ( "local[*]" ) \n       val  sc :  SparkContext  =   new  SparkContext ( conf ) \n       //sc.setLogLevel("WARN") \n       //* 2- \n       val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/words.txt" ) \n       //println(s"count value is:${fileRDD.count()}")//count value is:2 \n       //* 3-flatmap \n       val  valueRDD :  RDD [ String ]   =  fileRDD . flatMap ( x  =>  x . split ( "\\\\s+" ) ) \n       //* 4-map(word,1) \n       val  mapRDD :  RDD [ ( String ,   Int ) ]   =  valueRDD . map ( x  =>   ( x ,   1 ) ) \n       //* 5-reduceByKey() \n       val  resultRDD :  RDD [ ( String ,   Int ) ]   =  mapRDD . reduceByKey ( ( a ,  b )   =>  a  +  b ) \n       //* 6- \n      resultRDD . foreach ( x => println ( x ) )   //foreachRDD \n      println ( "=============================" ) \n      resultRDD . collect ( ) . foreach ( println ( _ ) ) // \n      println ( "=============================" ) \n      resultRDD . saveAsTextFile ( "data/baseoutput/output-1" ) \n      sc . stop ( ) \n     } \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 Java[] \n \n \n 4-JavaWordCount[] \n \n \n 1-JavaWordcount \n \n \n 2- \n \n 1-SparkContent \n 2- \n 3-flatmap \n 4-map(word,1) \n 5-reduceByKey() \n 6- \n \n \n \n 3- \n \n \n \n \n      package   cn . itcast . sparkbase ; \n    \n     import   org . apache . spark . SparkConf ; \n     import   org . apache . spark . api . java . JavaPairRDD ; \n     import   org . apache . spark . api . java . JavaRDD ; \n     import   org . apache . spark . api . java . JavaSparkContext ; \n     import   scala . Tuple2 ; \n    \n     import   java . util . Arrays ; \n     import   java . util . List ; \n    \n     /**\n     * DESCRIPTION:\n     * 1-SparkContent\n     * 2-\n     * 3-flatmap\n     * 4-map(word,1)\n     * 5-reduceByKey()\n     * 6-\n     */ \n     public   class  _01SparkFirst  { \n         public   static   void   main ( String [ ]  args )   { \n             //1-SparkContent \n             SparkConf  conf  =   new   SparkConf ( ) . setAppName ( "_01SparkFirst" ) . setMaster ( "local[*]" ) ; \n             JavaSparkContext  jsc  =   new   JavaSparkContext ( conf ) ; \n             //2- \n             JavaRDD < String >  fileRDD  =  jsc . textFile ( "data/baseinput/words.txt" ) ; \n             //3-flatmap() \n             JavaRDD < String >  flatMapRDD  =  fileRDD . flatMap ( x  ->   Arrays . asList ( x . split ( "\\\\s+" ) ) . iterator ( ) ) ; \n             //4-map(word,1)() \n             JavaPairRDD < String ,   Integer >  mapRDD  =  flatMapRDD . mapToPair ( x  ->   new   Tuple2 < > ( x ,   1 ) ) ; \n             //5-reduceByKey() \n             JavaPairRDD < String ,   Integer >  resultRDD  =  mapRDD . reduceByKey ( ( a ,  b )   ->  a  +  b ) ; \n             //6- \n             List < Tuple2 < String ,   Integer > >  collectResult  =  resultRDD . collect ( ) ; \n            collectResult . forEach ( System . out :: println ) ; \n             // \n            jsc . stop ( ) ; \n         } \n     } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n \n 1 \n \n setAppNamethis.getClass.getSimpleName.stripSuffix("$") \n \n \n \n 2 \n \n \n     //WebUIDAG\n    Thread.sleep(100*1000)\n \n 1 2 \n \n \n \n \n 3 \n \n SparkDriverExecutor \n \n Driverdag \n Executortask \n \n PC61212 \n \n \n \n JobDAG \n \n DriverExecutor \n  \n \n \n  \n \n \n  \n \n \n  \n \n \n 1-pompackage \n \n package \n \n \n \n 2- \n \n  \n \n \n \n 3- \nSpark Yarn-client \n    bin/spark-submit  \\ \n     --master   yarn   \\ \n    --deploy-mode client  \\ \n     --class  cn.itcast.sparkbase._01SparkWordCountTar  \\ \n    /export/data/spark-base_2.11-1.0.0.jar  \\ \n    hdfs://node1.itcast.cn:8020/wordcount/input  \\ \n    hdfs://node1.itcast.cn:8020/wordcount/output-8\n \n 1 2 3 4 5 6 7 8 \n \n  \n \n \n \n \n \n  \n SparkSubmit \n \n  \n \n   YARN\n   --master  MASTER_URL   spark://host:port,host1:port, mesos://host:port,  yarn \n                              k8s://https://host:port, or  local   ( Default: local [ * ] ) .\n  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally  ( "client" )  or\n                              on one of the worker machines inside the cluster  ( "cluster" ) \n                               ( Default: client ) .\n   --class  CLASS_NAME          Your application\'s main class  ( for Java / Scala apps ) .\n   --jars  JARS                 Comma-separated list of jars to include on the driverand executor classpaths.\n   --conf   PROP = VALUE           Arbitrary Spark configuration property.\nDriverExecutor \nDriver \n  --driver-memory MEM         Driver1G Memory  for  driver  ( e.g. 1000M, 2G )   ( Default: 1024M ) .\n  --driver-cores NUM          Number of cores used by the driver, only  in  cluster mode ( Default:  1 ) .\n                              localdriver-coreslocal [ * ] \nExecutor \n  --num-executors NUM         Number of executors to launch  ( Default:  2 ) .executors2\n                              If dynamic allocation is enabled, the initial number of\n                              executors will be at least NUM.\n  --executor-memory MEM       Executor1G  Memory per executor  ( e.g. 1000M, 2G )   ( Default: 1G ) .\n  --executor-cores NUM        executorcoresyarn1 \n                          Number of cores per executor.  ( Default:  1   in  YARN mode,\n                          or all available cores on the worker  in  standalone mode ) \n   --queue  QUEUE_NAME          The YARN queue to submit to  ( Default:  "default" ) .\n  \n  Executor\n  cpu-cores ( cpu-cores1cpucores ) \n  bin/spark-submit  \\ \n   --master   yarn   \\ \n   --deploy_mode  cluster  \\ \n  --driver-memory 2g  \\  \n  --driver-cores  2   \\ \n  --num-executors  10   \\ \n  --executor-memory 2g  \\ \n  --executor-cores  3   \\ \n   --class  cn.itcast.apple.mainclass  \\ \n  jar  \\ \n  \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 \n IDEAHDFS \n \n \n IDEAHDFS \n \n \n  \n \n 1-Hadoop \n 2-core-site.xmlhdfs-site.xml() \n 3-moduleresource \n 4- \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n groupbykeywordcount \n \n \n \n \n \n top \n \n \n \n \n \n  \n \n \n    package   cn . itcast . sparkbase \n  \n   import   org . apache . spark . rdd . RDD\n   import   org . apache . spark . { SparkConf ,  SparkContext } \n  \n   /**\n   * DESC:\n   * 1-SparkContextSparkConf\n   * 2-RDD\n   * 3-RDD\n   * 4-HDFS\n   */ \n   object  _01SparkWordCount  { \n     def  main ( args :  Array [ String ] ) :   Unit   =   { \n       //1-SparkContextSparkConf \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  sc  =   new  SparkContext ( conf ) \n      sc . setLogLevel ( "WARN" ) \n       //2-RDD \n       val  fileRDD :  RDD [ String ]   =  sc . textFile ( "hdfs://node1:8020/wordcount/input/" ) \n       //fileRDD.foreach(println(_)) \n       //3-RDD \n       val  flatMapRDD :  RDD [ String ]   =  fileRDD\n         . filter ( line  =>  line  !=   null   &&  line . trim . length  >   0 ) \n         . flatMap ( _ . split ( "\\\\s+" ) ) \n       val  resultRDD :  RDD [ ( String ,   Int ) ]   =  flatMapRDD . map ( ( _ ,   1 ) ) . groupByKey ( ) . map ( x  => ( x . _1 , x . _2 . sum )   ) \n       //Driver-all the data is loaded into the driver\'s memory. \n       val  resultArray :  Array [ ( String ,   Int ) ]   =  resultRDD . collect ( ) \n      println ( resultArray . toBuffer )   // \n       //Buffer((hello,4), (me,3), (you,2), (her,1)) \n       //4-,takeactionrddTransformation \n      println ( "sotyby oprations is:================" ) \n      resultRDD . sortBy ( _ . _2 , true ) . take ( 3 ) . foreach ( println ( _ ) ) \n      println ( "sotybykey oprations is:================" ) \n       //Take the first num elements of the RDD. \n      resultRDD . map ( _ . swap ) . sortByKey ( true ) . take ( 3 ) . foreach ( println ( _ ) ) \n      println ( "top oprations is:================" ) \n       //Returns the top k (largest) elements from this RDD \n      resultRDD . top ( 3 ) ( Ordering . by ( x => x . _2 ) ) . foreach ( println ( _ ) ) \n       //4-HDFS \n       //flatMapRDD.foreach(println(_)) \n      sc . stop ( ) \n     } \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 #  SparkCore--RDD \n  \n \n \n IDEAHDFS \n \n \n RDD \n \n \n RDD--[] \n \n \n RDD \n \n \n RDD \n \n \n RDD \n \n \n RDD[] \n RDD \n RDD*(Resilient Distributed Dataset)*   \n \n \n  \n \n \n  \n \n \n  \n \n \n RDD \n \n \n RDDpartition \n \n \n  \n \n \n \n \n \n  \n RDD \n \n 1-RDD \n 2-RDD \n 3-RDD \n 4-RDDHash-Partitioner \n 5- \n \n \n \n WordCount \n 1- \n \n 2- \n \n \n  \n \n \n RDD \n \n \n  \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n IDEA \n \n \n \n \n \n \n \n \n makerddparallisetotalcpucores2 \n \n \n makerddparallise \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n spark.default.parallelism \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n textFile \n \n \n \n \n \n \n \n \n \n \n \n Sparkrdd \n \n \n 1-userid-moviesid-rating-timestamp \n \n \n \n \n \n 2-sc.textFile \n \n \n Sparksc.wholetextFile \n \n \n package   cn . itcast . sparkbase . rddopration \n\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . { SparkConf ,  SparkContext } \n\n /**\n * DESC:\n */ \n object  _02SparkRDDFirst  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1- \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setMaster ( "local[*]" ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . set ( "spark.default.parallelism" ,   "4" ) \n     val  sc  =   new  SparkContext ( conf ) \n     //2- \n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/words.txt" ) \n     //sc.parallelize(1 to 10) \n     //3- \n    println ( "partitons length:"   +  fileRDD . getNumPartitions )   //2 \n    println ( s "partiiton length is: ${ fileRDD . partitions . length } " )   //2 \n     // \n     //textFile \n     val  filesRDD1 :  RDD [ String ]   =  sc . textFile ( "data/baseinput/ratings100/" ) \n    println ( "partitons length:"   +  filesRDD1 . getNumPartitions )   //100 \n     //Spark \n     val  filesRDD2 :  RDD [ ( String ,   String ) ]   =  sc . wholeTextFiles ( "data/baseinput/ratings100/" ) \n     //println("partitons length:" + filesRDD2.getNumPartitions) //2 \n    filesRDD2 . take ( 3 ) . foreach ( println ( _ ) ) \n     //23106,5214,3.0,1043445719 \n\n    sc . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \n \n  \n \n \n \n \n \n \n \n \n \n textFilepartition3xxx.logpartition(3partition=3)hadoopgoalsize->splitsize->1.1split \n \n \n textFile \n \n \n \n \n \n wholeTextFilesmaxSplitSize-> \n \n \n wholeTextFilehttps://blog.csdn.net/m0_37817767/article/details/125779863 \n \n \n \n spark wholeTextFilemapreduceCombineTextInputFormat \n https://blog.csdn.net/qq_35241080/article/details/106065442 \n RDD \n \n Transmormation \n Actionactionrdddriver \n RDDTransformation \n \n \n RDDValueRDD \n \n \n RDDValueRDD \n \n \n RDDKeyvalueRDD \n \n \n RDDValue \n map \n \n 1-funRDD \n 2- \n \n \n filter \n \n 1-funRDD \n 2- \n \n \n flatMap \n \n 1- \n 2- \n \n \n mapPartitions \n \n 1- \n \n 2- \n \n mapParittionWithIndex \n \n 1- \n \n 2- \n \n sample \n \n 1- \n \n seed \n 2- \n \n \n glom \n \n 1- \n 2- \n \n \n sortBy \n \n 1-keyvalue \n 2- \n \n \n coalese \n \n Shuffletrue \n 1- \n \n 2- \n \n \n repartition \n \n \n 1-coalesce(numPartitions,shuffle=true),coalesce \n \n \n 2- \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n repartitionAndSortWithinPartitions , \n \n \n RDDValue \n  \n union \n intersection \n distinct \n subtract \n \n zip \n  \n \n \n RDDKeyValue \n partitionBy \n \n 1- \n \n 2- \n \n \n ****Spark****sc.makerddHashRange[0,100]Range1018(10,20] \n \n reduceByKey \n \n 1-shuffleshuffle \n \n 2- \n \n \n groupByKey \n \n \n 1- \n \n \n \n \n \n 2- \n \n \n \n \n \n  \n \n \n \n reduceByKeykeyshufflecombineRDD[k,v]. \n groupByKeykeyshuffle \n reduceByKeygroupByKey \n \n \n \n GroupByKey---GroupByKey \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n package   cn . itcast . sparkbase . base \n\n import   org . apache . spark . rdd . { RDD ,  ShuffledRDD } \n import   org . apache . spark . { Aggregator ,  SparkConf ,  SparkContext } \n\n import   scala . collection . mutable . ArrayBuffer\n\n /**\n * DESC:\n */ \n object  _01groupByKeyOperation  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //* 1-SparkContext \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n     val  sc :  SparkContext  =   new  SparkContext ( conf ) \n     //* 2RDD \n     val  filerdd :  RDD [ String ]   =  sc . makeRDD ( Array ( "hello you" ,   "hello me hello she" ,   "hello spark" ) ) \n     val  flatmapRDD :  RDD [ String ]   =  filerdd . flatMap ( _ . split ( "\\\\s+" ) ) \n     val  mapRDD :  RDD [ ( String ,   Int ) ]   =  flatmapRDD . map ( x  =>   ( x ,   1 ) ) \n     //* 3groupBy \n     val  groupRDD1 :  RDD [ ( String ,  Iterable [ Int ] ) ]   =  mapRDD . groupByKey ( ) \n    groupRDD1 . collect ( ) . foreach ( println ( _ ) ) \n     //(me,CompactBuffer(1)) \n     //(spark,CompactBuffer(1)) \n     //(you,CompactBuffer(1)) \n     //(she,CompactBuffer(1)) \n     //(hello,CompactBuffer(1, 1, 1, 1)) \n     //CompactBuffersparkarraybuffer \n     //gropuByKeykeyvalue \n     //1-CompactBufferArrayBuffer \n     //val createCombiner = (v: V) => CompactBuffer(v) \n     //val mergeValue = (buf: CompactBuffer[V], v: V) => buf += v \n     //val mergeCombiners = (c1: CompactBuffer[V], c2: CompactBuffer[V]) => c1 ++= c2 \n     /*\n    key value valuearraybuffer\n     */ \n     val  createCombiner  =   ( v :   Int )   =>  ArrayBuffer ( v ) \n     val  mergeValue  =   ( buf :  ArrayBuffer [ Int ] ,  v :   Int )   =>  buf  +=  v\n     val  mergeCombiners  =   ( c1 :  ArrayBuffer [ Int ] ,  c2 :  ArrayBuffer [ Int ] )   =>  c1  ++ =  c2\n     val  valueRDD :  ShuffledRDD [ String ,   Int ,  ArrayBuffer [ Int ] ]   =   new  ShuffledRDD [ String ,   Int ,  ArrayBuffer [ Int ] ] ( mapRDD ,   new  org . apache . spark . HashPartitioner ( 4 ) ) \n       . setAggregator ( new  Aggregator ( \n        createCombiner , \n        mergeValue , \n        mergeCombiners ) ) \n       . setMapSideCombine ( false ) \n    println ( "source code create groupBykey" ) \n    valueRDD . foreach ( println ( _ ) ) \n    sc . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 reduceByKeyfoldByKeyaggregateByKeycombineByKey  \n combineByKeyWithClassTag \n reduceByKey:  key  \nFoldByKey:  key  \nAggregateByKey key  \nCombineByKey: \n CombineByKey combineByKeyWithClassTag\n This  method is here  for  backward  compatibility .  It  does not provide combiner classtag information  to   the  shuffle . \njvm T  K , U , V  ,  Java  JVM  . \n  1.  : ClassTag   scala . reflect . ClassTag  \n 2.  : Manifest \n \n 1 2 3 4 5  \nFoldByKeykeynkeyn \n \n aggreateByKey  \n reduceByKeyaggregateByKey \n To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.\n \n 1 \n combineByKey \n \n \n \n sortByKey \n  sortBy \nrdd. sortBy( _._2 ,false). foreach(println) //sortByforeach \n RangePartitioner shuffle \n \n \n 1- \n \n 2- \n \n \n join \n \n 1- \n \n 2- \n \n \n cogroup \n \n 1- \n \n 2- \n \n \n cartisian \n \n 1- \n \n 2- \n \n \n mapvalue \n \n 1- \n Value \n 2- \n RDDAction \n \n \n reduce \n \n \n collectdriverforeachsaveasTextFile \n \n \n count \n \n \n first \n \n \n takecollect \n \n \n \n \n \n takeSample \n \n \n \n \n \n takeorder \n \n \n aggreate \n \n \n fold \n \n \n \n \n \n countByKey \n \n \n \n \n \n foreach \n \n \n rdd.foreachExecutorExecutorforeachList(3,4)value.collect().foreach(println)printDriverDriver \n \n \n \n RDD \n \n \n \n \n \n RDD \n package   cn . itcast . sparkbase . base \n\n import   org . apache . spark . { SparkConf ,  SparkContext ,  TaskContext } \n import   org . junit . Test\n\n /**\n * DESC:\n */ \n class  _02RDDTest  { \n\n   private   val  sc  =   new  SparkContext ( new  SparkConf ( ) . setAppName ( "_02RDDTest" ) . setMaster ( "local[*]" ) ) \n\n   @Test \n   def  test01 ( ) :   Unit   =   { \n    sc . parallelize ( Seq ( 1 ,   2 ,   3 ,   4 ,   5 ) ) \n       . map ( x  =>  x  *   2 ) \n       . foreach ( println ( _ ) ) \n\n    sc . parallelize ( Seq ( 1 ,   2 ,   3 ,   4 ,   5 ) ) \n       . filter ( x  =>  x  >   3 ) \n       . foreach ( println ( _ ) ) \n\n   } \n\n   @Test \n   def  test02 :   Unit   =   { \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       . map ( x  =>   ( x  *   2 ) ) \n       . foreach ( println ( _ ) ) \n     //foreachiteratale \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       //f: Iterator[T] => Iterator[U], \n       . mapPartitions ( iter  =>   { \n        iter . foreach ( println ( _ ) ) \n        iter\n       } ) . collect ( ) \n     //2 \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       //f: Iterator[T] => Iterator[U], \n       . mapPartitions ( iter  =>   { \n         val  iterator :  Iterator [ Int ]   =  iter . map ( item  =>  item  *   2 ) \n        iterator\n       } ) . collect ( ) . foreach ( println ( _ ) ) \n\n    println ( "" ) \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       //f: Iterator[T] => Iterator[U], \n       . mapPartitions ( iter  =>   { \n        iter . map ( item  =>  item  *   2 ) \n       } ) . collect ( ) . foreach ( println ( _ ) ) \n   } \n\n   @Test \n   def  test03 :   Unit   =   { \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       // f: (Int, Iterator[T]) => Iterator[U] \n       . mapPartitionsWithIndex ( ( index ,  iter )   =>   { \n        println ( "index:"   +  index ) \n        iter . map ( _  *   2 ) \n       } ) . collect ( ) . foreach ( println ( _ ) ) \n    println ( "" ) \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       // f: (Int, Iterator[T]) => Iterator[U] \n       . mapPartitionsWithIndex ( ( index ,  iter )   =>   { \n        iter . map ( x  =>   "index is:"   +  index  +   "\\tvalue is:"   +  x  *   2 ) \n       } ) . collect ( ) . foreach ( println ( _ ) ) \n    println ( "mapPartition" ) \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       . mapPartitions ( iter  =>   { \n        println ( "partitionID:" ,  TaskContext . getPartitionId ( ) ) \n        iter . map ( x  =>  x  *   2 ) \n       } ) . collect ( ) . foreach ( println ( _ ) ) \n\n    println ( "mapPartition," ) \n    sc . parallelize ( Array ( 1 ,   2 ,   3 ,   4 ,   5 ,   6 ) ,   2 ) \n       . mapPartitions ( iter  =>   { \n        iter . map ( x  =>   "partitionID:"   +  TaskContext . getPartitionId ( )   +   "\\tvalue is:"   +  x  *   2 ) \n       } ) . collect ( ) . foreach ( println ( _ ) ) \n     //mapPartition, \n     //partitionID:0  value is:2 \n     //partitionID:0  value is:4 \n     //partitionID:0  value is:6 \n     //partitionID:1  value is:8 \n     //partitionID:1  value is:10 \n     //partitionID:1  value is:12 \n   } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88  \n \n IDEAHDFS\n \n core-site.xmlhdfs-site.xml \n \n \n RDD\n \n RDD \n RDD \n RDD \n \n \n RDD--[]\n \n 1- \n 2- \n 3- \n 4-key-value \n 5- \n \n \n RDD\n \n  \n  \n \n \n RDD\n \n value\n \n map \n filter \n repartiton \n colasese \n glom \n \n \n value\n \n  \n distinct \n zip \n \n \n key-value\n \n groupByKeycombineBykey \n \n \n \n \n RDD\n \n collect \n take \n \n \n RDD[] \n \n 1:groupByKey(sum) \n \n \n \n \n \n    @Test \n   def  test01 ( ) :   Unit   =   { \n     val  value :  RDD [ ( String ,   Int ) ]   =  sc . parallelize ( Seq ( ( "a" ,   1 ) ,   ( "a" ,   1 ) ,   ( "b" ,   1 ) ) ) \n    value\n       . groupByKey ( ) \n       . collect ( ) \n       . foreach ( println ( _ ) ) \n     //(a,CompactBuffer(1, 1)) \n     //(b,CompactBuffer(1)) \n    value\n       . groupByKey ( ) \n       . map ( x  =>   ( x . _1 ,  x . _2 . sum ) ) \n       . collect ( ) \n       . foreach ( println ( _ ) ) \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 2:combineBykey() \n \n \n \n    @Test \n   def  test02 :   Unit   =   { \n     val  rdd :  RDD [ ( String ,   Double ) ]   =  sc . parallelize ( Seq ( \n       ( "zhangsan" ,   99.0 ) , \n       ( "zhangsan" ,   96.0 ) , \n       ( "lisi" ,   97.0 ) , \n       ( "lisi" ,   98.0 ) , \n       ( "zhangsan" ,   97.0 ) ) \n     ) \n     // createCombiner: Value => C , \n     // mergeValue: (C, Value) => C,value \n     // mergeCombiners: (C, C) => C, \n     val  createCombiner  =   ( curr :   Double )   =>   ( curr ,   1 ) \n     val  mergeValue  =   ( curr :   ( Double ,   Int ) ,  nextValue :   Double )   =>   ( curr . _1  +  nextValue ,  curr . _2  +   1 ) \n     val  mergeCombiners  =   ( curr1 :   ( Double ,   Int ) ,  agg1 :   ( Double ,   Int ) )   =>   ( curr1 . _1  +  agg1 . _1 ,  curr1 . _2  +  agg1 . _2 ) \n     val  valueRDD :  RDD [ ( String ,   ( Double ,   Int ) ) ]   =  rdd . combineByKey ( createCombiner ,  mergeValue ,  mergeCombiners ) \n  \n    valueRDD . foreach ( println ( _ ) ) \n     // x._1      x._2._1 \n     //(zhangsan,(292.0,3)) \n     //(lisi,(195.0,2)) \n    valueRDD . map ( x  =>   ( x . _1 ,  x . _2 . _1  /  x . _2 . _2 ) ) . foreach ( println ( _ ) ) // \n     //(zhangsan,(292.0,3)) \n     //(zhangsan,97.33333333333333) \n     //(lisi,97.5) \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 3:foldByKey \n \n \n \n \n \n    @Test \n   def  test03 :   Unit   =   { \n     // \n    sc . parallelize ( Seq ( ( "a" ,   1 ) ,   ( "a" ,   1 ) ,   ( "b" ,   1 ) ) ) \n       . aggregateByKey ( 0 ) ( _  +  _ ,  _  +  _ ) \n       . collect ( ) \n       . foreach ( println ( _ ) ) \n  \n    sc . parallelize ( Seq ( ( "a" ,   1 ) ,   ( "a" ,   1 ) ,   ( "b" ,   1 ) ) ) \n       . foldByKey ( 0 ) ( _  +  _ ) \n       . collect ( ) \n       . foreach ( println ( _ ) ) \n  \n    sc . parallelize ( Seq ( ( "a" ,   1 ) ,   ( "a" ,   1 ) ,   ( "b" ,   1 ) ) ) \n       . foldByKey ( 0 ) ( ( a ,  b )   =>  a  +  b ) \n       . collect ( ) \n       . foreach ( println ( _ ) ) \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \n collectAsMap \n \n    @Test \n   def  test04 ( ) :   Unit   = { \n     val  rdd  =  sc . parallelize ( List ( ( "a" ,   1 ) ,   ( "a" ,   3 ) ,   ( "b" ,   2 ) ) ) \n    rdd . collectAsMap ( ) . foreach ( println ( _ ) ) \n   } \n \n 1 2 3 4 5 \n randomSplit \n \n    @Test \n   def  test04 ( ) :   Unit   = { \n     val  rdd  =  sc . parallelize ( List ( ( "a" ,   1 ) ,   ( "a" ,   3 ) ,   ( "b" ,   2 ) ) ) \n    rdd . collectAsMap ( ) . foreach ( println ( _ ) ) \n     //Randomly splits this RDD with the provided weights. \n     //weights: Array[Double], \n     //seed: Long = Utils.random.nextLong \n     val  array :  Array [ RDD [ ( String ,   Int ) ] ]   =  rdd . randomSplit ( Array ( 0.6 ,   0.4 ) ,   123L ) \n     val  traingSet :  RDD [ ( String ,   Int ) ]   =  array ( 0 ) \n     val  testSet :  RDD [ ( String ,   Int ) ]   =  array ( 1 ) \n    println ( "=======================" ) \n    traingSet . collect ( ) . foreach ( println ( _ ) ) \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 \n join \n \n    @Test \n   def  testJoin ( ) :   Unit   = { \n     //  \n     val  empRDD :  RDD [ ( Int ,   String ) ]   =  sc . parallelize ( \n      Seq ( ( 1001 ,   "zhangsan" ) ,   ( 1002 ,   "lisi" ) ,   ( 1003 ,   "wangwu" ) ,   ( 1004 ,   "zhangliu" ) ) \n     ) \n     val  deptRDD :  RDD [ ( Int ,   String ) ]   =  sc . parallelize ( \n      Seq ( ( 1001 ,   "sales" ) ,   ( 1002 ,   "tech" ) ) \n     ) \n    empRDD . join ( deptRDD ) . foreach ( println ( _ ) ) \n    println ( "============" ) \n    empRDD . leftOuterJoin ( deptRDD ) . foreach ( println ( _ ) ) \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13  \n \n 1SparkPV-UV-TOPK \n 2Spark \n 3Spark \n RDD \n RDD \n RDDDAG \n SparkShuffle \n Spark \n spark \n  \n 1SparkPVUVTOPN \n \n PVUVTOPN \n \n   \n  127.0.0.1 - - [28/Nov/2019:08:37:25 +0800] "GET / HTTP/1.1" 200 57621 "-" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36"\n  1pvpage viewPVPVPV\n  \n  2uvunique view\n \n 1 2 3 4 5 \n \n \n  \n \n PV-PageView \n 1- \n 2-mapPV \n 3-reduceByKey(AggerateByKeyFoldByKey) \n 4-PV \n 5- \n UV-UserView \n 1- \n 2-mapIP \n 3-distinct \n 4-reduceByKeykeyvalue \n 5-UV \n 6-saveAsTextFile \n TopK- \n 1- \n 2-mapURLAPPX(10) \n 3- \n 4-TopK \n 5-saveAsTextFile \n \n \n \n  \n \n \n    package   cn . itcast . sparkbase . pro \n  \n   import   org . apache . spark . rdd . RDD\n   import   org . apache . spark . { SparkConf ,  SparkContext } \n  \n   /**\n   * DESC:\n   * PV-PageView\n   * 1-\n   * 2-mapPV\n   * 3-reduceByKey(AggerateByKeyFoldByKey)\n   * 4-PV\n   * 5-\n   * UV-UserView\n   */ \n   object  _01SparkPvUvTopK  { \n     def  main ( args :  Array [ String ] ) :   Unit   =   { \n       // PV-PageView \n       val  sc :  SparkContext  =   { \n         val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n         val  sc  =   new  SparkContext ( conf ) \n        sc\n       } \n       // 1- \n       val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/access.log" ) \n       //fileRDD.take(5).foreach(println(_)) \n       //194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] "GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1" 304 0 "-" "Mozilla/4.0 (compatible;)" \n       // 2-mapPV \n       val  mapRDD :  RDD [ ( String ,   Int ) ]   =  fileRDD . map ( x  =>   ( "PV" ,   1 ) ) \n       //mapRDD.take(3).foreach(println(_)) \n       // 3-reduceByKey(AggerateByKeyFoldByKey) \n       val  resuleRDD :  RDD [ ( String ,   Int ) ]   =  mapRDD . reduceByKey ( _  +  _ ) \n       // 5- \n      resuleRDD . foreach ( println ( _ ) )   //(PV,14619) \n       // UV-UserView \n       // 1- - \n       // 2-mapIP \n       val  ipValue :  RDD [ String ]   =  fileRDD . map ( x  =>  x . split ( "\\\\s+" ) ) . map ( x  =>  x ( 0 ) ) \n       // 3-distinct \n       val  uvValue :  RDD [ ( String ,   Int ) ]   =  ipValue . distinct ( ) . map ( x  =>   ( "UV" ,   1 ) ) \n       // 4-reduceByKeykeyvalue \n       val  uvResult :  RDD [ ( String ,   Int ) ]   =  uvValue . reduceByKey ( _  +  _ ) \n      uvResult . foreach ( println ( _ ) )   //(UV,1050) \n       // 5-UV \n       //TopK- \n       //1--- \n       //2-mapURLAPPX(10) \n       val  value1RDD :  RDD [ ( String ,   Int ) ]   =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . filter ( x  =>  x . length  >   10 ) . map ( x  =>   ( x ( 10 ) ,   1 ) ) \n       //value1RDD.take(20).foreach(println(_)) \n       //3- \n       val  result1RDD :  RDD [ ( String ,   Int ) ]   =  value1RDD . reduceByKey ( _  +  _ ) . sortBy ( x  =>  x . _2 ,   false ) . filter ( x  =>  x . _1  !=   "\\"-\\"" ) \n       //sortBysortBykeycollectdriver \n      result1RDD . take ( 5 ) . foreach ( println ( _ ) ) \n       //("http://blog.fens.me/category/hadoop-action/",547) \n       //("http://blog.fens.me/",377) \n       //("http://blog.fens.me/wp-admin/post.php?post=2445&action=edit&message=10",360) \n       //("http://blog.fens.me/r-json-rjson/",274) \n       //("http://blog.fens.me/angularjs-webstorm-ide/",271) \n       //4-TopK \n      sc . stop ( ) \n     } \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 \n \n  \n \n RDD \n 2Spark \n \n \n  \n \n ipip \n \n \n \n  \n \n \n \n \n \n IPIP \n \n \n \n \n     20090121000132095572000|125.213.100.12|show.51.com|/\n    \n \n 1 2 \n IPIP \n \n     1.0.1.0|1.0.3.255|16777472|16778239|||||||350100|China|CN|119.306239|26.075302\n \n 1 \n \n 125.213.100.12  1.0.1.0|1.0.3.255   \n \n \n 1.0.1.0|1.0.3.255Long16777472|16778239 \n \n 125.213.100.12 LongIP.txt \n   \n \n \n \n \n IPIP \n \n \n \n \n \n \n \n \n  \n \n 1-SparkContext \n 2-ip \n 3-IPIP\n \n 3-1iplong \n 3-2longipip \n 3-3 \n \n \n 4- \n \n \n \n  \n \n \n    package   cn . itcast . sparkbase . pro \n  \n   import   org . apache . spark . broadcast . Broadcast\n   import   org . apache . spark . rdd . RDD\n   import   org . apache . spark . { SparkConf ,  SparkContext } \n  \n   /**\n   * DESC:\n   * 1-SparkContext\n   * 2-ip\n   * 3-IPIP\n   * 3-1iplong\n   * 3-2longipip\n   * 3-3\n   * 4-\n   */ \n   object  _05IPCheck  { \n   / \n     def  binarySearch ( ipLong :   Long ,  boradcastValue :  Array [ ( String ,   String ,   String ,   String ) ] ) :   Int   =   { \n       var  start  =   0 \n       var  end  =  boradcastValue . length  -   1 \n  \n       while   ( start  <=  end )   { \n         var  middle  =   ( start  +  end )   /   2 \n         //--startip \n         if   ( ipLong  >=  boradcastValue ( middle ) . _1 . toLong  &&  ipLong  <=  boradcastValue ( middle ) . _2 . toLong )   { \n           return  middle\n         } \n         // \n         if   ( ipLong  >  boradcastValue ( middle ) . _2 . toLong )   { \n          start  =  middle  +   1 \n         } \n         // \n         if   ( ipLong  <  boradcastValue ( middle ) . _1 . toLong )   { \n          end  =  middle  -   1 \n         } \n       } \n       - 1 \n     } \n  \n     def  ipToLong ( ip :   String ) :   Long   =   { \n       //:IP: \n       //10111111.10111010.11110000.11110000 \n       val  ipArr :  Array [ Int ]   =  ip . split ( "[.]" ) . map ( s  =>  Integer . parseInt ( s ) ) \n       var  ipnum  =   0L \n       for   ( i  <-  ipArr )   { \n         //<< ,0L0L,:00000000.00000000.00000000.00000000 \n         //,,0 \n         //|,,0, \n         //: \n         //00000000.00000000.00000000.10111111 \n         //00000000.00000000.00000000.00000000 \n         //00000000.00000000.00000000.10111111 \n         //: \n         //00000000.00000000.00000000.10111010 \n         //00000000.00000000.10111111.00000000 \n         //00000000.00000000.10111111.10111010 \n         //: \n         //00000000.00000000.00000000.11110000 \n         //00000000.10111111.10111010.00000000 \n         //00000000.10111111.10111010.11110000 \n         //: \n         //0000 \n           0.00000000 .00000000 .11110000 \n         //10111111.10111010.11110000.00000000 \n         //10111111.10111010.11110000.11110000 \n        ipnum  =  i  |   ( ipnum  <<   8 ) \n       } \n      ipnum\n     } \n  \n     def  main ( args :  Array [ String ] ) :   Unit   =   { \n       //1-SparkContext \n       val  sc :  SparkContext  =   { \n         val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n         val  sc  =   new  SparkContext ( conf ) \n        sc\n       } \n       //2-ip \n       val  userIPRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/ip/20190121000132.394251.http.format" ) \n       val  userSplitRDD :  RDD [ String ]   =  userIPRDD . map ( _ . split ( "\\\\|" ) ) . map ( x  =>  x ( 1 ) ) \n       //userSplitRDD.take(5).foreach(println(_))//125.213.100.123 \n  \n       val  ipRangeRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/ip/ip.txt" ) \n       //223.247.0.0|223.247.7.255|3757506560|3757508607|||||||341700|China|CN|117.489157|30.656037 \n       //iplongiplong \n       val  ipRDD :  RDD [ ( String ,   String ,   String ,   String ) ]   =  ipRangeRDD . map ( _ . split ( "\\\\|" ) ) . map ( x  =>   ( x ( 2 ) ,  x ( 3 ) ,  x ( x . length  -   2 ) ,  x ( x . length  -   1 ) ) ) \n       //ipRDD.take(5).foreach(println(_))//(16777472,16778239,119.306239,26.075302) \n       val  broadcastIpValue :  Broadcast [ Array [ ( String ,   String ,   String ,   String ) ] ]   =  sc . broadcast ( ipRDD . collect ( ) ) \n       //broadcastIpValue.value// \n       //3-IPIP \n       val  resultRDD :  RDD [ ( ( String ,   String ) ,   Int ) ]   =  userSplitRDD . mapPartitions ( iter  =>   { \n         val  boradcastValue :  Array [ ( String ,   String ,   String ,   String ) ]   =  broadcastIpValue . value\n        iter . map ( ip  =>   { \n           //* 3-1iplong \n           val  ipLong :   Long   =  ipToLong ( ip ) \n           //* 3-2longipip \n           val  index :   Int   =  binarySearch ( ipLong ,  boradcastValue ) \n           //* 3-3 \n           ( ( boradcastValue ( index ) . _3 ,  boradcastValue ( index ) . _4 ) ,   1 ) \n         } ) \n       } )   //end mapParttion \n       //4- \n      resultRDD . reduceByKey ( _  +  _ ) . sortBy ( _ . _2 ,   false ) . take ( 5 ) . foreach ( println ( _ ) ) \n       //((108.948024,34.263161),1824) \n       //((116.405285,39.904989),1535) \n       //((106.504962,29.533155),400) \n       //((114.502461,38.045474),383) \n       //((106.57434,29.60658),177) \n      sc . stop ( ) \n     } \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 \n \n  \n \n iplong \n  \n  \n 3Spark \n \n \n  \n \n \n http://download.labs.sogou.com/dl/sogoulabdown/SogouQ/SogouQ.reduced.zip \n \n \n \n \n \n    /////  TFIDF Word2Vec \n \n \n HanLP \n \n \n \n \n      < dependency > \n         < groupId > com.hankcs </ groupId > \n         < artifactId > hanlp </ artifactId > \n         < version > portable-1.7.7 </ version > \n     </ dependency > \n \n 1 2 3 4 5 \n \n  \n \n \n  \n \n \n  \n \n \n  \n \n \n \n \n \n  \n \n sc.textFile,mapmappartitionmap(x=>x.split("\\\\s+") \n : \n ID(ID) \n  \n \n \n \n  \n \n \n 1- \n \n \n package   cn . itcast . sparkbase . pro \n\n import   java . util \n\n import   com . hankcs . hanlp . HanLP\n import   com . hankcs . hanlp . seg . common . Term\n import   com . hankcs . hanlp . tokenizer . StandardTokenizer\n\n /**\n * DESC:\n */ \n import   scala . collection . JavaConverters . _\n\n object  _02HanLp  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n\n     val  terms :  util . List [ Term ]   =  HanLP . segment ( "" ) \n    println ( terms )   //[/r, /v, /n, /n, /uj, /m, /q, /n] \n    println ( terms . asScala . map ( _ . word . trim ) )   //ArrayBuffer(, , , , , , , ) \n\n     val  terms1 :  util . List [ Term ]   =  StandardTokenizer . segment ( "" ) \n    println ( terms1 . asScala . map ( _ . word . trim ) ) \n\n     //00:00:00 2982199073774412 [360]  8 3  download.it.com.cn/softweb/software/firewall/antivirus/20067/17938.html \n     val  arr :  Array [ String ]   =   """00:00:00 2982199073774412 [360]  8 3  download.it.com.cn/softweb/software/firewall/antivirus/20067/17938.html""" . split ( "\\\\s+" ) \n    println ( arr ( 2 ) . replaceAll ( "\\\\[|\\\\]" ,   "" ) ) //360 \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \n  \n \n package   cn . itcast . sparkbase . pro \n\n import   java . util \n\n import   com . hankcs . hanlp . HanLP\n import   com . hankcs . hanlp . seg . common . Term\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . { SparkConf ,  SparkContext } \n\n import   scala . collection . JavaConverters . _\n import   scala . collection . mutable \n\n /**\n * DESC:\n * \n * 1-SparkContext\n * 2-\n */ \n object  _04SougouCount  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-SparkContext \n     val  sc :  SparkContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  sc  =   new  SparkContext ( conf ) \n      sc\n     } \n     //2- \n     val  sougouRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sougu/SogouQ.reduced" ) \n    println ( s "sougouRDD count value is: ${ sougouRDD . count ( ) } " )   //1724264 \n     //3- \n     val  recordRDD :  RDD [ SogouRecord ]   =  sougouRDD\n       //length \n       . filter ( line  =>  line  !=   null   &&  line . trim . split ( "\\\\s+" ) . length  ==   6 ) \n       . mapPartitions ( iter  =>   { \n        iter . map ( record  =>   { \n           val  arr :  Array [ String ]   =  record . split ( "\\\\s+" ) \n          SogouRecord ( arr ( 0 ) ,  arr ( 1 ) ,  arr ( 2 ) . replaceAll ( "\\\\[|\\\\]" ,   "" ) ,  arr ( 3 ) . toInt ,  arr ( 4 ) . toInt ,  arr ( 5 ) ) \n         } ) \n       } ) \n       // \n    recordRDD . take ( 5 ) . foreach ( println ( _ ) ) \n     //SogouRecord(00:00:00,2982199073774412,360,8,3,download.it.com.cn/softweb/software/firewall/antivirus/20067/17938.html) \n     //SogouRecord(00:00:00,07594220010824798,,1,1,news.21cn.com/social/daqian/2008/05/29/4777194_1.shtml) \n     //3- \n     val  valueRDD :  RDD [ String ]   =  recordRDD . mapPartitions ( iter  =>   { \n      iter . flatMap ( record  =>   { \n         val  terms :  util . List [ Term ]   =  HanLP . segment ( record . queryWords ) \n        terms . asScala . map ( _ . word . trim ) \n       } ) \n     } )   //end  mappartition \n     //valueRDD.take(5).foreach(println(_)) \n     val  keyWordCount :  RDD [ ( String ,   Int ) ]   =  valueRDD . map ( x  =>   ( x ,   1 ) ) . reduceByKey ( _  +  _ ) . sortBy ( _ . _2 ,   false ) \n    println ( "====================" ) \n    keyWordCount . take ( 5 ) . foreach ( println ( _ ) ) \n     //(+,193939) \n     //(,93246) \n     //(.,90985) \n     //(,88451) \n     //(,69662) \n    \n\n  sc . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 #  RDD \n  \n  \n \n \n  \n \n 1-RDD  RDDRDD \n 2-RDDRDD \n 3-RDD,shuffle \n \n RDDRDD \n \n \n SparkDAGRDDRDDSpark \n  \n \n \n \n \n \n \n \n \n \n  \n \n RDDRDD() \n RDDRDD \n \n \n \n  \n \n RDDRDD () \n RDDRDD \n \n \n \n shuffle \n \n shuffleshuffleRDDRDDRDDshuffle \n \n \n \n  \n \n RDDRDD \n shuffle,RDD (,shuffle.) \n RDD \n \n \n \n Spark \n \n \n  \n \n     \n    \n \n \n  \n \n CachePersist \n \n \n \n \n 1- \n 2- \n 3-(off_heapjvm) \n 4---- \n 5---- \n \n \n \n \n \n \n  \n \n \n \n \n \n cache \n persist \n unpersist \n shuffle \n \n \n \n \n \n HDFS \n checkpoint \n RDDCheckPoint \n \n checkpoint\n \n checkpointSpark   \n cachepersistHDFScheckpointHDFS \n \n \n \n \n Checkpoint  ,CheckpointHDFS, , ,cachepersist. \n \n \n checkpoint\n \n sc.setCheckpontDir(hdfs:///)RDDHDFS \n rdd1.checkpoint() rddHDFS \n \n \n checkpoint \n \n \n \n  \n \n \n \n \n checkpointhdfs \n \n Spark\n \n  \n 1-Sparkcachepersist \n 2-checkpointHDFS \n 3-RDDrdd1rdd2rdd3 \n  \n \n  \n \n \n \n \n \n  \n RDDDAG \n \n \n \n \n \n \n \n \n DAG \n \n SparkSpark \n \n \n \n DAG \n \n SparkImpalaFlinkDAG \n 4040DAG \n \n \n \n DAG Stage  \n \n Stages jobstage \n \n \n \n DAGStage \n \n Stage \n \n \n \n Spark \n spark \n  1~1012~19 \n 12driverDAGSchedluerTaskSchedulerTaskSchedulerResourceManager \n \n \n \n (Spark \n  Applicationtask \n Applicationtasktasktasktaskstagejobapplication \n task \n MapReduce \n Applicationjobtasktask \n  \n tasktaskApplication \n  \n  \n \n \n \n \n \n \n \n \n  \n \n \n 1-driverexecutorexecutor \n \n \n 2-BlockManagerDriver \n \n \n 3-sc.broadcast(map.collect) \n \n \n  \n \n \n package   cn . itcast . sparkbase . base \n\n import   org . apache . spark . broadcast . Broadcast\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . { SparkConf ,  SparkContext } \n\n /**\n * DESC:\n */ \n object  _04broadcast  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n\n     // \n     val  sc : SparkContext = { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  sc  =   new  SparkContext ( conf ) \n      sc\n     } \n     //RDD \n     val  kvFruit :  RDD [ ( Int ,   String ) ]   =  sc . parallelize ( List ( ( 1 , "apple" ) , ( 2 , "orange" ) , ( 3 , "banana" ) , ( 4 , "grape" ) ) ) \n     val  fruitMap :  collection . Map [ Int ,   String ]   = kvFruit . collectAsMap\n     //fruitMap.foreach(println(_)) \n     // \n     val  fruitsIds :  RDD [ Int ]   =  sc . parallelize ( Array ( 2 ,   4 ,   1 ,   3 ) ) \n    fruitsIds . map ( x => fruitMap ( x ) ) . collect ( ) . foreach ( println ( _ ) ) \n     //fruitMap \n     val  broadMap :  Broadcast [ collection . Map [ Int ,   String ] ]   =  sc . broadcast ( fruitMap ) \n    fruitsIds . map ( x => broadMap . value ( x ) ) . collect ( ) . foreach ( println ( _ ) ) \n     //orange \n     //grape \n     //apple \n     //banana \n    sc . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #   \n \n - \n \n \n Accumulator \n \n \n DriverDriverTask \n \n \n scala \n rdd \n  \n \n package   cn . itcast . sparkbase . base \n\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . util . LongAccumulator\n import   org . apache . spark . { Accumulator ,  SparkConf ,  SparkContext } \n\n /**\n * DESC:\n */ \n object  _05accumulate  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // \n     val  sc :  SparkContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  sc  =   new  SparkContext ( conf ) \n      sc\n     } \n     //scala \n     var  counter1  =   0 \n     val  seq  =  Seq ( 1 ,   2 ,   3 ) \n    seq . map ( x  =>  counter1  +=  x ) \n    println ( "counter result is:"   +  counter1 ) \n     //rdd-0--driverexecutordriver \n     var  counter2  =   0 \n     val  rdd1 :  RDD [ Int ]   =  sc . parallelize ( seq ) \n    rdd1 . foreach ( x  =>  counter2  +=  x ) \n    println ( counter2 ) \n     //driverexecutor \n     //action \n     val  acc :  Accumulator [ Int ]   =  sc . accumulator ( 0 ) \n    rdd1 . foreach ( x => acc += x ) \n    println ( acc ) \n     //sc.accumulator2.0sc.longAccumulator \n     val  acc_count :  LongAccumulator  =  sc . longAccumulator ( "acc_count" ) \n    rdd1 . foreach ( x => acc_count . add ( x ) ) \n    println ( acc_count ) //LongAccumulator(id: 51, name: Some(acc_count), value: 6) \n    println ( acc_count . value ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  \n Sparktransformactionlazyaccumulator \n actionvalue \n actionvalue5 \n actionvalue10 \n action510 \n  \nactioncachecheckpointcachepersist \n  \n \n \n  \n \n \n  \n \n \n  \n \n 1- \n 2-listmapexecutor \n 2- \n 3-wordcout \n \n \n \n  \n \n \n package   cn . itcast . sparkbase . base \n\n import   org . apache . spark . broadcast . Broadcast\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . util . LongAccumulator\n import   org . apache . spark . { SparkConf ,  SparkContext } \n\n /**\n * DESC:\n * * 1-\n * * 2-listmapexecutor\n * * 2-\n * * 3-wordcout\n */ \n object  _06acc_broadcast  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // \n     val  sc :  SparkContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  sc  =   new  SparkContext ( conf ) \n      sc\n     } \n     // 1- \n     // \n     val  list :  List [ String ]   =  List ( "," ,   "." ,   "!" ,   "#" ,   "$" ,   "%" ) \n     val  broadcastList :  Broadcast [ List [ String ] ]   =  sc . broadcast ( list ) \n     // \n     val  acc_count :  LongAccumulator  =  sc . longAccumulator ( "acc_count" ) \n\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/words1.txt" ) \n     val  wordscount :  RDD [ String ]   =  fileRDD\n       . filter ( line  =>  line  !=   null   &&  line . trim . length  >   0 ) \n       . flatMap ( line  =>  line . split ( "\\\\s+" ) ) \n       . filter ( word  =>   { \n         val  listValue :  List [ String ]   =  broadcastList . value\n         val  isFlag :   Boolean   =  listValue . contains ( word ) \n         if   ( isFlag )   { \n          acc_count . add ( 1L ) \n         } \n         ! isFlag\n       } ) \n     // 2-listmapexecutor \n     // 2- \n     // 3-wordcout \n    println ( "wordcount" ) \n    wordscount . map ( ( _ ,   1 ) ) . reduceByKey ( _  +  _ ) . collect ( ) . foreach ( println ( _ ) ) \n    println ( "" ,  acc_count . value ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \n \n \n \n \n Thread.sleepWebUi \n Kyro \n \n \n \n \n \n \n \n \n kyro \n \n \n \n \n \n \n \n \n kryo \n \n \n \n \n \n kryojava10xkryo \n \n \n \n \n \n  \n \n \n   1.   spark.serializerorg.apache.spark.serializer.KryoSerializerworkerRDD \n   2.   spark.kryoserializer.bufferExecutorcorebufferspark.kryoserializer.buffer.max \n   3.   spark.kryoserializer.buffer.maxbuffer \n   4.   spark.kryo.classesToRegisterKryo \n   5.   spark.kryo.referenceTrackingfalse \n   6.   spark.kryo.registrationRequiredKryotrue \n   7.   spark.kryo.registratorKryoKryo \n   8.   spark.kryo.unsafeKryo unsafe \n \n 1 2 3 4 5 6 7 8 kryo \n    //java+scalascalasrcpom \n   // \n   //KryoRegistratorconf \n  public  class  MyKryoRegistrator implements KryoRegistrator  { \n       @Override \n      public void registerClasses (  Kryo kryo )   { \n          kryo . register ( User . class ) ; \n          kryo . register ( User [ ] . class ) ; \n          kryo . register ( scala . collection . convert . Wrappers$ . class ) ; \n       } \n   } \n   @Data \n  public  class  User implements Serializable  { \n          int id ; \n           String  name ; \n           String  city ; \n          List < String >  hobby ; \n   } \n  \n   //  conf \n   val  conf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName ) \n        conf\n         . set ( "spark.serializer" ,   "org.apache.spark.serializer.KryoSerializer" ) \n         . set ( "spark.kryo.registrationRequired" ,   "true" ) \n         . set ( "spark.kryo.registrator" , classOf [ MyKryoRegistrator ] . getName ) \n      \n  \n       val  sc  =   new  SparkContext ( conf ) \n  \n   // \n   conf\n         . set ( "spark.serializer" ,   "org.apache.spark.serializer.KryoSerializer" ) \n         . registerKryoClasses ( Array ( classOf [ User ] ) ) \n  \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 kryo \n \n \n \n \n 2groupBy \n  \n \n \n 1 \n \n SparkFlinkFlinkSpark \n SparkCore \n \n 2 \n Sparkshuffle \n \n \n Sparkshuffle \n \n 1.2HashShuffleManager \n 1.2SortShuffleManager \n \n \n \n Shuffle \n \n shuffle writemapperstage \nshuffle read reducestagestage \n \n \n \n HashShuffleManager \n \n hashShuffleManager \n \n \n \n hashShuffleManager\n \n shuffleFileGroup \n \n \n \n \n \n \n \n \n \n \n \n \n SortShuffleManager \n \n \n \n \n \n bypassMerge \n \n shuffle write taskspark.shuffle.sort.bypassMergeThreshold(****200****) \n reduceByKeymap side combinetrue \n \n \n \n \n  \n \n \n \n \n \n 1(5M)shufflereduceByKeyshuffleMapMap;joinshuffleArray \n 2    \n  \n shuffle \n applyMemory=nowMenory*2-oldMemory \n =*2- \n *2- \n SparkExecutor5M5.02M5.02*2-5=5.04M \n  \n aSpark5MMRBuffer100M \n b start offset  end offset  \n 3 \n key \n 4 \n batch100001 Java BufferedOutputStreamBufferedOutputStreamJavaIO \n 5merge \n task 1mergetasktaskReducestagetasktaskstart offsetend offset \n SortShuffleManagermergestage50task10ExecutorExecutor5taskstage100tasktaskExecutor5Executor50 \n \n \n \n \n \n \n bypass \n ; \n shuffle write \n 2M(map task number) \n ****SortShufflebypass(5M)shuffle map taskspark.shuffle.sort.bypassMergeThresholdshuffle(reduceByKey)SortShufflebypassSortShufflebypass \n \n \n  \n \n \n \n \n \n Shuffle \n \n \n Shuffle\n\n def  distinct ( ) \n def  distinct ( numPartitions :   Int ) \n\n def  reduceByKey ( func :   ( V ,  V )   =>  V ,  numPartitions :   Int ) :  RDD [ ( K ,  V ) ] \n def  reduceByKey ( partitioner :  Partitioner ,  func :   ( V ,  V )   =>  V ) :  RDD [ ( K ,  V ) ] \n def  groupBy [ K ] ( f :  T  =>  K ,  p :  Partitioner ) : RDD [ ( K ,  Iterable [ V ] ) ] \n def  groupByKey ( partitioner :  Partitioner ) : RDD [ ( K ,  Iterable [ V ] ) ] \n def  aggregateByKey [ U :  ClassTag ] ( zeroValue :  U ,  partitioner :  Partitioner ) :  RDD [ ( K ,  U ) ] \n def  aggregateByKey [ U :  ClassTag ] ( zeroValue :  U ,  numPartitions :   Int ) :  RDD [ ( K ,  U ) ] \n def  combineByKey [ C ] ( createCombiner :  V  =>  C ,  mergeValue :   ( C ,  V )   =>  C ,  mergeCombiners :   ( C ,  C )   =>  C ) :  RDD [ ( K ,  C ) ] \n def  combineByKey [ C ] ( createCombiner :  V  =>  C ,  mergeValue :   ( C ,  V )   =>  C ,  mergeCombiners :   ( C ,  C )   =>  C ,  numPartitions :   Int ) :  RDD [ ( K ,  C ) ] \n def  combineByKey [ C ] ( createCombiner :  V  =>  C ,  mergeValue :   ( C ,  V )   =>  C ,  mergeCombiners :   ( C ,  C )   =>  C ,  partitioner :  Partitioner ,  mapSideCombine :   Boolean   =   true ,  serializer :  Serializer  =   null ) :  RDD [ ( K ,  C ) ] \n\n def  sortByKey ( ascending :   Boolean   =   true ,  numPartitions :   Int   =   self . partitions . length ) :  RDD [ ( K ,  V ) ] \n def  sortBy [ K ] ( f :   ( T )   =>  K ,  ascending :   Boolean   =   true ,  numPartitions :   Int   =   this . partitions . length ) ( implicit  ord :  Ordering [ K ] ,  ctag :  ClassTag [ K ] ) :  RDD [ T ] \n\n def  coalesce ( numPartitions :   Int ,  shuffle :   Boolean   =   false ,  partitionCoalescer :  Option [ PartitionCoalescer ]   =  Option . empty ) \n def  repartition ( numPartitions :   Int ) ( implicit  ord :  Ordering [ T ]   =   null ) \n\n def  intersection ( other :  RDD [ T ] ) :  RDD [ T ] \n def  intersection ( other :  RDD [ T ] ,  partitioner :  Partitioner ) ( implicit  ord :  Ordering [ T ]   =   null ) :  RDD [ T ] \n def  intersection ( other :  RDD [ T ] ,  numPartitions :   Int ) :  RDD [ T ] \n def  subtract ( other :  RDD [ T ] ,  numPartitions :   Int ) :  RDD [ T ] \n def  subtract ( other :  RDD [ T ] ,  p :  Partitioner ) ( implicit  ord :  Ordering [ T ]   =   null ) :  RDD [ T ] \n def  subtractByKey [ W :  ClassTag ] ( other :  RDD [ ( K ,  W ) ] ) :  RDD [ ( K ,  V ) ] \n def  subtractByKey [ W :  ClassTag ] ( other :  RDD [ ( K ,  W ) ] ,  numPartitions :   Int ) :  RDD [ ( K ,  V ) ] \n def  subtractByKey [ W :  ClassTag ] ( other :  RDD [ ( K ,  W ) ] ,  p :  Partitioner ) :  RDD [ ( K ,  V ) ] \n def  join [ W ] ( other :  RDD [ ( K ,  W ) ] ,  partitioner :  Partitioner ) :  RDD [ ( K ,   ( V ,  W ) ) ] \n def  join [ W ] ( other :  RDD [ ( K ,  W ) ] ) :  RDD [ ( K ,   ( V ,  W ) ) ] \n def  join [ W ] ( other :  RDD [ ( K ,  W ) ] ,  numPartitions :   Int ) :  RDD [ ( K ,   ( V ,  W ) ) ] \n def  leftOuterJoin [ W ] ( other :  RDD [ ( K ,  W ) ] ) :  RDD [ ( K ,   ( V ,  Option [ W ] ) ) ] \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \n \n *spark.shuffle.file.buffer*   32K64K \n \n \n *spark.reducer.maxSizeInFlight*   48M \n \n \n *spark.shuffle.io.maxRetries*   \n \n \n ****spark.shuffle.io.retryWait**** \n \n \n ArrayMap5M \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n SparkSQL \n \n SparkCoreSparkSQL \n SparkSQLHive \n sparksql \n SparkSQLHive() \n \n \n SparkSQLHive: \n \n \n Hive \n \n \n \n \n \n Shark \n \n \n HQL on MapRedduce -> \n HQL on MapReduce -> HQL on Spark \n \n \n SparkSQL \n \n \n \n \n \n  \n SparkSQL \n \n \n SparkCoreRDD \n \n \n SparkSQLDataFrameDataSet \n \n \n  \n \n \n \n 2.0DataFrame = DataSet[Row] \n DataFrameDatasetDataSet \n \n \n SparkSQLDataFrame \n \n SparkSession=sqlcontext+hivecontext+sparkcontext \n \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . sql . { Dataset ,  SparkSession } \n\n /**\n * DESC:\n * SparkSession\n */ \n object  _01SparkSession  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //SparkSession \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     // \n     val  sc :  SparkContext  =  spark . sparkContext\n    sc . setLogLevel ( "WARN" ) \n     // \n     val  valueDS :  Dataset [ String ]   =  spark . read . textFile ( "data/baseinput/words.txt" ) \n    println ( "counts:"   +  valueDS . count ( ) ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \n \n RDDDF \n \n \n 1-RDDDF \n \n \n \n \n \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n\n /**\n * DESC:\n */ \n case   class  People ( id :   Int ,  name :   String ,  age :   Int ) \n\n object  _02toDFWay1  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //For implicit conversions from RDDs to DataFrames \n     import   spark . implicits . _\n     // \n     val  sc :  SparkContext  =  spark . sparkContext\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sql/people1.txt" ) \n     //person \n     val  peopleRDD :  RDD [ People ]   =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . map ( x  =>  People ( x ( 0 ) . toInt ,  x ( 1 ) ,  x ( 2 ) . toInt ) ) \n     //df \n     val  peopleDF :  DataFrame  =  peopleRDD . toDF ( ) \n     // \n    peopleDF . show ( ) \n     //+---+--------+---+ \n     //| id|    name|age| \n     //+---+--------+---+ \n     //|  1|zhangsan| 20| \n     //|  2|    lisi| 29| \n     //|  3|  wangwu| 25| \n     //scheme \n    peopleDF . printSchema ( ) \n     //root \n     // |-- id: integer (nullable = false) \n     // |-- name: string (nullable = true) \n     // |-- age: integer (nullable = false) \n     // \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \n \n 2-sparkDF() \n \n \n \n \n \n \n \n 4-RDDRow+StructTypeDF \n \n \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . types . { DataTypes ,  StructType } \n import   org . apache . spark . sql . { DataFrame ,  Row ,  SparkSession } \n\n /**\n * DESC:\n */ \n\n object  _04toDFWay3  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //For implicit conversions from RDDs to DataFrames \n     import   spark . implicits . _\n     // \n     val  sc :  SparkContext  =  spark . sparkContext\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sql/people1.txt" ) \n     //row \n     val  peopleRDD :  RDD [ Row ]   =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . map ( x  =>  Row ( x ( 0 ) . toInt ,  x ( 1 ) ,  x ( 2 ) . toInt ) ) \n     //structedFiled \n     val  schema :  StructType  =   new  StructType ( ) \n       . add ( "id" ,  DataTypes . IntegerType ,   true ) \n       . add ( "name" ,   "string" ,   true ) \n       . add ( "age" ,   "int" ,   true ) \n     // \n     val  peopleDF :  DataFrame  =  spark . createDataFrame ( peopleRDD ,  schema ) \n    peopleDF . show ( 2 , false ) \n     //+---+--------+---+ \n     //| id|    name|age| \n     //+---+--------+---+ \n     //|  1|zhangsan| 20| \n     //|  2|    lisi| 29| \n     //|  3|  wangwu| 25| \n     //scheme \n    peopleDF . printSchema ( ) \n     //root \n     // |-- id: integer (nullable = false) \n     // |-- name: string (nullable = true) \n     // |-- age: integer (nullable = false) \n     // \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \n 2 \n \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . types . { DataTypes ,  IntegerType ,  LongType ,  StringType ,  StructField ,  StructType } \n import   org . apache . spark . sql . { DataFrame ,  Row ,  SparkSession } \n\n /**\n * DESC:\n */ \n\n object  _04toDFWay4  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //For implicit conversions from RDDs to DataFrames \n     // \n     val  sc :  SparkContext  =  spark . sparkContext\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sql/people1.txt" ) \n     //row \n     val  peopleRDD :  RDD [ Row ]   =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . map ( x  =>  Row ( x ( 0 ) . toInt ,  x ( 1 ) ,  x ( 2 ) . toInt ) ) \n     //structedFiled \n\n     /* val schema: StructType = StructType(Array(\n       StructField("id", DataTypes.IntegerType, true),\n       StructField("name", DataTypes.StringType, true),\n       StructField("age", DataTypes.IntegerType, true)\n     ))*/ \n\n     val  schema :  StructType  =  StructType ( \n      StructField ( "f1" ,  IntegerType ,   true )   :: \n        StructField ( "f2" ,  StringType ,   false )   :: \n        StructField ( "f3" ,  IntegerType ,   false )   :: \n        Nil ) \n     // \n     val  peopleDF :  DataFrame  =  spark . createDataFrame ( peopleRDD ,  schema ) \n    peopleDF . show ( 2 ,   false ) \n     //+---+--------+---+ \n     //| id|    name|age| \n     //+---+--------+---+ \n     //|  1|zhangsan| 20| \n     //|  2|    lisi| 29| \n     //|  3|  wangwu| 25| \n     //scheme \n    peopleDF . printSchema ( ) \n     //root \n     // |-- id: integer (nullable = false) \n     // |-- name: string (nullable = true) \n     // |-- age: integer (nullable = false) \n     // \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 \n 5-toDF \n \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n\n /**\n * DESC:\n */ \n\n object  _03toDFWay2  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //For implicit conversions from RDDs to DataFrames \n     import   spark . implicits . _\n     // \n     val  sc :  SparkContext  =  spark . sparkContext\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sql/people1.txt" ) \n     //person \n     val  peopleRDD  =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . map ( x  =>   ( x ( 0 ) . toInt ,  x ( 1 ) ,  x ( 2 ) . toInt ) ) \n     //df \n     val  peopleDF :  DataFrame  =  peopleRDD . toDF ( "id" , "name" , "age" ) \n     // \n    peopleDF . show ( ) \n     //+---+--------+---+ \n     //| id|    name|age| \n     //+---+--------+---+ \n     //|  1|zhangsan| 20| \n     //|  2|    lisi| 29| \n     //|  3|  wangwu| 25| \n     //scheme \n    peopleDF . printSchema ( ) \n     //root \n     // |-- id: integer (nullable = false) \n     // |-- name: string (nullable = true) \n     // |-- age: integer (nullable = false) \n     // \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 \n  \n \n \n \n rdddf \n strucedType \n \n SparkSQLDataFrame \n DSL \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n\n /**\n * DESC:\n */ \n case   class  People1 ( id :   Int ,  name :   String ,  age :   Int ) \n\n object  _06DataFrameOpration  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //For implicit conversions from RDDs to DataFrames \n     import   spark . implicits . _\n     // \n     val  sc :  SparkContext  =  spark . sparkContext\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sql/people1.txt" ) \n     //person \n     val  peopleRDD :  RDD [ People1 ]   =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . map ( x  =>  People1 ( x ( 0 ) . toInt ,  x ( 1 ) ,  x ( 2 ) . toInt ) ) \n     //df \n     val  peopleDF :  DataFrame  =  peopleRDD . toDF ( ) \n     // \n    peopleDF . show ( ) \n     //+---+--------+---+ \n     //| id|    name|age| \n     //+---+--------+---+ \n     //|  1|zhangsan| 20| \n     //|  2|    lisi| 29| \n     //|  3|  wangwu| 25| \n     //scheme \n    peopleDF . printSchema ( ) \n     //root \n     // |-- id: integer (nullable = false) \n     // |-- name: string (nullable = true) \n     // |-- age: integer (nullable = false) \n     //Spark \n     //DSL \n     //1-name \n    peopleDF . select ( "name" ) . show ( ) \n    peopleDF . select ( col ( "name" ) ) . show ( ) \n    peopleDF . select ( column ( "name" ) ) . show ( ) \n    peopleDF . select ( \'name ) . show ( ) \n     //2-nameage \n    peopleDF . select ( "name" ,   "age" ) . show ( ) \n    peopleDF . select ( col ( "name" ) ,  col ( "age" ) ) . show ( ) \n    peopleDF . select ( column ( "name" ) ,  column ( "age" ) ) . show ( ) \n    peopleDF . select ( \'name, \' age ) . show ( ) \n     // \n    peopleDF . select ( col ( "name" ) ,  column ( "age" ) ) . show ( ) \n    peopleDF . select ( col ( "name" ) ,   \'age ) . show ( ) \n     //peopleDF.select("name",\'age).show() \n     //3-age+1 \n     //peopleDF.select(\'name,\'age+1).show() \n     //peopleDF.select("name","age"+1).show() \n    peopleDF . select ( col ( "name" ) ,  col ( "age" )   +   1 ) . show ( ) \n    peopleDF . select ( col ( "name" ) ,  column ( "age" )   +   1 ) . show ( ) \n    peopleDF . select ( \'name, \' age  +   1 ) . show ( ) \n     //4age25filter \n    peopleDF . filter ( col ( "age" )   >   25 ) . show ( ) \n    peopleDF . filter ( \'age   >   25 ) . show ( ) \n    println ( peopleDF . filter ( \'age   >   25 ) . count ( ) ) \n     //5 \n     val  re1 :  DataFrame  =  peopleDF . groupBy ( "age" ) . count ( )   //countschemcount \n    re1 . show ( ) \n    re1 . printSchema ( ) \n    re1 . orderBy ( "count" ) . show ( ) \n    re1 . orderBy ( \'count ) . show ( ) \n     // \n    peopleDF . groupBy ( "age" ) . count ( ) . orderBy ( "count" ) . show ( ) \n     //SQL \n     // \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 SQL \n package   cn . itcast . sparksql \n\n import   org . apache . spark . SparkContext\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n\n /**\n * DESC:\n */ \n\n object  _06_1DataFrameOpration  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     //For implicit conversions from RDDs to DataFrames \n     import   spark . implicits . _\n     // \n     val  sc :  SparkContext  =  spark . sparkContext\n     val  fileRDD :  RDD [ String ]   =  sc . textFile ( "data/baseinput/sql/people1.txt" ) \n     //person \n     val  peopleRDD :  RDD [ People1 ]   =  fileRDD . map ( _ . split ( "\\\\s+" ) ) . map ( x  =>  People1 ( x ( 0 ) . toInt ,  x ( 1 ) ,  x ( 2 ) . toInt ) ) \n     //df \n     val  peopleDF :  DataFrame  =  peopleRDD . toDF ( ) \n     // \n     //SQL \n    peopleDF . createOrReplaceTempView ( "peopleTable" ) \n     //1-name \n    spark . sql ( "select name from peopleTable" ) . show ( ) \n     //2-nameage \n    spark . sql ( "select name,age from peopleTable" ) . show ( ) \n     //3- \n    spark . sql ( "select * from peopleTable order by age desc limit 2" ) . show ( ) \n    spark . sql ( \n       """\n        |select *\n        |from peopleTable\n        |order by age desc\n        |limit 2\n        |""" . stripMargin ) . show ( ) \n     //SQL \n     // \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 SparkSQLDataSet \n \n \n wordcount \n \n \n DSL: \n \n \n package   cn . itcast . sparksql . wordcount \n\n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  Row ,  SparkSession } \n\n /**\n * DESC:\n */ \n object  _01DSL  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //SparkSession \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     // \n     val  df1 :  DataFrame  =  spark . read . text ( "data/baseinput/words.txt" ) \n     val  ds1 :  Dataset [ String ]   =  spark . read . textFile ( "data/baseinput/words.txt" ) \n     //ds \n     //df1.as[String].flatMap(_.split("\\\\s+")) \n     val  value :  Dataset [ String ]   =  ds1 . flatMap ( _ . split ( "\\\\s+" ) ) \n    value . show ( ) \n     //+----------+ \n     //|     value| \n     //+----------+ \n     //|     hello| \n     //|     spark| \n     //|     hello| \n     //|     flink| \n     //|     hello| \n     //dsl \n     val  result :  Dataset [ Row ]   =  value . groupBy ( "value" ) . count ( ) . orderBy ( \'count . desc ) \n    result . show ( ) \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 \n SQL: \n \n package   cn . itcast . sparksql . wordcount \n\n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  Row ,  SparkSession } \n\n /**\n * DESC:\n */ \n object  _02SQL  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //SparkSession \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     // \n     val  df1 :  DataFrame  =  spark . read . text ( "data/baseinput/words.txt" ) \n     val  ds1 :  Dataset [ String ]   =  spark . read . textFile ( "data/baseinput/words.txt" ) \n     //ds \n     //df1.as[String].flatMap(_.split("\\\\s+")) \n     val  value :  Dataset [ String ]   =  ds1 . flatMap ( _ . split ( "\\\\s+" ) ) \n    value . show ( ) \n     //+----------+ \n     //|     value| \n     //+----------+ \n     //|     hello| \n     //|     spark| \n     //|     hello| \n     //|     flink| \n     //|     hello| \n     //sql \n    value . createOrReplaceTempView ( "table" ) \n     val  result :  DataFrame  =  spark . sql ( \n       """\n        |select  value,count(value) as counts\n        |from table\n        |group by value\n        |order by counts desc\n        |""" . stripMargin ) \n     //+----------+-----+ \n     //|     value|count| \n     //+----------+-----+ \n     //|     hello|    6| \n     //|     sqoop|    1| \n     //|     flink|    1| \n     //|    pulsar|    1| \n     //|     doris|    1| \n     //|clickhouse|    1| \n     //|     spark|    1| \n     //+----------+-----+ \n    result . show ( ) \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #   SparkSQL() \n \n \n RDDDFDS \n RDDschemedfdfds \n dfrdddataset \n datasetspark2.0dataframedataSet[ROW]=dataframe \n () \n \n \n  \n \n \n  \n \n Top102000) \n \n \n \n  \n \n \n \n \n \n  \n \n 1- \n 2-RDDDF(case class) \n 3-SQL \n 4-DSL \n 5-csv \n 6-MySQL \n \n \n \n  \n \n \n package   cn . itcast . sparksql . moviesPro \n\n\n import   java . util . Properties\n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  Row ,  SaveMode ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n\n /**\n * DESC:\n * 1-\n * 2-RDDDF(case class)\n * 3-SQL\n * 4-DSL\n * 5-csv\n * 6-MySQL\n */ \n case   class  MoviesRatings ( userId :   String ,  moviesId :   String ,  ratings :   Double ,  timestamp :   String ) \n\n object  _01MoviesRatingLoader  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     val  spark :  SparkSession  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) \n         . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n         . setMaster ( "local[*]" ) \n         . set ( "spark.sql.shuffle.partitions" ,   "4" ) //200 \n       val  spark :  SparkSession  =  SparkSession . builder ( ) . config ( conf ) . getOrCreate ( ) \n      spark\n     } \n     import   spark . implicits . _\n     //1- \n     val  fileRDD :  RDD [ String ]   =  spark . sparkContext . textFile ( "data/baseinput/ml-1m/ratings.dat" ) \n     //2-RDDDF(case class) \n     val  moviesDF :  DataFrame  =  fileRDD\n       . filter ( line  =>  line  !=   null   &&  line . trim . split ( "::" ) . length  ==   4 ) \n       . mapPartitions ( iter  =>   { \n        iter . map ( line  =>   { \n           val  arr :  Array [ String ]   =  line . split ( "::" ) \n          MoviesRatings ( arr ( 0 ) ,  arr ( 1 ) ,  arr ( 2 ) . toDouble ,  arr ( 3 ) ) \n         } ) \n       } ) . toDF\n    moviesDF . show ( 3 ,   false ) \n     //+------+--------+-------+---------+ \n     //|userId|moviesId|ratings|timestamp| \n     //+------+--------+-------+---------+ \n     //|1     |1193    |5.0    |978300760| \n     //|1     |661     |3.0    |978302109| \n     //|1     |914     |3.0    |978301968| \n     //+------+--------+-------+---------+ \n    moviesDF . printSchema ( ) \n     //root \n     // |-- userId: string (nullable = true) \n     // |-- moviesId: string (nullable = true) \n     // |-- ratings: double (nullable = false) \n     // |-- timestamp: string (nullable = true) \n     //3-SQL \n    moviesDF . createOrReplaceTempView ( "movies_table" ) \n     //Top102000) \n     val  sql  = \n       """\n        |select moviesId,round(avg(ratings),2) as avg_rating,count(moviesId) as rnt_rating\n        |from movies_table\n        |group by moviesId\n        |having rnt_rating>2000\n        |order by avg_rating desc,rnt_rating desc\n        |limit 10\n        |""" . stripMargin\n    println ( "sql funtions way is:..........." ) \n     //spark.sql(sql).show() \n     //4-DSL \n     val  resultDF :  Dataset [ Row ]   =  moviesDF\n       . select ( "moviesId" ,   "ratings" ) \n       . groupBy ( "moviesId" ) \n       . agg ( \n        round ( avg ( "ratings" ) ,   2 ) . as ( "avg_rating" ) , \n        count ( "moviesId" ) . as ( "rnt_rating" ) \n       ) \n       . filter ( \'rnt_rating   >   2000 ) \n       //.filter($"rnt_rating" >2000) \n       //.filter(col("rnt_rating") >2000) \n       //.filter(column("rnt_rating") >2000) \n       . orderBy ( $ "avg_rating" . desc ,   \'rnt_rating . desc ) \n       . limit ( 10 ) \n    println ( "dsl funtions way is:..........." ) \n    resultDF . show ( ) \n     //5-csv \n     //+--------+----------+----------+ \n     //|moviesId|avg_rating|rnt_rating| \n     //+--------+----------+----------+ \n     //|     318|      4.55|      2227| \n     //resultDF \n     //  .coalesce(1) \n     //  .write \n     //  .mode(SaveMode.Overwrite) \n     //  .csv("data/baseoutput/output-2/") \n     // Thread.sleep(100 * 1000) \n     //6-MySQL \n     // \n     /*    CREATE DATABASE bigdata CHARACTER SET utf8;\n        CREATE TABLE `tb_top10_movies` (\n          `movieId` INT(11) NOT NULL,\n          `avg_rating` FLOAT(10) DEFAULT NULL,\n          `cnt_rating` INT(11) DEFAULT NULL,\n          PRIMARY KEY (`movieId`)\n        ) ENGINE=INNODB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;\n        SELECT * FROM tb_top10_movies*/ \n     //url1 \n     //val prop = new Properties() \n     //prop.setProperty("driver", "com.mysql.jdbc.Driver") \n     //prop.setProperty("user", "root") \n     //prop.setProperty("password", "root") \n     //resultDF \n     //  .coalesce(1) \n     //  .write \n     //  .mode(SaveMode.Overwrite) \n     //  .jdbc("jdbc:mysql://localhost:3306/bigdata", "tb_top10_movie", prop) \n     //2,https://spark.apache.org/docs/3.1.1/sql-data-sources-jdbc.html \n     // Saving data to a JDBC source \n    resultDF . write\n       . format ( "jdbc" ) \n       . mode ( SaveMode . Overwrite ) \n       . option ( "url" ,   "jdbc:mysql://localhost:3306/bigdata" ) \n       . option ( "dbtable" ,   "tb_top10_movie" ) \n       . option ( "user" ,   "root" ) \n       . option ( "password" ,   "root" ) \n       . save ( ) \n    println ( "data write finished!" ) \n     //1 \n     //val jdbcDF = spark.read \n     //  .format("jdbc") \n     //  .option("url", "jdbc:mysql://localhost:3306/bigdata") \n     //  .option("dbtable", "tb_top10_movie") \n     //  .option("user", "root") \n     //  .option("password", "root") \n     //  .load() \n    println ( "data reader finished!" ) \n     //2 \n     val  connectionProperties  =   new  Properties ( ) \n    connectionProperties . put ( "user" ,   "root" ) \n    connectionProperties . put ( "password" ,   "root" ) \n     val  jdbcDF2  =  spark . read\n       . jdbc ( "jdbc:mysql://localhost:3306/bigdata" ,   "tb_top10_movie" ,  connectionProperties ) \n    jdbcDF2 . show ( ) \n    \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 \n  \n \n \n  \n Spark SQLHive \n SparkSQLHive() \n \n \n SparkSQLHive \n \n \n 0- \n \n \n 1,zhangsan,30 \n 2,lisi,40 \n 3,wangwu,50 \n \n \n 1-mavenspark_hive \n \n \n \n \n < dependency > \n           < groupId > org.apache.spark </ groupId > \n           < artifactId > spark-hive_2.11 </ artifactId > \n           < version > 2.4.5 </ version > \n  </ dependency > \n \n 1 2 3 4 5 \n \n 2-sparkconfennableHiveSupprtHive \n \n \n 3-hivespark.sql(hivesql) \n \n \n 4- \n \n \n  \n \n \n package   cn . itcast . sparksql . toHive \n\n import   org . apache . spark . sql . SparkSession\n\n /**\n * DESC:\n *\n */ \n object  _01SparkToHive  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //SparkSession \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . enableHiveSupport ( ) //to a persistent Hive metastore \n       . getOrCreate ( ) \n     //hivederbymysql \n\n    spark . sql ( "show databases" ) . show ( ) \n    spark . sql ( "create table if not exists student2(id int,name String,age int) row format delimited fields terminated by \',\'" ) \n    spark . sql ( "load data local inpath \'data/baseinput/sql/hive/students.csv\' overwrite into table student2" ) \n    spark . sql ( "select * from student2 where age >30" ) . show ( ) \n\n\n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #  SparkHive \n \n \n SParkOnHive \n \n Hive---HiveOnMR \n Shark----HiveOnSpark---Hive \n SparkSQL---- \n SparkSQL+HIve---- SparkOnHive --HiveSparkSQL \n \n \n \n SparkOnHive \n \n \n 1-SparkHivehive-site.xmlsaprkconf \n \n \n \n \n \n \n \n \n 2-hive-site.xml \n \n \n 3-mysqljarsparkjars \n \n \n metastore \n \n \n \n \n nohup  /export/server/hive/bin/hive  --service  metastore  2 > &1   >>  /var/log.log  & \n \n 1 \n \n 4-spark-sqlspark-shell \n \n \n \n \n \n \n \n \n 4-IDEAHive \n \n \n \n \n \n \n \n \n  \n \n \n package   cn . itcast . sparkbase \n\n import   org . apache . spark . sql . SparkSession\n\n /**\n * DESC:\n */ \n object  _02SparkToHive  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       //2.0.0hive.sql.warehouse.dir2.0spark.sql.warehouse.dir \n       //warehousehive \n       . config ( "spark.sql.warehouse.dir" ,   "hdfs://node1:8020/user/hive/warehouse" ) \n       //metastorethrift \n       . config ( "hive.metastore.uris" ,   "thrift://node3:9083" ) \n       . enableHiveSupport ( )   //to a persistent Hive metastore \n       . getOrCreate ( ) \n\n    spark . sql ( "show databases" ) . show ( ) \n    spark . sql ( "use sparkhive3" ) \n    spark . sql ( "create table if not exists student2(id int,name String,age int) row format delimited fields terminated by \',\'" ) \n    spark . sql ( "show tables" ) . show ( ) \n    spark . sql ( "load data local inpath \'data/baseinput/sql/hive/students.csv\' overwrite into table student2" ) \n    spark . sql ( \n     """\n      |select *\n      |from student2\n      |where age >30\n      |""" . stripMargin ) . show ( ) \n     //+---+------+---+ \n     //| id|  name|age| \n     //+---+------+---+ \n     //|  2|  lisi| 40| \n     //|  3|wangwu| 50| \n     //+---+------+---+ \n    spark . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 \n  \n \n \n \n \n \n SparkCLI \n \n \n \n \n \n SparkBeelinethrift \n \n \n SparkThriftServer(HiveHiveServer2)BeelineSQL\n \n 1 \n \n \n SPARK_HOME = /export/server/spark\n $SPARK_HOME /sbin/start-thriftserver.sh  \\ \n --hiveconf   hive.server2.thrift.port = 10001   \\ \\ hiveport\n --hiveconf   hive.server2.thrift.bind.host = node3  \\ \n --master  local [ 2 ] \n \n 1 2 3 4 5 \n \n \n \n \n sparkonhive \n \n \n \n \n \n thriftsparkonhive \n SparkSQLUDF() \n \n \n UDFSpark \n \n \n UDAFSpark \n \n \n UDTFHiveUDTFSpark \n \n \n \n \n \n \n \n \n sparksessionUDF  \n spark.udf.register()  \n \n 1.udf \n \n //udf,  \nspark . udf . register ( "strLen" ,   ( str :   String )   =>  str . length ( ) ) \n //spark sqludf \nspark . sql ( "select name,strLen(name) as name_len from user" ) . show ( false ) \n \n 1 2 3 4 \n 2.udf \n \n // \n def  getStrLen ( str :   String ) :   Int   =   { \n  str . length\n } \n\n //udf, _() \nspark . udf . register ( "strLen" ,  getStrLen _ ) \n //spark sqludf \nspark . sql ( "select name,strLen(name) as name_len from user" ) . show ( false ) \n \n 1 2 3 4 5 6 7 8 9 \n SparkSQLUDF META-INF/MANIFEST.MFsrcresouces,main \n spark-sql--jars \n \n cd   $SPARK_HOME /bin\nspark-sql  --jars  /home/hadoop/lib/udf.jar\nCREATE TEMPORARY FUNCTION hello AS  \'com.luogankun.udf.HelloUDF\' ; \n select  hello ( url )  from page_views limit  1 ; \n \n 1 2 3 4 \n  \n \n 1udf.jarspark-env.shSPARK_CLASSPATH \n export   SPARK_CLASSPATH = $SPARK_CLASSPATH :/home/hadoop/software/mysql-connector-java-5.1.27-bin.jar:/home/hadoop/lib/udf.jar\n \n 1 2spark-sqlCREATE TEMPORARY FUNCTION \n cd   $SPARK_HOME /bin\nspark-sql\nCREATE TEMPORARY FUNCTION hello AS  \'com.luogankun.udf.HelloUDF\' ; \n select  hello ( url )  from page_views limit  1 ; \n \n 1 2 3 4 \n Thrift JDBC ServerUDF \n \n beeline \n create  function  base_analizer as  \'com.zhengkw.udf.BaseFieldUDF\'  using jar  \'hdfs://hadoop102:9000/user/hive/jars/hivefunction-1.0-SNAPSHOT.jar\' ; //hdfs\n\n add  jar /home/hadoop/lib/udf.jar ; //\nCREATE TEMPORARY FUNCTION hello AS  \'com.luogankun.udf.HelloUDF\' ; \n select  hello ( url )  from page_views limit  1 ; \n\n \n 1 2 3 4 5 6 \n spark/hive(hive udf) \n \n /**\nDataFrame(DataSet) Java(PythonScale) UDFSQL(SparkSQLHive) Spark(Hive)SparkHiveUDF\n**/ \n \n 1 2 3 \n UDF1 \n \n package   cn . itcast . sparksql . udf \n\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . api . java . UDF1\n import   org . apache . spark . sql . types . StringType\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n\n /**\n * DESC:\n * 1-\n * 2-\n * 3-UDF\n */ \n case   class  Smaller ( line :   String ) \n\n object  _01wordsToBigger  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1- \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     //2- \n     val  fileRDD :  RDD [ String ]   =  spark . sparkContext . textFile ( "data\\\\baseinput\\\\sql\\\\udf\\\\udf.txt" ) \n     //3-UDF \n     val  wordsDF :  DataFrame  =  fileRDD . map ( x  =>  Smaller ( x ) ) . toDF\n    wordsDF . show ( ) \n    wordsDF . printSchema ( )   // |-- line: string (nullable = true) \n     //user-defined function \n     //spark.udf.register("wordToBigger", new UDF1[String, String] { \n     //  override def call(t1: String): String = { \n     //    t1.toUpperCase() \n     //  } \n     //}, StringType) \n    spark . udf . register ( "wordToBigger" , ( line : String ) => { \n      line . toUpperCase ( ) \n     } ) \n     //SQL \n    wordsDF . createOrReplaceTempView ( "word_view" ) \n     val  result1 :  DataFrame  =  spark . sql ( "select line,wordToBigger(line) as bigger from word_view" ) \n    result1 . show ( ) \n     //DSL \n     val  result2 :  DataFrame  =  wordsDF . select ( \'line , \n      callUDF ( "wordToBigger" ,   \'line ) . as ( "bigger" ) ) \n    result2 . show ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \n  \n \n package   cn . itcast . sparksql . udf \n\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . sql . api . java . UDF1\n import   org . apache . spark . sql . types . StringType\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n\n /**\n * DESC:\n * 1-\n * 2-\n * 3-UDF\n */ \n\n object  _02udfDemo  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1- \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n\n     val  df  =  Seq ( ( "id1" ,   1 ) ,   ( "id2" ,   4 ) ,   ( "id3" ,   5 ) ) . toDF ( "id" ,   "value" ) \n    spark . udf . register ( "simpleUDF" ,   ( n :   Int )   =>  n  *  n ) \n    df . select ( $ "id" ,  callUDF ( "simpleUDF" ,  $ "value" ) . as ( "pow(x,2)" ) ) . show ( ) \n     val  df1  =  Seq ( ( "id1" ,   1 , 6 ) ,   ( "id2" ,   4 , 7 ) ,   ( "id3" ,   5 , 8 ) ) . toDF ( "id" ,   "value1" , "value2" ) \n    spark . udf . register ( "simpleUDF2" , ( v1 : Int , v2 : Int ) => { \n      v1 * v2\n     } ) \n    df1 . createOrReplaceTempView ( "table_df" ) \n    spark . sql ( \n       """\n        |select id,simpleUDF2(value1,value2) as simple\n        |from table_df\n        |""" . stripMargin ) . show ( ) \n\n    df1 . select ( $ "id" , \n      callUDF ( "simpleUDF2" , \'value1,\' value2 ) . as ( "bigger" ) ) . show ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 \n UDAF \n \n \n package   cn . itcast . sparksql . udf \n import   org . apache . spark . sql . { DataFrame ,  Row ,  SparkSession } \n import   org . apache . spark . sql . expressions . { MutableAggregationBuffer ,  UserDefinedAggregateFunction } \n import   org . apache . spark . sql . types . _\n\n class  SparkFunctionUDAF  extends  UserDefinedAggregateFunction { \n   //schema \n   override   def  inputSchema :  StructType  =   { \n    StructType ( StructField ( "input" , LongType ) :: Nil ) \n   } \n   //schemaschema \n   // \n   override   def  bufferSchema :  StructType  =   { \n    StructType ( StructField ( "sum" , LongType ) :: StructField ( "total" , LongType ) :: Nil ) \n   } \n   //---/ doubleType \n   override   def  dataType :  DataType  =   { \n    DoubleType\n   } \n   // \n   override   def  deterministic :   Boolean   =   { \n     true \n   } \n   // \n   override   def  initialize ( buffer :  MutableAggregationBuffer ) :   Unit   =   { \n    buffer ( 0 )   =   0L    //Buffer00 \n    buffer ( 1 )   =   0L    //buffer1 \n   } \n   // \n   override   def  update ( buffer :  MutableAggregationBuffer ,  input :  Row ) :   Unit   =   { \n     // \n    buffer ( 0 )   =  buffer . getLong ( 0 )   +  input . getLong ( 0 ) \n     // \n    buffer ( 1 )   =  buffer . getLong ( 1 )   +   1   //10--+ \n   } \n   // \n   override   def  merge ( buffer1 :  MutableAggregationBuffer ,  buffer2 :  Row ) :   Unit   =   { \n    buffer1 ( 0 )   = buffer1 . getLong ( 0 )   +  buffer2 . getLong ( 0 ) \n    buffer1 ( 1 )   =  buffer1 . getLong ( 1 )   +  buffer2 . getLong ( 1 ) \n   } \n   // \n   override   def  evaluate ( buffer :  Row ) :   Any   =   { \n    buffer . getLong ( 0 ) . toDouble  /  buffer . getLong ( 1 ) \n   } \n } \n\n object  SparkFunctionUDAF  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //sparkSession \n     val  sparkSession :  SparkSession  =  SparkSession . builder ( ) . appName ( "sparkUDAF" ) . master ( "local[2]" ) . getOrCreate ( ) \n     //sparkSessionjsonDataFrame \n     val  employeeDF :  DataFrame  =  sparkSession . read . json ( "data\\\\baseinput\\\\sql\\\\udf\\\\udaf.txt" ) \n     //DataFrame \n    employeeDF . createOrReplaceTempView ( "employee_table" ) \n     //UDAF \n    sparkSession . udf . register ( "avgSal" , new  SparkFunctionUDAF ) \n     //UDAF \n    sparkSession . sql ( "select avgSal(salary) from employee_table" ) . show ( ) \n    sparkSession . close ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 #  SparkSQL \n \n \n Hive \n \n \n  \n \n count() over(order by partition by) \n \n \n \n  \n \n row_number() over(partition by order by)   12345 \n rank() over(partition by order by)     446 \n dense_rank() over(partition by order by)   445 \n \n \n \n  \n \n \n package   cn . itcast . sparksql . func \n\n import   org . apache . spark . sql . SparkSession\n\n /**\n * DESC:\n */ \n case   class  Score ( name :   String ,  clazz :   Int ,  score :   Int ) \n\n object  _01class  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1- \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     val  scoreDF  =  spark . sparkContext . makeRDD ( Array ( \n      Score ( "a1" ,   1 ,   80 ) , \n      Score ( "a2" ,   1 ,   78 ) , \n      Score ( "a3" ,   1 ,   95 ) , \n      Score ( "a4" ,   2 ,   74 ) , \n      Score ( "a5" ,   2 ,   92 ) , \n      Score ( "a6" ,   3 ,   99 ) , \n      Score ( "a7" ,   3 ,   99 ) , \n      Score ( "a8" ,   3 ,   45 ) , \n      Score ( "a9" ,   3 ,   55 ) , \n      Score ( "a10" ,   3 ,   78 ) , \n      Score ( "a11" ,   3 ,   100 ) ) \n     ) . toDF ( "name" ,   "class" ,   "score" ) \n    scoreDF . createOrReplaceTempView ( "scores" ) \n    scoreDF . printSchema ( ) \n     // \n    spark . sql ( "select count(name) from scores" ) . show ( ) \n    spark . sql ( "select name,class,score,count(name) over() ccc from scores" ) . show ( ) \n    spark . sql ( "select name,class,score,count(name) over(partition by class) ccc from scores" ) . show ( ) \n     // 123 \n    spark . sql ( "select name,class,score,row_number() over(order by class) sss from scores" ) . show ( ) \n    spark . sql ( "select name,class,score,row_number() over(partition by class order by score) sss from scores" ) . show ( ) \n     //446 \n    spark . sql ( "select name,class,score,rank() over(order by class) sss from scores" ) . show ( ) \n    spark . sql ( "select name,class,score,rank() over(partition by class order by score) sss from scores" ) . show ( ) \n     //445 \n    spark . sql ( "select name,class,score,dense_rank() over(order by class) sss from scores" ) . show ( ) \n    spark . sql ( "select name,class,score,dense_rank() over(partition by class order by score) sss from scores" ) . show ( ) \n     //ntile \n    spark . sql ( "select name,class,score,ntile(6) over(order by class) sss from scores" ) . show ( ) \n    spark . sql ( "select name,class,score,ntile(6) over(partition by class order by score) sss from scores" ) . show ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #  SparkSQLRDD \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n 1WebUI \n \n \n \n \n \n 2spark-shell4040 \n \n \n \n \n \n \n \n \n SparkSQL \n SparkStreaming--RDD--DStream \n  \n \n \n \n \n \n  \n \n \n \n \n \n SparkStreaming \n SparkStreaming \n SparkStreaming \n SparkStreaming \n  \n SparkStreaming \n \n SparkStreaming\n \n Receiver200ms50ms \n \n \n \n https://spark.apache.org/docs/3.1.1/configuration.html \n SparkStreaming5s \n DStream \n \n RDD@Time \n RDDTransormationActionDStream \n TransormationOutPutOpration \n \n \n \n \n \n \n SparkSTreamingDStream \n SparkStreamingtransformDStreamRDDRDDtransform \n SparkStreaming- \n \n \n 5swordcount \n \n \n nc -lk 9999/9998 socket \n \n \n  \n \n 1- \n 2- \n 3-flatMap \n 4-map \n 5-reduceBykey \n 6-start \n 7-awaitTermination \n \n \n \n  \n \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n *1-\n *2-\n *3-flatMap\n *4-map\n *5-reduceBykey\n *6-start\n *7-awaitTermination\n */ \n object  _01baseStreaming  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1- \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n     //batchDuration the time interval at which streaming data will be divided into batches \n     val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n     // 2---socketlinuxwindowsncncyum install -y nc   nc -lk 9999 \n     val  receiveRDD :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n     // 3-flatMap \n     val  flatRDD :  DStream [ String ]   =  receiveRDD . flatMap ( _ . split ( "\\\\s+" ) ) \n     // 4-map \n     val  mapRDD :  DStream [ ( String ,   Int ) ]   =  flatRDD . map ( x  =>   ( x ,   1 ) ) \n     // 5-reduceBykey \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapRDD . reduceByKey ( ( a :   Int ,  b :   Int )   =>  a  +  b ) \n    resultRDD . print ( ) \n     // 6-start \n     //Start the execution of the streams. \n    ssc . start ( ) \n     // 7-awaitTermination--Wait for the execution to stop \n    ssc . awaitTermination ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 \n \n  \n  \n updateStateByKey \n \n \n transform \n \n \n  \n \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-\n * 2-\n * 3-flatMap\n * 4-map\n * 5-reduceBykey\n * 6-start\n * 7-awaitTermination\n */ \n object  _02baseStreaming  { \n   /**\n   * @param currentValue \n   * @param historyValue \n   * @return Option-none some\n   */ \n   def  updateFunc ( currentValue :  Seq [ Int ] ,  historyValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sumV :   Int   =  currentValue . sum  +  historyValue . getOrElse ( 0 ) //0 \n     //Some(sumV) \n    Option ( sumV ) \n   } \n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1- \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n     //batchDuration the time interval at which streaming data will be divided into batches \n     val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n    ssc . checkpoint ( "data/baseoutput/output-4" ) \n     // 2---socketlinuxwindowsncncyum install -y nc   nc -lk 9999 \n     val  receiveRDD :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n     // 3-flatMap \n     val  flatRDD :  DStream [ String ]   =  receiveRDD . flatMap ( _ . split ( "\\\\s+" ) ) \n     // 4-map \n     val  mapRDD :  DStream [ ( String ,   Int ) ]   =  flatRDD . map ( x  =>   ( x ,   1 ) ) \n     // 5-reduceBykey \n       //ScalaJavaScala \n\n //Scala  defval def  \n       //++_ \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapRDD . updateStateByKey ( updateFunc ) \n     //--tranformrdd \n     val  sortResultDS :  DStream [ ( String ,   Int ) ]   =  resultRDD . transform ( rdd  =>   { \n      rdd . sortBy ( _ . _2 ,   false ) \n     } ) \n    sortResultDS . print ( ) \n      \n       // SparkStreaming \n     new  Thread ( new  Runnable  { \n       override   def  run ( ) :   Unit   =   { \n\n         while   (   true   )   { \n           try   { \n            Thread . sleep ( 5000 ) \n           }   catch   { \n             case  ex  :  Exception  =>  println ( ex ) \n           } \n\n           // HDFS \n           val  fs :  FileSystem  =  FileSystem . get ( new  URI ( "hdfs://node1:8020" ) ,   new  Configuration ( ) ,   "root" ) \n\n           val  state :  StreamingContextState  =  ssc . getState ( ) \n           //  \n           if   (  state  ==  StreamingContextState . ACTIVE  )   { \n\n             //  \n             val  flg :   Boolean   =  fs . exists ( new  Path ( "hdfs://node1:8020/spark/stopSparkHistory/" + appName + "@" + ssc . sparkContext . getConf . getAppId ) ) \n             if   (  flg  )   { \n               // Driver: \n              ssc . stop ( true ,   true ) \n              System . exit ( 0 ) \n             } \n\n           } \n         } \n\n       } \n     } ) . start ( ) \n     // 6-start \n     //Start the execution of the streams. \n    ssc . start ( ) \n      \n     \n     // 7-awaitTermination--Wait for the execution to stopawaitTermination()driver \n    ssc . awaitTermination ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 \n  \n \n union \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-\n * 2-\n * 3-flatMap\n * 4-map\n * 5-reduceBykey\n * 6-start\n * 7-awaitTermination\n */ \n object  _06twoSourceStreaming  { \n\n   def  updateFunc ( curentValue :  Seq [ Int ] ,  histouryValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  curentValue . sum  +  histouryValue . getOrElse ( 0 ) \n    Option ( sum ) \n   } \n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1- \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[3]" ) \n     //batchDuration the time interval at which streaming data will be divided into batches \n     val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n    ssc . checkpoint ( "data/baseoutput/output-6" ) \n     // 2---socketlinuxwindowsncncyum install -y nc   nc -lk 9999 \n     val  receiveRDD1 :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n     val  receiveRDD2 :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9998 ) \n     val  receiveRDD :  DStream [ String ]   =  receiveRDD1 . union ( receiveRDD2 ) \n     // 3-flatMap \n     val  flatRDD :  DStream [ String ]   =  receiveRDD . flatMap ( _ . split ( "\\\\s+" ) ) \n     // 4-map \n     val  mapRDD :  DStream [ ( String ,   Int ) ]   =  flatRDD . map ( x  =>   ( x ,   1 ) ) \n     // 5-reduceBykey \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapRDD . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n     // 6-start \n     //Start the execution of the streams. \n    ssc . start ( ) \n     // 7-awaitTermination--Wait for the execution to stop \n    ssc . awaitTermination ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 #  mapWithState \n \n \n updateStateByKeyDriver \n \n \n mapWithStatedriver \n \n \n \n \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . commons . lang3 . StringUtils\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { MapWithStateDStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  State ,  StateSpec ,  StreamingContext } \n\n /**\n * DESC:\n * 1-StreamingContextsparkconf\n * 2-socket\n * 3-flatMap\n * 4-map\n * 5-mapwithstate\n * 6-print\n * 7-ssc.start\n * 8-ssc.awaitTermination\n */ \n object  _04mapWithState  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-StreamingContextsparkconf \n     val  ssc :  StreamingContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc\n     } \n    ssc . checkpoint ( "data/baseoutput/output-5" ) \n     //2-socket \n     val  receiveStream :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n     //3-flatMap \n     //4-map \n     //5-mapwithstate \n     val  result :  MapWithStateDStream [ String ,   Int ,   Int ,   Any ]   =  receiveStream\n       . filter ( line  =>  StringUtils . isNotBlank ( line ) ) \n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . mapWithState ( StateSpec . function ( mappingFunction ) ) \n     //6-print \n    result . print ( ) \n     //7-ssc.start \n    ssc . start ( ) \n     //8-ssc.awaitTermination \n    ssc . awaitTermination ( ) \n   } \n\n   /**\n   * word-\n   * option-\n   * state-\n   */ \n   val  mappingFunction  =   ( word :   String ,  option :  Option [ Int ] ,  state :  State [ Int ] )   =>   { \n     //1- \n     if   ( state . isTimingOut ( ) )   { \n      println ( word  +   "is time out.." ) \n       //2- \n     }   else   { \n       //3-option \n       val  sum :   Int   =  option . getOrElse ( 0 )   +  state . getOption ( ) . getOrElse ( 0 ) \n       val  keyFreq :   ( String ,   Int )   =   ( word ,  sum ) \n       //4-stateupdate \n      state . update ( sum ) \n      keyFreq\n     } \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 #   \n \n \n \n \n \n : \n \n \n  \n \n \n 5sstreamigcontect(second(5)) \n \n \n \n \n \n 5s \n \n \n  \n \n \n  \n \n \n = \n \n \n < \n \n \n > \n  5min1  \n \n \n 17s \n \n \n rddrdd \n \n \n  \n \n \n 10s10s \n \n \n \n \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-\n * 2-\n * 3-flatMap\n * 4-map\n * 5-reduceBykey\n * 6-start\n * 7-awaitTermination\n */ \n object  _07windowsOpration  { \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1- \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[3]" ) \n     //batchDuration the time interval at which streaming data will be divided into batches \n     val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n    ssc . checkpoint ( "data/baseoutput/output-6" ) \n     // 2---socketlinuxwindowsncncyum install -y nc   nc -lk 9999 \n     val  receiveRDD :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n\n     // 3-flatMap \n     val  flatRDD :  DStream [ String ]   =  receiveRDD . flatMap ( _ . split ( "\\\\s+" ) ) \n     // 4-map \n     val  mapRDD :  DStream [ ( String ,   Int ) ]   =  flatRDD . map ( x  =>   ( x ,   1 ) ) \n     // 5-reduceBykey \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapRDD . reduceByKeyAndWindow ( \n       ( a :   Int ,  b :   Int )   =>  a  +  b , \n      Seconds ( 10 ) , \n      Seconds ( 10 ) ) \n    resultRDD . print ( ) \n     // 6-start \n     //Start the execution of the streams. \n    ssc . start ( ) \n     // 7-awaitTermination--Wait for the execution to stop \n    ssc . awaitTermination ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \n \n API \n \n \n \n \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-\n * 2-\n * 3-flatMap\n * 4-map\n * 5-reduceBykey\n * 6-start\n * 7-awaitTermination\n */ \n object  _08windowsOpration  { \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1- \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[3]" ) \n     //batchDuration the time interval at which streaming data will be divided into batches \n     val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n    ssc . checkpoint ( "data/baseoutput/output-6" ) \n     // 2---socketlinuxwindowsncncyum install -y nc   nc -lk 9999 \n     val  receiveRDD :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n\n     // 3-flatMap \n     val  flatRDD :  DStream [ String ]   =  receiveRDD . flatMap ( _ . split ( "\\\\s+" ) ) \n     // 4-map \n     val  mapRDD :  DStream [ ( String ,   Int ) ]   =  flatRDD . map ( x  =>   ( x ,   1 ) ) \n     // 5-reduceBykey \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapRDD . window ( Seconds ( 10 ) ,  Seconds ( 10 ) ) \n     // 6-tranform \n     val  resultRDD1 :  DStream [ ( String ,   Int ) ]   =  resultRDD . transform ( rdd  =>   { \n      rdd . reduceByKey ( _  +  _ ) \n     } ) \n    resultRDD1 . print ( ) \n     // 6-start \n     //Start the execution of the streams. \n    ssc . start ( ) \n     // 7-awaitTermination--Wait for the execution to stop \n    ssc . awaitTermination ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \n \n \n \n \n  \n \n \n package   cn . itcast . sparkstreaming \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . rdd . RDD\n import   org . apache . spark . streaming . dstream . { DStream ,  ReceiverInputDStream } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-\n * 2-\n * 3-flatMap\n * 4-map\n * 5-reduceBykey\n * 6-start\n * 7-awaitTermination\n */ \n object  _08windowsOpration  { \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1- \n     val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[3]" ) \n     //batchDuration the time interval at which streaming data will be divided into batches \n     val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n    ssc . checkpoint ( "data/baseoutput/output-6" ) \n     // 2---socketlinuxwindowsncncyum install -y nc   nc -lk 9999 \n     val  receiveRDD :  ReceiverInputDStream [ String ]   =  ssc . socketTextStream ( "node1.itcast.cn" ,   9999 ) \n\n     // 3-flatMap \n     val  flatRDD :  DStream [ String ]   =  receiveRDD . flatMap ( _ . split ( "\\\\s+" ) ) \n     // 4-map \n     val  mapRDD :  DStream [ ( String ,   Int ) ]   =  flatRDD . map ( x  =>   ( x ,   1 ) ) \n     // 5-reduceBykey \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapRDD . window ( Seconds ( 10 ) ,  Seconds ( 5 ) ) \n     // 6-tranform \n     val  resultRDD1 :  DStream [ ( String ,   Int ) ]   =  resultRDD . transform ( rdd  =>   { \n       val  reduceRDD :  RDD [ ( String ,   Int ) ]   =  rdd . reduceByKey ( _  +  _ ) \n      reduceRDD . sortBy ( _ . _2 ,   false ) \n     } ) \n    resultRDD1 . print ( ) \n     // 6-start \n     //Start the execution of the streams. \n    ssc . start ( ) \n     // 7-awaitTermination--Wait for the execution to stop \n    ssc . awaitTermination ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #  SparkStreamingSparkSQL \n SparkStreaming(HDFS) \n \n  \n \n package   cn . itcast . sparkstreaming . filesource \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . DStream\n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-\n * 2-\n * 3-flatMapmapupdateStateByKey\n * 4-ssc.start\n * 5-ssc.awitTermination\n * 6-ssc.stop\n */ \n object  _01textFileSource  { \n\n   def  updateFunc ( currenValue :  Seq [ Int ] ,  historyValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  currenValue . sum  +  historyValue . getOrElse ( 0 ) \n    Some ( sum ) \n   } \n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1- \n     val  ssc :  StreamingContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc\n     } \n    ssc . checkpoint ( "data/baseoutput/output-7" ) \n     // 2- \n     val  rddDS :  DStream [ String ]   =  ssc . textFileStream ( "hdfs://node1.itcast.cn:8020/wordcount/trans/" ) \n     // 3-flatMapmapupdateStateByKey \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  rddDS\n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n     // 4-ssc.start \n    ssc . start ( ) \n     // 5-ssc.awitTermination \n    ssc . awaitTermination ( ) \n     // 6-ssc.stop \n    ssc . stop ( true , true ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \n \n \n \n \n SparkSQLSparkSTreaming \n \n \n package   cn . itcast . sparkstreaming . filesource \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . streaming . dstream . DStream\n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-\n * 2-\n * 3-flatMapmapupdateStateByKey\n * 4-ssc.start\n * 5-ssc.awitTermination\n * 6-ssc.stop\n */ \n object  _03textFileSourceSparkSQL  { \n\n   def  updateFunc ( currenValue :  Seq [ Int ] ,  historyValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  currenValue . sum  +  historyValue . getOrElse ( 0 ) \n    Some ( sum ) \n   } \n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1- \n     val  ssc :  StreamingContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc\n     } \n    ssc . checkpoint ( "data/baseoutput/output-7" ) \n     // 2- \n     val  rddDS :  DStream [ String ]   =  ssc . textFileStream ( "hdfs://node1.itcast.cn:8020/wordcount/trans/" ) \n     // 3-flatMapmapupdateStateByKey \n     val  resultRDD :  DStream [ String ]   =  rddDS . flatMap ( _ . split ( "\\\\s+" ) ) \n\n     val  resultValue :  DStream [ ( String ,   Int ) ]   =  resultRDD . map ( ( _ ,   1 ) ) . updateStateByKey ( updateFunc ) \n\n    resultValue . foreachRDD ( rdd  =>   { \n       // Get the singleton instance of SparkSession \n       val  spark  =  SparkSession . builder . config ( rdd . sparkContext . getConf ) . getOrCreate ( ) \n       import   spark . implicits . _\n       val  df :  DataFrame  =  rdd . toDF ( "word" ,   "count" ) \n       //dsl \n       //sql \n      df . createOrReplaceTempView ( "table_view" ) \n       val  result :  DataFrame  =  spark . sql ( \n         """\n          |select *\n          |from table_view\n          |""" . stripMargin ) \n      result . show ( ) \n     } ) \n\n     // 4-ssc.start \n    ssc . start ( ) \n     // 5-ssc.awitTermination \n    ssc . awaitTermination ( ) \n     // 6-ssc.stop \n    ssc . stop ( true ,   true ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 #  SparkStreamingKafka \n kafka \n \n Kafka \n  \n  \n  \n  \n KafkaScala+JavaLinkedIn(ActiveMQ) \n KafkaBroker \n Kafka1,1.02,4,1boostrap-serverzk \n \n \n Kafka \n \n  \n1.1.0 2.4.1 \n1-kafka  \n nohup  bin/kafka-server-start.sh config/server.properties   & \n2-kafkatopic \n/export/server/kafka/bin/kafka-topics.sh  --list   --zookeeper  node1:2181\n #3-topic \n/export/server/kafka/bin/kafka-topics.sh  --describe   --zookeeper  node1:2181  --topic  spark_kafka\ntopic \n/export/server/kafka/bin/kafka-topics.sh  --create   --zookeeper  node1:2181 --replication-factor  1   --partitions   3   --topic  spark_kafka\n #topic \n/export/server/kafka/bin/kafka-topics.sh  --describe   --zookeeper  node1:2181  --topic  spark_kafka\nopic \n/export/server/kafka/bin/kafka-topics.sh  --zookeeper  node1:2181  --delete   --topic   test \n\n # \n/export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092  --topic  spark_kafka\n/export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092  --topic  spark_kafka --from-beginning \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #   \n \n SparkStreamingkafka \n 0.80.100.10 \n \n \n ReceiverReceiverkafkaWALcheckpount\n \n 1Receiver \n 2HighLevelAPIoffsetZK \n 3WALcheckpoint \n \n \n Directkafkatopicpartitionoffset \n \n 1sparkrddkafkapartition \n 2lowlevel apioffsetkafkatopic \n 3WAL \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n kafka+sparkstreaming \n SparkStreaming010Kafka \n \n \n 010kafkaDirectly \n \n \n sparkstreamingAPIkafkawordcount \n \n \n  \n \n 1-kafkasparkJar \n \n \n \n < dependency > \n\t < groupId > org.apache.spark </ groupId > \n\t < artifactId > spark-streaming-kafka-0-10_2.11 </ artifactId > \n\t < version > 2.4.5 </ version > \n </ dependency > \n \n 1 2 3 4 5 \n \n 2-streamingCOntext \n \n \n 3-KafkaUtils.creatDriectlyStreamKafka \n \n \n \n \n \n \n \n \n  \n \n \n LocationStrategies  \n \n \n 1.  LocationStrategies . PreferBrokers ( ) \n \n 1  spark  executor   kafka  broker  \n    2.  LocationStrategies . PreferConsistent ( ) ; \n \n 1  executor  \n    3.  LocationStrategies . PreferFixed ( hostMap :  collection . Map [ TopicPartition ,   String ] ) \n   4.  LocationStrategies . PreferFixed ( hostMap :  ju . Map [ TopicPartition ,   String ] ) \n \n 1 2  map  preferConsistent()  \n \n \n \n \n \n \n \n \n \n \n \n 4-recordvalue \n \n \n 5-valuewordcount \n \n \n 6-ssc.statrt \n \n \n 7-ssc.awaitTermination \n \n \n 8-ssc.stop(true,true) \n \n \n  \n \n \n package   cn . itcast . sparkstreaming . kafka \n\n import   org . apache . kafka . clients . consumer . ConsumerRecord\n import   org . apache . kafka . common . serialization . StringDeserializer\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  InputDStream } \n import   org . apache . spark . streaming . kafka010 . { ConsumerStrategies ,  KafkaUtils ,  LocationStrategies } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-kafkasparkJar\n * 2-streamingCOntext\n * 3-KafkaUtils.creatDriectlyStreamKafka\n * 4-recordvalue\n * 5-valuewordcount\n * 6-ssc.statrt\n * 7-ssc.awaitTermination\n * 8-ssc.stop(true,true)\n */ \n object  _01SparkStreamingKafkaAuto  { \n   def  updateFunc ( curentValue :  Seq [ Int ] ,  histouryValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  curentValue . sum  +  histouryValue . getOrElse ( 0 ) \n    Option ( sum ) \n   } \n\n   val  kafkaParams  =  Map [ String ,  Object ] ( \n     "bootstrap.servers"   ->   "node1:9092" , \n     "key.deserializer"   ->  classOf [ StringDeserializer ] , \n     "value.deserializer"   ->  classOf [ StringDeserializer ] , \n     "group.id"   ->   "spark_group" , \n     //offset \n     // //auto.offset.resetoffsetlatest \n     "auto.offset.reset"   ->   "latest" , \n     //kafka__consumeroffsetkafkafalseckeckpointoffsetmysql \n     // //falseSparkStreamingcheckpointoffsetmysqlredis \n     "enable.auto.commit"   ->   ( true :  java . lang . Boolean ) , \n     // \n     "auto.commit.interval.ms"   ->   "1000" \n   ) \n\n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-kafkasparkJar \n     //2-streamingCOntext \n     val  ssc :  StreamingContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc\n     } \n    ssc . checkpoint ( "data/baseoutput/cck1" ) \n     //3-KafkaUtils.creatDriectlyStreamKafka \n     //ssc: StreamingContext, \n     //locationStrategy: LocationStrategy, \n     //consumerStrategy: ConsumerStrategy[K, V] \n     val  streamRDD :  InputDStream [ ConsumerRecord [ String ,   String ] ]   =  KafkaUtils . createDirectStream [ String ,   String ] ( ssc , \n      LocationStrategies . PreferConsistent , \n      ConsumerStrategies . Subscribe [ String ,   String ] ( Array ( "spark_kafka" ) ,  kafkaParams ) ) \n     //4-recordvalue \n     val  mapValue :  DStream [ String ]   =  streamRDD . map ( _ . value ( ) ) \n     //5-valuewordcount \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapValue\n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n     //6-ssc.statrt \n    ssc . start ( ) \n     //7-ssc.awaitTermination \n    ssc . awaitTermination ( ) \n     //8-ssc.stop(true,true) \n    ssc . stop ( true ,   true ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 #   \n \t  \n \n \n https://spark.apache.org/docs/3.1.1/streaming-kafka-0-10-integration.html \n \n \n \n \n \n `checkpointcheckpointhdfshdfs \n \n \n package   cn . itcast . sparkstreaming . kafka \n\n import   org . apache . kafka . clients . consumer . ConsumerRecord\n import   org . apache . kafka . common . serialization . StringDeserializer\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  InputDStream } \n import   org . apache . spark . streaming . kafka010 . { CanCommitOffsets ,  ConsumerStrategies ,  HasOffsetRanges ,  KafkaUtils ,  LocationStrategies ,  OffsetRange } \n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-kafkasparkJar\n * 2-streamingCOntext\n * 3-KafkaUtils.creatDriectlyStreamKafka\n * 4-recordvalue\n * 5-valuewordcount\n * 6-ssc.statrt\n * 7-ssc.awaitTermination\n * 8-ssc.stop(true,true)\n */ \n object  _02SparkStreamingKafkaByPass  { \n   def  updateFunc ( curentValue :  Seq [ Int ] ,  histouryValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  curentValue . sum  +  histouryValue . getOrElse ( 0 ) \n    Option ( sum ) \n   } \n\n   val  kafkaParams  =  Map [ String ,  Object ] ( \n     "bootstrap.servers"   ->   "node1:9092" , \n     "key.deserializer"   ->  classOf [ StringDeserializer ] , \n     "value.deserializer"   ->  classOf [ StringDeserializer ] , \n     "group.id"   ->   "spark_group" , \n     //offset \n     //auto.offset.resetoffsetlatest \n     "auto.offset.reset"   ->   "latest" , \n     //falseSparkStreamingcheckpointoffsetmysqlredis \n     "enable.auto.commit"   ->   ( false :  java . lang . Boolean ) \n   ) \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-kafkasparkJar \n     //2-streamingCOntext \n     val  ssc :  StreamingContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc\n     } \n    ssc . checkpoint ( "data/baseoutput/cck2" ) \n     //3-KafkaUtils.creatDriectlyStreamKafka \n     //ssc: StreamingContext, \n     //locationStrategy: LocationStrategy, \n     //consumerStrategy: ConsumerStrategy[K, V] \n     val  streamRDD :  InputDStream [ ConsumerRecord [ String ,   String ] ]   =  KafkaUtils . createDirectStream [ String ,   String ] ( ssc , \n      LocationStrategies . PreferConsistent , \n      ConsumerStrategies . Subscribe [ String ,   String ] ( Array ( "spark_kafka" ) ,  kafkaParams ) ) \n     //offsetoffset \n    streamRDD . foreachRDD ( f  =>   { \n       if   ( f . count ( )   >   0 )   { \n         // \n        println ( "rdd is:" ,  f ) \n        f . foreach ( record  =>   { \n          println ( "record result is:" ,  record ) \n           val  value :   String   =  record . value ( ) \n          println ( "value is:" ,  value ) \n         } ) \n       }   //end id \n       //offset \n       val  offsetRanges :  Array [ OffsetRange ]   =  f . asInstanceOf [ HasOffsetRanges ] . offsetRanges\n       // \n       for ( offsetRange  <-  offsetRanges ) { \n        println ( s "topic: ${ offsetRange . topic }  partition: ${ offsetRange . partition }  fromoffset: ${ offsetRange . fromOffset }  endoffset: ${ offsetRange . untilOffset } " ) \n       } //end for \n       //offset,checkpoint \n      streamRDD . asInstanceOf [ CanCommitOffsets ] . commitAsync ( offsetRanges ) \n     } ) \n\n\n     //4-recordvalue \n     val  mapValue :  DStream [ String ]   =  streamRDD . map ( _ . value ( ) ) \n     //5-valuewordcount \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapValue\n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n     //6-ssc.statrt \n    ssc . start ( ) \n     //7-ssc.awaitTermination \n    ssc . awaitTermination ( ) \n     //8-ssc.stop(true,true) \n    ssc . stop ( true ,   true ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 \n \n \n \n \n \n \n \n  \n \n package   cn . itcast . sparkstreaming . kafka \n\n import   org . apache . kafka . clients . consumer . ConsumerRecord\n import   org . apache . kafka . common . serialization . StringDeserializer\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  InputDStream } \n import   org . apache . spark . streaming . kafka010 . _\n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-kafkasparkJar\n * 2-streamingCOntext\n * 3-KafkaUtils.creatDriectlyStreamKafka\n * 4-recordvalue\n * 5-valuewordcount\n * 6-ssc.statrt\n * 7-ssc.awaitTermination\n * 8-ssc.stop(true,true)\n */ \n object  _03SparkStreamingKafkaModel  { \n   def  updateFunc ( curentValue :  Seq [ Int ] ,  histouryValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  curentValue . sum  +  histouryValue . getOrElse ( 0 ) \n    Option ( sum ) \n   } \n\n   val  kafkaParams  =  Map [ String ,  Object ] ( \n     "bootstrap.servers"   ->   "node1:9092" , \n     "key.deserializer"   ->  classOf [ StringDeserializer ] , \n     "value.deserializer"   ->  classOf [ StringDeserializer ] , \n     "group.id"   ->   "spark_group" , \n     //offset \n     //auto.offset.resetoffsetlatest \n     "auto.offset.reset"   ->   "latest" , \n     //falseSparkStreamingcheckpointoffsetmysqlredis \n     "enable.auto.commit"   ->   ( false :  java . lang . Boolean ) \n   ) \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-kafkasparkJar \n     //2-streamingCOntext \n     val  ssc :  StreamingContext  =   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc\n     } \n    ssc . checkpoint ( "data/baseoutput/cck3" ) \n    compute ( ssc ) \n     //6-ssc.statrt \n    ssc . start ( ) \n     //7-ssc.awaitTermination \n    ssc . awaitTermination ( ) \n     //8-ssc.stop(true,true) \n    ssc . stop ( true ,   true ) \n   } \n\n\n   def  compute ( ssc :  StreamingContext ) :   Unit   =   { \n     //3-KafkaUtils.creatDriectlyStreamKafka \n     //ssc: StreamingContext, \n     //locationStrategy: LocationStrategy, \n     //consumerStrategy: ConsumerStrategy[K, V] \n     val  streamRDD :  InputDStream [ ConsumerRecord [ String ,   String ] ]   =  KafkaUtils . createDirectStream [ String ,   String ] ( ssc , \n      LocationStrategies . PreferConsistent , \n      ConsumerStrategies . Subscribe [ String ,   String ] ( Array ( "spark_kafka" ) ,  kafkaParams ) ) \n     //offsetoffset \n    streamRDD . foreachRDD ( f  =>   { \n       if   ( f . count ( )   >   0 )   { \n         // \n        println ( "rdd is:" ,  f ) \n        f . foreach ( record  =>   { \n          println ( "record result is:" ,  record ) \n           val  value :   String   =  record . value ( ) \n          println ( "value is:" ,  value ) \n         } ) \n       }   //end id \n       //offset \n       val  offsetRanges :  Array [ OffsetRange ]   =  f . asInstanceOf [ HasOffsetRanges ] . offsetRanges\n       // \n       for   ( offsetRange  <-  offsetRanges )   { \n        println ( s "topic: ${ offsetRange . topic }  partition: ${ offsetRange . partition }  fromoffset: ${ offsetRange . fromOffset }  endoffset: ${ offsetRange . untilOffset } " ) \n       }   //end for \n       //offset,checkpoint \n      streamRDD . asInstanceOf [ CanCommitOffsets ] . commitAsync ( offsetRanges ) \n     } ) \n     //4-recordvalue \n     val  mapValue :  DStream [ String ]   =  streamRDD . map ( _ . value ( ) ) \n     //5-valuewordcount \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapValue\n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 #  Checkpoint  \n \n \n \n \n \n \n \n \n \n Streaming ApplicationCheckpoint     \n \n ERROR Utils: Exception encountered   java.lang.ClassCastException: cannot assign instance of  cn.itcast.spark.ckpt.StreamingCkptState$$anonfun$streamingProcess$1 to field  org.apache.spark.streaming.dstream.ForEachDStream.org$apache$spark$streaming$dstream$ForEachDStream$$foreachFunc  of type scala.Function2 in instance of  org.apache.spark.streaming.dstream.ForEachDStream     at  java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2133)     at  java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1305)     at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2024) \n \n DStreamClassCastException \n SparkStreamingCheckpoint \n l  1MySQLRedis \n l  2MySQLZookeeperHBaseRedis \n \n \n \n  \n \n \n package   cn . itcast . sparkstreaming . kafka \n\n import   org . apache . kafka . clients . consumer . ConsumerRecord\n import   org . apache . kafka . common . serialization . StringDeserializer\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  InputDStream } \n import   org . apache . spark . streaming . kafka010 . _\n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n /**\n * DESC:\n * 1-kafkasparkJar\n * 2-streamingCOntext\n * 3-KafkaUtils.creatDriectlyStreamKafka\n * 4-recordvalue\n * 5-valuewordcount\n * 6-ssc.statrt\n * 7-ssc.awaitTermination\n * 8-ssc.stop(true,true)\n */ \n object  _04getActiveOrCreate  { \n   def  updateFunc ( curentValue :  Seq [ Int ] ,  histouryValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  curentValue . sum  +  histouryValue . getOrElse ( 0 ) \n    Option ( sum ) \n   } \n\n   val  kafkaParams  =  Map [ String ,  Object ] ( \n     "bootstrap.servers"   ->   "node1:9092" , \n     "key.deserializer"   ->  classOf [ StringDeserializer ] , \n     "value.deserializer"   ->  classOf [ StringDeserializer ] , \n     "group.id"   ->   "spark_group" , \n     //offset \n     //auto.offset.resetoffsetlatest \n     "auto.offset.reset"   ->   "latest" , \n     //falseSparkStreamingcheckpointoffsetmysqlredis \n     "enable.auto.commit"   ->   ( false :  java . lang . Boolean ) \n   ) \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-kafkasparkJar \n     val  CHECKPOINT  =   "data/baseoutput/cck5" \n     //2-streamingCOntext \n     val  ssc :  StreamingContext  =  StreamingContext . getActiveOrCreate ( CHECKPOINT ,   ( )   =>   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc . checkpoint ( CHECKPOINT ) \n      compute ( ssc ) \n      ssc\n     } ) \n     //6-ssc.statrt \n    ssc . start ( ) \n     //7-ssc.awaitTermination \n    ssc . awaitTermination ( ) \n     //8-ssc.stop(true,true) \n    ssc . stop ( true ,   true ) \n   } \n\n\n   def  compute ( ssc :  StreamingContext ) :   Unit   =   { \n     //3-KafkaUtils.creatDriectlyStreamKafka \n     //ssc: StreamingContext, \n     //locationStrategy: LocationStrategy, \n     //consumerStrategy: ConsumerStrategy[K, V] \n     val  streamRDD :  InputDStream [ ConsumerRecord [ String ,   String ] ]   =  KafkaUtils . createDirectStream [ String ,   String ] ( ssc , \n      LocationStrategies . PreferConsistent , \n      ConsumerStrategies . Subscribe [ String ,   String ] ( Array ( "spark_kafka" ) ,  kafkaParams ) ) \n     //offsetoffset \n    streamRDD . foreachRDD ( f  =>   { \n       if   ( f . count ( )   >   0 )   { \n         // \n        println ( "rdd is:" ,  f ) \n        f . foreach ( record  =>   { \n          println ( "record result is:" ,  record ) \n           val  value :   String   =  record . value ( ) \n          println ( "value is:" ,  value ) \n         } ) \n       }   //end id \n       //offset \n       val  offsetRanges :  Array [ OffsetRange ]   =  f . asInstanceOf [ HasOffsetRanges ] . offsetRanges\n       // \n       for   ( offsetRange  <-  offsetRanges )   { \n        println ( s "topic: ${ offsetRange . topic }  partition: ${ offsetRange . partition }  fromoffset: ${ offsetRange . fromOffset }  endoffset: ${ offsetRange . untilOffset } " ) \n       }   //end for \n       //offset,checkpoint \n      streamRDD . asInstanceOf [ CanCommitOffsets ] . commitAsync ( offsetRanges ) \n     } ) \n     //4-recordvalue \n     val  mapValue :  DStream [ String ]   =  streamRDD . map ( _ . value ( ) ) \n     //5-valuewordcount \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapValue\n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 MySQL \n \n \n offsetmysqlredisredismysqloffsetoffset \n \n \n \n \n \n MySQLDriverManager \n \n \n     CREATE TABLE `t_offset` (\n      `topic` varchar(255) NOT NULL,\n      `partition` int(11) NOT NULL,\n      `groupid` varchar(255) NOT NULL,\n      `offset` bigint(20) DEFAULT NULL,\n      PRIMARY KEY (`topic`,`partition`,`groupid`)\n    ) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n \n 1 2 3 4 5 6 7 \n \n topic \n \n \n \n \n \n TopicPerition \n \n \n \n \n \n \n \n \n 1-MySQL \n \n \n package   cn . itcast . sparkstreaming . kafka . toMySQL \n\n import   java . sql . { DriverManager ,  ResultSet } \n\n import   org . apache . kafka . common . TopicPartition\n import   org . apache . spark . streaming . kafka010 . OffsetRange\n\n import   scala . collection . mutable \n\n object  OffsetUtil  { \n\n     // \n     def  getOffsetMap ( groupid :   String ,  topic :   String )   =   { \n       val  connection  =  DriverManager . getConnection ( "jdbc:mysql://localhost:3306/bigdata?characterEncoding=UTF-8" ,   "root" ,   "root" ) \n       val  pstmt  =  connection . prepareStatement ( "select * from t_offset where groupid=? and topic=?" ) \n      pstmt . setString ( 1 ,  groupid ) \n      pstmt . setString ( 2 ,  topic ) \n       val  rs :  ResultSet  =  pstmt . executeQuery ( ) \n       val  offsetMap  =  mutable . Map [ TopicPartition ,   Long ] ( ) \n       while   ( rs . next ( ) )   { \n        offsetMap  +=   new  TopicPartition ( rs . getString ( "topic" ) ,  rs . getInt ( "partition" ) )   ->  rs . getLong ( "offset" ) \n       } \n      rs . close ( ) \n      pstmt . close ( ) \n      connection . close ( ) \n      offsetMap\n     } \n\n     // \n     def  saveOffsetRanges ( groupid :   String ,  offsetRange :  Array [ OffsetRange ] )   =   { \n       val  connection  =  DriverManager . getConnection ( "jdbc:mysql://localhost:3306/bigdata?characterEncoding=UTF-8" ,   "root" ,   "root" ) \n       //replace into, \n       val  pstmt  =  connection . prepareStatement ( "replace into t_offset (`topic`, `partition`, `groupid`, `offset`) values(?,?,?,?)" ) \n       for   ( o  <-  offsetRange )   { \n        pstmt . setString ( 1 ,  o . topic ) \n        pstmt . setInt ( 2 ,  o . partition ) \n        pstmt . setString ( 3 ,  groupid ) \n        pstmt . setLong ( 4 ,  o . untilOffset ) \n        pstmt . executeUpdate ( ) \n       } \n      pstmt . close ( ) \n      connection . close ( ) \n     } \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \n \n 2-MySQL \n \n \n mysql \n \n MysQLoffsetMap \n Mysql \n MySQLsubscribe \n \n \n \n \n Mysql \n \n MySQLOffset(utilOffset) \n \n \n \n \n  \n \n \n package   cn . itcast . sparkstreaming . kafka . toMySQL \n\n import   org . apache . kafka . clients . consumer . ConsumerRecord\n import   org . apache . kafka . common . TopicPartition\n import   org . apache . kafka . common . serialization . StringDeserializer\n import   org . apache . spark . SparkConf\n import   org . apache . spark . streaming . dstream . { DStream ,  InputDStream } \n import   org . apache . spark . streaming . kafka010 . _\n import   org . apache . spark . streaming . { Seconds ,  StreamingContext } \n\n import   scala . collection . mutable \n\n /**\n * DESC:\n * 1-kafkasparkJar\n * 2-streamingCOntext\n * 3-KafkaUtils.creatDriectlyStreamKafka\n * 4-recordvalue\n * 5-valuewordcount\n * 6-ssc.statrt\n * 7-ssc.awaitTermination\n * 8-ssc.stop(true,true)\n */ \n object  _01getActiveOrCreate  { \n   def  updateFunc ( curentValue :  Seq [ Int ] ,  histouryValue :  Option [ Int ] ) :  Option [ Int ]   =   { \n     val  sum :   Int   =  curentValue . sum  +  histouryValue . getOrElse ( 0 ) \n    Option ( sum ) \n   } \n\n   val  kafkaParams  =  Map [ String ,  Object ] ( \n     "bootstrap.servers"   ->   "node1:9092" , \n     "key.deserializer"   ->  classOf [ StringDeserializer ] , \n     "value.deserializer"   ->  classOf [ StringDeserializer ] , \n     "group.id"   ->   "spark_group" , \n     //offset \n     //auto.offset.resetoffsetlatest \n     "auto.offset.reset"   ->   "latest" , \n     //falseSparkStreamingcheckpointoffsetmysqlredis \n     "enable.auto.commit"   ->   ( false :  java . lang . Boolean ) \n   ) \n\n\n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-kafkasparkJar \n     val  CHECKPOINT  =   "data/baseoutput/cck6" \n     //2-streamingCOntext \n     val  ssc :  StreamingContext  =  StreamingContext . getActiveOrCreate ( CHECKPOINT ,   ( )   =>   { \n       val  conf :  SparkConf  =   new  SparkConf ( ) . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) . setMaster ( "local[*]" ) \n       val  ssc  =   new  StreamingContext ( conf ,  Seconds ( 5 ) ) \n      ssc . checkpoint ( CHECKPOINT ) \n      compute ( ssc ) \n      ssc\n     } ) \n     //6-ssc.statrt \n    ssc . start ( ) \n     //7-ssc.awaitTermination \n    ssc . awaitTermination ( ) \n     //8-ssc.stop(true,true) \n    ssc . stop ( true ,   true ) \n   } \n\n\n   def  compute ( ssc :  StreamingContext ) :   Unit   =   { \n     //1- \n     val  offsetMap :  mutable . Map [ TopicPartition ,   Long ]   =  OffsetUtil . getOffsetMap ( "spark_group" ,   "spark_kafka" ) \n     var  streamRDD :  InputDStream [ ConsumerRecord [ String ,   String ] ]   =   null \n     if   ( offsetMap . size  >   0 )   { \n      println ( "MySQLoffset,offset" ) \n       //3-KafkaUtils.creatDriectlyStreamKafka \n      streamRDD  =  KafkaUtils . createDirectStream [ String ,   String ] ( ssc , \n        LocationStrategies . PreferConsistent , \n        ConsumerStrategies . Subscribe [ String ,   String ] ( Array ( "spark_kafka" ) ,  kafkaParams ,  offsetMap ) ) \n     }   else   { \n      println ( "MySQLoffset,,latest" ) \n       //3-KafkaUtils.creatDriectlyStreamKafka \n      streamRDD  =  KafkaUtils . createDirectStream [ String ,   String ] ( ssc , \n        LocationStrategies . PreferConsistent , \n        ConsumerStrategies . Subscribe [ String ,   String ] ( Array ( "spark_kafka" ) ,  kafkaParams ) ) \n     } \n     //offsetoffset \n    streamRDD . foreachRDD ( f  =>   { \n       if   ( f . count ( )   >   0 )   { \n         // \n         //println("rdd is:", f) \n        f . foreach ( record  =>   { \n          println ( "record result is:" ,  record ) \n           val  value :   String   =  record . value ( ) \n           //println("value is:", value) \n         } ) \n       }   //end id \n       //offset \n       val  offsetRanges :  Array [ OffsetRange ]   =  f . asInstanceOf [ HasOffsetRanges ] . offsetRanges\n       // \n       for   ( offsetRange  <-  offsetRanges )   { \n        println ( s "topic: ${ offsetRange . topic }  partition: ${ offsetRange . partition }  fromoffset: ${ offsetRange . fromOffset }  endoffset: ${ offsetRange . untilOffset } " ) \n       }   //end for \n       //offset,checkpoint \n       //streamRDD.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges) \n       //MySQL \n      OffsetUtil . saveOffsetRanges ( "spark_group" , offsetRanges ) \n     } ) \n     //4-recordvalue \n     val  mapValue :  DStream [ String ]   =  streamRDD . map ( _ . value ( ) ) \n     //5-valuewordcount \n     val  resultRDD :  DStream [ ( String ,   Int ) ]   =  mapValue\n       . flatMap ( _ . split ( "\\\\s+" ) ) \n       . map ( ( _ ,   1 ) ) \n       . updateStateByKey ( updateFunc ) \n    resultRDD . print ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 \n \n \n \n \n \n \n \n ZookeeperRedis \n l  1ZookeeperKafka Kafka Topic \n l  2RedisRedis \n  RDDMySQL \n StructuredStreamig \n StructuredStreamig \n \n StructuredStreaming  --() \n  1 Flink2  SParkStreaming \n  SparkSQLDataFrameDataSet \n StructuredStreamingsparkstreaming \n SparkStreaming \n \n 1-SparkStreaingProcessingTimeEventTime \n 2-SparkStreamingAPIRDDDAG \n 3-SparkStreaming, DStream  exactly-once   input  Spark Streaming  Spark Straming  \n 4-SparkStreaming,,RDD \n \n StructuredStreaming \n \n SparkStreaing \n 1-EventTime \n 2-DataFrameDataSetAPI \n 3-sourcesinkexactly-once \n 4- \n API\n \n ProcesstimgTime-- \n CotinuceTime- \n  \n  \n StructuredStreaming-Socket \n \n \n wordcount \n \n \n \n \n \n  \n \n \n \n \n \n l socket \n l 1 \n l unbound table" \n l wordCounts \n \n \n  \n \n \n package   cn . iitcast . structedstreaming \n\n import   java . util . concurrent . TimeUnit\n\n import   org . apache . commons . lang3 . StringUtils\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  Row ,  SparkSession } \n\n /**\n * DESC:\n * 1-sparksessionsparksql\n * 2-socket\n * 3-wordcount\n * 4-\n */ \n object  _01socketSource  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-sparksessionsparksql \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       //.set("spark.sql.shuffle.partitions","4") \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" , "4" ) \n       . getOrCreate ( ) \n     //spark.sparkContext.setLogLevel("WARN") \n     import   spark . implicits . _\n     //2-socket \n     val  streamData :  DataFrame  =  spark . readStream\n       . format ( "socket" ) \n       . option ( "host" ,   "node1" ) \n       . option ( "port" ,   9999 ) \n       . load ( ) \n    streamData . printSchema ( )   // |-- value: string (nullable = true) \n     //3-wordcount \n     val  result :  Dataset [ Row ]   =  streamData\n       . as [ String ] \n       . filter ( StringUtils . isNoneBlank ( _ ) ) \n       . flatMap ( x  =>  x . split ( "\\\\s+" ) ) \n       . groupBy ( "value" ) \n       . count ( ) \n       . orderBy ( \'count . desc ) \n     //value.printSchema() \n     //4- \n     val  query :  StreamingQuery  =  result . writeStream\n       . format ( "console" ) \n       //.outputMode("complete") \n       . outputMode ( OutputMode . Complete ( ) ) \n       . option ( "numRows" ,   5 ) \n       . option ( "truncate" ,   "false" ) \n       //.trigger(Trigger.ProcessingTime(10)) \n       . trigger ( Trigger . ProcessingTime ( 0 ) ) \n       . start ( )   //Starts the execution of the streaming query, \n\n    query . awaitTermination ( ) \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \n \n  \n \n \n spark.sql.shuffle.partition200 \n StructuredStreaming \n \n \n  25 \n \n \n  \n \n \n 1-SparkSession \n \n \n 2- \n \n \n 3- \n \n \n 4- \n \n \n 5-query.awaitTermination \n \n \n 6-query.stop \n \n \n  \n \n \n package   cn . iitcast . structedstreaming . filesource \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . types . { DataTypes ,  StructField ,  StructType } \n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n\n /**\n * DESC:\n */ \n object  _01FileSource  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-SparkSession \n     //1-sparksessionsparksql \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       . getOrCreate ( ) \n     //2---- \n     val  schema :  StructType  =  StructType ( \n      StructField ( "name" ,  DataTypes . StringType ,   true )   :: \n        StructField ( "age" ,  DataTypes . IntegerType ,   true )   :: \n        StructField ( "hobby" ,  DataTypes . StringType ,   true )   ::  Nil\n     ) \n       // \n     val  streamDF :  DataFrame  =  spark . readStream\n       . format ( "csv" ) \n       . option ( "sep" ,   ";" ) \n       . option ( "header" ,   "false" ) \n       . schema ( schema ) \n       . load ( "data/baseinput/struct/" ) \n    streamDF . printSchema ( ) \n     import   spark . implicits . _\n     //3--25 \n     val  result :  DataFrame  =  streamDF\n       . filter ( \'age   <   25 ) \n       . groupBy ( \'hobby ) \n       . count ( ) \n     //4- \n     val  query :  StreamingQuery  =  result . writeStream\n       . format ( "console" ) \n       . outputMode ( OutputMode . Complete ( ) ) \n       . option ( "numRows" ,   10 ) \n       . option ( "truncate" ,   "false" ) \n       . trigger ( Trigger . ProcessingTime ( 0 ) ) \n       . start ( ) \n     //5-query.awaitTermination \n    query . awaitTermination ( ) \n     //6-query.stop \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 \n \n  \n \n \n  \n StructuredStreamingKakfa \n \n \n StructuredStreamingKakfaKafka \n \n \n  \n \n 1- \n 2-Kafka \n 3-Kafka \n 4- \n 5.query.awaitTermination \n 6-query.stop \n \n \n \n  \n \n \n package   cn . iitcast . structedstreaming . kafka \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  Row ,  SparkSession } \n\n /**\n * DESC:\n * * 1-\n * * 2-Kafka\n * * 3-Kafka\n * * 4-\n * * 5.query.awaitTermination\n * * 6-query.stop\n */ \n object  _01KafkaSourceWordcount  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1- \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       . getOrCreate ( ) \n     //spark.sparkContext.setLogLevel("WARN") \n     import   spark . implicits . _\n     //2-Kafka \n     val  streamDF :  DataFrame  =  spark . readStream\n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "subscribe" ,   "wordstopic" ) \n       . load ( ) \n     //streamDF.printSchema() \n     //root \n     // |-- key: binary (nullable = true) \n     // |-- value: binary (nullable = true) \n     // |-- topic: string (nullable = true) \n     // |-- partition: integer (nullable = true) \n     // |-- offset: long (nullable = true) \n     // |-- timestamp: timestamp (nullable = true) \n     // |-- timestampType: integer (nullable = true) \n     //3-Kafka \n     val  result :  Dataset [ Row ]   =  streamDF\n       . selectExpr ( "cast (value as string)" )   //kafkabinarycast \n       . as [ String ] \n       . flatMap ( x  =>  x . split ( "\\\\s+" ) )   // |-- value: string (nullable = true) \n       . groupBy ( $ "value" ) \n       . count ( ) \n       . orderBy ( \'count . desc ) \n     //.groupBy("value") \n     //4- \n     val  query :  StreamingQuery  =  result\n       . writeStream\n       . format ( "console" ) \n       . outputMode ( OutputMode . Complete ( ) ) \n       . trigger ( Trigger . ProcessingTime ( 0 ) ) \n       . option ( "numRows" ,   10 ) \n       . option ( "truncate" ,   false ) \n       . start ( ) \n     //5.query.awaitTermination \n    query . awaitTermination ( ) \n     //6-query.stop \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \n \n  \n \n \n Kafka \n \n \n \n \n \n \n \n \n \n \n \n kafka \n \n \n \n \n \n // Write key-value data from a DataFrame to a specific Kafka topic specified in an option \n val  ds  =  df\n   . selectExpr ( "CAST(key AS STRING)" ,   "CAST(value AS STRING)" ) \n   . writeStream\n   . format ( "kafka" ) \n   . option ( "kafka.bootstrap.servers" ,   "host1:port1,host2:port2" ) \n   . option ( "topic" ,   "topic1" ) \n   . start ( ) \n \n 1 2 3 4 5 6 7 8 #  ETL \n \n \n  \n \n \n \n \n \n Kafkatopic \nETL1 \n #topic \n/export/server/kafka/bin/kafka-topics.sh  --list   --zookeeper  node1:2181\n #topic \n/export/server/kafka/bin/kafka-topics.sh  --delete   --zookeeper  node1:2181  --topic  stationTopic\n/export/server/kafka/bin/kafka-topics.sh  --delete   --zookeeper  node1:2181  --topic  etlTopic\n\n #topic \n/export/server/kafka/bin/kafka-topics.sh  --create   --zookeeper  node1:2181 --replication-factor  1   --partitions   3   --topic  stationTopic\n/export/server/kafka/bin/kafka-topics.sh  --create   --zookeeper  node1:2181 --replication-factor  1   --partitions   3   --topic  etlTopic\n\n # \n/export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092  --topic  stationTopic\n/export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092  --topic  etlTopic\n\n #----------stationTopic \n/export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092  --topic  stationTopic --from-beginning\n/export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092  --topic  etlTopic --from-beginning\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \n \n MockKafkaProducerStation \n \n \n \n \n \n package   cn . iitcast . structedstreaming . kafka \n\n import   java . util . Properties\n\n import   org . apache . kafka . clients . producer . { KafkaProducer ,  ProducerRecord } \n import   org . apache . kafka . common . serialization . StringSerializer\n\n import   scala . util . Random\n\n /**\n * Kafka Topic\n * ID, , , , \n */ \n object  MockStationLog  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // Kafka Topic \n     val  props  =   new  Properties ( ) \n    props . put ( "bootstrap.servers" ,   "node1:9092" ) \n    props . put ( "acks" ,   "1" ) \n    props . put ( "retries" ,   "3" ) \n    props . put ( "key.serializer" ,  classOf [ StringSerializer ] . getName ) \n    props . put ( "value.serializer" ,  classOf [ StringSerializer ] . getName ) \n     val  producer  =   new  KafkaProducer [ String ,   String ] ( props ) \n\n     val  random  =   new  Random ( ) \n     val  allStatus  =  Array ( \n       "fail" ,   "busy" ,   "barring" ,   "success" ,   "success" ,   "success" , \n       "success" ,   "success" ,   "success" ,   "success" ,   "success" ,   "success" \n     ) \n\n     while   ( true )   { \n       val  callOut :   String   =   "1860000%04d" . format ( random . nextInt ( 10000 ) ) \n       val  callIn :   String   =   "1890000%04d" . format ( random . nextInt ( 10000 ) ) \n       val  callStatus :   String   =  allStatus ( random . nextInt ( allStatus . length ) ) \n       val  callDuration  =   if   ( "success" . equals ( callStatus ) )   ( 1   +  random . nextInt ( 10 ) )   *   1000L   else   0L \n\n       //  \n       val  stationLog :  StationLog  =  StationLog ( \n         "station_"   +  random . nextInt ( 10 ) , \n        callOut , \n        callIn , \n        callStatus , \n        System . currentTimeMillis ( ) , \n        callDuration\n       ) \n      println ( stationLog . toString ) \n      Thread . sleep ( 100   +  random . nextInt ( 100 ) ) \n\n       val  record  =   new  ProducerRecord [ String ,   String ] ( "stationTopic" ,  stationLog . toString ) \n      producer . send ( record ) \n     } \n\n    producer . close ( )   //  \n   } \n\n   /**\n   * \n   */ \n   case   class  StationLog ( \n                         stationId :   String ,   //ID \n                         callOut :   String ,   // \n                         callIn :   String ,   // \n                         callStatus :   String ,   // \n                         callTime :   Long ,   // \n                         duration :   Long   // \n                        )   { \n     override   def  toString :   String   =   { \n       s " $ stationId , $ callOut , $ callIn , $ callStatus , $ callTime , $ duration " \n     } \n   } \n\n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 \n \n StationTopicProducer \n \n \n staiontopickafka \n \n \n  \n \n 1-KafkaTopic \n 2-stationstaiontopic \n 3-StructuredStreamingstaiontopic \n 4-etl---------(callStatus=success) \n 5-etltopic \n 6-etltopic \n \n  \n package   cn . iitcast . structedstreaming . kafka \n\n import   org . apache . commons . lang3 . StringUtils\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . streaming . StreamingQuery\n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  SparkSession } \n\n /**\n * DESC:\n */ \n object  _02StationDataProcess  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-KafkaTopic \n     //1- \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     //2-stationstaiontopic \n     //3-StructuredStreamingstaiontopic \n     val  streamDF :  DataFrame  =  spark . readStream\n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "subscribe" ,   "stationTopic" ) \n       . load ( ) \n     //4-etl---------(callStatus=success) \n     val  result :  Dataset [ String ]   =  streamDF\n       . selectExpr ( "cast (value as string)" ) \n       . as [ String ] \n       . filter ( line  =>  StringUtils . isNotBlank ( line )   &&   "success" . equals ( line . trim . split ( "," ) ( 3 ) ) ) \n     //5-etltopic \n     val  query :  StreamingQuery  =  result . writeStream\n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "topic" ,   "etlTopic" ) \n       . option ( "checkpointLocation" , "data/baseoutput/checkpoint-2" ) \n       . start ( ) \n     //6-etltopic \n    query . awaitTermination ( ) \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  \n  \n \n \n IOT \n \n \n  \n \n #topic \n/export/server/kafka/bin/kafka-topics.sh  --list   --zookeeper  node1:2181\n #topic \n/export/server/kafka/bin/kafka-topics.sh  --delete   --zookeeper  node1:2181  --topic  iotTopic\n\n #topic \n/export/server/kafka/bin/kafka-topics.sh  --create   --zookeeper  node1:2181 --replication-factor  1   --partitions   3   --topic  iotTopic\n\n # \n/export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092  --topic  iotTopic\n # \n/export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092  --topic  iotTopic --from-beginning\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 \n \n : \n 130 \n 2 \n 3 \n \n \n  \n \n \n 1-IOTIOTtopic \n \n \n 2-StructuredStreamingKafka \n \n \n 3-IOTTopic--DSL&SQL \n \n \n 4- \n \n \n 5-query.awaitTermination \n \n \n 6-query.stop \n \n \n  \n \n \n SQL \n \n \n package   cn . iitcast . structedstreaming . iot \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n import   org . apache . spark . sql . functions . _\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . types . DoubleType\n\n /**\n * DESC:\n * 1-IOTIOTtopic\n * 2-StructuredStreamingKafka\n * 3-IOTTopic--DSL&SQL\n * 4-\n * 5-query.awaitTermination\n * 6-query.stop\n */ \n object  _01IOTStreamProcess  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-IOTIOTtopic \n     //2-StructuredStreamingKafka \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     val  streamDF :  DataFrame  =  spark . readStream\n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "subscribe" ,   "iotTopic" ) \n       . load ( ) \n     //3-IOTTopic--DSL&SQL \n     val  parseJsonData :  DataFrame  =  streamDF\n       . selectExpr ( "cast (value as string)" ) \n       . as [ String ] \n       //{"device":"device_10","deviceType":"db","signal":74.0,"time":1617612014757} \n       . select ( \n        get_json_object ( $ "value" ,   "$.device" ) . as ( "device" ) , \n        get_json_object ( $ "value" ,   "$.deviceType" ) . as ( "deviceType" ) , \n        get_json_object ( $ "value" ,   "$.signal" ) . cast ( DoubleType ) . as ( "signal" ) , \n        get_json_object ( $ "value" ,   "$.time" ) . as ( "time" ) \n       ) \n     //SQL:signal > 30   \n    parseJsonData . createOrReplaceTempView ( "table_view" ) \n     val  sql  = \n       """\n        |select round(avg(signal),2) as avg_signal,count(deviceType) as device_counts\n        |from table_view\n        |where signal >30\n        |group by deviceType\n        |""" . stripMargin\n     val  result :  DataFrame  =  spark . sql ( sql ) \n     //4- \n     val  query :  StreamingQuery  =  result . writeStream\n       . format ( "console" ) \n       . outputMode ( OutputMode . Complete ( ) ) \n       . option ( "numRows" ,   10 ) \n       . option ( "truncate" ,   false ) \n       . trigger ( Trigger . ProcessingTime ( 0 ) ) \n       . start ( ) \n     //5-query.awaitTermination \n    query . awaitTermination ( ) \n     //6-query.stop \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 \n 2DSL \n \n package   cn . iitcast . structedstreaming . iot \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . functions . _\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . types . DoubleType\n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n\n /**\n * DESC:\n * 1-IOTIOTtopic\n * 2-StructuredStreamingKafka\n * 3-IOTTopic--DSL&SQL\n * 4-\n * 5-query.awaitTermination\n * 6-query.stop\n */ \n object  _02IOTStreamProcessDSL  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     //1-IOTIOTtopic \n     //2-StructuredStreamingKafka \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       . getOrCreate ( ) \n     import   spark . implicits . _\n     val  streamDF :  DataFrame  =  spark . readStream\n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "subscribe" ,   "iotTopic" ) \n       . load ( ) \n     //3-IOTTopic--DSL&SQL \n     val  parseJsonData :  DataFrame  =  streamDF\n       . selectExpr ( "cast (value as string)" ) \n       . as [ String ] \n       //{"device":"device_10","deviceType":"db","signal":74.0,"time":1617612014757} \n       . select ( \n        get_json_object ( $ "value" ,   "$.device" ) . as ( "device" ) , \n        get_json_object ( $ "value" ,   "$.deviceType" ) . as ( "deviceType" ) , \n        get_json_object ( $ "value" ,   "$.signal" ) . cast ( DoubleType ) . as ( "signal" ) , \n        get_json_object ( $ "value" ,   "$.time" ) . as ( "time" ) \n       ) \n     //SQL:signal > 30   \n    parseJsonData . createOrReplaceTempView ( "table_view" ) \n     /* val sql =\n       """\n         |select round(avg(signal),2) as avg_signal,count(deviceType) as device_counts\n         |from table_view\n         |where signal >30\n         |group by deviceType\n         |""".stripMargin\n     val result: DataFrame = spark.sql(sql)*/ \n     val  result :  DataFrame  =  parseJsonData\n       //.select("signal", "deviceType") \n       . filter ( \'signal   >   30 ) \n       . groupBy ( "deviceType" ) \n       . agg ( \n        round ( avg ( "signal" ) ,   2 ) . as ( "avg_signal" ) , \n        count ( "deviceType" ) . as ( "device_counts" ) \n       ) \n\n     //4- \n     val  query :  StreamingQuery  =  result . writeStream\n       . format ( "console" ) \n       . outputMode ( OutputMode . Complete ( ) ) \n       . option ( "numRows" ,   10 ) \n       . option ( "truncate" ,   false ) \n       . trigger ( Trigger . ProcessingTime ( 0 ) ) \n       . start ( ) \n     //5-query.awaitTermination \n    query . awaitTermination ( ) \n     //6-query.stop \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 \n \n  \n \n \n JsonSparkSQLgetJsonObject \n \n \n Json \n \n \n  \n \n  \n StructureedStreamingForeachForeachBatch \n \n MySQL \n \n \n Foreach \n \n \n foreach \n \n \n 1- \n \n \n   url="jdbc:mysql://localhost:3306/database_name"\n  com.mysql.jdbc.Driver\n  Maven:\n   \x3c!-- MySQL Client 5.1.38 --\x3e \n   < dependency > \n  \t < groupId > mysql </ groupId > \n  \t < artifactId > mysql-connector-java </ artifactId > \n  \t < version > 5.1.38 </ version > \n   </ dependency > \n \n 1 2 3 4 5 6 7 8 9 \n 2-MySQL \n \n CREATE   TABLE   ` t_word `   ( \n     ` id `   int ( 11 )   NOT   NULL   AUTO_INCREMENT , \n     ` word `   varchar ( 255 )   NOT   NULL , \n     ` count `   int ( 11 )   DEFAULT   NULL , \n     PRIMARY   KEY   ( ` id ` ) , \n     UNIQUE   KEY   ` word `   ( ` word ` ) \n   )   ENGINE = InnoDB   AUTO_INCREMENT = 26   DEFAULT   CHARSET = utf8 ; \n\n \n 1 2 3 4 5 6 7 8 \n 2-JDBC \n \n //format(console) \n   class  JDBCSink ( url :   String ,  username :   String ,  password :   String ) \n     extends  ForeachWriter [ Row ]   with  Serializable  { \n    var  connection :  Connection  =  _  //_, \n     var  preparedStatement :  PreparedStatement  =  _\n\n     // \n     override   def   open ( partitionId :   Long ,  version :   Long ) :   Boolean   =   { \n      connection  =  DriverManager . getConnection ( url ,  username ,  password ) \n       true \n     } \n\n     /*\n    CREATE TABLE `t_word` (\n        `id` int(11) NOT NULL AUTO_INCREMENT,\n        `word` varchar(255) NOT NULL,\n        `count` int(11) DEFAULT NULL,\n        PRIMARY KEY (`id`),\n        UNIQUE KEY `word` (`word`)\n      ) ENGINE=InnoDB AUTO_INCREMENT=26 DEFAULT CHARSET=utf8;\n     */ \n     //replace INTO `bigdata`.`t_word` (`id`, `word`, `count`) VALUES (NULL, NULL, NULL); \n     //--MySQL \n     override   def  process ( row :  Row ) :   Unit   =   { \n       val  word :   String   =  row . get ( 0 ) . toString\n       val  count :   String   =  row . get ( 1 ) . toString  //introw.getLong() \n      println ( word  +   ":"   +  count ) \n       //REPLACE INTO:, \n       //:REPLACE INTO \n       val  sql  =   "REPLACE INTO `t_word` (`id`, `word`, `count`) VALUES (NULL, ?, ?);" \n      preparedStatement  =  connection . prepareStatement ( sql ) \n      preparedStatement . setString ( 1 ,  word ) \n      preparedStatement . setInt ( 2 ,  Integer . parseInt ( count ) ) \n      preparedStatement . executeUpdate ( ) \n     } \n\n     // \n     override   def  close ( errorOrNull :  Throwable ) :   Unit   =   { \n       if   ( connection  !=   null )   { \n        connection . close ( ) \n       } \n       if   ( preparedStatement  !=   null )   { \n        preparedStatement . close ( ) \n       } \n     } \n   } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \n  \n \n package   cn . iitcast . structedstreaming . toMySQL \n\n import   java . sql . { Connection ,  DriverManager ,  PreparedStatement } \n\n import   org . apache . spark . SparkConf\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . { ForeachWriter ,  Row ,  SparkSession } \n\n /**\n * DESC:\n * 1-\n * 2-SparkSocketwordcount\n * 3-wordcountt\n * 4-wordcountMySQL\n * 5-query.awaitTermination\n * 6-query.stop\n */ \n object  _01ForeachWayToMySQL  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1- \n     //1-sparksessionsparksql \n     val  conf :  SparkConf  =   new  SparkConf ( ) \n       . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . setMaster ( "local[*]" ) \n     val  spark :  SparkSession  =  SparkSession\n       . builder ( ) \n       . config ( conf ) \n       . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n       . getOrCreate ( ) \n     //spark.sparkContext.setLogLevel("WARN") \n     import   spark . implicits . _\n     // 2-SparkSocketwordcount \n     val  lines  =  spark . readStream\n       . format ( "socket" ) \n       . option ( "host" ,   "node1" ) \n       . option ( "port" ,   9999 ) \n       . load ( ) \n      \n      \n     // 3-wordcount \n     // Split the lines into words \n     val  words  =  lines\n       . as [ String ] \n       . flatMap ( _ . split ( "\\\\s+" ) ) \n     // Generate running word count \n     val  wordCounts  =  words\n       . groupBy ( "value" ) \n       . count ( ) \n     val  jDBCSink  =   new  JDBCSink ( "jdbc:mysql://localhost:3306/bigdata?characterEncoding=UTF-8" ,   "root" ,   "root" ) \n     // 4-wordcountMySQL \n     val  query :  StreamingQuery  =  wordCounts\n       . writeStream\n       . foreach ( jDBCSink ) \n       . outputMode ( OutputMode . Complete ( ) ) \n       . trigger ( Trigger . ProcessingTime ( 0 ) ) \n       . start ( ) \n     // 5-query.awaitTermination \n    query . awaitTermination ( ) \n     // 6-query.stop \n    query . stop ( ) \n   } \n\n   //format(console) \n   class  JDBCSink ( url :   String ,  username :   String ,  password :   String ) \n     extends  ForeachWriter [ Row ]   with  Serializable  { \n     var  connection :  Connection  =  _  //_, \n     var  preparedStatement :  PreparedStatement  =  _\n\n     // \n     override   def   open ( partitionId :   Long ,  version :   Long ) :   Boolean   =   { \n      connection  =  DriverManager . getConnection ( url ,  username ,  password ) \n       true \n     } \n\n     /*\n    CREATE TABLE `t_word` (\n        `id` int(11) NOT NULL AUTO_INCREMENT,\n        `word` varchar(255) NOT NULL,\n        `count` int(11) DEFAULT NULL,\n        PRIMARY KEY (`id`),\n        UNIQUE KEY `word` (`word`)\n      ) ENGINE=InnoDB AUTO_INCREMENT=26 DEFAULT CHARSET=utf8;\n     */ \n     //replace INTO `bigdata`.`t_word` (`id`, `word`, `count`) VALUES (NULL, NULL, NULL); \n     //--MySQL \n     override   def  process ( row :  Row ) :   Unit   =   { \n       val  word :   String   =  row . get ( 0 ) . toString\n       val  count :   String   =  row . get ( 1 ) . toString  //introw.getLong() \n      println ( word  +   ":"   +  count ) \n       //REPLACE INTO:, \n       //:REPLACE INTO \n       val  sql  =   "REPLACE INTO `t_word` (`id`, `word`, `count`) VALUES (NULL, ?, ?);" \n      preparedStatement  =  connection . prepareStatement ( sql ) \n      preparedStatement . setString ( 1 ,  word ) \n      preparedStatement . setInt ( 2 ,  Integer . parseInt ( count ) ) \n      preparedStatement . executeUpdate ( ) \n     } \n\n     // \n     override   def  close ( errorOrNull :  Throwable ) :   Unit   =   { \n       if   ( connection  !=   null )   { \n        connection . close ( ) \n       } \n       if   ( preparedStatement  !=   null )   { \n        preparedStatement . close ( ) \n       } \n     } \n   } \n\n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 \n ForeachWriteropenprocesscloseforeach \n \n ForeachBatch \n    package   cn . iitcast . structedstreaming . toMySQL \n  \n   import   java . sql . { Connection ,  DriverManager ,  PreparedStatement } \n  \n   import   org . apache . spark . SparkConf\n   import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n   import   org . apache . spark . sql . { DataFrame ,  ForeachWriter ,  Row ,  SaveMode ,  SparkSession } \n  \n   /**\n   * DESC:\n   * 1-\n   * 2-SparkSocketwordcount\n   * 3-wordcountt\n   * 4-wordcountMySQL\n   * 5-query.awaitTermination\n   * 6-query.stop\n   */ \n   object  _02ForeeachBatchWayToMySQL  { \n     def  main ( args :  Array [ String ] ) :   Unit   =   { \n       // 1- \n       //1-sparksessionsparksql \n       val  conf :  SparkConf  =   new  SparkConf ( ) \n         . setAppName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n         . setMaster ( "local[*]" ) \n       val  spark :  SparkSession  =  SparkSession\n         . builder ( ) \n         . config ( conf ) \n         . config ( "spark.sql.shuffle.partitions" ,   "4" ) \n         . getOrCreate ( ) \n       //spark.sparkContext.setLogLevel("WARN") \n       import   spark . implicits . _\n       // 2-SparkSocketwordcount \n       val  lines  =  spark . readStream\n         . format ( "socket" ) \n         . option ( "host" ,   "node1" ) \n         . option ( "port" ,   9999 ) \n         . load ( ) \n        \n        \n       // 3-wordcount \n       // Split the lines into words \n       val  words  =  lines\n         . as [ String ] \n         . flatMap ( _ . split ( "\\\\s+" ) ) \n       // Generate running word count \n       val  wordCounts  =  words\n         . groupBy ( "value" ) \n         . count ( ) \n       // 4-MySQL \n       val  query :  StreamingQuery  =  wordCounts\n         . writeStream\n         . outputMode ( OutputMode . Complete ( ) ) \n         . foreachBatch ( ( data :  DataFrame ,  batchID :   Long )   =>   { \n          println ( "BatchId is:" ,  batchID ) \n          data\n             . coalesce ( 1 ) \n             . write\n             . mode ( SaveMode . Overwrite ) \n             . format ( "jdbc" ) \n             . option ( "driver" ,   "com.mysql.jdbc.Driver" ) \n             . option ( "url" ,   "jdbc:mysql://localhost:3306/bigdata?characterEncoding=UTF-8" ) \n             . option ( "user" ,   "root" ) \n             . option ( "password" ,   "root" ) \n             . option ( "dbtable" ,   "bigdata.tb_word_count2" ) \n             . save ( ) \n         } ) . start ( ) \n  \n       // 5-query.awaitTermination \n      query . awaitTermination ( ) \n       // 6-query.stop \n      query . stop ( ) \n     } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 \n \n foreachbatchId \n \n \n foreachBatch batchId \n foreachBatch foreach \n  \n kafka \n spark.streaming \n -- \n \n \n \n \n \n SparkStreamingProcessTime \n StructuredStreaming \n Flink \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n Spark SQL****EventTime**** \n \n \n processing timewartermark>= \n \n \n key \n \n \n \n --\x3e()--\x3eupdate \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n package   cn . iitcast . structedstreaming . kafka \n\n import   java . sql . Timestamp\n\n import   org . apache . commons . lang3 . StringUtils\n import   org . apache . spark . SparkContext\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . { DataFrame ,  SparkSession } \n\n /**\n * Structured Streaming TCP Socket\n * 510WordCount)Watermark10\n * 2019-10-10 12:00:07,dog\n * 2019-10-10 12:00:08,owl\n *\n * 2019-10-10 12:00:14,dog\n * 2019-10-10 12:00:09,cat\n *\n * 2019-10-10 12:00:15,cat\n * 2019-10-10 12:00:08,dog\n * 2019-10-10 12:00:13,owl\n * 2019-10-10 12:00:21,owl\n *\n * 2019-10-10 12:00:04,donkey  --\n * 2019-10-10 12:00:17,owl     --\n */ \n object  StructuredWindow  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1. SparkSessionsparkConf \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . config ( "spark.sql.shuffle.partitions" ,   "3" ) \n       . getOrCreate ( ) \n     val  sc :  SparkContext  =  spark . sparkContext\n    sc . setLogLevel ( "WARN" ) \n     import   org . apache . spark . sql . functions . _\n     import   spark . implicits . _\n\n     // 2. SparkSessionTCP Socket \n     val  inputStreamDF :  DataFrame  =  spark . readStream\n       . format ( "socket" ) \n       . option ( "host" ,   "node1" ) \n       . option ( "port" ,   9999 ) \n       . load ( ) \n\n     // 3. DStream \n     val  resultStreamDF  =  inputStreamDF\n       . as [ String ] \n       . filter ( StringUtils . isNotBlank ( _ ) ) \n       // : 2019-10-12 09:00:02,cat dog \n       . flatMap ( line  =>   { \n         val  arr  =  line . trim . split ( "," ) \n         val  timestampStr :   String   =  arr ( 0 ) \n         val  wordsStr :   String   =  arr ( 1 ) \n        wordsStr\n           . split ( "\\\\s+" ) \n           //(,) \n           . map ( ( Timestamp . valueOf ( timestampStr ) ,  _ ) ) \n       } ) \n       //  \n       . toDF ( "timestamp" ,   "word" ) \n       // TODOWatermark \n       . withWatermark ( "timestamp" ,   "10 seconds" ) \n       // \n       // TODOevent time -> time, 510 \n       . groupBy ( \n           //structed streaming window spark streaming  ssc batch  \n        window ( $ "timestamp" ,   "10 seconds" ,   "5 seconds" ) , \n        $ "word" \n       ) . count ( ) \n       //  \n       //.orderBy($"window") \n\n     /*\n        root\n         |-- window: struct (nullable = true)\n         |    |-- start: timestamp (nullable = true)\n         |    |-- end: timestamp (nullable = true)\n         |-- word: string (nullable = true)\n         |-- count: long (nullable = false)\n     */ \n     //resultStreamDF.printSchema() \n\n     // 4.  \n     val  query :  StreamingQuery  =  resultStreamDF . writeStream\n       . outputMode ( OutputMode . Update ( ) ) \n       . format ( "console" ) \n       . option ( "numRows" ,   "100" ) \n       . option ( "truncate" ,   "false" ) \n       . trigger ( Trigger . ProcessingTime ( "5 seconds" ) ) \n       . start ( ) \n    query . awaitTermination ( ) \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 \n StructuredStreamingContinue \n \n \n Continuous ProcessingSpark 2.3(StormFlink)At-Least-Once  \n micro-batch processingExactly-Once 100ms \n \n \n Continue \n \n \n writeStream.trigger(Trigger.ContinueTime()) \n \n \n import   org . apache . commons . lang3 . StringUtils\n import   org . apache . spark . SparkContext\n import   org . apache . spark . sql . streaming . { OutputMode ,  StreamingQuery ,  Trigger } \n import   org . apache . spark . sql . { DataFrame ,  Dataset ,  SparkSession } \n\n /**\n * Spark 2.3StructuredStreamingContinuous processing\n * StormFlink100ms\n */ \n object  StructuredContinuous  { \n   def  main ( args :  Array [ String ] ) :   Unit   =   { \n     // 1. SparkSession \n     val  spark :  SparkSession  =  SparkSession . builder ( ) \n       . appName ( this . getClass . getSimpleName . stripSuffix ( "$" ) ) \n       . master ( "local[*]" ) \n       . config ( "spark.sql.shuffle.partitions" ,   "3" ) \n       . getOrCreate ( ) \n     val  sc :  SparkContext  =  spark . sparkContext\n    sc . setLogLevel ( "WARN" ) \n     import   org . apache . spark . sql . functions . _\n     import   spark . implicits . _\n\n     // 1. KAFKA \n     val  kafkaStreamDF :  DataFrame  =  spark . readStream\n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "subscribe" ,   "stationTopic" ) \n       . load ( ) \n\n     // 2. ETL \n     // station_0,18600004405,18900009049,success,1589711564033,9000 \n     val  etlStreamDF :  Dataset [ String ]   =  kafkaStreamDF\n       // valueString \n       . selectExpr ( "CAST(value AS STRING)" ) \n       // Dataset \n       . as [ String ] \n       // success \n       . filter ( log  =>  StringUtils . isNotBlank ( log )   &&   "success" . equals ( log . trim . split ( "," ) ( 3 ) ) ) \n\n     // 3.  \n     val  query :  StreamingQuery  =  etlStreamDF . writeStream\n       . outputMode ( OutputMode . Append ( ) ) \n       . format ( "kafka" ) \n       . option ( "kafka.bootstrap.servers" ,   "node1:9092" ) \n       . option ( "topic" ,   "etlTopic" ) \n       . option ( "checkpointLocation" ,   "./ckp"   +  System . currentTimeMillis ( ) ) \n       // TODO:  Continuous Processing, CKPT \n       //the continuous processing engine will records the progress of the query every second \n       //1Query \n       . trigger ( Trigger . Continuous ( "1 second" ) ) \n       . start ( ) \n    query . awaitTermination ( ) \n    query . stop ( ) \n   } \n } \n \n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 \n  \n \n'},{frontmatter:{layout:"Tags",title:"Tags"},regularPath:"/tag/",key:"v-b1564aac",path:"/tag/",content:""},{frontmatter:{layout:"FrontmatterKey",title:"Categories"},regularPath:"/categories/",key:"v-ef9325c4",path:"/categories/",content:""},{frontmatter:{layout:"TimeLines",title:"Timeline"},regularPath:"/timeline/",key:"v-6319eb4e",path:"/timeline/",content:""},{frontmatter:{layout:"Tag",title:"markdown Tags"},regularPath:"/tag/markdown/",key:"v-3ae5b494",path:"/tag/markdown/",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E6%8E%A8%E8%8D%90/",key:"v-584666fc",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E8%B5%84%E6%BA%90%E5%88%86%E4%BA%AB/",key:"v-23a8b635",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/",key:"v-c481210e",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:"- Tags"},regularPath:"/tag/%E7%94%9F%E4%BA%A7%E8%80%85-%E6%B6%88%E8%B4%B9%E8%80%85/",key:"v-598aa1ae",path:"/tag/-/",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E4%BA%91%E5%8E%9F%E7%94%9F/",key:"v-6b29cdd0",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/",key:"v-7ed06156",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E8%BF%9C%E7%A8%8B%E8%BF%87%E7%A8%8B%E8%B0%83%E7%94%A8/",key:"v-1f026d94",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E7%B4%A2%E5%BC%95/",key:"v-036115ab",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E7%A9%BA%E9%97%B4%E7%B4%A2%E5%BC%95/",key:"v-6dd70f48",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E9%99%84%E8%BF%91%E7%9A%84%E4%BA%BA/",key:"v-5f2b2f45",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:"k8s Tags"},regularPath:"/tag/k8s/",key:"v-32360c9a",path:"/tag/k8s/",content:""},{frontmatter:{layout:"Tag",title:"Flink Tags"},regularPath:"/tag/Flink/",key:"v-f57983ce",path:"/tag/Flink/",content:""},{frontmatter:{layout:"Tag",title:"beam Tags"},regularPath:"/tag/beam/",key:"v-1560bc14",path:"/tag/beam/",content:""},{frontmatter:{layout:"Tag",title:"k8s Tags"},regularPath:"/tag/k8s%E9%83%A8%E7%BD%B2/",key:"v-53d8f7ba",path:"/tag/k8s/",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E7%A6%BB%E7%BA%BF%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE/",key:"v-b6d24872",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E8%BF%91%E5%AE%9E%E6%97%B6/",key:"v-70e3a6de",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:"linux Tags"},regularPath:"/tag/linux%E5%91%BD%E4%BB%A4/",key:"v-70980f1d",path:"/tag/linux/",content:""},{frontmatter:{layout:"Tag",title:"CICD Tags"},regularPath:"/tag/CICD/",key:"v-18e4cd44",path:"/tag/CICD/",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E9%83%A8%E7%BD%B2/",key:"v-9f88a2c0",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:"pipeline Tags"},regularPath:"/tag/pipeline/",key:"v-6ac82e63",path:"/tag/pipeline/",content:""},{frontmatter:{layout:"Tag",title:"redis Tags"},regularPath:"/tag/redis/",key:"v-60190584",path:"/tag/redis/",content:""},{frontmatter:{layout:"Tag",title:"hash Tags"},regularPath:"/tag/%E4%B8%80%E8%87%B4%E6%80%A7hash/",key:"v-7e42d028",path:"/tag/hash/",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E6%B0%B4%E5%B9%B3%E6%89%A9%E5%AE%B9/",key:"v-257df835",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E9%BB%91%E5%90%8D%E5%8D%95%E8%BF%87%E6%BB%A4/",key:"v-38e037b7",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E4%BC%98%E5%8C%96/",key:"v-2ad4af63",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E5%88%86%E5%B8%83%E5%BC%8F/",key:"v-08174efe",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E4%B8%80%E8%87%B4%E6%80%A7/",key:"v-35aa74ba",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E5%A2%9E%E9%87%8F%E6%9B%B4%E6%96%B0/",key:"v-1aaea625",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:"bisdiff/bispatch Tags"},regularPath:"/tag/bisdiff/bispatch/",key:"v-2370020b",path:"/tag/bisdiff/bispatch/",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E5%BA%8F%E5%88%97%E5%8C%96/",key:"v-84b6fb3e",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E7%BC%93%E5%AD%98/",key:"v-d5dea5f8",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E4%B8%80%E7%BA%A7%E7%BC%93%E5%AD%98/",key:"v-812754e8",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:"id Tags"},regularPath:"/tag/%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90/",key:"v-2a06d83f",path:"/tag/id/",content:""},{frontmatter:{layout:"Tag",title:"kafka Tags"},regularPath:"/tag/kafka%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD%E5%8E%9F%E7%90%86/",key:"v-5b26dd53",path:"/tag/kafka/",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93/",key:"v-0dd50f9b",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%AE%BE%E8%AE%A1/",key:"v-19522da4",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:"spring Tags"},regularPath:"/tag/spring/",key:"v-76ecf1d8",path:"/tag/spring/",content:""},{frontmatter:{layout:"Tag",title:"IO Tags"},regularPath:"/tag/%E7%BD%91%E7%BB%9CIO/",key:"v-7ed250eb",path:"/tag/IO/",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8/",key:"v-6367c596",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E6%90%9C%E7%B4%A2/",key:"v-771b1f98",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/",key:"v-1fdf3cdf",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93%E7%BC%96%E7%A8%8B%E6%A1%86%E6%9E%B6/",key:"v-402a1f0e",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/",key:"v-134dedee",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93/",key:"v-099ad802",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:" Tags"},regularPath:"/tag/%E7%A6%BB%E7%BA%BF%E8%AE%A1%E7%AE%97/",key:"v-dd643e04",path:"/tag//",content:""},{frontmatter:{layout:"Tag",title:"sparksql Tags"},regularPath:"/tag/sparksql/",key:"v-093485d8",path:"/tag/sparksql/",content:""},{frontmatter:{layout:"Category",title:"tool Categories"},regularPath:"/categories/tool/",key:"v-638ddf39",path:"/categories/tool/",content:""},{frontmatter:{layout:"Category",title:" Categories"},regularPath:"/categories/%E8%BD%AF%E4%BB%B6%E8%B5%84%E6%BA%90/",key:"v-b571f312",path:"/categories//",content:""},{frontmatter:{layout:"Category",title:" Categories"},regularPath:"/categories/%E9%9A%8F%E7%AC%94/",key:"v-2bf76980",path:"/categories//",content:""},{frontmatter:{layout:"Category",title:" Categories"},regularPath:"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/",key:"v-ab31fcde",path:"/categories//",content:""},{frontmatter:{layout:"Category",title:" Categories"},regularPath:"/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/",key:"v-c77fd4f6",path:"/categories//",content:""},{frontmatter:{layout:"Category",title:" Categories"},regularPath:"/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/",key:"v-6f5d94e8",path:"/categories//",content:""},{frontmatter:{layout:"Category",title:" Categories"},regularPath:"/categories/%E5%85%B6%E4%BB%96/",key:"v-7fc479ec",path:"/categories//",content:""},{frontmatter:{layout:"Category",title:" Categories"},regularPath:"/categories/%E7%AE%97%E6%B3%95/",key:"v-f37f30be",path:"/categories//",content:""},{frontmatter:{layout:"Category",title:" Categories"},regularPath:"/categories/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/",key:"v-5db6d2ed",path:"/categories//",content:""},{frontmatter:{layout:"Category",title:"CICD Categories"},regularPath:"/categories/CICD/",key:"v-60c96f6a",path:"/categories/CICD/",content:""},{frontmatter:{layout:"Category",title:" Categories"},regularPath:"/categories/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/",key:"v-2dd62492",path:"/categories//",content:""},{frontmatter:{layout:"Category",title:"nosql Categories"},regularPath:"/categories/nosql/",key:"v-03f2e000",path:"/categories/nosql/",content:""},{frontmatter:{layout:"Category",title:" Categories"},regularPath:"/categories/%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/",key:"v-c1e9d86a",path:"/categories//",content:""}],themeConfig:{repo:"GordonChanFZ/gordonchanfz.github.io/",docsDir:"docs",docsBranch:"main",editLinks:!1,subSidebar:"auto",codeTheme:"solarizedlight",activeHeaderLinks:!1,nav:[{text:"",link:"/",icon:"reco-home"},{text:"",items:[{text:"flink",link:"//flink.md"},{text:"spark",link:"//spark.md"}]},{text:"",items:[{text:"ElasticSearch",link:"//ElasticSearch.md"},{text:"hbase",link:"//hbase.md"},{text:"redis",link:"//redis.md"}]},{text:"",items:[{text:"Docker",link:"//Docker.md"}]},{text:"",items:[{text:"kafka",link:"//kafka.md"},{text:"Sqoop",link:"//Sqoop.md"}]},{text:"",link:"/tool/emoji/emoji"},{text:"",link:"/aboutme",icon:"reco-account"}],sidebar:{"/sql/":[{title:"Mysql",collapsable:!1,sidebarDepth:2,children:["mysql/MySQL","mysql/MySQL","mysql/MySQL B+","mysql/MySQLMVCC","mysql/MySQL","mysql/MySQL","mysql/MySQL","mysql/sql","mysql/"]},{title:"Redis",collapsable:!1,sidebarDepth:2,children:["redis/Redis","redis/Redis","redis/Redis"]},{title:"ElasticSearch",collapsable:!1,sidebarDepth:2,children:["es/ElasticSearch","es/ElasticSearch","es/ES"]}],"/java/":[{title:"Java ",collapsable:!1,sidebarDepth:2,children:["basics/","basics/","basics/","basics/","basics/","basics/Java8","basics/Java"]},{title:"Java ",collapsable:!1,sidebarDepth:2,children:["collection/HashMap","collection/ConcurrentHashMap","collection/HashSet","collection/"]},{title:"Java IO",collapsable:!1,sidebarDepth:2,children:["io/IO.md"]},{title:"Java ",collapsable:!1,sidebarDepth:2,children:["reflect/","reflect/","reflect/","reflect/"]},{title:"Java ",collapsable:!1,sidebarDepth:2,children:["thread/","thread/","thread/Java","thread/synchronized ","thread/volatile","thread/AQSLOCK","thread/  Atomic","thread/ThreadLocal","thread/","thread/"]},{title:"JVM",collapsable:!1,sidebarDepth:2,children:["jvm/JVM","jvm/JVM"]}],"/java/":[{title:"Spring",collapsable:!1,sidebarDepth:2,children:["spring family/Spring/Spring.md","spring family/Spring/Spring.md"]},{title:"SpringMVC",collapsable:!1,sidebarDepth:2,children:["spring family/SpringMVC/SpringMVC.md"]},{title:"SpringBoot",collapsable:!1,sidebarDepth:2,children:["spring family/SpringBoot/SpringBootStarter.md","spring family/SpringBoot/SpringBoot.md","spring family/SpringBoot/SpringBoot requestbodyjson.md","spring family/SpringBoot/SpringBoot.md","spring family/SpringBoot/SpringBoot requestbodyjson.md","spring family/SpringBoot/SpringBoot LocalDateTime.md"]},{title:"JWT",collapsable:!1,sidebarDepth:2,children:["JWT.md"]},{title:"Netty",collapsable:!1,sidebarDepth:2,children:["Netty/Netty.md","Netty/Netty2.md","Netty/Netty.md","Netty/WebSocket.md"]}],"//":[{title:"Hadoop",collapsable:!1,sidebarDepth:2,children:["Hadoop/Hadoop","Hadoop/HDFS","Hadoop/MapReduce","Hadoop/Yarn"]},{title:"Hive",collapsable:!1,sidebarDepth:2,children:["Hive/Hive","Hive/HiveDDLDML","Hive/Hive","Hive/Hive","Hive/Hive"]},{title:"HBase",collapsable:!1,sidebarDepth:2,children:["HBase/HBase"]},{title:"Flink",collapsable:!1,sidebarDepth:2,children:["Flink/flink"]}],"//":[{title:"",collapsable:!1,sidebarDepth:2,children:["/","/","/","/","/","/","/"]},{title:"",collapsable:!1,sidebarDepth:2,children:["/01","/02","/03","/04","/05","/06DFS","/07","/08","/09","/10","/11BFS","/12"]},{title:"codetop",collapsable:!1,sidebarDepth:2,children:["codetop/codetop(21-40)"]},{title:"offer",collapsable:!1,sidebarDepth:2,children:["offer"]}],"//":[{title:"",collapsable:!1,sidebarDepth:2,children:["/ "]},{title:"ID",collapsable:!1,sidebarDepth:2,children:["ID/ID","ID/RedisID","ID/ZooKeeperID"]},{title:"",collapsable:!1,sidebarDepth:2,children:["/"]},{title:"",collapsable:!1,sidebarDepth:2,children:["/"]},{title:"OAuth2.0",collapsable:!1,sidebarDepth:2,children:["OAuth2.0/OAuth2.0"]},{title:"",collapsable:!1,sidebarDepth:2,children:["/K8S"]},{title:"",collapsable:!1,sidebarDepth:2,children:["","Gateway"]}]},type:"blog",blogConfig:{socialLinks:[{icon:"reco-github",link:"https://gordonchanfz.github.io/"}]},logo:"/logo.png",search:!0,searchMaxSuggestions:10,lastUpdated:"Last Updated",author:"Gordon",authorAvatar:"/avatar.jpeg",record:"xxxxxx-1",startYear:"2018"},locales:{"/":{lang:"zh-CN",title:"",path:"/"}}};var Me={data:()=>({username:"",password:""}),methods:{login(){if(this.username&&this.password){var n={user1:"password1",user2:"password2"},e=!1;for(var t in n){var r=t,a=n[t];if(this.username===r&&this.password===a){e=!0;break}}if(e){const n=JSON.stringify({name:this.username,time:(new Date).getTime()});window.localStorage.setItem("user_auth_xxxxxxxxxxxx",n),this.$emit("close",!0)}else this.$dlg.alert("",{messageType:"warning"})}else this.$dlg.alert("",{messageType:"warning"})}}},je=(t(275),Object(Ce.a)(Me,(function(){var n=this,e=n._self._c;return e("div",{staticClass:"login-form"},[e("div",{staticClass:"form-header"},[n._v("")]),n._v(" "),e("div",[e("input",{directives:[{name:"model",rawName:"v-model",value:n.username,expression:"username"}],staticClass:"form-control",attrs:{type:"text"},domProps:{value:n.username},on:{input:function(e){e.target.composing||(n.username=e.target.value)}}})]),n._v(" "),e("div",{staticClass:"form-header"},[n._v("")]),n._v(" "),e("div",[e("input",{directives:[{name:"model",rawName:"v-model",value:n.password,expression:"password"}],staticClass:"form-control",attrs:{type:"password"},domProps:{value:n.password},on:{input:function(e){e.target.composing||(n.password=e.target.value)}}})]),n._v(" "),e("div",{staticClass:"btn-row"},[e("button",{staticClass:"btn",on:{click:n.login}},[n._v("\n      \n    ")])])])}),[],!1,null,null,null).exports),Le=(t(17),t(28)),Ne={computed:{$recoPosts(){let n=this.$site.pages;return n=Object(Le.a)(n,!1),Object(Le.c)(n),n},$recoPostsForTimeline(){let n=this.$recoPosts;const e={},t=[];n=Object(Le.a)(n,!0),this.pages=0==n.length?[]:n;for(let t=0,r=n.length;t<r;t++){const r=n[t],a=$e(r.frontmatter.date,"year");e[a]?e[a].push(r):e[a]=[r]}for(const n in e){const r=e[n];Object(Le.b)(r),t.unshift({year:n,data:r})}return t},$categoriesList(){return this.$categories.list.map(n=>(n.pages=n.pages.filter(n=>!1!==n.frontmatter.publish),n))},$tagesList(){return this.$tags.list.map(n=>(n.pages=n.pages.filter(n=>!1!==n.frontmatter.publish),n))},$showSubSideBar(){const{$themeConfig:{subSidebar:n,sidebar:e},$frontmatter:{subSidebar:t,sidebar:r}}=this,a=this.$page.headers||[];return!([t,r].indexOf(!1)>-1)&&([t,r].indexOf("auto")>-1&&a.length>0||[n,e].indexOf("auto")>-1&&a.length>0)}}};function $e(n,e){n=function(n){var e=new Date(n).toJSON();return new Date(+new Date(e)+288e5).toISOString().replace(/T/g," ").replace(/\.[\d]{3}Z/,"").replace(/-/g,"/")}(n);const t=new Date(n),r=t.getFullYear(),a=t.getMonth()+1,o=t.getDate();return"year"==e?r:`${a}-${o}`}var Ue={all:"",article:"",tag:"",category:"",friendLink:"",timeLine:"",timeLineMsg:""},ze={all:"",article:"",tag:"",category:"",friendLink:"",timeLine:"",timeLineMsg:""},He={all:"All",article:"Articles",tag:"Tags",category:"Categories",friendLink:"Friend Links",timeLine:"TimeLine",timeLineMsg:"Yesterday Once More!"},qe={all:"",article:"",tag:"",category:"",friendLink:"",timeLine:"",timeLineMsg:""},Ve={all:"",article:"",tag:"",category:"",friendLink:" ",timeLine:" ",timeLineMsg:" !"},Ke={all:"Todas",article:"Artculos",tag:"Etiquetas",category:"Categoras",friendLink:"Pginas amigas",timeLine:"Cronologa",timeLineMsg:"Ayer otra vez!"},We={computed:{$recoLocales(){const n=this.$themeLocaleConfig.recoLocales||{};return/^zh\-(CN|SG)$/.test(this.$lang)?{...Ue,...n}:/^zh\-(HK|MO|TW)$/.test(this.$lang)?{...ze,...n}:/^ja\-JP$/.test(this.$lang)?{...qe,...n}:/^ko\-KR$/.test(this.$lang)?{...Ve,...n}:/^es(\-[A-Z]+)?$/.test(this.$lang)?{...Ke,...n}:{...He,...n}}}},Ge=t(42);t(276);r.b.component("tongji",()=>Promise.all([t.e(0),t.e(33)]).then(t.bind(null,1564))),r.b.component("Badge",()=>Promise.all([t.e(0),t.e(35)]).then(t.bind(null,1607)));var Je={name:"BackToTop",data:()=>({visible:!1,customStyle:{right:"1rem",bottom:"6rem",width:"2.5rem",height:"2.5rem","border-radius":".25rem","line-height":"2.5rem"},visibilityHeight:400}),mounted(){window.addEventListener("scroll",this.throttle(this.handleScroll,500))},beforeDestroy(){window.removeEventListener("scroll",this.throttle(this.handleScroll,500))},methods:{handleScroll(){this.visible=window.pageYOffset>this.visibilityHeight},backToTop(){window.scrollTo(0,0)},throttle(n,e){let t=null,r=Date.now();return function(){const a=Date.now(),o=e-(a-r),i=this,s=arguments;clearTimeout(t),o<=0?(n.apply(i,s),r=Date.now()):t=setTimeout(n,o)}}}},Ye=(t(277),Object(Ce.a)(Je,(function(){var n=this._self._c;return n("transition",{attrs:{name:"fade"}},[n("div",{directives:[{name:"show",rawName:"v-show",value:this.visible,expression:"visible"}],staticClass:"back-to-ceiling",style:this.customStyle,on:{click:this.backToTop}},[n("svg",{staticClass:"icon",attrs:{t:"1574745035067",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"5404"}},[n("path",{attrs:{d:"M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z","p-id":"5405"}}),n("path",{attrs:{d:"M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z","p-id":"5406"}})])])])}),[],!1,null,"c6073ba8",null).exports);t(278),t(284);const Qe={prev:"",next:"",go:"",jump:""},Xe={prev:"",next:"",go:"",jump:""},Ze={prev:"Prev",next:"Next",go:"Go",jump:"Jump To"},nt={prev:"",next:"",go:"",jump:""},et={prev:" ",next:" ",go:"",jump:""};var tt={data:()=>({changePage:"",pageSize:10}),props:{total:{type:Number,default:10},perPage:{type:Number,default:10},currentPage:{type:Number,default:1}},computed:{pages(){return Math.ceil(this.total/this.pageSize)},efont:function(){return this.pages>7},indexes:function(){var n=1,e=this.pages,t=[];for(this.pages>=7&&(this.currentPage>5&&this.currentPage<this.pages-4?(n=Number(this.currentPage)-3,e=Number(this.currentPage)+3):this.currentPage<=5?(n=1,e=7):(e=this.pages,n=this.pages-6));n<=e;)t.push(n),n++;return t},pagationLocales(){return function(n){const{$lang:e,$recoLocales:{pagation:t}={}}=n;return t||(/^zh\-(CN|SG)$/.test(e)?Qe:/^zh\-(HK|MO|TW)$/.test(e)?Xe:/^ja\-JP$/.test(e)?nt:/^ko\-KR$/.test(e)?et:Ze)}(this)},showStartFakePageNum:function(){return this.efont&&!this.indexes.includes(1)},showLastFakePageNum:function(){return this.efont&&!this.indexes.includes(this.pages)}},methods:{goPrev(){let n=this.currentPage;this.currentPage>1&&this.emit(--n)},goNext(){let n=this.currentPage;n<this.pages&&this.emit(++n)},jumpPage:function(n){const e=parseInt(n);e<=this.pages&&e>0?this.emit(e):alert(`0${this.pages}`)},emit(n){this.$emit("getCurrentPage",n)}}},rt=(t(285),Object(Ce.a)(tt,(function(){var n=this,e=n._self._c;return e("div",{directives:[{name:"show",rawName:"v-show",value:n.pages>1,expression:"pages > 1"}],staticClass:"pagation"},[e("div",{staticClass:"pagation-list"},[e("span",{directives:[{name:"show",rawName:"v-show",value:n.currentPage>1,expression:"currentPage > 1"}],staticClass:"jump",attrs:{unselectable:"on"},on:{click:n.goPrev}},[n._v(n._s(n.pagationLocales.prev))]),n._v(" "),n.showStartFakePageNum?e("span",{staticClass:"jump",on:{click:function(e){return n.jumpPage(1)}}},[n._v("1")]):n._e(),n._v(" "),n.showStartFakePageNum&&n.indexes[0]>2?e("span",{staticClass:"ellipsis"},[n._v("...")]):n._e(),n._v(" "),n._l(n.indexes,(function(t){return e("span",{key:t,staticClass:"jump",class:{bgprimary:n.currentPage==t},on:{click:function(e){return n.jumpPage(t)}}},[n._v(n._s(t))])})),n._v(" "),n.showLastFakePageNum&&n.pages-n.indexes.at(-1)>1?e("span",{staticClass:"ellipsis"},[n._v("...")]):n._e(),n._v(" "),n.showLastFakePageNum?e("span",{staticClass:"jump",on:{click:function(e){return n.jumpPage(n.pages)}}},[n._v(n._s(n.pages))]):n._e(),n._v(" "),n.currentPage<n.pages?e("span",{staticClass:"jump",on:{click:n.goNext}},[n._v(n._s(n.pagationLocales.next))]):n._e(),n._v(" "),e("span",{staticClass:"jumppoint"},[n._v(n._s(n.pagationLocales.jump))]),n._v(" "),e("span",{staticClass:"jumpinp"},[e("input",{directives:[{name:"model",rawName:"v-model",value:n.changePage,expression:"changePage"}],attrs:{type:"text"},domProps:{value:n.changePage},on:{input:function(e){e.target.composing||(n.changePage=e.target.value)}}})]),n._v(" "),e("span",{staticClass:"jump gobtn",on:{click:function(e){return n.jumpPage(n.changePage)}}},[n._v(n._s(n.pagationLocales.go))])],2)])}),[],!1,null,"22b6649c",null).exports),at={name:"Valine",props:{options:{type:Object,default:()=>({})}},mounted:function(){this.initValine()},methods:{initValine(){new(t(286))({el:"#valine",placeholder:"just go go",notify:!1,verify:!1,avatar:"retro",visitor:!0,recordIP:!1,path:window.location.pathname,...this.options})}},watch:{$route(n,e){n.path!==e.path&&setTimeout(()=>{this.initValine()},300)}}},ot=(t(287),Object(Ce.a)(at,(function(){this._self._c;return this._m(0)}),[function(){var n=this._self._c;return n("div",{staticClass:"valine-wrapper"},[n("div",{attrs:{id:"valine"}})])}],!1,null,null,null).exports),it=t(2);function st(n){return Object(it.a)((function(e,t){void 0===e.inject&&(e.inject={}),Array.isArray(e.inject)||(e.inject[t]=n||t)}))}function lt(n){return"function"!=typeof n||!n.managed&&!n.managedReactive}function ct(n){var e=function(){var t=this,r="function"==typeof n?n.call(this):n;for(var a in(r=Object.create(r||null))[pt]=Object.create(this[pt]||{}),e.managed)r[e.managed[a]]=this[a];var o=function(n){r[e.managedReactive[n]]=i[n],Object.defineProperty(r[pt],e.managedReactive[n],{enumerable:!0,configurable:!0,get:function(){return t[n]}})},i=this;for(var a in e.managedReactive)o(a);return r};return e.managed={},e.managedReactive={},e}var pt="__reactiveInject__";function ut(n){Array.isArray(n.inject)||(n.inject=n.inject||{},n.inject[pt]={from:pt,default:{}})}var dt="undefined"!=typeof Reflect&&void 0!==Reflect.getMetadata;function mt(n,e,t){if(dt&&!Array.isArray(n)&&"function"!=typeof n&&!n.hasOwnProperty("type")&&void 0===n.type){var r=Reflect.getMetadata("design:type",e,t);r!==Object&&(n.type=r)}}function gt(n){return void 0===n&&(n={}),function(e,t){mt(n,e,t),Object(it.a)((function(e,t){(e.props||(e.props={}))[t]=n}))(e,t)}}function ft(n,e){void 0===e&&(e={});var t=e.deep,r=void 0!==t&&t,a=e.immediate,o=void 0!==a&&a;return Object(it.a)((function(e,t){"object"!=typeof e.watch&&(e.watch=Object.create(null));var a=e.watch;"object"!=typeof a[n]||Array.isArray(a[n])?void 0===a[n]&&(a[n]=[]):a[n]=[a[n]],a[n].push({handler:t,deep:r,immediate:o})}))}var ht=t(21);const vt=n=>Object(ht.stringify)(n),bt=(n,e)=>`${n}${Object(ht.stringify)(e,{addQueryPrefix:!0})}`,kt=(n,e)=>`${n.replace(/\/$/,"")}/${e.replace(/^\//,"")}`;var yt=t(134),St=t.n(yt);const xt=n=>St()(n,"YYYY-MM-DD HH:mm:ss"),wt=n=>(n.split("#")[0]||"").split("?")[0]||"",Et=n=>Object(ht.parse)(n,{ignoreQueryPrefix:!0})
/*!
 * vue-i18n v8.28.2 
 * (c) 2022 kazuya kawaguchi
 * Released under the MIT License.
 */;var Dt=["compactDisplay","currency","currencyDisplay","currencySign","localeMatcher","notation","numberingSystem","signDisplay","style","unit","unitDisplay","useGrouping","minimumIntegerDigits","minimumFractionDigits","maximumFractionDigits","minimumSignificantDigits","maximumSignificantDigits"],Ct=["dateStyle","timeStyle","calendar","localeMatcher","hour12","hourCycle","timeZone","formatMatcher","weekday","era","year","month","day","hour","minute","second","timeZoneName"];function It(n,e){"undefined"!=typeof console&&(console.warn("[vue-i18n] "+n),e&&console.warn(e.stack))}var Tt=Array.isArray;function Ot(n){return null!==n&&"object"==typeof n}function At(n){return"string"==typeof n}var _t=Object.prototype.toString;function Rt(n){return"[object Object]"===_t.call(n)}function Pt(n){return null==n}function Ft(n){return"function"==typeof n}function Bt(){for(var n=[],e=arguments.length;e--;)n[e]=arguments[e];var t=null,r=null;return 1===n.length?Ot(n[0])||Tt(n[0])?r=n[0]:"string"==typeof n[0]&&(t=n[0]):2===n.length&&("string"==typeof n[0]&&(t=n[0]),(Ot(n[1])||Tt(n[1]))&&(r=n[1])),{locale:t,params:r}}function Mt(n){return JSON.parse(JSON.stringify(n))}function jt(n,e){return!!~n.indexOf(e)}var Lt=Object.prototype.hasOwnProperty;function Nt(n,e){return Lt.call(n,e)}function $t(n){for(var e=arguments,t=Object(n),r=1;r<arguments.length;r++){var a=e[r];if(null!=a){var o=void 0;for(o in a)Nt(a,o)&&(Ot(a[o])?t[o]=$t(t[o],a[o]):t[o]=a[o])}}return t}function Ut(n,e){if(n===e)return!0;var t=Ot(n),r=Ot(e);if(!t||!r)return!t&&!r&&String(n)===String(e);try{var a=Tt(n),o=Tt(e);if(a&&o)return n.length===e.length&&n.every((function(n,t){return Ut(n,e[t])}));if(a||o)return!1;var i=Object.keys(n),s=Object.keys(e);return i.length===s.length&&i.every((function(t){return Ut(n[t],e[t])}))}catch(n){return!1}}function zt(n){return null!=n&&Object.keys(n).forEach((function(e){"string"==typeof n[e]&&(n[e]=n[e].replace(/</g,"&lt;").replace(/>/g,"&gt;").replace(/"/g,"&quot;").replace(/'/g,"&apos;"))})),n}var Ht={name:"i18n",functional:!0,props:{tag:{type:[String,Boolean,Object],default:"span"},path:{type:String,required:!0},locale:{type:String},places:{type:[Array,Object]}},render:function(n,e){var t=e.data,r=e.parent,a=e.props,o=e.slots,i=r.$i18n;if(i){var s=a.path,l=a.locale,c=a.places,p=o(),u=i.i(s,l,function(n){var e;for(e in n)if("default"!==e)return!1;return Boolean(e)}(p)||c?function(n,e){var t=e?function(n){0;return Array.isArray(n)?n.reduce(Vt,{}):Object.assign({},n)}(e):{};if(!n)return t;var r=(n=n.filter((function(n){return n.tag||""!==n.text.trim()}))).every(Kt);0;return n.reduce(r?qt:Vt,t)}(p.default,c):p),d=a.tag&&!0!==a.tag||!1===a.tag?a.tag:"span";return d?n(d,t,u):u}}};function qt(n,e){return e.data&&e.data.attrs&&e.data.attrs.place&&(n[e.data.attrs.place]=e),n}function Vt(n,e,t){return n[t]=e,n}function Kt(n){return Boolean(n.data&&n.data.attrs&&n.data.attrs.place)}var Wt,Gt={name:"i18n-n",functional:!0,props:{tag:{type:[String,Boolean,Object],default:"span"},value:{type:Number,required:!0},format:{type:[String,Object]},locale:{type:String}},render:function(n,e){var t=e.props,r=e.parent,a=e.data,o=r.$i18n;if(!o)return null;var i=null,s=null;At(t.format)?i=t.format:Ot(t.format)&&(t.format.key&&(i=t.format.key),s=Object.keys(t.format).reduce((function(n,e){var r;return jt(Dt,e)?Object.assign({},n,((r={})[e]=t.format[e],r)):n}),null));var l=t.locale||o.locale,c=o._ntp(t.value,l,i,s),p=c.map((function(n,e){var t,r=a.scopedSlots&&a.scopedSlots[n.type];return r?r(((t={})[n.type]=n.value,t.index=e,t.parts=c,t)):n.value})),u=t.tag&&!0!==t.tag||!1===t.tag?t.tag:"span";return u?n(u,{attrs:a.attrs,class:a.class,staticClass:a.staticClass},p):p}};function Jt(n,e,t){Xt(n,t)&&Zt(n,e,t)}function Yt(n,e,t,r){if(Xt(n,t)){var a=t.context.$i18n;(function(n,e){var t=e.context;return n._locale===t.$i18n.locale})(n,t)&&Ut(e.value,e.oldValue)&&Ut(n._localeMessage,a.getLocaleMessage(a.locale))||Zt(n,e,t)}}function Qt(n,e,t,r){if(t.context){var a=t.context.$i18n||{};e.modifiers.preserve||a.preserveDirectiveContent||(n.textContent=""),n._vt=void 0,delete n._vt,n._locale=void 0,delete n._locale,n._localeMessage=void 0,delete n._localeMessage}else It("Vue instance does not exists in VNode context")}function Xt(n,e){var t=e.context;return t?!!t.$i18n||(It("VueI18n instance does not exists in Vue instance"),!1):(It("Vue instance does not exists in VNode context"),!1)}function Zt(n,e,t){var r,a,o=function(n){var e,t,r,a;At(n)?e=n:Rt(n)&&(e=n.path,t=n.locale,r=n.args,a=n.choice);return{path:e,locale:t,args:r,choice:a}}(e.value),i=o.path,s=o.locale,l=o.args,c=o.choice;if(i||s||l)if(i){var p=t.context;n._vt=n.textContent=null!=c?(r=p.$i18n).tc.apply(r,[i,c].concat(nr(s,l))):(a=p.$i18n).t.apply(a,[i].concat(nr(s,l))),n._locale=p.$i18n.locale,n._localeMessage=p.$i18n.getLocaleMessage(p.$i18n.locale)}else It("`path` is required in v-t directive");else It("value type not supported")}function nr(n,e){var t=[];return n&&t.push(n),e&&(Array.isArray(e)||Rt(e))&&t.push(e),t}function er(n,e){void 0===e&&(e={bridge:!1}),er.installed=!0;var t;(Wt=n).version&&Number(Wt.version.split(".")[0]);(t=Wt).prototype.hasOwnProperty("$i18n")||Object.defineProperty(t.prototype,"$i18n",{get:function(){return this._i18n}}),t.prototype.$t=function(n){for(var e=[],t=arguments.length-1;t-- >0;)e[t]=arguments[t+1];var r=this.$i18n;return r._t.apply(r,[n,r.locale,r._getMessages(),this].concat(e))},t.prototype.$tc=function(n,e){for(var t=[],r=arguments.length-2;r-- >0;)t[r]=arguments[r+2];var a=this.$i18n;return a._tc.apply(a,[n,a.locale,a._getMessages(),this,e].concat(t))},t.prototype.$te=function(n,e){var t=this.$i18n;return t._te(n,t.locale,t._getMessages(),e)},t.prototype.$d=function(n){for(var e,t=[],r=arguments.length-1;r-- >0;)t[r]=arguments[r+1];return(e=this.$i18n).d.apply(e,[n].concat(t))},t.prototype.$n=function(n){for(var e,t=[],r=arguments.length-1;r-- >0;)t[r]=arguments[r+1];return(e=this.$i18n).n.apply(e,[n].concat(t))},Wt.mixin(function(n){function e(){this!==this.$root&&this.$options.__INTLIFY_META__&&this.$el&&this.$el.setAttribute("data-intlify",this.$options.__INTLIFY_META__)}return void 0===n&&(n=!1),n?{mounted:e}:{beforeCreate:function(){var n=this.$options;if(n.i18n=n.i18n||(n.__i18nBridge||n.__i18n?{}:null),n.i18n)if(n.i18n instanceof vr){if(n.__i18nBridge||n.__i18n)try{var e=n.i18n&&n.i18n.messages?n.i18n.messages:{};(n.__i18nBridge||n.__i18n).forEach((function(n){e=$t(e,JSON.parse(n))})),Object.keys(e).forEach((function(t){n.i18n.mergeLocaleMessage(t,e[t])}))}catch(n){0}this._i18n=n.i18n,this._i18nWatcher=this._i18n.watchI18nData()}else if(Rt(n.i18n)){var t=this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof vr?this.$root.$i18n:null;if(t&&(n.i18n.root=this.$root,n.i18n.formatter=t.formatter,n.i18n.fallbackLocale=t.fallbackLocale,n.i18n.formatFallbackMessages=t.formatFallbackMessages,n.i18n.silentTranslationWarn=t.silentTranslationWarn,n.i18n.silentFallbackWarn=t.silentFallbackWarn,n.i18n.pluralizationRules=t.pluralizationRules,n.i18n.preserveDirectiveContent=t.preserveDirectiveContent),n.__i18nBridge||n.__i18n)try{var r=n.i18n&&n.i18n.messages?n.i18n.messages:{};(n.__i18nBridge||n.__i18n).forEach((function(n){r=$t(r,JSON.parse(n))})),n.i18n.messages=r}catch(n){0}var a=n.i18n.sharedMessages;a&&Rt(a)&&(n.i18n.messages=$t(n.i18n.messages,a)),this._i18n=new vr(n.i18n),this._i18nWatcher=this._i18n.watchI18nData(),(void 0===n.i18n.sync||n.i18n.sync)&&(this._localeWatcher=this.$i18n.watchLocale()),t&&t.onComponentInstanceCreated(this._i18n)}else 0;else this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof vr?this._i18n=this.$root.$i18n:n.parent&&n.parent.$i18n&&n.parent.$i18n instanceof vr&&(this._i18n=n.parent.$i18n)},beforeMount:function(){var n=this.$options;n.i18n=n.i18n||(n.__i18nBridge||n.__i18n?{}:null),n.i18n?(n.i18n instanceof vr||Rt(n.i18n))&&(this._i18n.subscribeDataChanging(this),this._subscribing=!0):(this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof vr||n.parent&&n.parent.$i18n&&n.parent.$i18n instanceof vr)&&(this._i18n.subscribeDataChanging(this),this._subscribing=!0)},mounted:e,beforeDestroy:function(){if(this._i18n){var n=this;this.$nextTick((function(){n._subscribing&&(n._i18n.unsubscribeDataChanging(n),delete n._subscribing),n._i18nWatcher&&(n._i18nWatcher(),n._i18n.destroyVM(),delete n._i18nWatcher),n._localeWatcher&&(n._localeWatcher(),delete n._localeWatcher)}))}}}}(e.bridge)),Wt.directive("t",{bind:Jt,update:Yt,unbind:Qt}),Wt.component(Ht.name,Ht),Wt.component(Gt.name,Gt),Wt.config.optionMergeStrategies.i18n=function(n,e){return void 0===e?n:e}}var tr=function(){this._caches=Object.create(null)};tr.prototype.interpolate=function(n,e){if(!e)return[n];var t=this._caches[n];return t||(t=function(n){var e=[],t=0,r="";for(;t<n.length;){var a=n[t++];if("{"===a){r&&e.push({type:"text",value:r}),r="";var o="";for(a=n[t++];void 0!==a&&"}"!==a;)o+=a,a=n[t++];var i="}"===a,s=rr.test(o)?"list":i&&ar.test(o)?"named":"unknown";e.push({value:o,type:s})}else"%"===a?"{"!==n[t]&&(r+=a):r+=a}return r&&e.push({type:"text",value:r}),e}(n),this._caches[n]=t),function(n,e){var t=[],r=0,a=Array.isArray(e)?"list":Ot(e)?"named":"unknown";if("unknown"===a)return t;for(;r<n.length;){var o=n[r];switch(o.type){case"text":t.push(o.value);break;case"list":t.push(e[parseInt(o.value,10)]);break;case"named":"named"===a&&t.push(e[o.value]);break;case"unknown":0}r++}return t}(t,e)};var rr=/^(?:\d)+/,ar=/^(?:\w)+/;var or=[];or[0]={ws:[0],ident:[3,0],"[":[4],eof:[7]},or[1]={ws:[1],".":[2],"[":[4],eof:[7]},or[2]={ws:[2],ident:[3,0],0:[3,0],number:[3,0]},or[3]={ident:[3,0],0:[3,0],number:[3,0],ws:[1,1],".":[2,1],"[":[4,1],eof:[7,1]},or[4]={"'":[5,0],'"':[6,0],"[":[4,2],"]":[1,3],eof:8,else:[4,0]},or[5]={"'":[4,0],eof:8,else:[5,0]},or[6]={'"':[4,0],eof:8,else:[6,0]};var ir=/^\s?(?:true|false|-?[\d.]+|'[^']*'|"[^"]*")\s?$/;function sr(n){if(null==n)return"eof";switch(n.charCodeAt(0)){case 91:case 93:case 46:case 34:case 39:return n;case 95:case 36:case 45:return"ident";case 9:case 10:case 13:case 160:case 65279:case 8232:case 8233:return"ws"}return"ident"}function lr(n){var e,t,r,a=n.trim();return("0"!==n.charAt(0)||!isNaN(n))&&(r=a,ir.test(r)?(t=(e=a).charCodeAt(0))!==e.charCodeAt(e.length-1)||34!==t&&39!==t?e:e.slice(1,-1):"*"+a)}var cr=function(){this._cache=Object.create(null)};cr.prototype.parsePath=function(n){var e=this._cache[n];return e||(e=function(n){var e,t,r,a,o,i,s,l=[],c=-1,p=0,u=0,d=[];function m(){var e=n[c+1];if(5===p&&"'"===e||6===p&&'"'===e)return c++,r="\\"+e,d[0](),!0}for(d[1]=function(){void 0!==t&&(l.push(t),t=void 0)},d[0]=function(){void 0===t?t=r:t+=r},d[2]=function(){d[0](),u++},d[3]=function(){if(u>0)u--,p=4,d[0]();else{if(u=0,void 0===t)return!1;if(!1===(t=lr(t)))return!1;d[1]()}};null!==p;)if(c++,"\\"!==(e=n[c])||!m()){if(a=sr(e),8===(o=(s=or[p])[a]||s.else||8))return;if(p=o[0],(i=d[o[1]])&&(r=void 0===(r=o[2])?e:r,!1===i()))return;if(7===p)return l}}(n))&&(this._cache[n]=e),e||[]},cr.prototype.getPathValue=function(n,e){if(!Ot(n))return null;var t=this.parsePath(e);if(0===t.length)return null;for(var r=t.length,a=n,o=0;o<r;){var i=a[t[o]];if(null==i)return null;a=i,o++}return a};var pr,ur=/<\/?[\w\s="/.':;#-\/]+>/,dr=/(?:@(?:\.[a-zA-Z]+)?:(?:[\w\-_|./]+|\([\w\-_:|./]+\)))/g,mr=/^@(?:\.([a-zA-Z]+))?:/,gr=/[()]/g,fr={upper:function(n){return n.toLocaleUpperCase()},lower:function(n){return n.toLocaleLowerCase()},capitalize:function(n){return""+n.charAt(0).toLocaleUpperCase()+n.substr(1)}},hr=new tr,vr=function(n){var e=this;void 0===n&&(n={}),!Wt&&"undefined"!=typeof window&&window.Vue&&er(window.Vue);var t=n.locale||"en-US",r=!1!==n.fallbackLocale&&(n.fallbackLocale||"en-US"),a=n.messages||{},o=n.dateTimeFormats||n.datetimeFormats||{},i=n.numberFormats||{};this._vm=null,this._formatter=n.formatter||hr,this._modifiers=n.modifiers||{},this._missing=n.missing||null,this._root=n.root||null,this._sync=void 0===n.sync||!!n.sync,this._fallbackRoot=void 0===n.fallbackRoot||!!n.fallbackRoot,this._fallbackRootWithEmptyString=void 0===n.fallbackRootWithEmptyString||!!n.fallbackRootWithEmptyString,this._formatFallbackMessages=void 0!==n.formatFallbackMessages&&!!n.formatFallbackMessages,this._silentTranslationWarn=void 0!==n.silentTranslationWarn&&n.silentTranslationWarn,this._silentFallbackWarn=void 0!==n.silentFallbackWarn&&!!n.silentFallbackWarn,this._dateTimeFormatters={},this._numberFormatters={},this._path=new cr,this._dataListeners=new Set,this._componentInstanceCreatedListener=n.componentInstanceCreatedListener||null,this._preserveDirectiveContent=void 0!==n.preserveDirectiveContent&&!!n.preserveDirectiveContent,this.pluralizationRules=n.pluralizationRules||{},this._warnHtmlInMessage=n.warnHtmlInMessage||"off",this._postTranslation=n.postTranslation||null,this._escapeParameterHtml=n.escapeParameterHtml||!1,"__VUE_I18N_BRIDGE__"in n&&(this.__VUE_I18N_BRIDGE__=n.__VUE_I18N_BRIDGE__),this.getChoiceIndex=function(n,t){var r=Object.getPrototypeOf(e);if(r&&r.getChoiceIndex)return r.getChoiceIndex.call(e,n,t);var a,o;return e.locale in e.pluralizationRules?e.pluralizationRules[e.locale].apply(e,[n,t]):(a=n,o=t,a=Math.abs(a),2===o?a?a>1?1:0:1:a?Math.min(a,2):0)},this._exist=function(n,t){return!(!n||!t)&&(!Pt(e._path.getPathValue(n,t))||!!n[t])},"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||Object.keys(a).forEach((function(n){e._checkLocaleMessage(n,e._warnHtmlInMessage,a[n])})),this._initVM({locale:t,fallbackLocale:r,messages:a,dateTimeFormats:o,numberFormats:i})},br={vm:{configurable:!0},messages:{configurable:!0},dateTimeFormats:{configurable:!0},numberFormats:{configurable:!0},availableLocales:{configurable:!0},locale:{configurable:!0},fallbackLocale:{configurable:!0},formatFallbackMessages:{configurable:!0},missing:{configurable:!0},formatter:{configurable:!0},silentTranslationWarn:{configurable:!0},silentFallbackWarn:{configurable:!0},preserveDirectiveContent:{configurable:!0},warnHtmlInMessage:{configurable:!0},postTranslation:{configurable:!0},sync:{configurable:!0}};vr.prototype._checkLocaleMessage=function(n,e,t){var r=function(n,e,t,a){if(Rt(t))Object.keys(t).forEach((function(o){var i=t[o];Rt(i)?(a.push(o),a.push("."),r(n,e,i,a),a.pop(),a.pop()):(a.push(o),r(n,e,i,a),a.pop())}));else if(Tt(t))t.forEach((function(t,o){Rt(t)?(a.push("["+o+"]"),a.push("."),r(n,e,t,a),a.pop(),a.pop()):(a.push("["+o+"]"),r(n,e,t,a),a.pop())}));else if(At(t)){if(ur.test(t)){var o="Detected HTML in message '"+t+"' of keypath '"+a.join("")+"' at '"+e+"'. Consider component interpolation with '<i18n>' to avoid XSS. See https://bit.ly/2ZqJzkp";"warn"===n?It(o):"error"===n&&function(n,e){"undefined"!=typeof console&&(console.error("[vue-i18n] "+n),e&&console.error(e.stack))}(o)}}};r(e,n,t,[])},vr.prototype._initVM=function(n){var e=Wt.config.silent;Wt.config.silent=!0,this._vm=new Wt({data:n,__VUE18N__INSTANCE__:!0}),Wt.config.silent=e},vr.prototype.destroyVM=function(){this._vm.$destroy()},vr.prototype.subscribeDataChanging=function(n){this._dataListeners.add(n)},vr.prototype.unsubscribeDataChanging=function(n){!function(n,e){if(n.delete(e));}(this._dataListeners,n)},vr.prototype.watchI18nData=function(){var n=this;return this._vm.$watch("$data",(function(){for(var e,t,r=(e=n._dataListeners,t=[],e.forEach((function(n){return t.push(n)})),t),a=r.length;a--;)Wt.nextTick((function(){r[a]&&r[a].$forceUpdate()}))}),{deep:!0})},vr.prototype.watchLocale=function(n){if(n){if(!this.__VUE_I18N_BRIDGE__)return null;var e=this,t=this._vm;return this.vm.$watch("locale",(function(r){t.$set(t,"locale",r),e.__VUE_I18N_BRIDGE__&&n&&(n.locale.value=r),t.$forceUpdate()}),{immediate:!0})}if(!this._sync||!this._root)return null;var r=this._vm;return this._root.$i18n.vm.$watch("locale",(function(n){r.$set(r,"locale",n),r.$forceUpdate()}),{immediate:!0})},vr.prototype.onComponentInstanceCreated=function(n){this._componentInstanceCreatedListener&&this._componentInstanceCreatedListener(n,this)},br.vm.get=function(){return this._vm},br.messages.get=function(){return Mt(this._getMessages())},br.dateTimeFormats.get=function(){return Mt(this._getDateTimeFormats())},br.numberFormats.get=function(){return Mt(this._getNumberFormats())},br.availableLocales.get=function(){return Object.keys(this.messages).sort()},br.locale.get=function(){return this._vm.locale},br.locale.set=function(n){this._vm.$set(this._vm,"locale",n)},br.fallbackLocale.get=function(){return this._vm.fallbackLocale},br.fallbackLocale.set=function(n){this._localeChainCache={},this._vm.$set(this._vm,"fallbackLocale",n)},br.formatFallbackMessages.get=function(){return this._formatFallbackMessages},br.formatFallbackMessages.set=function(n){this._formatFallbackMessages=n},br.missing.get=function(){return this._missing},br.missing.set=function(n){this._missing=n},br.formatter.get=function(){return this._formatter},br.formatter.set=function(n){this._formatter=n},br.silentTranslationWarn.get=function(){return this._silentTranslationWarn},br.silentTranslationWarn.set=function(n){this._silentTranslationWarn=n},br.silentFallbackWarn.get=function(){return this._silentFallbackWarn},br.silentFallbackWarn.set=function(n){this._silentFallbackWarn=n},br.preserveDirectiveContent.get=function(){return this._preserveDirectiveContent},br.preserveDirectiveContent.set=function(n){this._preserveDirectiveContent=n},br.warnHtmlInMessage.get=function(){return this._warnHtmlInMessage},br.warnHtmlInMessage.set=function(n){var e=this,t=this._warnHtmlInMessage;if(this._warnHtmlInMessage=n,t!==n&&("warn"===n||"error"===n)){var r=this._getMessages();Object.keys(r).forEach((function(n){e._checkLocaleMessage(n,e._warnHtmlInMessage,r[n])}))}},br.postTranslation.get=function(){return this._postTranslation},br.postTranslation.set=function(n){this._postTranslation=n},br.sync.get=function(){return this._sync},br.sync.set=function(n){this._sync=n},vr.prototype._getMessages=function(){return this._vm.messages},vr.prototype._getDateTimeFormats=function(){return this._vm.dateTimeFormats},vr.prototype._getNumberFormats=function(){return this._vm.numberFormats},vr.prototype._warnDefault=function(n,e,t,r,a,o){if(!Pt(t))return t;if(this._missing){var i=this._missing.apply(null,[n,e,r,a]);if(At(i))return i}else 0;if(this._formatFallbackMessages){var s=Bt.apply(void 0,a);return this._render(e,o,s.params,e)}return e},vr.prototype._isFallbackRoot=function(n){return(this._fallbackRootWithEmptyString?!n:Pt(n))&&!Pt(this._root)&&this._fallbackRoot},vr.prototype._isSilentFallbackWarn=function(n){return this._silentFallbackWarn instanceof RegExp?this._silentFallbackWarn.test(n):this._silentFallbackWarn},vr.prototype._isSilentFallback=function(n,e){return this._isSilentFallbackWarn(e)&&(this._isFallbackRoot()||n!==this.fallbackLocale)},vr.prototype._isSilentTranslationWarn=function(n){return this._silentTranslationWarn instanceof RegExp?this._silentTranslationWarn.test(n):this._silentTranslationWarn},vr.prototype._interpolate=function(n,e,t,r,a,o,i){if(!e)return null;var s,l=this._path.getPathValue(e,t);if(Tt(l)||Rt(l))return l;if(Pt(l)){if(!Rt(e))return null;if(!At(s=e[t])&&!Ft(s))return null}else{if(!At(l)&&!Ft(l))return null;s=l}return At(s)&&(s.indexOf("@:")>=0||s.indexOf("@.")>=0)&&(s=this._link(n,e,s,r,"raw",o,i)),this._render(s,a,o,t)},vr.prototype._link=function(n,e,t,r,a,o,i){var s=t,l=s.match(dr);for(var c in l)if(l.hasOwnProperty(c)){var p=l[c],u=p.match(mr),d=u[0],m=u[1],g=p.replace(d,"").replace(gr,"");if(jt(i,g))return s;i.push(g);var f=this._interpolate(n,e,g,r,"raw"===a?"string":a,"raw"===a?void 0:o,i);if(this._isFallbackRoot(f)){if(!this._root)throw Error("unexpected error");var h=this._root.$i18n;f=h._translate(h._getMessages(),h.locale,h.fallbackLocale,g,r,a,o)}f=this._warnDefault(n,g,f,r,Tt(o)?o:[o],a),this._modifiers.hasOwnProperty(m)?f=this._modifiers[m](f):fr.hasOwnProperty(m)&&(f=fr[m](f)),i.pop(),s=f?s.replace(p,f):s}return s},vr.prototype._createMessageContext=function(n,e,t,r){var a=this,o=Tt(n)?n:[],i=Ot(n)?n:{},s=this._getMessages(),l=this.locale;return{list:function(n){return o[n]},named:function(n){return i[n]},values:n,formatter:e,path:t,messages:s,locale:l,linked:function(n){return a._interpolate(l,s[l]||{},n,null,r,void 0,[n])}}},vr.prototype._render=function(n,e,t,r){if(Ft(n))return n(this._createMessageContext(t,this._formatter||hr,r,e));var a=this._formatter.interpolate(n,t,r);return a||(a=hr.interpolate(n,t,r)),"string"!==e||At(a)?a:a.join("")},vr.prototype._appendItemToChain=function(n,e,t){var r=!1;return jt(n,e)||(r=!0,e&&(r="!"!==e[e.length-1],e=e.replace(/!/g,""),n.push(e),t&&t[e]&&(r=t[e]))),r},vr.prototype._appendLocaleToChain=function(n,e,t){var r,a=e.split("-");do{var o=a.join("-");r=this._appendItemToChain(n,o,t),a.splice(-1,1)}while(a.length&&!0===r);return r},vr.prototype._appendBlockToChain=function(n,e,t){for(var r=!0,a=0;a<e.length&&"boolean"==typeof r;a++){var o=e[a];At(o)&&(r=this._appendLocaleToChain(n,o,t))}return r},vr.prototype._getLocaleChain=function(n,e){if(""===n)return[];this._localeChainCache||(this._localeChainCache={});var t=this._localeChainCache[n];if(!t){e||(e=this.fallbackLocale),t=[];for(var r,a=[n];Tt(a);)a=this._appendBlockToChain(t,a,e);(a=At(r=Tt(e)?e:Ot(e)?e.default?e.default:null:e)?[r]:r)&&this._appendBlockToChain(t,a,null),this._localeChainCache[n]=t}return t},vr.prototype._translate=function(n,e,t,r,a,o,i){for(var s,l=this._getLocaleChain(e,t),c=0;c<l.length;c++){var p=l[c];if(!Pt(s=this._interpolate(p,n[p],r,a,o,i,[r])))return s}return null},vr.prototype._t=function(n,e,t,r){for(var a,o=[],i=arguments.length-4;i-- >0;)o[i]=arguments[i+4];if(!n)return"";var s=Bt.apply(void 0,o);this._escapeParameterHtml&&(s.params=zt(s.params));var l=s.locale||e,c=this._translate(t,l,this.fallbackLocale,n,r,"string",s.params);if(this._isFallbackRoot(c)){if(!this._root)throw Error("unexpected error");return(a=this._root).$t.apply(a,[n].concat(o))}return c=this._warnDefault(l,n,c,r,o,"string"),this._postTranslation&&null!=c&&(c=this._postTranslation(c,n)),c},vr.prototype.t=function(n){for(var e,t=[],r=arguments.length-1;r-- >0;)t[r]=arguments[r+1];return(e=this)._t.apply(e,[n,this.locale,this._getMessages(),null].concat(t))},vr.prototype._i=function(n,e,t,r,a){var o=this._translate(t,e,this.fallbackLocale,n,r,"raw",a);if(this._isFallbackRoot(o)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.i(n,e,a)}return this._warnDefault(e,n,o,r,[a],"raw")},vr.prototype.i=function(n,e,t){return n?(At(e)||(e=this.locale),this._i(n,e,this._getMessages(),null,t)):""},vr.prototype._tc=function(n,e,t,r,a){for(var o,i=[],s=arguments.length-5;s-- >0;)i[s]=arguments[s+5];if(!n)return"";void 0===a&&(a=1);var l={count:a,n:a},c=Bt.apply(void 0,i);return c.params=Object.assign(l,c.params),i=null===c.locale?[c.params]:[c.locale,c.params],this.fetchChoice((o=this)._t.apply(o,[n,e,t,r].concat(i)),a)},vr.prototype.fetchChoice=function(n,e){if(!n||!At(n))return null;var t=n.split("|");return t[e=this.getChoiceIndex(e,t.length)]?t[e].trim():n},vr.prototype.tc=function(n,e){for(var t,r=[],a=arguments.length-2;a-- >0;)r[a]=arguments[a+2];return(t=this)._tc.apply(t,[n,this.locale,this._getMessages(),null,e].concat(r))},vr.prototype._te=function(n,e,t){for(var r=[],a=arguments.length-3;a-- >0;)r[a]=arguments[a+3];var o=Bt.apply(void 0,r).locale||e;return this._exist(t[o],n)},vr.prototype.te=function(n,e){return this._te(n,this.locale,this._getMessages(),e)},vr.prototype.getLocaleMessage=function(n){return Mt(this._vm.messages[n]||{})},vr.prototype.setLocaleMessage=function(n,e){"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||this._checkLocaleMessage(n,this._warnHtmlInMessage,e),this._vm.$set(this._vm.messages,n,e)},vr.prototype.mergeLocaleMessage=function(n,e){"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||this._checkLocaleMessage(n,this._warnHtmlInMessage,e),this._vm.$set(this._vm.messages,n,$t(void 0!==this._vm.messages[n]&&Object.keys(this._vm.messages[n]).length?Object.assign({},this._vm.messages[n]):{},e))},vr.prototype.getDateTimeFormat=function(n){return Mt(this._vm.dateTimeFormats[n]||{})},vr.prototype.setDateTimeFormat=function(n,e){this._vm.$set(this._vm.dateTimeFormats,n,e),this._clearDateTimeFormat(n,e)},vr.prototype.mergeDateTimeFormat=function(n,e){this._vm.$set(this._vm.dateTimeFormats,n,$t(this._vm.dateTimeFormats[n]||{},e)),this._clearDateTimeFormat(n,e)},vr.prototype._clearDateTimeFormat=function(n,e){for(var t in e){var r=n+"__"+t;this._dateTimeFormatters.hasOwnProperty(r)&&delete this._dateTimeFormatters[r]}},vr.prototype._localizeDateTime=function(n,e,t,r,a,o){for(var i=e,s=r[i],l=this._getLocaleChain(e,t),c=0;c<l.length;c++){var p=l[c];if(i=p,!Pt(s=r[p])&&!Pt(s[a]))break}if(Pt(s)||Pt(s[a]))return null;var u,d=s[a];if(o)u=new Intl.DateTimeFormat(i,Object.assign({},d,o));else{var m=i+"__"+a;(u=this._dateTimeFormatters[m])||(u=this._dateTimeFormatters[m]=new Intl.DateTimeFormat(i,d))}return u.format(n)},vr.prototype._d=function(n,e,t,r){if(!t)return(r?new Intl.DateTimeFormat(e,r):new Intl.DateTimeFormat(e)).format(n);var a=this._localizeDateTime(n,e,this.fallbackLocale,this._getDateTimeFormats(),t,r);if(this._isFallbackRoot(a)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.d(n,t,e)}return a||""},vr.prototype.d=function(n){for(var e=[],t=arguments.length-1;t-- >0;)e[t]=arguments[t+1];var r=this.locale,a=null,o=null;return 1===e.length?(At(e[0])?a=e[0]:Ot(e[0])&&(e[0].locale&&(r=e[0].locale),e[0].key&&(a=e[0].key)),o=Object.keys(e[0]).reduce((function(n,t){var r;return jt(Ct,t)?Object.assign({},n,((r={})[t]=e[0][t],r)):n}),null)):2===e.length&&(At(e[0])&&(a=e[0]),At(e[1])&&(r=e[1])),this._d(n,r,a,o)},vr.prototype.getNumberFormat=function(n){return Mt(this._vm.numberFormats[n]||{})},vr.prototype.setNumberFormat=function(n,e){this._vm.$set(this._vm.numberFormats,n,e),this._clearNumberFormat(n,e)},vr.prototype.mergeNumberFormat=function(n,e){this._vm.$set(this._vm.numberFormats,n,$t(this._vm.numberFormats[n]||{},e)),this._clearNumberFormat(n,e)},vr.prototype._clearNumberFormat=function(n,e){for(var t in e){var r=n+"__"+t;this._numberFormatters.hasOwnProperty(r)&&delete this._numberFormatters[r]}},vr.prototype._getNumberFormatter=function(n,e,t,r,a,o){for(var i=e,s=r[i],l=this._getLocaleChain(e,t),c=0;c<l.length;c++){var p=l[c];if(i=p,!Pt(s=r[p])&&!Pt(s[a]))break}if(Pt(s)||Pt(s[a]))return null;var u,d=s[a];if(o)u=new Intl.NumberFormat(i,Object.assign({},d,o));else{var m=i+"__"+a;(u=this._numberFormatters[m])||(u=this._numberFormatters[m]=new Intl.NumberFormat(i,d))}return u},vr.prototype._n=function(n,e,t,r){if(!vr.availabilities.numberFormat)return"";if(!t)return(r?new Intl.NumberFormat(e,r):new Intl.NumberFormat(e)).format(n);var a=this._getNumberFormatter(n,e,this.fallbackLocale,this._getNumberFormats(),t,r),o=a&&a.format(n);if(this._isFallbackRoot(o)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.n(n,Object.assign({},{key:t,locale:e},r))}return o||""},vr.prototype.n=function(n){for(var e=[],t=arguments.length-1;t-- >0;)e[t]=arguments[t+1];var r=this.locale,a=null,o=null;return 1===e.length?At(e[0])?a=e[0]:Ot(e[0])&&(e[0].locale&&(r=e[0].locale),e[0].key&&(a=e[0].key),o=Object.keys(e[0]).reduce((function(n,t){var r;return jt(Dt,t)?Object.assign({},n,((r={})[t]=e[0][t],r)):n}),null)):2===e.length&&(At(e[0])&&(a=e[0]),At(e[1])&&(r=e[1])),this._n(n,r,a,o)},vr.prototype._ntp=function(n,e,t,r){if(!vr.availabilities.numberFormat)return[];if(!t)return(r?new Intl.NumberFormat(e,r):new Intl.NumberFormat(e)).formatToParts(n);var a=this._getNumberFormatter(n,e,this.fallbackLocale,this._getNumberFormats(),t,r),o=a&&a.formatToParts(n);if(this._isFallbackRoot(o)){if(!this._root)throw Error("unexpected error");return this._root.$i18n._ntp(n,e,t,r)}return o||[]},Object.defineProperties(vr.prototype,br),Object.defineProperty(vr,"availabilities",{get:function(){if(!pr){var n="undefined"!=typeof Intl;pr={dateTimeFormat:n&&void 0!==Intl.DateTimeFormat,numberFormat:n&&void 0!==Intl.NumberFormat}}return pr}}),vr.install=er,vr.version="8.28.2";var kr=vr;
/*!
 * vssue - A vue-powered issue-based comment plugin
 *
 * @version v1.4.8
 * @link https://vssue.js.org
 * @license MIT
 * @copyright 2018-2021 meteorlxy
 */
/*! *****************************************************************************
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use
this file except in compliance with the License. You may obtain a copy of the
License at http://www.apache.org/licenses/LICENSE-2.0

THIS CODE IS PROVIDED ON AN *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
MERCHANTABLITY OR NON-INFRINGEMENT.

See the Apache Version 2.0 License for specific language governing permissions
and limitations under the License.
***************************************************************************** */function yr(n,e,t,r){var a,o=arguments.length,i=o<3?e:null===r?r=Object.getOwnPropertyDescriptor(e,t):r;if("object"==typeof Reflect&&"function"==typeof Reflect.decorate)i=Reflect.decorate(n,e,t,r);else for(var s=n.length-1;s>=0;s--)(a=n[s])&&(i=(o<3?a(i):o>3?a(e,t,i):a(e,t))||i);return o>3&&i&&Object.defineProperty(e,t,i),i}var Sr=r.b.extend({name:"Iconfont"});function xr(n,e,t,r,a,o,i,s,l,c){"boolean"!=typeof i&&(l=s,s=i,i=!1);const p="function"==typeof t?t.options:t;let u;if(n&&n.render&&(p.render=n.render,p.staticRenderFns=n.staticRenderFns,p._compiled=!0,a&&(p.functional=!0)),r&&(p._scopeId=r),o?(u=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),e&&e.call(this,l(n)),n&&n._registeredComponents&&n._registeredComponents.add(o)},p._ssrRegister=u):e&&(u=i?function(n){e.call(this,c(n,this.$root.$options.shadowRoot))}:function(n){e.call(this,s(n))}),u)if(p.functional){const n=p.render;p.render=function(e,t){return u.call(t),n(e,t)}}else{const n=p.beforeCreate;p.beforeCreate=n?[].concat(n,u):[u]}return t}"undefined"!=typeof navigator&&/msie [6-9]\\b/.test(navigator.userAgent.toLowerCase());const wr=xr({render:function(n,e){var t=e._c;return t("svg",{directives:[{name:"show",rawName:"v-show",value:!1,expression:"false"}]},[t("symbol",{attrs:{id:"vssue-icon-bitbucket",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M579.5522464 489.45249493q4.8371808 38.38537173-30.81752427 61.55702827t-67.95459093 3.66689493q-23.79580907-10.37653333-32.6119616-35.34262826t-0.31207573-50.01020907 31.67573333-35.34262827q21.92335253-11.00068587 44.1587808-7.33379093t39.00952427 21.61127573 16.77409493 41.1160384zM647.19476053 476.65737173q-8.50407573-65.22392427-68.8908192-99.9424t-120.07131413-7.9579424q-38.38537173 17.08617173-61.24495253 53.9111616t-21.0651424 78.95527574q2.41859093 55.4715424 47.20152426 94.48106666t100.87862827 34.1723424q55.4715424-4.8371808 92.60860907-51.18049493t30.50544746-102.43900907zM792.93434133 146.32472427q-12.17097173-16.4620192-34.1723424-27.15062827t-35.34262826-13.41927573-43.30057174-7.64586667q-177.33729493-28.63299093-345.00022826 1.24830507-26.2144 4.29104747-40.25782827 7.33379093t-33.54819093 13.41927573-30.50544747 26.2144q18.2564576 17.08617173 46.34331413 27.6967616t44.78293334 13.41927574 53.36502826 7.02171413q138.95192427 17.71032427 273.06666667 0.62415253 38.38537173-4.8371808 54.53531413-7.33379093t44.1587808-13.1072 45.7191616-28.32091413zM827.65281813 777.10872427q-4.8371808 15.83786667-9.44030506 46.65539093t-8.50407574 51.18049493-17.39824746 42.6764192-35.34262827 34.4064q-52.4288 29.2571424-115.46819093 43.61264747t-123.1140576 13.41927573-122.8019808-11.3127616q-28.0088384-4.8371808-49.69813334-11.00068586t-46.65539093-16.4620192-44.4708576-26.52647574-31.67573333-37.4491424q-15.21371413-58.51428587-34.71847574-177.96144746l3.66689494-9.7523808 11.00068586-5.46133334q135.9091808 90.1900192 308.72137174 90.1900192t309.34552426-90.1900192q12.79512427 3.66689493 14.5895616 14.04342827t-3.0427424 27.46270507-4.8371808 22.54750506zM937.97175147 191.41973333q-15.83786667 101.8148576-67.64251414 399.22346667-3.0427424 18.2564576-16.4620192 34.1723424t-26.52647573 24.3419424-33.23611413 18.88060907q-153.61950507 76.7707424-371.8387808 53.67710506-151.12289493-16.4620192-240.14262827-84.72868586-9.12822827-7.33379093-15.52579093-16.1499424t-10.37653334-21.2992-5.46133333-20.75306667-3.66689493-24.10788587-3.3548192-21.2992q-5.46133333-30.50544747-16.1499424-91.43832426t-17.08617174-98.4600384-14.35550506-89.8779424-13.41927574-96.27550507q1.7944384-15.83786667 10.68860907-29.5692192t19.19268587-22.8595808 27.46270506-18.2564576 28.0088384-13.73135253 29.2571424-11.3127616q76.22460907-28.0088384 190.75657174-39.00952427 231.0144-22.54750507 412.01859093 30.50544747 94.48106667 28.0088384 131.072 74.35215253 9.7523808 12.17097173 10.0644576 31.0515808t-3.3548192 32.9240384z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitea",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M184.31868985 236.10860742C106.94832667 235.94086648 3.32655508 285.13080468 9.02973665 408.46209936c8.93218827 192.65010787 206.32096845 210.5144844 285.20099725 212.06608453 8.63864186 36.14810496 101.48307766 160.77938883 170.21479898 167.32127321h301.09442177c180.57278288-11.99345499 315.77172611-546.07960359 215.54670217-548.09249109-165.7696721 7.79993906-264.02374305 11.74184405-348.27147151 12.41280591v166.69224585l-26.25140843-11.61603761-0.16773997-154.99233728c-96.70246985-0.04193548-181.83083757-4.52899687-343.4069947-12.49667687-20.21274496-0.12580547-48.39316992-3.5644886-78.67035236-3.64835859z m10.94507577 68.14462849h9.22573371c10.98701124 98.75729283 28.85138778 156.50200291 64.99949274 244.73357185-92.25734394-10.90314029-170.75995634-37.69970509-185.18564974-137.75698809-7.46445813-51.78991757 17.69663558-105.84433456 110.96042329-107.01851827z m358.83913087 97.07988723c6.29027343 0.08386999 12.70635233 1.25805468 18.74501482 4.02577499l31.40943263 13.54505513-22.51917887 41.05451824a28.18042496 25.03528825 0 0 0-10.10637297 1.59353561 28.18042496 25.03528825 0 0 0-16.98373825 32.038459 28.18042496 25.03528825 0 0 0 4.69673781 7.29671718l-38.83195528 70.70267333a28.18042496 25.03528825 0 0 0-9.30960467 1.59353659 28.18042496 25.03528825 0 0 0-16.98373825 32.038459 28.18042496 25.03528825 0 0 0 36.06423497 15.09665623 28.18042496 25.03528825 0 0 0 16.94180276-32.08039449 28.18042496 25.03528825 0 0 0-6.62575434-9.22573468l37.82551056-68.85752581a28.18042496 25.03528825 0 0 0 12.28700044-1.25805469 28.18042496 25.03528825 0 0 0 8.93218826-4.69673783c14.59343435 6.12253248 26.54495386 11.11281671 35.14166122 15.34826717 12.91602778 6.37414341 17.48696012 10.60959485 18.87082027 15.30633169 1.38386015 4.61286685-0.12580547 13.50312062-7.42252263 29.10299872-5.45157063 11.61603859-14.46762889 28.09655497-25.11915823 47.51253164a28.18042496 25.03528825 0 0 0-10.52572486 1.59353659 28.18042496 25.03528825 0 0 0-16.98373826 32.038459 28.18042496 25.03528825 0 0 0 36.06423498 15.09665623 28.18042496 25.03528825 0 0 0 16.94180278-32.03845901 28.18042496 25.03528825 0 0 0-5.74511608-8.47090188c10.52572388-19.20630122 19.58371762-35.72875308 25.41270465-48.14155897 7.88380904-16.85793279 11.99345499-29.39654416 8.38703091-41.51580463-3.60642311-12.11926046-14.67730434-20.0030695-29.35460966-27.25785217-9.6450856-4.73867233-21.68047607-9.77089106-36.06423399-15.80955357a28.18042496 25.03528825 0 0 0-1.59353562-10.022502 28.18042496 25.03528825 0 0 0-6.08059796-8.7644483l22.14176246-40.38355541 122.61839638 52.96410227c22.14176247 9.6031511 31.2836262 33.12877372 20.54822685 52.8382968l-84.28966393 154.32137544c-10.77733482 19.66758857-37.23841869 27.80300855-59.38018118 18.24179293l-173.48574115-74.98005927c-22.14176247-9.5612156-31.32556167-33.12877372-20.54822687-52.83829679l84.28966395-154.27943995c7.38058716-13.54505513 22.22563246-21.59660511 37.951317-22.22563246h2.68384935z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitee",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M978.404275 409.561604H455.061338c-25.117645 0-45.499734 20.382089-45.499734 45.499734l-0.031997 113.781333c0 25.117645 20.350092 45.499734 45.499734 45.531731h318.594132c25.117645 0 45.499734 20.382089 45.499734 45.499735v22.749867a136.5312 136.5312 0 0 1-136.5312 136.5312H250.248539a45.499734 45.499734 0 0 1-45.499734-45.499734V341.343999a136.5312 136.5312 0 0 1 136.5312-136.5312L978.308284 204.780802c25.117645 0 45.499734-20.350092 45.499734-45.467738L1023.904009 45.531731h0.031997A45.499734 45.499734 0 0 0 978.468269 0h-0.031997L341.343999 0.031997C152.84967 0.031997 0.031997 152.84967 0.031997 341.343999v637.092273c0 25.117645 20.382089 45.499734 45.499734 45.499734h671.233072a307.171203 307.171203 0 0 0 307.171203-307.171203v-261.671468c0-25.117645-20.382089-45.499734-45.499734-45.499734z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-github",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M512 20.4425c-278.334 0-504 225.6345-504 504 0 222.6735 144.4275 411.6105 344.673 478.233 25.2 4.662 34.461-10.9305 34.461-24.255 0-12.0015-0.4725-51.723-0.693-93.8385-140.238 30.492-169.8165-59.472-169.8165-59.472-22.932-58.2435-55.944-73.7415-55.944-73.7415-45.738-31.2795 3.465-30.6495 3.465-30.6495 50.589 3.5595 77.238 51.9435 77.238 51.9435 44.9505 77.049 117.9045 54.7785 146.664 41.895 4.5045-32.571 17.577-54.81 32.004-67.41-111.951-12.726-229.635-55.9755-229.635-249.0705 0-55.0305 19.6875-99.981 51.9435-135.2925-5.229-12.6945-22.491-63.945 4.8825-133.371 0 0 42.336-13.545 138.6315 51.66 40.194-11.1825 83.3175-16.758 126.1575-16.9785 42.8085 0.189 85.9635 5.796 126.252 16.9785 96.201-65.205 138.4425-51.66 138.4425-51.66 27.4365 69.426 10.1745 120.6765 4.9455 133.371 32.319 35.28 51.8805 80.262 51.8805 135.2925 0 193.5675-117.9045 236.187-230.139 248.6925 18.081 15.6555 34.1775 46.305 34.1775 93.3345 0 67.4415-0.5985 121.716-0.5985 138.3165 0 13.419 9.072 29.1375 34.6185 24.192 200.151-66.717 344.3895-255.5595 344.3895-478.17 0-278.3655-225.666-504-504-504z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitlab",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M66.61375986 405.11600042L512.11376028 976.03999972 23.84576 621.65599958a39.312 39.312 0 0 1-14.07600042-43.30799944l56.8080007-173.26800028z m259.88400014 0h371.26800014L512.14975986 976.03999972zM215.11376 60.88400042l111.384 344.232H66.61375986l111.384-344.232a19.72800014 19.72800014 0 0 1 37.11600014 0z m742.49999972 344.232l56.8080007 173.2679993a39.23999986 39.23999986 0 0 1-14.07600042 43.30800042l-488.26800028 354.38400014 445.50000042-570.92400028z m0 0h-259.88400014l111.384-344.232a19.72800014 19.72800014 0 0 1 37.11600014 0z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-loading",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M843.307 742.24c0 3.217 2.607 5.824 5.824 5.824s5.824-2.607 5.824-5.824a5.823 5.823 0 0 0-5.824-5.824 5.823 5.823 0 0 0-5.824 5.824zM714.731 874.912c0 6.398 5.186 11.584 11.584 11.584s11.584-5.186 11.584-11.584-5.186-11.584-11.584-11.584-11.584 5.186-11.584 11.584zM541.419 943.2c0 9.614 7.794 17.408 17.408 17.408s17.408-7.794 17.408-17.408-7.794-17.408-17.408-17.408-17.408 7.794-17.408 17.408z m-186.56-9.152c0 12.795 10.373 23.168 23.168 23.168s23.168-10.373 23.168-23.168-10.373-23.168-23.168-23.168-23.168 10.373-23.168 23.168zM189.355 849.12c0 16.012 12.98 28.992 28.992 28.992s28.992-12.98 28.992-28.992-12.98-28.992-28.992-28.992-28.992 12.98-28.992 28.992zM74.731 704.736c0 19.228 15.588 34.816 34.816 34.816s34.816-15.588 34.816-34.816-15.588-34.816-34.816-34.816-34.816 15.588-34.816 34.816z m-43.008-177.28c0 22.41 18.166 40.576 40.576 40.576s40.576-18.166 40.576-40.576-18.166-40.576-40.576-40.576-40.576 18.166-40.576 40.576z m35.392-176.128c0 25.626 20.774 46.4 46.4 46.4s46.4-20.774 46.4-46.4c0-25.626-20.774-46.4-46.4-46.4-25.626 0-46.4 20.774-46.4 46.4z m106.176-142.016c0 28.843 23.381 52.224 52.224 52.224s52.224-23.381 52.224-52.224c0-28.843-23.381-52.224-52.224-52.224-28.843 0-52.224 23.381-52.224 52.224z m155.904-81.344c0 32.024 25.96 57.984 57.984 57.984s57.984-25.96 57.984-57.984-25.96-57.984-57.984-57.984-57.984 25.96-57.984 57.984z m175.104-5.056c0 35.24 28.568 63.808 63.808 63.808s63.808-28.568 63.808-63.808c0-35.24-28.568-63.808-63.808-63.808-35.24 0-63.808 28.568-63.808 63.808z m160.32 72.128c0 38.421 31.147 69.568 69.568 69.568s69.568-31.147 69.568-69.568-31.147-69.568-69.568-69.568-69.568 31.147-69.568 69.568z m113.92 135.488c0 41.638 33.754 75.392 75.392 75.392s75.392-33.754 75.392-75.392-33.754-75.392-75.392-75.392-75.392 33.754-75.392 75.392z m45.312 175.488c0 44.854 36.362 81.216 81.216 81.216s81.216-36.362 81.216-81.216c0-44.854-36.362-81.216-81.216-81.216-44.854 0-81.216 36.362-81.216 81.216z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-like",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M885.9 533.7c16.8-22.2 26.1-49.4 26.1-77.7 0-44.9-25.1-87.4-65.5-111.1a67.67 67.67 0 0 0-34.3-9.3H572.4l6-122.9c1.4-29.7-9.1-57.9-29.5-79.4-20.5-21.5-48.1-33.4-77.9-33.4-52 0-98 35-111.8 85.1l-85.9 311H144c-17.7 0-32 14.3-32 32v364c0 17.7 14.3 32 32 32h601.3c9.2 0 18.2-1.8 26.5-5.4 47.6-20.3 78.3-66.8 78.3-118.4 0-12.6-1.8-25-5.4-37 16.8-22.2 26.1-49.4 26.1-77.7 0-12.6-1.8-25-5.4-37 16.8-22.2 26.1-49.4 26.1-77.7-0.2-12.6-2-25.1-5.6-37.1zM184 852V568h81v284h-81z m636.4-353l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 16.5-7.2 32.2-19.6 43l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 16.5-7.2 32.2-19.6 43l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 22.4-13.2 42.6-33.6 51.8H329V564.8l99.5-360.5c5.2-18.9 22.5-32.2 42.2-32.3 7.6 0 15.1 2.2 21.1 6.7 9.9 7.4 15.2 18.6 14.6 30.5l-9.6 198.4h314.4C829 418.5 840 436.9 840 456c0 16.5-7.2 32.1-19.6 43z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-unlike",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M885.9 490.3c3.6-12 5.4-24.4 5.4-37 0-28.3-9.3-55.5-26.1-77.7 3.6-12 5.4-24.4 5.4-37 0-28.3-9.3-55.5-26.1-77.7 3.6-12 5.4-24.4 5.4-37 0-51.6-30.7-98.1-78.3-118.4-8.3-3.6-17.2-5.4-26.5-5.4H144c-17.7 0-32 14.3-32 32v364c0 17.7 14.3 32 32 32h129.3l85.8 310.8C372.9 889 418.9 924 470.9 924c29.7 0 57.4-11.8 77.9-33.4 20.5-21.5 31-49.7 29.5-79.4l-6-122.9h239.9c12.1 0 23.9-3.2 34.3-9.3 40.4-23.5 65.5-66.1 65.5-111 0-28.3-9.3-55.5-26.1-77.7zM184 456V172h81v284h-81z m627.2 160.4H496.8l9.6 198.4c0.6 11.9-4.7 23.1-14.6 30.5-6.1 4.5-13.6 6.8-21.1 6.7-19.6-0.1-36.9-13.4-42.2-32.3L329 459.2V172h415.4c20.4 9.2 33.6 29.4 33.6 51.8 0 9.7-2.3 18.9-6.9 27.3l-13.9 25.4 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 9.7-2.3 18.9-6.9 27.3l-13.9 25.4 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 9.7-2.3 18.9-6.9 27.3l-14 25.5 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 19.1-11 37.5-28.8 48.4z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-heart",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M923 283.6c-13.4-31.1-32.6-58.9-56.9-82.8-24.3-23.8-52.5-42.4-84-55.5-32.5-13.5-66.9-20.3-102.4-20.3-49.3 0-97.4 13.5-139.2 39-10 6.1-19.5 12.8-28.5 20.1-9-7.3-18.5-14-28.5-20.1-41.8-25.5-89.9-39-139.2-39-35.5 0-69.9 6.8-102.4 20.3-31.4 13-59.7 31.7-84 55.5-24.4 23.9-43.5 51.7-56.9 82.8-13.9 32.3-21 66.6-21 101.9 0 33.3 6.8 68 20.3 103.3 11.3 29.5 27.5 60.1 48.2 91 32.8 48.9 77.9 99.9 133.9 151.6 92.8 85.7 184.7 144.9 188.6 147.3l23.7 15.2c10.5 6.7 24 6.7 34.5 0l23.7-15.2c3.9-2.5 95.7-61.6 188.6-147.3 56-51.7 101.1-102.7 133.9-151.6 20.7-30.9 37-61.5 48.2-91 13.5-35.3 20.3-70 20.3-103.3 0.1-35.3-7-69.6-20.9-101.9zM512 814.8S156 586.7 156 385.5C156 283.6 240.3 201 344.3 201c73.1 0 136.5 40.8 167.7 100.4C543.2 241.8 606.6 201 679.7 201c104 0 188.3 82.6 188.3 184.5 0 201.2-356 429.3-356 429.3z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-edit",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M723.2 917.76H286.72c-65.28 0-118.4-51.2-118.4-113.92V261.76C168.32 198.4 221.44 147.2 286.72 147.2h375.04c17.92 0 32 14.08 32 32s-14.08 32-32 32H286.72c-30.08 0-54.4 22.4-54.4 49.92v542.08c0 27.52 24.32 49.92 54.4 49.92H723.2c30.08 0 54.4-22.4 54.4-49.92V440.32c0-17.92 14.08-32 32-32s32 14.08 32 32v363.52c0 62.72-53.12 113.92-118.4 113.92z"}}),e._v(" "),t("path",{attrs:{d:"M499.84 602.24c-7.68 0-14.72-2.56-21.12-7.68-13.44-11.52-14.72-32-3.2-45.44L780.16 198.4c11.52-13.44 32-14.72 45.44-3.2s14.72 32 3.2 45.44L524.16 591.36c-6.4 7.04-15.36 10.88-24.32 10.88z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-delete",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M677.647059 256l0-90.352941c0-37.436235-23.461647-60.235294-61.771294-60.235294L408.094118 105.411765c-38.249412 0-61.741176 22.799059-61.741176 60.235294l0 90.352941-180.705882 0 0 60.235294 60.235294 0 0 512c0 54.272 33.972706 90.352941 90.352941 90.352941l391.529412 0c55.085176 0 90.352941-33.490824 90.352941-90.352941l0-512 60.235294 0 0-60.235294L677.647059 256zM406.588235 165.647059l210.823529 0-1.264941 90.352941L406.588235 256 406.588235 165.647059zM737.882353 858.352941l-451.764706 0 0-542.117647 451.764706 0L737.882353 858.352941zM466.823529 376.470588l-58.729412 0-1.505882 391.529412 60.235294 0L466.823529 376.470588zM617.411765 376.470588l-60.235294 0 0 391.529412 60.235294 0L617.411765 376.470588z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-reply",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M426.666667 384 426.666667 213.333333 128 512 426.666667 810.666667 426.666667 635.733333C640 635.733333 789.333333 704 896 853.333333 853.333333 640 725.333333 426.666667 426.666667 384Z"}})]),e._v(" "),t("symbol",{attrs:{id:"vssue-icon-error",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M512 720m-48 0a48 48 0 1 0 96 0 48 48 0 1 0-96 0Z"}}),e._v(" "),t("path",{attrs:{d:"M480 416v184c0 4.4 3.6 8 8 8h48c4.4 0 8-3.6 8-8V416c0-4.4-3.6-8-8-8h-48c-4.4 0-8 3.6-8 8z"}}),e._v(" "),t("path",{attrs:{d:"M955.7 856l-416-720c-6.2-10.7-16.9-16-27.7-16s-21.6 5.3-27.7 16l-416 720C56 877.4 71.4 904 96 904h832c24.6 0 40-26.6 27.7-48z m-783.5-27.9L512 239.9l339.8 588.2H172.2z"}})])])},staticRenderFns:[]},void 0,Sr,void 0,!0,void 0,!1,void 0,void 0,void 0);const Er=xr({},void 0,r.b.extend({name:"TransitionFade",functional:!0,props:{group:{type:Boolean,required:!1,default:!1},tag:{type:String,required:!1,default:"div"}},render:(n,{props:e,children:t})=>n(e.group?"TransitionGroup":"Transition",{props:{name:"fade",mode:"out-in",appear:!0,tag:e.tag}},t)}),void 0,void 0,void 0,!1,void 0,void 0,void 0);const Dr=xr({},void 0,r.b.extend({name:"VssueIcon",functional:!0,props:{name:{type:String,required:!0},title:{type:String,required:!1,default:null}},render:(n,{props:e,data:t})=>n("svg",Object.assign(Object.assign({},t),{class:["vssue-icon","vssue-icon-"+e.name],attrs:{"aria-hidden":"true"}}),[n("title",e.title),n("use",{attrs:{"xlink:href":"#vssue-icon-"+e.name}})])}),void 0,void 0,void 0,!1,void 0,void 0,void 0);let Cr=class extends r.b{constructor(){super(...arguments),this.editMode=!1,this.editContent=this.comment.contentRaw,this.creatingReactions=[],this.isPutingComment=!1,this.isDeletingComment=!1}get currentUser(){return this.vssue.user?this.vssue.user.username:null}get content(){return this.comment.content}get author(){return this.comment.author}get createdAt(){return xt(this.comment.createdAt)}get updatedAt(){return xt(this.comment.updatedAt)}get showReactions(){return Boolean(this.vssue.API&&this.vssue.API.platform.meta.reactable&&this.comment.reactions&&!this.editMode)}get reactionKeys(){return["heart","like","unlike"]}get editContentRows(){return this.editContent.split("\n").length-1}get editInputRows(){return this.editContentRows<3?5:this.editContentRows+2}async postReaction({reaction:n}){try{if(this.creatingReactions.includes(n))return;this.creatingReactions.push(n);await this.vssue.postCommentReaction({commentId:this.comment.id,reaction:n})||this.vssue.$emit("error",new Error(this.vssue.$t("reactionGiven",{reaction:this.vssue.$t(n)})));const e=await this.vssue.getCommentReactions({commentId:this.comment.id});e&&(this.comment.reactions=e)}finally{this.creatingReactions.splice(this.creatingReactions.findIndex(e=>e===n),1)}}enterEdit(){this.editMode=!0,this.$nextTick(()=>{this.$refs.input.focus()})}resetEdit(){this.editMode=!1,this.editContent=this.comment.contentRaw}async putComment(){try{if(this.vssue.isPending)return;if(this.editContent!==this.comment.contentRaw){this.isPutingComment=!0,this.vssue.isUpdatingComment=!0;const n=await this.vssue.putComment({commentId:this.comment.id,content:this.editContent});n&&this.vssue.comments.data.splice(this.vssue.comments.data.findIndex(n=>n.id===this.comment.id),1,n)}this.editMode=!1}finally{this.isPutingComment=!1,this.vssue.isUpdatingComment=!1}}async deleteComment(){try{if(this.vssue.isPending)return;if(!window.confirm(this.vssue.$t("deleteConfirm")))return;this.isDeletingComment=!0,this.vssue.isUpdatingComment=!0;await this.vssue.deleteComment({commentId:this.comment.id})?(this.vssue.comments.count-=1,this.vssue.comments.data.length>1&&this.vssue.comments.data.splice(this.vssue.comments.data.findIndex(n=>n.id===this.comment.id),1),this.vssue.query.page>1&&this.vssue.query.page>Math.ceil(this.vssue.comments.count/this.vssue.query.perPage)?this.vssue.query.page-=1:await this.vssue.getComments()):this.vssue.$emit("error",new Error(this.vssue.$t("deleteFailed")))}finally{this.isDeletingComment=!1,this.vssue.isUpdatingComment=!1}}};yr([gt({type:Object,required:!0})],Cr.prototype,"comment",void 0),yr([st()],Cr.prototype,"vssue",void 0),Cr=yr([Object(it.b)({components:{VssueIcon:Dr}})],Cr);const Ir=xr({render:function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"vssue-comment",class:{"vssue-comment-edit-mode":n.editMode,"vssue-comment-disabled":n.isDeletingComment||n.isPutingComment}},[t("div",{staticClass:"vssue-comment-avatar"},[t("a",{attrs:{href:n.author.homepage,title:n.author.username,target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:n.author.avatar,alt:n.author.username}})])]),n._v(" "),t("div",{staticClass:"vssue-comment-body"},[n._t("body",[t("div",{staticClass:"vssue-comment-header"},[t("span",{staticClass:"vssue-comment-author"},[t("a",{attrs:{href:n.author.homepage,title:n.author.username,target:"_blank",rel:"noopener noreferrer"}},[n._v("\n            "+n._s(n.author.username)+"\n          ")])]),n._v(" "),t("span",{staticClass:"vssue-comment-created-at"},[n._v("\n          "+n._s(n.createdAt)+"\n        ")])]),n._v(" "),t("div",{staticClass:"vssue-comment-main"},[n.editMode?t("textarea",{directives:[{name:"model",rawName:"v-model",value:n.editContent,expression:"editContent"}],ref:"input",staticClass:"vssue-edit-comment-input",attrs:{rows:n.editInputRows},domProps:{value:n.editContent},on:{keyup:function(e){return!e.type.indexOf("key")&&n._k(e.keyCode,"enter",13,e.key,"Enter")?null:e.ctrlKey?n.putComment():null},input:function(e){e.target.composing||(n.editContent=e.target.value)}}}):t("article",{staticClass:"markdown-body",domProps:{innerHTML:n._s(n.content)}})]),n._v(" "),t("div",{staticClass:"vssue-comment-footer"},[n.editMode?t("span",{staticClass:"vssue-comment-hint"},[n._v("\n          "+n._s(n.vssue.$t("editMode"))+"\n        ")]):n._e(),n._v(" "),n.showReactions?t("span",{staticClass:"vssue-comment-reactions"},n._l(n.reactionKeys,(function(e){return t("span",{key:e,staticClass:"vssue-comment-reaction",attrs:{title:n.vssue.$t(n.creatingReactions.includes(e)?"loading":e)},on:{click:function(t){return n.postReaction({reaction:e})}}},[t("VssueIcon",{attrs:{name:n.creatingReactions.includes(e)?"loading":e,title:n.vssue.$t(n.creatingReactions.includes(e)?"loading":e)}}),n._v(" "),t("span",{staticClass:"vssue-comment-reaction-number"},[n._v("\n              "+n._s(n.comment.reactions[e])+"\n            ")])],1)})),0):n._e(),n._v(" "),t("span",{staticClass:"vssue-comment-operations"},[n.comment.author.username===n.currentUser&&n.editMode?t("span",{staticClass:"vssue-comment-operation",class:{"vssue-comment-operation-muted":n.isPutingComment},attrs:{title:n.vssue.$t(n.isPutingComment?"loading":"submit")},on:{click:function(e){return n.putComment()}}},[t("VssueIcon",{directives:[{name:"show",rawName:"v-show",value:n.isPutingComment,expression:"isPutingComment"}],attrs:{name:"loading",title:n.vssue.$t("loading")}}),n._v("\n\n            "+n._s(n.vssue.$t("submit"))+"\n          ")],1):n._e(),n._v(" "),n.comment.author.username===n.currentUser&&n.editMode?t("span",{staticClass:"vssue-comment-operation vssue-comment-operation-muted",attrs:{title:n.vssue.$t("cancel")},on:{click:function(e){return n.resetEdit()}}},[n._v("\n            "+n._s(n.vssue.$t("cancel"))+"\n          ")]):n._e(),n._v(" "),n.comment.author.username===n.currentUser?t("span",{directives:[{name:"show",rawName:"v-show",value:!n.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(e){return n.enterEdit()}}},[t("VssueIcon",{attrs:{name:"edit",title:n.vssue.$t("edit")}})],1):n._e(),n._v(" "),n.comment.author.username===n.currentUser||n.vssue.isAdmin?t("span",{directives:[{name:"show",rawName:"v-show",value:!n.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(e){return n.deleteComment()}}},[t("VssueIcon",{attrs:{name:n.isDeletingComment?"loading":"delete",title:n.vssue.$t(n.isDeletingComment?"loading":"delete")}})],1):n._e(),n._v(" "),t("span",{directives:[{name:"show",rawName:"v-show",value:!n.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(e){return n.vssue.$emit("reply-comment",n.comment)}}},[t("VssueIcon",{attrs:{name:"reply",title:n.vssue.$t("reply")}})],1)])])])],2)])},staticRenderFns:[]},void 0,Cr,void 0,!1,void 0,!1,void 0,void 0,void 0);let Tr=class extends r.b{get disabled(){return this.vssue.isPending}get pageCount(){const n=Math.ceil(this.vssue.comments.count/this.vssue.comments.perPage);return n>1?n:1}get perPageOptions(){const n=[5,10,20,50];return!n.includes(this.vssue.options.perPage)&&this.vssue.options.perPage<100&&n.push(this.vssue.options.perPage),n.sort((n,e)=>n-e)}get page(){return this.vssue.query.page>this.pageCount?this.pageCount:this.vssue.query.page}set page(n){n>0&&n<=this.pageCount&&(this.vssue.query.page=n)}get perPage(){return this.vssue.query.perPage}set perPage(n){this.perPageOptions.includes(n)&&(this.vssue.query.perPage=n)}};yr([st()],Tr.prototype,"vssue",void 0),Tr=yr([Object(it.b)({components:{VssueIcon:Dr}})],Tr);const Or=xr({render:function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"vssue-pagination"},[t("div",{staticClass:"vssue-pagination-per-page"},[t("label",[t("select",{directives:[{name:"model",rawName:"v-model",value:n.perPage,expression:"perPage"}],staticClass:"vssue-pagination-select",attrs:{disabled:n.disabled},on:{change:function(e){var t=Array.prototype.filter.call(e.target.options,(function(n){return n.selected})).map((function(n){return"_value"in n?n._value:n.value}));n.perPage=e.target.multiple?t:t[0]}}},n._l(n.perPageOptions,(function(e){return t("option",{key:e,domProps:{value:e}},[n._v("\n          "+n._s(e)+"\n        ")])})),0),n._v(" "),t("span",[n._v("\n        "+n._s(n.vssue.$t("perPage"))+"\n      ")])]),n._v(" "),n.vssue.API.platform.meta.sortable?t("span",{class:{"vssue-pagination-link":!0,disabled:n.disabled},attrs:{title:n.vssue.$t("sort")},on:{click:function(e){n.vssue.query.sort="asc"===n.vssue.query.sort?"desc":"asc"}}},[n._v("\n      "+n._s("asc"===n.vssue.query.sort?"":"")+"\n    ")]):n._e()]),n._v(" "),t("div",{staticClass:"vssue-pagination-page"},[t("span",{class:{"vssue-pagination-link":!0,disabled:1===n.page||n.disabled},attrs:{title:n.vssue.$t("prev")},domProps:{textContent:n._s("<")},on:{click:function(e){n.page-=1}}}),n._v(" "),t("label",[t("span",[n._v("\n        "+n._s(n.vssue.$t("page"))+"\n      ")]),n._v(" "),t("select",{directives:[{name:"show",rawName:"v-show",value:n.pageCount>1,expression:"pageCount > 1"},{name:"model",rawName:"v-model",value:n.page,expression:"page"}],staticClass:"vssue-pagination-select",attrs:{disabled:n.disabled},on:{change:function(e){var t=Array.prototype.filter.call(e.target.options,(function(n){return n.selected})).map((function(n){return"_value"in n?n._value:n.value}));n.page=e.target.multiple?t:t[0]}}},n._l(n.pageCount,(function(e){return t("option",{key:e,domProps:{value:e}},[n._v("\n          "+n._s(e)+"\n        ")])})),0),n._v(" "),t("span",{directives:[{name:"show",rawName:"v-show",value:n.pageCount<2,expression:"pageCount < 2"}],domProps:{textContent:n._s(n.page)}}),n._v(" "),t("span",{domProps:{textContent:n._s(" / "+n.pageCount+" ")}})]),n._v(" "),t("span",{class:{"vssue-pagination-link":!0,disabled:n.page===n.pageCount||n.disabled},attrs:{title:n.vssue.$t("next")},domProps:{textContent:n._s(">")},on:{click:function(e){n.page+=1}}})])])},staticRenderFns:[]},void 0,Tr,void 0,!1,void 0,!1,void 0,void 0,void 0);let Ar=class extends r.b{};yr([st()],Ar.prototype,"vssue",void 0),Ar=yr([Object(it.b)({components:{TransitionFade:Er,VssueComment:Ir,VssuePagination:Or}})],Ar);const _r=xr({render:function(){var n=this.$createElement,e=this._self._c||n;return e("div",{staticClass:"vssue-comments"},[e("VssuePagination"),this._v(" "),e("TransitionFade",{attrs:{group:""}},this._l(this.vssue.comments.data,(function(n){return e("VssueComment",{key:n.id,attrs:{comment:n}})})),1),this._v(" "),e("VssuePagination",{directives:[{name:"show",rawName:"v-show",value:this.vssue.comments.data.length>5,expression:"vssue.comments.data.length > 5"}]})],1)},staticRenderFns:[]},void 0,Ar,void 0,!1,void 0,!1,void 0,void 0,void 0);const Rr=xr({},void 0,r.b.extend({name:"VssueIcon",functional:!0,props:{type:{type:String,required:!1,default:"default"}},render:(n,{props:e,data:t,children:r})=>n("button",Object.assign(Object.assign({},t),{class:["vssue-button","vssue-button-"+e.type]}),r)}),void 0,void 0,void 0,!1,void 0,void 0,void 0);let Pr=class extends r.b{constructor(){super(...arguments),this.content=""}get user(){return this.vssue.user}get platform(){return this.vssue.API&&this.vssue.API.platform.name}get isInputDisabled(){return this.loading||null===this.user||null===this.vssue.issue}get isSubmitDisabled(){return""===this.content||this.vssue.isPending||null===this.vssue.issue}get loading(){return this.vssue.isCreatingComment}get contentRows(){return this.content.split("\n").length-1}get inputRows(){return this.contentRows<3?5:this.contentRows+2}created(){this.vssue.$on("reply-comment",n=>{const e=n.contentRaw.replace(/\n/g,"\n> "),t=`@${n.author.username}\n\n> ${e}\n\n`;this.content=this.content.concat(t),this.focus()})}beforeDestroy(){this.vssue.$off("reply-comment")}focus(){this.$refs.input.focus()}async submit(){this.isSubmitDisabled||(await this.vssue.postComment({content:this.content}),this.content="",await this.vssue.getComments())}};yr([st()],Pr.prototype,"vssue",void 0),Pr=yr([Object(it.b)({components:{VssueButton:Rr,VssueIcon:Dr}})],Pr);const Fr=xr({render:function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"vssue-new-comment"},[t("div",{staticClass:"vssue-comment-avatar"},[n.user?t("a",{attrs:{href:n.user.homepage,title:n.user.username,target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:n.user.avatar,alt:n.user.username}})]):t("VssueIcon",{attrs:{name:n.platform.toLowerCase(),title:n.vssue.$t("loginToComment",{platform:n.platform})},on:{click:function(e){return n.vssue.login()}}})],1),n._v(" "),t("div",{staticClass:"vssue-new-comment-body"},[t("textarea",{directives:[{name:"model",rawName:"v-model",value:n.content,expression:"content"}],ref:"input",staticClass:"vssue-new-comment-input",attrs:{rows:n.inputRows,disabled:n.isInputDisabled,placeholder:n.vssue.$t(n.user?"placeholder":"noLoginPlaceHolder"),spellcheck:!1,"aria-label":"leave a comment"},domProps:{value:n.content},on:{keyup:function(e){return!e.type.indexOf("key")&&n._k(e.keyCode,"enter",13,e.key,"Enter")?null:e.ctrlKey?n.submit():null},input:function(e){e.target.composing||(n.content=e.target.value)}}})]),n._v(" "),t("div",{staticClass:"vssue-new-comment-footer"},[n.user?t("span",{staticClass:"vssue-current-user"},[t("span",[n._v(n._s(n.vssue.$t("currentUser"))+" - "+n._s(n.user.username)+" - ")]),n._v(" "),t("a",{staticClass:"vssue-logout",on:{click:function(e){return n.vssue.logout()}}},[n._v("\n        "+n._s(n.vssue.$t("logout"))+"\n      ")])]):t("span",{staticClass:"vssue-current-user"},[n._v("\n      "+n._s(n.vssue.$t("loginToComment",{platform:n.platform}))+"\n    ")]),n._v(" "),t("div",{staticClass:"vssue-new-comment-operations"},[n.user?t("VssueButton",{staticClass:"vssue-button-submit-comment",attrs:{type:"primary",disabled:n.isSubmitDisabled},on:{click:function(e){return n.submit()}}},[t("VssueIcon",{directives:[{name:"show",rawName:"v-show",value:n.loading,expression:"loading"}],attrs:{name:"loading"}}),n._v("\n\n        "+n._s(n.vssue.$t(n.loading?"submitting":"submitComment"))+"\n      ")],1):t("VssueButton",{staticClass:"vssue-button-login",attrs:{type:"primary",title:n.vssue.$t("loginToComment",{platform:n.platform})},on:{click:function(e){return n.vssue.login()}}},[n._v("\n        "+n._s(n.vssue.$t("login",{platform:n.platform}))+"\n      ")])],1)])])},staticRenderFns:[]},void 0,Pr,void 0,!1,void 0,!1,void 0,void 0,void 0);let Br=class extends r.b{constructor(){super(...arguments),this.progress={show:!1,percent:0,timer:null,speed:200},this.alert={show:!1,message:null,timer:null}}onLoadingCommentsChange(n){this.vssue.comments&&(n?this.progressStart():this.progressDone())}created(){this.vssue.$on("error",n=>this.alertShow(n.message))}beforeDestroy(){this.vssue.$off("error"),null!==this.progress.timer&&window.clearTimeout(this.progress.timer),null!==this.alert.timer&&window.clearTimeout(this.alert.timer)}progressStart(){this.progress.show=!0,this.progress.percent=0,this.progress.timer=window.setInterval(()=>{this.progress.percent+=5,this.progress.percent>94&&null!==this.progress.timer&&window.clearInterval(this.progress.timer)},this.progress.speed)}progressDone(){this.progress.percent=100,null!==this.progress.timer&&window.clearTimeout(this.progress.timer),this.progress.timer=null,window.setTimeout(()=>{this.progress.show=!1},this.progress.speed)}alertShow(n){this.alert.show=!0,this.alert.message=n,null!==this.alert.timer&&window.clearTimeout(this.alert.timer),this.alert.timer=window.setTimeout(()=>{this.alertHide()},3e3)}alertHide(){this.alert.show=!1,null!==this.alert.timer&&window.clearTimeout(this.alert.timer),this.alert.timer=null}};yr([st()],Br.prototype,"vssue",void 0),yr([ft("vssue.isLoadingComments")],Br.prototype,"onLoadingCommentsChange",null),Br=yr([Object(it.b)({components:{TransitionFade:Er}})],Br);const Mr=xr({render:function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"vssue-notice"},[t("div",{directives:[{name:"show",rawName:"v-show",value:n.progress.show,expression:"progress.show"}],staticClass:"vssue-progress",style:{width:n.progress.percent+"%",transition:"all "+n.progress.speed+"ms linear"}}),n._v(" "),t("TransitionFade",[t("div",{directives:[{name:"show",rawName:"v-show",value:n.alert.show,expression:"alert.show"}],staticClass:"vssue-alert",domProps:{textContent:n._s(n.alert.message)},on:{click:function(e){return n.alertHide()}}})])],1)},staticRenderFns:[]},void 0,Br,void 0,!1,void 0,!1,void 0,void 0,void 0);let jr=class extends r.b{get status(){return this.vssue.isFailed?"failed":this.vssue.isInitializing?"initializing":this.vssue.isIssueNotCreated&&!this.vssue.isCreatingIssue?this.vssue.isAdmin||!this.vssue.isLogined?"issueNotCreated":"failed":this.vssue.isLoginRequired?"loginRequired":!this.vssue.comments||this.vssue.isCreatingIssue?"loadingComments":0===this.vssue.comments.data.length?"noComments":null}handleClick(){"issueNotCreated"===this.status?this.vssue.postIssue():"loginRequired"===this.status&&this.vssue.login()}};yr([st()],jr.prototype,"vssue",void 0),jr=yr([Object(it.b)({components:{TransitionFade:Er,VssueIcon:Dr}})],jr);const Lr=xr({render:function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("TransitionFade",[n.status?t("div",{key:n.status,staticClass:"vssue-status"},[["failed","loadingComments","initializing"].includes(n.status)?t("VssueIcon",{attrs:{name:"failed"===n.status?"error":"loading"}}):n._e(),n._v(" "),t("p",{staticClass:"vssue-status-info"},[t(["issueNotCreated","loginRequired"].includes(n.status)?"a":"span",{tag:"Component",on:{click:n.handleClick}},[n._v("\n        "+n._s(n.vssue.$t(n.status))+"\n      ")])],1)],1):n._e()])},staticRenderFns:[]},void 0,jr,void 0,!1,void 0,!1,void 0,void 0,void 0);let Nr=class extends r.b{};yr([st()],Nr.prototype,"vssue",void 0),Nr=yr([Object(it.b)({components:{TransitionFade:Er,VssueIcon:Dr,VssueComments:_r,VssueNewComment:Fr,VssueNotice:Mr,VssueStatus:Lr}})],Nr);const $r=xr({render:function(){var n=this.$createElement,e=this._self._c||n;return e("TransitionFade",[this.vssue.isInitializing?e("VssueStatus"):e("div",{staticClass:"vssue-body"},[this.vssue.API?e("VssueNewComment"):this._e(),this._v(" "),e("VssueNotice"),this._v(" "),e("TransitionFade",[this.vssue.comments&&this.vssue.comments.data.length>0?e("VssueComments"):e("VssueStatus")],1)],1)],1)},staticRenderFns:[]},void 0,Nr,void 0,!1,void 0,!1,void 0,void 0,void 0);let Ur=class extends r.b{};yr([st()],Ur.prototype,"vssue",void 0),Ur=yr([it.b],Ur);const zr=xr({render:function(){var n=this,e=n.$createElement,t=n._self._c||e;return t("div",{staticClass:"vssue-header"},[t("a",{staticClass:"vssue-header-comments-count",attrs:{href:n.vssue.issue?n.vssue.issue.link:null,target:"_blank",rel:"noopener noreferrer"}},[t("span",[n._v("\n      "+n._s(n.vssue.comments?n.vssue.$tc("comments",n.vssue.comments.count,{count:n.vssue.comments.count}):n.vssue.$tc("comments",0))+"\n    ")])]),n._v(" "),t("span",{staticClass:"vssue-header-powered-by"},[t("span",[n._v("Powered by")]),n._v(" "),n.vssue.API?t("span",[t("a",{attrs:{href:n.vssue.API.platform.link,title:n.vssue.API.platform.name+" API "+n.vssue.API.platform.version,target:"_blank",rel:"noopener noreferrer"}},[n._v("\n        "+n._s(n.vssue.API.platform.name)+"\n      ")]),n._v(" "),t("span",[n._v("&")])]):n._e(),n._v(" "),t("a",{attrs:{href:"https://github.com/meteorlxy/vssue",title:"Vssue v"+n.vssue.version,target:"_blank",rel:"noopener noreferrer"}},[n._v("\n      Vssue\n    ")])])])},staticRenderFns:[]},void 0,Ur,void 0,!1,void 0,!1,void 0,void 0,void 0),Hr={login:"Login with {platform}",logout:"Logout",currentUser:"Current User",loading:"Loading",submit:"Submit",submitting:"Submitting",submitComment:"Submit Comment",cancel:"Cancel",edit:"Edit",editMode:"Edit Mode",delete:"Delete",reply:"Reply",heart:"Heart",like:"Like",unlike:"Unlike",perPage:"Comments per page",sort:"Click to change the sort direction",page:"Page",prev:"Previous Page",next:"Next Page",comments:"Comments | {count} Comment | {count} Comments",loginToComment:"Login with {platform} account to leave a comment",placeholder:"Leave a comment. Styling with Markdown is supported. Ctrl + Enter to submit.",noLoginPlaceHolder:"Login to leave a comment. Styling with Markdown is supported. ",failed:"Failed to load comments",initializing:"Initializing...",issueNotCreated:"Click to create issue",loadingComments:"Loading comments...",loginRequired:"Login to view comments",noComments:"No comments yet. Leave the first comment !",reactionGiven:"Already given '{reaction}' reaction",deleteConfirm:"Confirm to delete this comment ?",deleteFailed:"Failed to delete comment"},qr={login:" {platform} ",logout:"",currentUser:"",loading:"",submit:"",submitting:"",submitComment:"",cancel:"",edit:"",editMode:"",delete:"",reply:"",heart:"",like:"",unlike:"",perPage:"",sort:"",page:"",prev:"",next:"",comments:" | {count}  | {count} ",loginToComment:" {platform} ",placeholder:" Markdown Ctrl + Enter ",noLoginPlaceHolder:" Markdown ",failed:"",initializing:"...",issueNotCreated:" Issue",loadingComments:"...",loginRequired:"",noComments:"",reactionGiven:" '{reaction}' ",deleteConfirm:"",deleteFailed:""},Vr={login:"Entrar com {platform}",logout:"Sair",currentUser:"Usurio Atual",loading:"Carregando",submit:"Enviar",submitting:"Enviando",submitComment:"Enviar Comentrio",cancel:"Cancelar",edit:"Editar",editMode:"Modo de Edio",delete:"Apagar",reply:"Responder",heart:"Heart",like:"Like",unlike:"Unlike",perPage:"Comentrios por pgina",sort:"Clique para alterar a ordenao",page:"Pgina",prev:"Pgina Anterior",next:"Prxima Pgina",comments:"Comentrios | {count} Comentrio | {count} Comentrios",loginToComment:"Entre com uma conta {platform} para deixar um comentrio",placeholder:"Deixe um comentrio. Estilos com Markdown suportados. Ctrl + Enter para enviar.",noLoginPlaceHolder:"Entre para deixar um comentrio. Estilos com Markdown suportados. ",failed:"Falha ao carregar comentrios",initializing:"Inicializando...",issueNotCreated:"Click to create issue",loadingComments:"Carregando comentrios...",loginRequired:"Entrar para visualizar comentrios",noComments:"Nenhum comentrio. Deixe o primeiro comentrio!",reactionGiven:"J reagiu com '{reaction}'",deleteConfirm:"Apagar este comentrio?",deleteFailed:"Falha ao apagar comentrio"},Kr={login:"{platform} ",logout:"",currentUser:"",loading:"",submit:"",submitting:"",submitComment:"",cancel:"",edit:"",editMode:"",delete:"",reply:"",heart:"",like:"",unlike:"",perPage:"/",sort:"",page:"",prev:"",next:"",comments:" | {count}  | {count} ",loginToComment:" {platform} ",placeholder:"Markdown  Ctrl + Enter ",noLoginPlaceHolder:"",failed:"",initializing:"...",issueNotCreated:"Click to create issue",loadingComments:"...",loginRequired:"",noComments:"",reactionGiven:" '{reaction}' ",deleteConfirm:"",deleteFailed:""},Wr={login:"  {platform}",logout:"",currentUser:"/ /",loading:"",submit:"",submitting:"",submitComment:" ",cancel:"",edit:"",editMode:" ",delete:"",reply:"",heart:"",like:"",unlike:"",perPage:" ",sort:"     ",page:"",prev:" ",next:" ",comments:" | {count}  | {count} ",loginToComment:"   {platform}   ",placeholder:" .     Markdown. Ctrl + Enter  .",noLoginPlaceHolder:"   .     Markdown. ",failed:"  ",initializing:"...",issueNotCreated:"  issue",loadingComments:" ...",loginRequired:"   ",noComments:"  .    !",reactionGiven:"   '{reaction}'",deleteConfirm:"   ?",deleteFailed:"  "};Object.prototype.hasOwnProperty.call(r.b,"$i18n")||r.b.use(kr);const Gr=new kr({locale:"en",fallbackLocale:"en",messages:{en:Hr,"en-US":Hr,zh:qr,"zh-CN":qr,pt:Vr,"pt-BR":Vr,ja:Kr,"ja-JP":Kr,he:Wr,"he-IL":Wr}});let Jr=class extends r.b{constructor(){super(...arguments),this.title=n=>`${n.prefix}${document.title}`,this.issueId=null,this.options=null,this.API=null,this.accessToken=null,this.user=null,this.issue=null,this.comments=null,this.query={page:1,perPage:10,sort:"desc"},this.isInitializing=!0,this.isIssueNotCreated=!1,this.isLoginRequired=!1,this.isFailed=!1,this.isCreatingIssue=!1,this.isLoadingComments=!1,this.isCreatingComment=!1,this.isUpdatingComment=!1}get version(){return"1.4.8"}get issueTitle(){return null===this.options?"":"function"==typeof this.title?this.title(this.options):`${this.options.prefix}${this.title}`}get isPending(){return this.isLoadingComments||this.isCreatingComment||this.isUpdatingComment}get isLogined(){return null!==this.accessToken&&null!==this.user}get isAdmin(){return null!==this.options&&null!==this.accessToken&&null!==this.user&&(this.user.username===this.options.owner||this.options.admins.includes(this.user.username))}get accessTokenKey(){return this.API?`Vssue.${this.API.platform.name.toLowerCase()}.access_token`:""}onQueryPerPageChange(){this.query.page=1,this.getComments()}onQueryChange(){this.getComments()}setOptions(n){this.options=Object.assign({labels:["Vssue"],state:"Vssue",prefix:"[Vssue]",admins:[],perPage:10,proxy:n=>"https://cors-anywhere.azm.workers.dev/"+n,issueContent:({url:n})=>n,autoCreateIssue:!1},n);const e=["api","owner","repo","clientId"];for(const n of e)this.options[n]||console.warn(`[Vssue] the option '${n}' is required`);if(this.options.locale)this.$i18n.locale=this.options.locale;else{const n=Object.keys(this.$i18n.messages),e=window.navigator.languages;this.$i18n.locale=e.filter(e=>n.includes(e)).shift()||"en"}}async init(){try{await this.initStore(),await this.initComments()}catch(n){n.response&&[401,403].includes(n.response.status)?this.isLoginRequired=!0:this.isFailed=!0,console.error(n)}}async initStore(){try{if(!this.options)throw new Error("Options are required to initialize Vssue");this.API=null,this.accessToken=null,this.user=null,this.issue=null,this.comments=null,this.query={page:1,perPage:this.options.perPage,sort:"desc"},this.isInitializing=!0,this.isIssueNotCreated=!1,this.isLoginRequired=!1,this.isFailed=!1,this.isCreatingIssue=!1,this.isLoadingComments=!1,this.isCreatingComment=!1,this.isUpdatingComment=!1;const n=this.options.api;this.API=new n({baseURL:this.options.baseURL,labels:this.options.labels,state:this.options.state,owner:this.options.owner,repo:this.options.repo,clientId:this.options.clientId,clientSecret:this.options.clientSecret,proxy:this.options.proxy}),await this.handleAuth()}finally{this.isInitializing=!1}}async initComments(){if(this.API&&this.options)if(this.issueId){const[n,e]=await Promise.all([this.API.getIssue({accessToken:this.accessToken,issueId:this.issueId}),this.API.getComments({accessToken:this.accessToken,issueId:this.issueId,query:this.query})]);this.issue=n,this.comments=e}else this.issue=await this.API.getIssue({accessToken:this.accessToken,issueTitle:this.issueTitle}),null===this.issue?(this.isIssueNotCreated=!0,this.options.autoCreateIssue&&await this.postIssue()):await this.getComments()}async postIssue(){if(this.API&&this.options&&!this.issue&&!this.issueId&&(this.isLogined||this.login(),this.isAdmin))try{this.isCreatingIssue=!0;const n=await this.API.postIssue({title:this.issueTitle,content:await this.options.issueContent({options:this.options,url:wt(window.location.href)}),accessToken:this.accessToken});this.issue=n,this.isIssueNotCreated=!1,await this.getComments()}catch(n){this.isFailed=!0}finally{this.isCreatingIssue=!1}}async getComments(){try{if(!this.API||!this.issue||this.isLoadingComments)return;this.isLoadingComments=!0;const n=await this.API.getComments({accessToken:this.accessToken,issueId:this.issue.id,query:this.query});return this.comments=n,this.query.page!==n.page&&(this.query.page=n.page),this.query.perPage!==n.perPage&&(this.query.perPage=n.perPage),n}catch(n){if(!n.response||![401,403].includes(n.response.status)||this.isLogined)throw this.$emit("error",n),n;this.isLoginRequired=!0}finally{this.isLoadingComments=!1}}async postComment({content:n}){try{if(!this.API||!this.issue||this.isCreatingComment)return;this.isCreatingComment=!0;return await this.API.postComment({accessToken:this.accessToken,content:n,issueId:this.issue.id})}catch(n){throw this.$emit("error",n),n}finally{this.isCreatingComment=!1}}async putComment({commentId:n,content:e}){try{if(!this.API||!this.issue)return;return await this.API.putComment({accessToken:this.accessToken,issueId:this.issue.id,commentId:n,content:e})}catch(n){throw this.$emit("error",n),n}}async deleteComment({commentId:n}){try{if(!this.API||!this.issue)return;return await this.API.deleteComment({accessToken:this.accessToken,issueId:this.issue.id,commentId:n})}catch(n){throw this.$emit("error",n),n}}async getCommentReactions({commentId:n}){try{if(!this.API||!this.issue)return;return await this.API.getCommentReactions({accessToken:this.accessToken,issueId:this.issue.id,commentId:n})}catch(n){throw this.$emit("error",n),n}}async postCommentReaction({commentId:n,reaction:e}){try{if(!this.API||!this.issue)return!1;return await this.API.postCommentReaction({accessToken:this.accessToken,issueId:this.issue.id,commentId:n,reaction:e})}catch(n){throw this.$emit("error",n),n}}login(){this.API&&this.API.redirectAuth()}logout(){this.setAccessToken(null),this.user=null}async handleAuth(){if(!this.API)return;const n=await this.API.handleAuth();n?(this.setAccessToken(n),this.user=await this.API.getUser({accessToken:n})):this.getAccessToken()?this.user=await this.API.getUser({accessToken:this.accessToken}):(this.setAccessToken(null),this.user=null)}getAccessToken(){return this.accessToken=window.localStorage.getItem(this.accessTokenKey),this.accessToken}setAccessToken(n){null===n?window.localStorage.removeItem(this.accessTokenKey):window.localStorage.setItem(this.accessTokenKey,n),this.accessToken=n}};yr([ft("query.perPage")],Jr.prototype,"onQueryPerPageChange",null),yr([ft("query.page"),ft("query.sort")],Jr.prototype,"onQueryChange",null),Jr=yr([Object(it.b)({i18n:Gr})],Jr);var Yr=Jr;let Qr=class extends r.b{constructor(){super(...arguments),this.vssue=new Yr}onOptionsChange(n){this.vssue.setOptions(n)}mounted(){null!==this.title&&(this.vssue.title=this.title),null!==this.issueId&&(this.vssue.issueId=this.issueId),this.vssue.setOptions(this.options),this.vssue.init()}};var Xr;yr([gt({type:[String,Function],required:!1,default:null})],Qr.prototype,"title",void 0),yr([gt({type:[String,Number],required:!1,default:null})],Qr.prototype,"issueId",void 0),yr([gt({type:Object,required:!1,default:()=>({})})],Qr.prototype,"options",void 0),yr([(Xr="vssue",Object(it.a)((function(n,e){var t=n.provide;ut(n),lt(t)&&(t=n.provide=ct(t)),t.managed[e]=Xr||e})))],Qr.prototype,"vssue",void 0),yr([ft("options",{deep:!0})],Qr.prototype,"onOptionsChange",null),Qr=yr([Object(it.b)({components:{Iconfont:wr,VssueBody:$r,VssueHeader:zr}})],Qr);const Zr=xr({render:function(){var n=this.$createElement,e=this._self._c||n;return e("div",{staticClass:"vssue"},[e("Iconfont"),this._v(" "),e("VssueHeader"),this._v(" "),e("VssueBody")],1)},staticRenderFns:[]},void 0,Qr,void 0,!1,void 0,!1,void 0,void 0,void 0);var na=t(10),ea=t.n(na);function ta(n){return{username:n.login,avatar:n.avatar_url,homepage:n.html_url}}function ra(n){return{id:n.number,title:n.title,content:n.body,link:n.html_url}}function aa(n){return{like:n["+1"],unlike:n[-1],heart:n.heart}}function oa(n){return{id:n.id,content:n.body_html,contentRaw:n.body,author:ta(n.user),createdAt:n.created_at,updatedAt:n.updated_at,reactions:aa(n.reactions)}}function ia(n){return"like"===n?"+1":"unlike"===n?"-1":n}class sa{constructor({baseURL:n="https://github.com",owner:e,repo:t,labels:r,clientId:a,clientSecret:o,state:i,proxy:s}){if(void 0===o||void 0===s)throw new Error("clientSecret and proxy is required for GitHub V3");this.baseURL=n,this.owner=e,this.repo=t,this.labels=r,this.clientId=a,this.clientSecret=o,this.state=i,this.proxy=s,this.$http=ea.a.create({baseURL:"https://github.com"===n?"https://api.github.com":kt(n,"api/v3"),headers:{Accept:"application/vnd.github.v3+json"}}),this.$http.interceptors.response.use(n=>n.data&&n.data.error?Promise.reject(new Error(n.data.error_description)):n,n=>(void 0===n.response&&"Network Error"===n.message&&(n.response={status:403}),Promise.reject(n)))}get platform(){return{name:"GitHub",link:this.baseURL,version:"v3",meta:{reactable:!0,sortable:!1}}}redirectAuth(){window.location.href=bt(kt(this.baseURL,"login/oauth/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,scope:"public_repo",state:this.state})}async handleAuth(){const n=Et(window.location.search);if(n.code){if(n.state!==this.state)return null;const e=n.code;delete n.code,delete n.state;const t=bt(wt(window.location.href),n)+window.location.hash;window.history.replaceState(null,"",t);return await this.getAccessToken({code:e})}return null}async getAccessToken({code:n}){const e=kt(this.baseURL,"login/oauth/access_token"),t="function"==typeof this.proxy?this.proxy(e):this.proxy,{data:r}=await this.$http.post(t,{client_id:this.clientId,client_secret:this.clientSecret,code:n},{headers:{Accept:"application/json"}});return r.access_token}async getUser({accessToken:n}){const{data:e}=await this.$http.get("user",{headers:{Authorization:"token "+n}});return ta(e)}async getIssue({accessToken:n,issueId:e,issueTitle:t}){const r={};if(n&&(r.headers={Authorization:"token "+n}),!e){r.params={q:[`"${t}"`,"is:issue","in:title",`repo:${this.owner}/${this.repo}`,"is:public",...this.labels.map(n=>"label:"+n)].join(" "),timestamp:Date.now()};const{data:n}=await this.$http.get("search/issues",r);return n.items.map(ra).find(n=>n.title===t)||null}try{r.params={timestamp:Date.now()};const{data:n}=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/${e}`,r);return ra(n)}catch(n){if(n.response&&404===n.response.status)return null;throw n}}async postIssue({accessToken:n,title:e,content:t}){const{data:r}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues`,{title:e,body:t,labels:this.labels},{headers:{Authorization:"token "+n}});return ra(r)}async getComments({accessToken:n,issueId:e,query:{page:t=1,perPage:r=10}={}}){const a={params:{timestamp:Date.now()}},o={params:{page:t,per_page:r,timestamp:Date.now()},headers:{Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}};n&&(a.headers={Authorization:"token "+n},o.headers.Authorization="token "+n);const[i,s]=await Promise.all([this.$http.get(`repos/${this.owner}/${this.repo}/issues/${e}`,a),this.$http.get(`repos/${this.owner}/${this.repo}/issues/${e}/comments`,o)]),l=s.headers.link||null,c=/rel="next"/.test(l)?Number(l.replace(/^.*[^_]page=(\d*).*rel="next".*$/,"$1"))-1:/rel="prev"/.test(l)?Number(l.replace(/^.*[^_]page=(\d*).*rel="prev".*$/,"$1"))+1:1,p=l?Number(l.replace(/^.*per_page=(\d*).*$/,"$1")):r;return{count:Number(i.data.comments),page:c,perPage:p,data:s.data.map(oa)}}async postComment({accessToken:n,issueId:e,content:t}){const{data:r}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues/${e}/comments`,{body:t},{headers:{Authorization:"token "+n,Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}});return oa(r)}async putComment({accessToken:n,commentId:e,content:t}){const{data:r}=await this.$http.patch(`repos/${this.owner}/${this.repo}/issues/comments/${e}`,{body:t},{headers:{Authorization:"token "+n,Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}});return oa(r)}async deleteComment({accessToken:n,commentId:e}){const{status:t}=await this.$http.delete(`repos/${this.owner}/${this.repo}/issues/comments/${e}`,{headers:{Authorization:"token "+n}});return 204===t}async getCommentReactions({accessToken:n,commentId:e}){const{data:t}=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/comments/${e}`,{params:{timestamp:Date.now()},headers:{Authorization:"token "+n,Accept:"application/vnd.github.squirrel-girl-preview"}});return aa(t.reactions)}async postCommentReaction({accessToken:n,commentId:e,reaction:t}){const r=await this.$http.post(`repos/${this.owner}/${this.repo}/issues/comments/${e}/reactions`,{content:ia(t)},{headers:{Authorization:"token "+n,Accept:"application/vnd.github.squirrel-girl-preview"}});return 200===r.status?this.deleteCommentReaction({accessToken:n,commentId:e,reactionId:r.data.id}):201===r.status}async deleteCommentReaction({accessToken:n,commentId:e,reactionId:t}){return 204===(await this.$http.delete(`repos/${this.owner}/${this.repo}/issues/comments/${e}/reactions/${t}`,{headers:{Authorization:"token "+n,Accept:"application/vnd.github.squirrel-girl-preview"}})).status}}function la(n){return null===n?{username:"ghost",avatar:"https://avatars3.githubusercontent.com/u/10137?v=4",homepage:"https://github.com/ghost"}:{username:n.login,avatar:n.avatarUrl,homepage:n.url}}function ca(n){return{id:n.number,title:n.title,content:n.body,link:n.url}}function pa(n){return{like:n.find(n=>"THUMBS_UP"===n.content).users.totalCount,unlike:n.find(n=>"THUMBS_DOWN"===n.content).users.totalCount,heart:n.find(n=>"HEART"===n.content).users.totalCount}}function ua(n){return{id:n.id,content:n.bodyHTML,contentRaw:n.body,author:la(n.author),createdAt:n.createdAt,updatedAt:n.updatedAt,reactions:pa(n.reactionGroups)}}function da(n){return"like"===n?"THUMBS_UP":"unlike"===n?"THUMBS_DOWN":"heart"===n?"HEART":n}class ma{constructor({baseURL:n="https://github.com",owner:e,repo:t,labels:r,clientId:a,clientSecret:o,state:i,proxy:s}){if(void 0===o||void 0===s)throw new Error("clientSecret and proxy is required for GitHub V4");this.baseURL=n,this.owner=e,this.repo=t,this.labels=r,this.clientId=a,this.clientSecret=o,this.state=i,this.proxy=s,this._pageInfo={page:1,startCursor:null,endCursor:null,sort:null,perPage:null},this._issueNodeId=null,this.$http=ea.a.create({baseURL:"https://github.com"===n?"https://api.github.com":kt(n,"api"),headers:{Accept:"application/vnd.github.v3+json"}}),this.$http.interceptors.response.use(n=>n.data.error?Promise.reject(n.data.error_description):n.data.errors?Promise.reject(n.data.errors[0].message):n)}get platform(){return{name:"GitHub",link:this.baseURL,version:"v4",meta:{reactable:!0,sortable:!0}}}redirectAuth(){window.location.href=bt(kt(this.baseURL,"login/oauth/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,scope:"public_repo",state:this.state})}async handleAuth(){const n=Et(window.location.search);if(n.code){if(n.state!==this.state)return null;const e=n.code;delete n.code,delete n.state;const t=bt(wt(window.location.href),n)+window.location.hash;window.history.replaceState(null,"",t);return await this.getAccessToken({code:e})}return null}async getAccessToken({code:n}){const e=kt(this.baseURL,"login/oauth/access_token"),t="function"==typeof this.proxy?this.proxy(e):this.proxy,{data:r}=await this.$http.post(t,{client_id:this.clientId,client_secret:this.clientSecret,code:n},{headers:{Accept:"application/json"}});return r.access_token}async getUser({accessToken:n}){const{data:e}=await this.$http.post("graphql",{query:"query getUser {\n  viewer {\n    login\n    avatarUrl\n    url\n  }\n}"},{headers:{Authorization:"token "+n}});return la(e.data.viewer)}async getIssue({accessToken:n,issueId:e,issueTitle:t}){const r={};if(n&&(r.headers={Authorization:"token "+n}),!e){const n=[`"${t}"`,"in:title",`repo:${this.owner}/${this.repo}`,"is:public",...this.labels.map(n=>"label:"+n)].join(" "),{data:e}=await this.$http.post("graphql",{variables:{query:n},query:"query getIssueByTitle(\n  $query: String!\n) {\n  search(\n    query: $query\n    type: ISSUE\n    first: 20\n    ) {\n      nodes {\n      ... on Issue {\n        id\n        number\n        title\n        body\n        url\n      }\n    }\n  }\n}"},r),a=e.data.search.nodes.find(n=>n.title===t);return a?(this._issueNodeId=a.id,ca(a)):null}try{const{data:n}=await this.$http.post("graphql",{query:`query getIssueById {\n  repository(owner: "${this.owner}", name: "${this.repo}") {\n    issue (number: ${e}) {\n      id\n      number\n      title\n      body\n      url\n    }\n  }\n}`},r);return this._issueNodeId=n.data.repository.issue.id,ca(n.data.repository.issue)}catch(n){if(n.response&&404===n.response.status)return null;throw n}}async postIssue({accessToken:n,title:e,content:t}){const{data:r}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues`,{title:e,body:t,labels:this.labels},{headers:{Authorization:"token "+n}});return r.url=r.html_url,this._issueNodeId=r.node_id,ca(r)}async getComments({accessToken:n,issueId:e,query:{page:t=1,perPage:r=10,sort:a="desc"}={}}){const o={};n&&(o.headers={Authorization:"token "+n}),null!==this._pageInfo.sort&&a!==this._pageInfo.sort&&(t=1);const{firstOrLast:i,afterOrBefore:s,cursor:l}=this._getQueryParams({page:t,sort:a}),{data:c}=await this.$http.post("graphql",{variables:{owner:this.owner,repo:this.repo,issueId:e,perPage:r},query:`query getComments(\n  $owner: String!\n  $repo: String!\n  $issueId: Int!\n  $perPage: Int!\n) {\n  repository(owner: $owner, name: $repo) {\n    issue(number: $issueId) {\n      comments(\n        ${i}: $perPage\n        ${null===s?"":`${s}: "${l}"`}\n      ) {\n        totalCount\n        pageInfo {\n          endCursor\n          startCursor\n        }\n        nodes {\n          id\n          body\n          bodyHTML\n          createdAt\n          updatedAt\n          author {\n            avatarUrl\n            login\n            url\n          }\n          reactionGroups {\n            users (first: 0) {\n              totalCount\n            }\n            content\n          }\n        }\n      }\n    }\n  }\n}`},o),p=c.data.repository.issue.comments;return"desc"===a&&p.nodes.reverse(),this._pageInfo={page:t,startCursor:p.pageInfo.startCursor,endCursor:p.pageInfo.endCursor,sort:a,perPage:r},{count:p.totalCount,page:t,perPage:r,data:p.nodes.map(ua)}}async postComment({accessToken:n,content:e}){const{data:t}=await this.$http.post("graphql",{variables:{issueNodeId:this._issueNodeId,content:e},query:"mutation postComment(\n  $issueNodeId: ID!\n  $content: String!\n) {\n  addComment(\n    input: {\n      subjectId: $issueNodeId\n      body: $content\n    }\n  ) {\n    commentEdge {\n      node {\n        id\n        body\n        bodyHTML\n        createdAt\n        updatedAt\n        author {\n          avatarUrl\n          login\n          url\n        }\n        reactionGroups {\n          users (first: 0) {\n            totalCount\n          }\n          content\n        }\n      }\n    }\n  }\n}"},{headers:{Authorization:"token "+n}});return ua(t.data.addComment.commentEdge.node)}async putComment({accessToken:n,commentId:e,content:t}){const{data:r}=await this.$http.post("graphql",{variables:{commentId:e,content:t},query:"mutation putComment(\n  $commentId: ID!,\n  $content: String!,\n) {\n  updateIssueComment(input: {\n    id: $commentId\n    body: $content\n  }) {\n    issueComment {\n      id\n      body\n      bodyHTML\n      createdAt\n      updatedAt\n      author {\n        avatarUrl\n        login\n        url\n      }\n      reactionGroups {\n        users (first: 0) {\n          totalCount\n        }\n        content\n      }\n    }\n  }\n}"},{headers:{Authorization:"token "+n}});return ua(r.data.updateIssueComment.issueComment)}async deleteComment({accessToken:n,commentId:e}){return await this.$http.post("graphql",{variables:{commentId:e},query:"mutation deleteComment(\n  $commentId: ID!,\n) {\n  deleteIssueComment(input: {\n    id: $commentId\n  }) {\n    clientMutationId\n  }\n}"},{headers:{Authorization:"token "+n}}),!0}async getCommentReactions({accessToken:n,issueId:e,commentId:t}){const{firstOrLast:r,afterOrBefore:a,cursor:o}=this._getQueryParams(),{data:i}=await this.$http.post("graphql",{variables:{owner:this.owner,repo:this.repo,issueId:e,perPage:this._pageInfo.perPage},query:`query getComments(\n  $owner: String!\n  $repo: String!\n  $issueId: Int!\n  $perPage: Int!\n) {\n  repository(owner: $owner, name: $repo) {\n    issue(number: $issueId) {\n      comments(\n        ${r}: $perPage\n        ${null===a?"":`${a}: "${o}"`}\n      ) {\n        nodes {\n          id\n          reactionGroups {\n            users (first: 0) {\n              totalCount\n            }\n            content\n          }\n        }\n      }\n    }\n  }\n}`},{headers:{Authorization:"token "+n}});return pa(i.data.repository.issue.comments.nodes.find(n=>n.id===t).reactionGroups)}async postCommentReaction({accessToken:n,commentId:e,reaction:t}){return await this.$http.post("graphql",{variables:{commentId:e,content:da(t)},query:"mutation postCommentReaction(\n  $commentId: ID!,\n  $content: ReactionContent!,\n) {\n  addReaction(input: {\n    subjectId: $commentId\n    content: $content\n  }) {\n    reaction {\n      databaseId\n    }\n  }\n}"},{headers:{Authorization:"token "+n}}),!0}_getQueryParams({page:n=this._pageInfo.page,sort:e=this._pageInfo.sort}={}){let t,r,a;return 1===n?(t="asc"===e?"first":"last",r=null,a=null):"asc"===e?n>this._pageInfo.page?(t="first",r="after",a=this._pageInfo.endCursor):(t="last",r="before",a=this._pageInfo.startCursor):n>this._pageInfo.page?(t="last",r="before",a=this._pageInfo.startCursor):(t="first",r="after",a=this._pageInfo.endCursor),{firstOrLast:t,afterOrBefore:r,cursor:a}}}function fa(n){return{username:n.username,avatar:n.avatar_url,homepage:n.web_url}}function ha(n){return{id:n.iid,title:n.title,content:n.description,link:n.web_url}}function va(n){return{id:n.id,content:n.body_html||"",contentRaw:n.body,author:fa(n.author),createdAt:n.created_at,updatedAt:n.updated_at,reactions:n.reactions}}function ba(n){return{like:n.filter(n=>"thumbsup"===n.name).length,unlike:n.filter(n=>"thumbsdown"===n.name).length,heart:n.filter(n=>"heart"===n.name).length}}function ka(n){return"like"===n?"thumbsup":"unlike"===n?"thumbsdown":n}class ya{constructor({baseURL:n="https://gitlab.com",owner:e,repo:t,labels:r,clientId:a,state:o}){this.baseURL=n,this.owner=e,this.repo=t,this.labels=r,this.clientId=a,this.state=o,this._encodedRepo=encodeURIComponent(`${this.owner}/${this.repo}`),this.$http=ea.a.create({baseURL:kt(n,"api/v4"),headers:{Accept:"application/json"}})}get platform(){return{name:"GitLab",link:this.baseURL,version:"v4",meta:{reactable:!0,sortable:!0}}}redirectAuth(){window.location.href=bt(kt(this.baseURL,"oauth/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,response_type:"token",state:this.state})}async handleAuth(){const n=Et(window.location.hash.slice(1));if(!n.access_token||n.state!==this.state)return null;const e=n.access_token;delete n.access_token,delete n.token_type,delete n.expires_in,delete n.state;const t=vt(n),r=t?"#"+t:"",a=`${wt(window.location.href)}${window.location.search}${r}`;return window.history.replaceState(null,"",a),e}async getUser({accessToken:n}){const{data:e}=await this.$http.get("user",{headers:{Authorization:"Bearer "+n}});return fa(e)}async getIssue({accessToken:n,issueId:e,issueTitle:t}){const r={};if(n&&(r.headers={Authorization:"Bearer "+n}),!e){r.params={labels:this.labels.join(","),order_by:"created_at",sort:"asc",search:t};const{data:n}=await this.$http.get(`projects/${this._encodedRepo}/issues`,r);return n.map(ha).find(n=>n.title===t)||null}try{const{data:n}=await this.$http.get(`projects/${this._encodedRepo}/issues/${e}`,r);return ha(n)}catch(n){if(n.response&&404===n.response.status)return null;throw n}}async postIssue({accessToken:n,title:e,content:t}){const{data:r}=await this.$http.post(`projects/${this._encodedRepo}/issues`,{title:e,description:t,labels:this.labels.join(",")},{headers:{Authorization:"Bearer "+n}});return ha(r)}async getComments({accessToken:n,issueId:e,query:{page:t=1,perPage:r=10,sort:a="desc"}={}}){const o={params:{page:t,per_page:r,order_by:"created_at",sort:a}};n&&(o.headers={Authorization:"Bearer "+n});const i=await this.$http.get(`projects/${this._encodedRepo}/issues/${e}/notes`,o),s=i.data,l=[];for(const t of s)l.push((async()=>{t.body_html=await this.getMarkdownContent({accessToken:n,contentRaw:t.body})})()),l.push((async()=>{t.reactions=await this.getCommentReactions({accessToken:n,issueId:e,commentId:t.id})})());return await Promise.all(l),{count:Number(i.headers["x-total"]),page:Number(i.headers["x-page"]),perPage:Number(i.headers["x-per-page"]),data:s.map(va)}}async postComment({accessToken:n,issueId:e,content:t}){const{data:r}=await this.$http.post(`projects/${this._encodedRepo}/issues/${e}/notes`,{body:t},{headers:{Authorization:"Bearer "+n}});return va(r)}async putComment({accessToken:n,issueId:e,commentId:t,content:r}){const{data:a}=await this.$http.put(`projects/${this._encodedRepo}/issues/${e}/notes/${t}`,{body:r},{headers:{Authorization:"Bearer "+n}}),[o,i]=await Promise.all([this.getMarkdownContent({accessToken:n,contentRaw:a.body}),this.getCommentReactions({accessToken:n,issueId:e,commentId:a.id})]);return a.body_html=o,a.reactions=i,va(a)}async deleteComment({accessToken:n,issueId:e,commentId:t}){const{status:r}=await this.$http.delete(`projects/${this._encodedRepo}/issues/${e}/notes/${t}`,{headers:{Authorization:"Bearer "+n}});return 204===r}async getCommentReactions({accessToken:n,issueId:e,commentId:t}){const{data:r}=await this.$http.get(`projects/${this._encodedRepo}/issues/${e}/notes/${t}/award_emoji`,{headers:{Authorization:"Bearer "+n}});return ba(r)}async postCommentReaction({issueId:n,commentId:e,reaction:t,accessToken:r}){try{return 201===(await this.$http.post(`projects/${this._encodedRepo}/issues/${n}/notes/${e}/award_emoji`,{name:ka(t)},{headers:{Authorization:"Bearer "+r}})).status}catch(n){if(n.response&&404===n.response.status)return!1;throw n}}async getMarkdownContent({accessToken:n,contentRaw:e}){const t={};n&&(t.headers={Authorization:"Bearer "+n});const{data:r}=await this.$http.post("markdown",{text:e,gfm:!0},t);return r.html}}function Sa(n){return{username:n.nickname,avatar:n.links.avatar.href,homepage:n.links.html.href}}function xa(n){return{id:n.id,title:n.title,content:n.content.raw,link:n.links.html.href}}function wa(n){return{id:n.id,content:n.content.html,contentRaw:n.content.raw,author:Sa(n.user),createdAt:n.created_on,updatedAt:n.updated_on,reactions:null}}class Ea{constructor({baseURL:n="https://bitbucket.org",owner:e,repo:t,clientId:r,state:a}){this.baseURL=n,this.owner=e,this.repo=t,this.clientId=r,this.state=a,this.$http=ea.a.create({baseURL:"https://api.bitbucket.org/2.0",headers:{Accept:"application/json"}})}get platform(){return{name:"Bitbucket",link:this.baseURL,version:"v2",meta:{reactable:!1,sortable:!0}}}redirectAuth(){window.location.href=bt(kt(this.baseURL,"site/oauth2/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,response_type:"token",state:this.state})}async handleAuth(){const n=Et(window.location.hash.slice(1));if(!n.access_token||n.state!==this.state)return null;const e=n.access_token;delete n.access_token,delete n.token_type,delete n.expires_in,delete n.state,delete n.scopes;const t=vt(n),r=t?"#"+t:"",a=`${wt(window.location.href)}${window.location.search}${r}`;return window.history.replaceState(null,"",a),e}async getUser({accessToken:n}){const{data:e}=await this.$http.get("user",{headers:{Authorization:"Bearer "+n}});return Sa(e)}async getIssue({accessToken:n,issueId:e,issueTitle:t}){const r={};if(n&&(r.headers={Authorization:"Bearer "+n}),!e){r.params={sort:"created_on",q:`title="${t}"`,timestamp:Date.now()};const{data:n}=await this.$http.get(`repositories/${this.owner}/${this.repo}/issues`,r);return n.size>0?xa(n.values[0]):null}try{r.params={timestamp:Date.now()};const{data:n}=await this.$http.get(`repositories/${this.owner}/${this.repo}/issues/${e}`,r);return xa(n)}catch(n){if(n.response&&404===n.response.status)return null;throw n}}async postIssue({accessToken:n,title:e,content:t}){const{data:r}=await this.$http.post(`repositories/${this.owner}/${this.repo}/issues`,{title:e,content:{raw:t},priority:"trivial",kind:"task"},{headers:{Authorization:"Bearer "+n}});return r.links.html={href:kt(this.baseURL,`${this.owner}/${this.repo}/issues/${r.id}`)},xa(r)}async getComments({accessToken:n,issueId:e,query:{page:t=1,perPage:r=10,sort:a="desc"}={}}){const o={params:{page:t,pagelen:r,sort:"desc"===a?"-created_on":"created_on",timestamp:Date.now()}};n&&(o.headers={Authorization:"Bearer "+n});const{data:i}=await this.$http.get(`repositories/${this.owner}/${this.repo}/issues/${e}/comments`,o);return{count:i.size,page:i.page,perPage:i.pagelen,data:i.values.filter(n=>null!==n.content.raw).map(wa)}}async postComment({accessToken:n,issueId:e,content:t}){const{data:r}=await this.$http.post(`repositories/${this.owner}/${this.repo}/issues/${e}/comments`,{content:{raw:t}},{headers:{Authorization:"Bearer "+n}});return wa(r)}async putComment({accessToken:n,issueId:e,commentId:t,content:r}){const{data:a}=await this.$http.put(`repositories/${this.owner}/${this.repo}/issues/${e}/comments/${t}`,{content:{raw:r}},{headers:{Authorization:"Bearer "+n}});return wa(a)}async deleteComment({accessToken:n,issueId:e,commentId:t}){const{status:r}=await this.$http.delete(`repositories/${this.owner}/${this.repo}/issues/${e}/comments/${t}`,{headers:{Authorization:"Bearer "+n}});return 204===r}async getCommentReactions(n){throw new Error("501 Not Implemented")}async postCommentReaction(n){throw new Error("501 Not Implemented")}}function Da(n){return{username:n.login,avatar:n.avatar_url,homepage:n.html_url}}function Ca(n){return{id:n.number,title:n.title,content:n.body,link:n.html_url}}function Ia(n){return{id:n.id,content:n.body_html||"",contentRaw:n.body,author:Da(n.user),createdAt:n.created_at,updatedAt:n.updated_at||"",reactions:null}}class Ta{constructor({baseURL:n="https://gitee.com",owner:e,repo:t,labels:r,clientId:a,clientSecret:o,state:i,proxy:s}){if(void 0===o||void 0===s)throw new Error("clientSecret and proxy is required for Gitee V5");this.baseURL=n,this.owner=e,this.repo=t,this.labels=r,this.clientId=a,this.clientSecret=o,this.state=i,this.proxy=s,this.$http=ea.a.create({baseURL:kt(n,"api/v5")}),this.$http.interceptors.response.use(n=>n,n=>(n.response.data&&n.response.data.message&&(n.message=n.response.data.message),Promise.reject(n)))}get platform(){return{name:"Gitee",link:this.baseURL,version:"v5",meta:{reactable:!1,sortable:!1}}}redirectAuth(){window.location.href=bt(kt(this.baseURL,"oauth/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,scope:"user_info issues notes",response_type:"code",state:this.state})}async handleAuth(){const n=Et(window.location.search);if(n.code){if(n.state!==this.state)return null;const e=n.code;delete n.code,delete n.state;const t=bt(wt(window.location.href),n)+window.location.hash;window.history.replaceState(null,"",t);return await this.getAccessToken({code:e})}return null}async getAccessToken({code:n}){const e=kt(this.baseURL,"oauth/token"),t="function"==typeof this.proxy?this.proxy(e):this.proxy,{data:r}=await this.$http.post(t,{client_id:this.clientId,client_secret:this.clientSecret,code:n,grant_type:"authorization_code",redirect_uri:window.location.href});return r.access_token}async getUser({accessToken:n}){const{data:e}=await this.$http.get("user",{params:{access_token:n}});return Da(e)}async getIssue({accessToken:n,issueId:e,issueTitle:t}){const r={params:{timestamp:Date.now()}};if(n&&(r.params.access_token=n),!e){Object.assign(r.params,{q:t,repo:`${this.owner}/${this.repo}`,per_page:1});const{data:n}=await this.$http.get("search/issues",r);return n.map(Ca).find(n=>n.title===t)||null}try{const{data:n}=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/${e}`,r);return Ca(n)}catch(n){if(n.response&&404===n.response.status)return null;throw n}}async postIssue({accessToken:n,title:e,content:t}){const{data:r}=await this.$http.post(`repos/${this.owner}/issues`,{access_token:n,repo:this.repo,title:e,body:t,labels:this.labels.join(",")});return Ca(r)}async getComments({accessToken:n,issueId:e,query:{page:t=1,perPage:r=10}={}}){const a={params:{page:t,per_page:r,timestamp:Date.now()},headers:{Accept:["application/vnd.gitee.html+json"]}};n&&(a.params.access_token=n);const o=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/${e}/comments`,a);return{count:Number(o.headers.total_count),page:t,perPage:r,data:o.data.map(Ia)}}async postComment({accessToken:n,issueId:e,content:t}){const{data:r}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues/${e}/comments`,{body:t,access_token:n},{headers:{Accept:["application/vnd.gitee.html+json"]}});return Ia(r)}async putComment({accessToken:n,commentId:e,content:t}){const{data:r}=await this.$http.patch(`repos/${this.owner}/${this.repo}/issues/comments/${e}`,{body:t,access_token:n},{headers:{Accept:["application/vnd.gitee.html+json"]}});return Ia(r)}async deleteComment({accessToken:n,commentId:e}){const{status:t}=await this.$http.delete(`repos/${this.owner}/${this.repo}/issues/comments/${e}`,{params:{access_token:n}});return 204===t}async getCommentReactions(n){throw new Error("501 Not Implemented")}async postCommentReaction(n){throw new Error("501 Not Implemented")}}t(331);var Oa={name:"Vssue",components:{VssueComponent:Zr},props:{options:{type:Object,default:()=>({})}},data:()=>({key:"key",platformOptions:{github:sa,"github-v4":ma,gitlab:ya,bitbucket:Ea,gitee:Ta}}),computed:{vssueOptions(){const{platformOptions:n,options:e}=this,t=n[e.platform];return{...e,api:t}}},watch:{$route(n,e){n.path!==e.path&&setTimeout(()=>{this.key="reco-"+(new Date).getTime()},300)}}},Aa=(t(332),{components:{Valine:ot,Vssue:Object(Ce.a)(Oa,(function(){return(0,this._self._c)("VssueComponent",{key:this.key,staticClass:"vssue-wrapper",attrs:{options:this.vssueOptions}})}),[],!1,null,null,null).exports},props:{isShowComments:{type:Boolean,default:!0}},data:()=>({commentsOptions:{}}),computed:{solution(){const{commentsOptions:{solution:n},$themeConfig:{valineConfig:e,vssueConfig:t},$themeLocaleConfig:{valineConfig:r,vssueConfig:a}}=this;let o="";return void 0!==n?o=n:void 0!==r||void 0!==e?o="valine":void 0===a&&void 0===t||(o="vssue"),o},options(){const{commentsOptions:{options:n},$themeConfig:{valineConfig:e,vssueConfig:t},$themeLocaleConfig:{valineConfig:r,vssueConfig:a}}=this;return void 0!==n?n:void 0!==r||void 0!==e?r||e:void 0!==a||void 0!==t?a||t:null},componentName(){const n=this.solution;return"valine"===n?"Valine":"vssue"===n?"Vssue":""}},mounted(){this.$themeConfig.commentsSolution=this.solution}}),_a=Object(Ce.a)(Aa,(function(){var n=this._self._c;return n("div",{directives:[{name:"show",rawName:"v-show",value:this.isShowComments,expression:"isShowComments"}],staticClass:"comments-wrapper"},[n("ClientOnly",[n(this.componentName,{tag:"component",attrs:{options:this.options}})],1)],1)}),[],!1,null,null,null).exports,Ra={props:{idVal:String,numStyle:Object,flagTitle:{type:String,default:"Your Article Title"}},methods:{getIdVal(n){const e=this.$site.base;return e.slice(0,e.length-1)+n}}},Pa=Object(Ce.a)(Ra,(function(){var n=this._self._c;return n("span",{staticClass:"leancloud-visitors",attrs:{id:this.getIdVal(this.idVal),"data-flag-title":this.flagTitle}},[n("a",{staticClass:"leancloud-visitors-count",style:this.numStyle})])}),[],!1,null,null,null).exports,Fa=(t(333),{tags:{markdown:{key:"markdown",scope:"tags",path:"/tag/markdown/",pageKeys:["v-0cbdd054"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-6048fa40"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-6048fa40"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-7d8ca27f"]},"-":{key:"-",scope:"tags",path:"/tag/-/",pageKeys:["v-7d8ca27f"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-33a310e8","v-83d8af48","v-4f7f9be4"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-33a310e8","v-4f7f9be4"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-700a68a1"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-01787dc2"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-01787dc2"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-01787dc2"]},k8s:{key:"k8s",scope:"tags",path:"/tag/k8s/",pageKeys:["v-83d8af48"]},Flink:{key:"Flink",scope:"tags",path:"/tag/Flink/",pageKeys:["v-83d8af48"]},beam:{key:"beam",scope:"tags",path:"/tag/beam/",pageKeys:["v-83d8af48"]},"k8s":{key:"k8s",scope:"tags",path:"/tag/k8s/",pageKeys:["v-c38fec2a"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-7e69d236"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-7e69d236","v-24ffb3db"]},"linux":{key:"linux",scope:"tags",path:"/tag/linux/",pageKeys:["v-cdeeea2e"]},CICD:{key:"CICD",scope:"tags",path:"/tag/CICD/",pageKeys:["v-10e8b782"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-10e8b782"]},pipeline:{key:"pipeline",scope:"tags",path:"/tag/pipeline/",pageKeys:["v-10e8b782","v-36184f82"]},redis:{key:"redis",scope:"tags",path:"/tag/redis/",pageKeys:["v-851571e2"]},"hash":{key:"hash",scope:"tags",path:"/tag/hash/",pageKeys:["v-851571e2"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-851571e2"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-7350f07e"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-7350f07e"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-22a43ff6"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-22a43ff6"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-0ddfb2e2"]},"bisdiff/bispatch":{key:"bisdiff/bispatch",scope:"tags",path:"/tag/bisdiff/bispatch/",pageKeys:["v-0ddfb2e2"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-41ccc7e2","v-e463dc58"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-29f87cb0"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-29f87cb0"]},"id":{key:"id",scope:"tags",path:"/tag/id/",pageKeys:["v-2e09059c"]},"kafka":{key:"kafka",scope:"tags",path:"/tag/kafka/",pageKeys:["v-61c5b94b"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-61c5b94b"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-5692179e"]},spring:{key:"spring",scope:"tags",path:"/tag/spring/",pageKeys:["v-5692179e"]},"IO":{key:"IO",scope:"tags",path:"/tag/IO/",pageKeys:["v-227baaf0"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-24ffb3db"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-007b24d3"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-007b24d3"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-36184f82"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-49f14a1b"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-49f14a1b"]},"":{key:"",scope:"tags",path:"/tag//",pageKeys:["v-9f20b3be"]},sparksql:{key:"sparksql",scope:"tags",path:"/tag/sparksql/",pageKeys:["v-9f20b3be"]}},categories:{tool:{key:"tool",scope:"categories",path:"/categories/tool/",pageKeys:["v-0cbdd054"]},"":{key:"",scope:"categories",path:"/categories//",pageKeys:["v-6048fa40"]},"":{key:"",scope:"categories",path:"/categories//",pageKeys:["v-6048fa40"]},"":{key:"",scope:"categories",path:"/categories//",pageKeys:["v-7d8ca27f","v-7e69d236"]},"":{key:"",scope:"categories",path:"/categories//",pageKeys:["v-7d8ca27f"]},"":{key:"",scope:"categories",path:"/categories//",pageKeys:["v-33a310e8","v-83d8af48","v-4f7f9be4"]},"":{key:"",scope:"categories",path:"/categories//",pageKeys:["v-700a68a1","v-c38fec2a","v-cdeeea2e","v-22a43ff6","v-41ccc7e2","v-29f87cb0","v-61c5b94b","v-5692179e","v-227baaf0","v-e463dc58"]},"":{key:"",scope:"categories",path:"/categories//",pageKeys:["v-01787dc2","v-851571e2","v-7350f07e","v-0ddfb2e2","v-2e09059c"]},"":{key:"",scope:"categories",path:"/categories//",pageKeys:["v-7e69d236"]},CICD:{key:"CICD",scope:"categories",path:"/categories/CICD/",pageKeys:["v-10e8b782"]},"":{key:"",scope:"categories",path:"/categories//",pageKeys:["v-24ffb3db","v-007b24d3"]},nosql:{key:"nosql",scope:"categories",path:"/categories/nosql/",pageKeys:["v-24ffb3db"]},"":{key:"",scope:"categories",path:"/categories//",pageKeys:["v-36184f82","v-49f14a1b","v-9f20b3be"]}},timeline:{}});class Ba{constructor(n,e){this._metaMap=Object.assign({},n),Object.keys(this._metaMap).forEach(n=>{const{pageKeys:t}=this._metaMap[n];this._metaMap[n].pages=t.map(n=>Object(Jn.b)(e,n))})}get length(){return Object.keys(this._metaMap).length}get map(){return this._metaMap}get pages(){return this.list}get list(){return this.toArray()}toArray(){const n=[];return Object.keys(this._metaMap).forEach(e=>{const{pages:t,path:r}=this._metaMap[e];n.push({name:e,pages:t,path:r})}),n}getItemByName(n){return this._metaMap[n]}}var Ma={tags:(n,e)=>{const r=t(126);return r(n.frontmatter.date)-r(e.frontmatter.date)>0?-1:1},categories:(n,e)=>{const r=t(126);return r(n.frontmatter.date)-r(e.frontmatter.date)>0?-1:1}},ja={tags:function(n,e,t){const r=e;return["tags"].some(e=>{const t=n.frontmatter[e];return Array.isArray(t)?t.some(n=>n===r):t===r})},categories:function(n,e,t){const r=e;return["categories"].some(e=>{const t=n.frontmatter[e];return Array.isArray(t)?t.some(n=>n===r):t===r})}},La=[{pid:"tags",id:"markdown",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/markdown/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"-",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/-/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,3]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,2]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"k8s",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/k8s/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"Flink",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/Flink/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"beam",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/beam/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"k8s",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/k8s/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,2]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"linux",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/linux/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"CICD",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/CICD/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"pipeline",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/pipeline/",interval:[0,2]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"redis",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/redis/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"hash",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/hash/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"bisdiff/bispatch",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/bisdiff/bispatch/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,2]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"id",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/id/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"kafka",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/kafka/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"spring",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/spring/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"IO",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/IO/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"tags",id:"sparksql",filter:ja.tags,sorter:Ma.tags,pages:[{path:"/tag/sparksql/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"tool",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/tool/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories//",interval:[0,2]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories//",interval:[0,3]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories//",interval:[0,9]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories//",interval:[0,5]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories//",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"CICD",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/CICD/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories//",interval:[0,2]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"nosql",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories/nosql/",interval:[0,1]}],prevText:"Prev",nextText:"Next"},{pid:"categories",id:"",filter:ja.categories,sorter:Ma.categories,pages:[{path:"/categories//",interval:[0,3]}],prevText:"Prev",nextText:"Next"}],Na=t(135);const $a=t.n(Na)()("plugin-blog:pagination");class Ua{constructor(n,e,t){$a("pagination",n);const{pages:r,prevText:a,nextText:o}=n,{path:i}=t;this._prevText=a,this._nextText=o;for(let n=0,e=r.length;n<e;n++){if(r[n].path===i){this.paginationIndex=n;break}}this.paginationIndex||(this.paginationIndex=0),this._paginationPages=r,this._currentPage=r[this.paginationIndex],this._matchedPages=e.filter(e=>n.filter(e,n.id,n.pid)).sort(n.sorter)}setIndexPage(n){this._indexPage=n}get length(){return this._paginationPages.length}get pages(){const[n,e]=this._currentPage.interval;return this._matchedPages.slice(n,e+1)}get hasPrev(){return 0!==this.paginationIndex}get prevLink(){return this.hasPrev?this.paginationIndex-1==0&&this._indexPage?this._indexPage:this._paginationPages[this.paginationIndex-1].path:null}get hasNext(){return this.paginationIndex!==this.length-1}get nextLink(){return this.hasNext?this._paginationPages[this.paginationIndex+1].path:null}get prevText(){return this._prevText}get nextText(){return this._nextText}getSpecificPageLink(n){return this._paginationPages[n].path}}const za=new class{constructor(n){this.paginations=n}get pages(){return r.b.$vuepress.$get("siteData").pages}getPagination(n,e,t){$a("id",e),$a("pid",n);const r=this.paginations.filter(t=>t.id===e&&t.pid===n)[0];return new Ua(r,this.pages,t)}}(La);var Ha={comment:{enabled:!1,service:""},email:{enabled:!1},feed:{rss:!1,atom:!1,json:!1}};t(336);function qa(n){const e=document.documentElement.getBoundingClientRect(),t=n.getBoundingClientRect();return{x:t.left-e.left,y:t.top-e.top}}class Va{constructor(n){Object.defineProperty(this,"registration",{value:n,configurable:!0,writable:!0})}update(){return this.registration.update()}skipWaiting(){const n=this.registration.waiting;return n?(console.log("[vuepress:sw] Doing worker.skipWaiting()."),new Promise((e,t)=>{const r=new MessageChannel;r.port1.onmessage=n=>{console.log("[vuepress:sw] Done worker.skipWaiting()."),n.data.error?t(n.data.error):e(n.data)},n.postMessage({type:"skip-waiting"},[r.port2])})):Promise.resolve()}}var Ka=t(22);r.b.component("SWUpdatePopup",()=>Promise.all([t.e(0),t.e(34)]).then(t.bind(null,1558)));var Wa=[({Vue:n,options:e,router:r,siteData:a})=>{n.mixin({mounted(){const e=()=>{(function(){var n=JSON.parse(localStorage.getItem("user_auth_xxxxxxxxxxxx"));if(console.log(n),n&&n.time){var e=new Date(n.time);return!((new Date).setHours(-1)>e)&&(n&&Object.keys(n).length)}return!1})()||this.$dlg.modal(je,{width:400,height:350,title:"",singletonKey:"user-login",maxButton:!1,closeButton:!1,callback:n=>{}})};this.$dlg?e():t.e(56).then(t.t.bind(null,1554,7)).then(t=>{n.use(t.default),this.$nextTick(()=>{e()})})}})},({Vue:n,siteData:e,isServer:t,router:r})=>{n.mixin(Ne),n.mixin(We),Object(Ge.c)(r),Object(Ge.a)(r)},{},({Vue:n})=>{n.mixin({computed:{$dataBlock(){return this.$options.__data__block__}}})},{},({Vue:n})=>{n.component("BackToTop",Ye)},({Vue:n})=>{n.component("Pagation",rt)},({Vue:n})=>{n.mixin({computed:{$perPage:()=>10}})},({Vue:n})=>{n.component("Comments",_a),n.component("AccessNumber",Pa)},{},({Vue:n})=>{const e=Object.keys(Fa).map(n=>{const e=Fa[n],t="$"+n;return{[t](){const{pages:n}=this.$site;return new Ba(e,n)},["$current"+(n.charAt(0).toUpperCase()+n.slice(1))](){const n=this.$route.meta.id;return this[t].getItemByName(n)}}}).reduce((n,e)=>(Object.assign(n,e),n),{});e.$frontmatterKey=function(){const n=this["$"+this.$route.meta.id];return n||null},n.mixin({computed:e})},({Vue:n})=>{n.mixin({computed:{$pagination(){return this.$route.meta.pid&&this.$route.meta.id?this.$getPagination(this.$route.meta.pid,this.$route.meta.id):null}},methods:{$getPagination(n,e){return e=e||n,za.getPagination(n,e,this.$route)}}})},({Vue:n})=>{const e={$service:()=>Ha};n.mixin({computed:e})},({Vue:n,router:e})=>{e.options.scrollBehavior=(e,t,r)=>{if(r)return window.scrollTo({top:r.y,behavior:"smooth"});if(e.hash){if(n.$vuepress.$get("disableScrollBehavior"))return!1;const t=document.querySelector(e.hash);return!!t&&window.scrollTo({top:qa(t).y,behavior:"smooth"})}return window.scrollTo({top:0,behavior:"smooth"})}},async({router:n,isServer:e})=>{if(!e){const{register:e}=await t.e(55).then(t.bind(null,1555));n.onReady(()=>{e("./service-worker.js",{registrationOptions:{},ready(){console.log("[vuepress:sw] Service worker is active."),Ka.a.$emit("sw-ready")},cached(n){console.log("[vuepress:sw] Content has been cached for offline use."),Ka.a.$emit("sw-cached",new Va(n))},updated(n){console.log("[vuepress:sw] Content updated."),Ka.a.$emit("sw-updated",new Va(n))},offline(){console.log("[vuepress:sw] No internet connection found. App is running in offline mode."),Ka.a.$emit("sw-offline")},error(n){console.error("[vuepress:sw] Error during service worker registration:",n),Ka.a.$emit("sw-error",n),GA_ID&&ga("send","exception",{exDescription:n.message,exFatal:!1})}})})}},({Vue:n})=>{n.component("CodeCopy",Ae)}],Ga=["BackToTop","SWUpdatePopup"];class Ja extends class{constructor(){this.store=new r.b({data:{state:{}}})}$get(n){return this.store.state[n]}$set(n,e){r.b.set(this.store.state,n,e)}$emit(...n){this.store.$emit(...n)}$on(...n){this.store.$on(...n)}}{}Object.assign(Ja.prototype,{getPageAsyncComponent:Jn.e,getLayoutAsyncComponent:Jn.d,getAsyncComponent:Jn.c,getVueComponent:Jn.f});var Ya={install(n){const e=new Ja;n.$vuepress=e,n.prototype.$vuepress=e}};function Qa(n,e){const t=e.toLowerCase();return n.options.routes.some(n=>n.path.toLowerCase()===t)}var Xa={props:{pageKey:String,slotKey:{type:String,default:"default"}},render(n){const e=this.pageKey||this.$parent.$page.key;return Object(Jn.i)("pageKey",e),r.b.component(e)||r.b.component(e,Object(Jn.e)(e)),r.b.component(e)?n(e):n("")}},Za={functional:!0,props:{slotKey:String,required:!0},render:(n,{props:e,slots:t})=>n("div",{class:["content__"+e.slotKey]},t()[e.slotKey])},no={computed:{openInNewWindowTitle(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},eo=(t(337),t(338),Object(Ce.a)(no,(function(){var n=this._self._c;return n("span",[n("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[n("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),n("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),n("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports),to={functional:!0,render(n,{parent:e,children:t}){if(e._isMounted)return t;e.$once("hook:mounted",()=>{e.$forceUpdate()})}};r.b.config.productionTip=!1,r.b.use(Wn),r.b.use(Ya),r.b.mixin(function(n,e,t=r.b){!function(n){n.locales&&Object.keys(n.locales).forEach(e=>{n.locales[e].path=e});Object.freeze(n)}(e),t.$vuepress.$set("siteData",e);const a=new(n(t.$vuepress.$get("siteData"))),o=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(a)),i={};return Object.keys(o).reduce((n,e)=>(e.startsWith("$")&&(n[e]=o[e].get),n),i),{computed:i}}(n=>class{setPage(n){this.__page=n}get $site(){return n}get $themeConfig(){return this.$site.themeConfig}get $frontmatter(){return this.$page.frontmatter}get $localeConfig(){const{locales:n={}}=this.$site;let e,t;for(const r in n)"/"===r?t=n[r]:0===this.$page.path.indexOf(r)&&(e=n[r]);return e||t||{}}get $siteTitle(){return this.$localeConfig.title||this.$site.title||""}get $canonicalUrl(){const{canonicalUrl:n}=this.$page.frontmatter;return"string"==typeof n&&n}get $title(){const n=this.$page,{metaTitle:e}=this.$page.frontmatter;if("string"==typeof e)return e;const t=this.$siteTitle,r=n.frontmatter.home?null:n.frontmatter.title||n.title;return t?r?r+" | "+t:t:r||"VuePress"}get $description(){const n=function(n){if(n){const e=n.filter(n=>"description"===n.name)[0];if(e)return e.content}}(this.$page.frontmatter.meta);return n||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}get $lang(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}get $localePath(){return this.$localeConfig.path||"/"}get $themeLocaleConfig(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}get $page(){return this.__page?this.__page:function(n,e){for(let t=0;t<n.length;t++){const r=n[t];if(r.path.toLowerCase()===e.toLowerCase())return r}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}},Be)),r.b.component("Content",Xa),r.b.component("ContentSlotsDistributor",Za),r.b.component("OutboundLink",eo),r.b.component("ClientOnly",to),r.b.component("Layout",Object(Jn.d)("Layout")),r.b.component("NotFound",Object(Jn.d)("NotFound")),r.b.prototype.$withBase=function(n){const e=this.$site.base;return"/"===n.charAt(0)?e+n.slice(1):n},window.__VUEPRESS__={version:"1.9.9",hash:"793c1e2"},async function(n){const e="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:Be.routerBase||Be.base,t=new Wn({base:e,fallback:!1,routes:Fe,scrollBehavior:(n,e,t)=>t||(n.hash?!r.b.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(n.hash)}:{x:0,y:0})});!function(n){n.beforeEach((e,t,r)=>{if(Qa(n,e.path))r();else if(/(\/|\.html)$/.test(e.path))if(/\/$/.test(e.path)){const t=e.path.replace(/\/$/,"")+".html";Qa(n,t)?r(t):r()}else r();else{const t=e.path+"/",a=e.path+".html";Qa(n,a)?r(a):Qa(n,t)?r(t):r()}})}(t);const a={};try{await Promise.all(Wa.filter(n=>"function"==typeof n).map(e=>e({Vue:r.b,options:a,router:t,siteData:Be,isServer:n})))}catch(n){console.error(n)}return{app:new r.b(Object.assign(a,{router:t,render:n=>n("div",{attrs:{id:"app"}},[n("RouterView",{ref:"layout"}),n("div",{class:"global-ui"},Ga.map(e=>n(e)))])})),router:t}}(!1).then(({app:n,router:e})=>{e.onReady(()=>{n.$mount("#app")})})}]);